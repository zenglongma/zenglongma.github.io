<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>boss_and_job</title>
      <link href="/project/posts/c0bca80f.html"/>
      <url>/project/posts/c0bca80f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>网站重建，梦想开启</title>
      <link href="/project/posts/c318edab.html"/>
      <url>/project/posts/c318edab.html</url>
      
        <content type="html"><![CDATA[<h1>网站重建，梦想开启</h1><h2 id="博客重启的巴拉巴拉（必备工具）">博客重启的巴拉巴拉（必备工具）</h2><p><a href="https://blog.cuijiacai.com/blog-building/">https://blog.cuijiacai.com/blog-building/</a></p><p><a href="https://www.bilibili.com/video/BV1qD4y1z783/?vd_source=3d5976f4f96ce120bf88891647d386cb">超详细的个人博客搭建教程（无需服务器）- 从原理到实践手把手带你搭建属于自己的个人博客_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb</a></p><p><a href="https://www.fomal.cc/posts/e593433d.html">🐖抓到你啦～ (fomal.cc)</a></p><h1>重启操作教程</h1><p>熟能生巧</p><p>github 用git访问+chatgpt的回答帮助</p><ol><li>nodejs + nmp  安装+版本匹配<ol><li>删除nmp</li><li>更改下载源</li></ol></li><li>hexo 下载 单独安装配置up项目</li><li></li><li></li></ol><p><img src="../assets/image-20240906221418850.png" alt="image-20240906221418850"></p><h1>心路历程</h1><blockquote><p>2024年5月3日 gitee的page服务 被关闭，不在对外访问，导致自己的网站无法访问，也不能再继续写笔记和完成自己的博客l</p></blockquote><p>在2024年9.6日 重新为了封装简历，想着哪怕拿出一天的时间，全部花进去，用来把自己的博客后台迁移，从不能访问的gitee 迁移到github</p><p>花了一天的时间，从一开始7:00左右的信心满满，动力十足，到快到10：20左右，【在这期间成功的把之前的GitHub不能用git访问的问题解决了这个问题困扰了好久】然后跟我家美女一起改简历，搬到小教室里边，用自己的手机开热点，直到11：40左右，成功的能够跟之前一样，用hexo d 和hexo -g 完成数据往远端传输，搜着网页上说问题是网络不好</p><p>而且在用hexo d 传输的时候，蹦出一个网页，用来验证github的账号登录情况，这时候就知道能够完成数据传输了</p><p>当时是开着热点在小教室里，就没去吃饭，一直开着热点，给手机充电，期望能够成功，幸好最后成功传输上去了！！！</p><h4 id="但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs-和hexo-版本问题，导致hexo下载主题失败">但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs 和hexo 版本问题，导致hexo下载主题失败</h4><p>npm  与 node 匹配</p><p>下载使用的node.js 是12.19.0版本</p><h6 id="nodejs和npm版本不匹配：ERROR-npm-is-known-not-to-run-on-Node-js-v10-16-2">nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2</h6><p><a href="https://blog.csdn.net/qq_41992943/article/details/124925758">nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_45056216/article/details/101163126">关于node js和 npm 版本不匹配的情况_linux中npm版本个nodejs版本不匹配解决方法-CSDN博客</a></p><p>删除node 重装</p><p>node 12.19.0 重装</p><p>更改 npd 源头</p><h6 id="npm报错：request-to-https-registry-npm-taobao-org-failed-reason-certificate-has-expired">npm报错：request to <a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a> failed, reason certificate has expired</h6><p><a href="https://blog.csdn.net/maoge_666/article/details/136038003">npm报错：request to https://registry.npm.taobao.org failed, reason certificate has expired-CSDN博客</a></p><p>hexo  重装——直接在新文件夹中安装</p><h6 id="win10系统使用npm安装hexo失败">win10系统使用npm安装hexo失败!</h6><p><a href="https://github.com/hexojs/hexo/issues/2741">win10系统使用npm安装hexo失败! · Issue #2741 · hexojs/hexo (github.com)</a></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="selector-tag">i</span> -<span class="selector-tag">g</span> hexo-cli    </span><br><span class="line">sudo npm install gulp-cli -<span class="selector-tag">g</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://github.com/hexojs/hexo-cli/issues/23">hexo-cli 安装后失败 ·问题 #23 ·hexojs/hexo-cli 命令 (github.com)</a></p><p>git上传文件和文件夹</p><h6 id="Github如何上传项目-超详细小白教程">Github如何上传项目(超详细小白教程)</h6><p><a href="https://blog.csdn.net/KevinRay_0854/article/details/140408003">Github如何上传项目(超详细小白教程)_github上传项目-CSDN博客</a></p><p>[git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤_github推送文件夹视图工具-CSDN博客](<a href="https://blog.csdn.net/qq_40413898/article/details/104287509#:~:text=git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%EF%BC%9A%E5%B0%86%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B9%E5%86%85%E5%AE%B9%E6%88%96%E4%BB%A3%E7%A0%81%E6%96%87%E4%BB%B6%E6%8E%A8%E9%80%81%E5%88%B0GitHub%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4">https://blog.csdn.net/qq_40413898/article/details/104287509#:~:text=git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤</a> 1 1、在GitHub中新建一个仓库 在创建仓库是，一些配置说明如下： Repository name%3A 仓库名称 Description,. … 5 5、最终结果 将本地仓库与远程仓库连接 再次要推送文件时，需要与远程仓库进行交互，使用git remote指令。 )</p><h4 id="验证登录框">验证登录框</h4><blockquote><p>为了方便下次再碰到这种问题，记录一下蹦出的网页情况</p></blockquote><p>网页蹦出来是个小的登录界面，可以直接打开链接，然后验证与github账号的关联，同时确定能够跟这个电脑设备关联上。</p><p>打开链接，是一个github页面，然后输入设备登录码！！！很重要的成功标志</p><blockquote><p>幸好chatgpt帮助 解决 git 访问github</p></blockquote><blockquote><ol><li>配置对应的github库，需要设置好远程库与用户名一致，<a href="http://xn--github-9o7i562adkda6123feszd.io">然后后边加上github.io</a></li><li>先将up的模板放到github库中，详见配置教程</li><li>再将自己的test文件上传到对应的目录中，在进行hexo d 上传</li></ol></blockquote><h1>秋招文件收集</h1><p>数据库项目</p><p><a href="https://articles.zsxq.com/id_gsuoxn7qnvip.html">轮子实战项目：手写数据库 MYDB (zsxq.com)</a></p><p><a href="https://blog.csdn.net/qq_40856284/article/details/121580249">一起写个数据库 —— 0. 项目结构和一些不得不说的话-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_40856284/category_11504274.html">一起写个数据库_何人听我楚狂声的博客-CSDN博客</a></p><p>技术派</p><p><a href="https://paicoding.com/article/detail/472">如何将技术派写到简历上？-技术派 (paicoding.com)</a></p><p>PmHub</p><p><a href="https://www.yuque.com/canghe-u0ocv/laigeoffer-pmhub/mdoff4twogexi4ri">✅人人都是产品经理，打造竞争优势（🌟新人必看） (yuque.com)</a>   cgd9</p><p>八股学习</p><p><a href="https://javabetter.cn/sidebar/sanfene/nixi.html">面渣逆袭必看，面试题八股文Java基础、Java 集合框架、Java 并发编程、JVM、Spring、Redis、MyBatis、MySQL、操作系统、计算机网络、RocketMQ、分布式、微服务、设计模式、Linux👍 | 二哥的Java进阶之路 (javabetter.cn)</a></p><p>面试题整理</p><p><a href="https://top.interviewguide.cn/">InterviewGuide大厂面试真题</a></p><h2 id="面试经验">面试经验</h2><blockquote><p>刷面经</p><p><a href="https://www.yuque.com/snailclimb/mf2z3k/guh0u9hb3pr70rtk">大厂四年，2024 阿里、字节、蚂蚁、小红书面试经历分享 (yuque.com)</a>      《面试指北》cnk4</p></blockquote><p><a href="https://interviewguide.cn/notes/05-xiustar/02-campus_prepare/04-01-%E4%BA%92%E8%81%94%E7%BD%91%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html">⭐阿秀三个多月、50余场面试经验浓缩为经验和总结 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>简历修改</p><p>优质简历样例——<a href="https://articles.zsxq.com/id_5p3fcdmwpw4h.html">精选简历：基本信息、教育背景、专业技能、工作经验、项目经历、实习经历、荣誉证书写的比较好的例子 (zsxq.com)</a></p><p><a href="https://wx.zsxq.com/dweb2/index/topic_detail/182882241124882">知识星球 | 深度连接铁杆粉丝，运营高品质社群，知识变现的工具 (zsxq.com)</a></p><p>如何写好简历——<a href="https://www.yuque.com/itwanger/gykdzg/grvrtx?singleDoc=#e9486e82">如何写好简历？（完结） (yuque.com)</a>——王二</p><p>毒液   <a href="https://erdengk.top/archives/jian-li--mian-shi">简历、面试 | 我的知识海洋 (erdengk.top)</a></p><h2 id="知识星球常用">知识星球常用</h2><p><a href="https://wx.zsxq.com/dweb2/index/favorites">收藏</a></p><p>秀哥资源——<a href="https://articles.zsxq.com/id_dmf00wjw9dtm.html">🏆资源沉淀 (zsxq.com)</a></p><p>秀哥问答——<a href="https://articles.zsxq.com/id_wm06pp9dnogj.html">📖知识图谱（星球问答文章精华） (zsxq.com)</a></p><p><a href="https://interviewguide.cn/notes/05-xiustar/03-resume/01-00-%E7%AE%80%E5%8E%86%E5%BC%80%E7%AF%87%E8%AF%8D.html">简历远远比你想要的重要的多 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>公司列表  <a href="https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u">https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u</a> 面试指南 gqya</p><p>优质主题——<a href="https://articles.zsxq.com/id_v6xfrmw5637h.html">JavaGuide 知识星球优质主题汇总（2024-04-01） (zsxq.com)</a></p><h2 id="AI">AI</h2><p><a href="https://www.coze.cn/">扣子 - AI 智能体开发平台 (coze.cn)</a></p><h1>准备</h1><blockquote><p>测试八股文/测开八股文</p></blockquote><p>算法题目</p><p>数据库基础知识+ sql题目</p><p>项目梳理</p><p>java八股文</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/project/posts/0.html"/>
      <url>/project/posts/0.html</url>
      
        <content type="html"><![CDATA[<h2 id="社科人文认知突破">社科人文认知突破</h2><h2 id="社会科学入门">社会科学入门</h2><p>是这么回事，学习社会科学，学习文科，其实用处没你想得那么大。<br><strong>你如果想靠这个挣钱呢，那还是很难的。</strong></p><h2 id="学文科的好处">学文科的好处</h2><p><strong>但是呢，学这玩意用处很大的，一个是陶冶自身的情操。另一个能避免被政府欺骗。</strong></p><p>学文科的话，其实最重要的是多读书，多积累，然后能学会万事万物的联系。</p><p>这个万事万物的联系，说的比较笼统，就是<strong>你读一本书，能想到别的书之间和这本书的联系。</strong></p><p>比如你读唐代的历史书，能联系到北周的柱国，再联系到皇帝的更迭， 那恭喜你，你已经学会一点了。</p><p><strong>学文科的话，最高的水平是能做预测分析，其次是做事后分析。</strong></p><p>做预测分析这玩意属于有点是占卜，没有固定的套路，但是做事后分析，寻找历史的真相，其实如果多积累，多思考，总是能找到路径的。<strong>但是呢，这个时间可能需要很多年的积累。</strong><br>学文科另一个方面是<strong>有利于你炒股</strong>，如果你将来会看工信部的数据，懂得如何分析走向，不敢说带你走向暴富，但是穷应该是可以避免。</p><h2 id="文科的坏处">文科的坏处</h2><p>上面说了那么多学文科的好处，我也要说说学文科的坏处，一方面是要<strong>花费非常多的时间，</strong><br>另一方面是<strong>没有很明显的进步提示</strong>，他不像学数学，学了一本就有一本的知识，而是你要在庞大的积累后，<br>才能实现质的飞跃。</p><p>除此之外，另一个问题是<strong>容易走火入魔</strong>。关于走火入魔，比较明显的例子就是各种魔怔念经人，佐佐右右托派。</p><p>因为你读一些书，你就是和作者在产生交流和对话，作者会讲他的思考模式，然后把你说服，<br><strong>如果你能力不行，就很容易被他说服。</strong>      比较明显的例子就是哈耶克 和佛利德曼的书。</p><h2 id="怎么学习社科人文">怎么学习社科人文</h2><p>好了，我要正视开始讲学文科的事情了。<br>首先呢，<strong>你需要积累一定的历史，</strong>（方便在了解一系列的东西之后，能够分析出其发生的背景以及对应的因果关系，方便结合时势分析）（其实也可以不积累，但是不积累的话，在看一些东西的时候，不能结合时代背景去认知）。<br>第二是<strong>根据自己的爱好，去决定学什么。</strong><br>文科里面，我个人首推的是<strong>逻辑学导论</strong>。这本书会教你如何去正确的思考，会让你避免一些基础的逻辑推导错误。<br>第二 我是推荐看<strong>经济历史</strong>。经济历史对于人的收获是，你能理解说一些事，是如何演化过来的。<br>比如说我们曾经是社会主义，为啥会90年代破产， 为啥会cr ，为啥到现在又重新国进民退。<br>第三，我推荐是看<strong>历史类的书</strong>，历史类的书，其实我个人推荐是看法国人还有英国人写的书，<br>这两个国家的特点就是有丰富的国外交流经验，是认真的理解国外的，英国的博物学家是一群很不可思议的人，<br>19世纪的英国探险者能穿越喜马拉雅山跑到拉萨和当地的活佛交流，20世纪的劳伦斯一个人就能推翻伊拉克帝国，<br>瓦解德国在阿拉伯地区的努力，而法国在非洲的殖民，甚至一度让美国人都感到绝望。</p><p>​</p><h3 id="个人体验share">个人体验share</h3><p><strong>哲学类的书我不推荐看，因为这个很耗时间，容易学不到东西。</strong><br>关于道德类的讨论，还有国学这一块，旧文化毫无疑问是死掉了，现在什么复兴国学啊，<br>无非是对着一具僵尸祈祷。如果你们想学这个呢，可以<strong>大概扫扫孔子孟子荀子加王阳明的传习录，</strong><br>**不过心学讲的更多的是人的解放。**扫一扫就行，学深了没啥意义，这玩意本质上是一种道德教化，<br>但是实际上来说，真有用的还是看人能不能敢于斗争。。。。<br>学文科没啥固定的套路，<strong>有用就用，没用扔掉，别变成那只腐臭文人，指挥掉书袋。</strong><br>今天就暂时写那么多。欢迎大家一起互相讨论。</p><blockquote><p>天道有感</p></blockquote><ol><li>传统文化 强势文化 弱势文化 ==》演变成应该以政治文化为主，以道德和传统文化作为人文性情掌控的加锁</li><li>逐渐去学习现在很多事情的运行规律，多去尝试体会理解事物发展的本质规律，只有这样借势，慢慢去利用规律，才可能会帮助自己更好的认识本来的发展方向，更好的借力打力</li><li></li></ol><h3 id="政治cr是什么意思"><strong>政治cr是什么意思</strong></h3><p><a href="https://zh.wikipedia.org/wiki/CR">CR - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>极度濒危<br>极危<br>严重濒危<br>极度濒危物种</p><p><strong>政治cr是一种政治符号语言，CR是Community Relief（社区救济）的缩写，指政治上中左派团体对给弱势群体提供的各种救济措施的简称。这个术语是在网络上出现的，源自于社交媒体上用户对政治、文化、社会问题的讨论。这个术语主要出现在中国网络空间中，因为中国社交媒体的特殊环境，这个术语主要指代的是一种执政党在为人民办事的同时，也是在为自己、为党派、为官僚机构、为强势集团利益服务的政治现实。</strong><br><strong>政治CR的主要表现形式包括但不限于各种策划、活动、宣传、演讲、集会、网络言论、社区服务等。其中，社区服务在中左派政治圈子中被认为是最典型、最重要的CR方式。中左派倡导政治CR，不仅是为了弱势群体的合法权益，而且也是为了塑造中左派的良好形象和生存空间。当今中国的政治环境不容乐观，中左派圈子在应对复杂政治环境和外部压力时需要将社区服务和救济作为其政治理念和社区组织中的核心内容，以此来表明自己的政治态度和提高自己的公信力。</strong><br><strong>政治CR还有一层意义，它是在对传统CR概念的一个批判和拓展。传统CR是指在弱势群体受到自然灾害等不可抗因素影响时，社会团体或个人通过捐款、物资等方式向遭受不幸的群体提供救济的行为。这种救济行为受到了许多批评，人们认为它追求的仅仅是短期效应，而没有深入研究问题的根源。而政治CR则更注重于问题的根源研究，在提供救助的同时，也更注重于倡导相关的政治观点，从而更具有持续性和影响力。</strong><br><strong>总而言之，政治CR作为一种政治行动形式，在中国等一些国家有着广泛的意义。它是中左派的一种社会实践方式，在提供弱势群体救济的同时，也是在呼吁社会公正和推动社会进步。</strong></p><p>个人人生复盘</p><blockquote><p>个人角色</p><p>个人规划</p></blockquote><h1>润宇笔记</h1><p>拆解笔记，解读拆分   带货小时榜  咋没有知识付费或者教育类的呀  润宇老师<br>这个小时榜就代表了限时限刻的流行风<br>我今天也要调整我的带货顺序了[偷笑]<br>拆解他到底在买什么？  把每个直播间最撑着他直播销售的量的商品【100w中 有80w的商品】<br>10个商品 找出货盘，找出排名的前几个，哪些是主推的【人 货 场 】短视频的量？  投流来的量 还是转发来的</p><p>套路话术  看懂有什么过程  会逼单<br>如何卖？话术、时间循环、塑造价值、当下购买的理由<br>对哪些人   说那些话  卖什么货  产生了价值</p><p>先去看哪些有用  再去看他的逻辑【从消费者行为 去想为什么去买】<br>1、货盘、热卖的产品 2、引流品、信任品、利润品 3、流量是怎么来的：场观怎么来的<br>【ai 视频好 中华文化 疗愈  情绪价值 小红书【平台相关】  健康 】</p><p>普通人 ai 不是周鸿祎 则大可不必焦虑</p><blockquote><p>ai只是辅助创作的手段，从来不觉得ai是风口</p></blockquote><p>对于刚刚起步的商业体   找机会的话，一定冲着一个目标去：离现金流一定要近</p><blockquote><p>离现金流近，就一定要离交易近【交易场景明确】</p></blockquote><p>一定是需求和供给已经贼明确或者自己可以发现的时候: 人 货 场 形成干柴烈火的状态，一定最近且一定会发生</p><p>卖 ai 课的人，对ai感到焦虑的需求，视频直播平台提供课程 卖ai课是三者清晰的</p><blockquote><p>技术开发者  消费者  应用者</p></blockquote><p>AI写作，产生内容呢?</p><p>李一舟的课程一环套一环的感觉</p><p>写视频文案</p><blockquote><blockquote><p>技术开发者，需要有先机，抢开发机会</p></blockquote><p>技术抢先机，会形成商业壁垒</p></blockquote><p>哈哈，我现在都用ai来接文案商单了</p><blockquote><p>ai消费者</p></blockquote><p>新能源汽车</p><p>技术的应用者</p><blockquote><p>比亚迪 dmi的电池技术  宁德时代的玻璃 和 电池</p></blockquote><p>好不好用，好不好看，是否有高性价比</p><blockquote><p>伺候好消费者 是所有消费者的义务</p></blockquote><p>消费者好用便宜，不关注技术</p><blockquote><p>最终的技术最后都会不断降低壁垒，是不用学的</p><p>最后都会推动到消费者</p></blockquote><p>好的东西是不用学的</p><blockquote><p>但是你提前学会了，就有信息差，也能够提高你的生产效率。</p></blockquote><p>应用者 光有信息差，只能卖课，也需要产生出其他的价值帮助别人</p><p>大部分想要赚钱的 或者是利用好产品赚钱的，是应用者</p><blockquote><p>光写出文案 ，没有流量。光有数字人替代直播，但是卖不出去货</p><p>光有生成的视频，但是不能有浏览量，也没什么用</p></blockquote><blockquote><p>商业的底层逻辑</p><p>光帮助了生成量产，但是不能扩宽边界，仍然</p></blockquote><p>极其理解用户的交易场景，制造了一个想像空间，让用户感受到他也能，他也行的想法<br>价格由供求关系来决定 不是你的数量来决定[东西稀缺才值钱]</p><blockquote><p>热辣滚烫</p><p>用了一个很土的方法，100斤不稀缺，但是他在电影里真有的一个</p></blockquote><p>土是2024最大的稀缺<br>数字人没有情感与直播间的人不能产生共鸣。<br>手工写信，是因为你愿意花时间精力来干这样的事情，说明你很重要，被尊重，被重视，被需要<br>缺了真情实感<br>技术需要了解用户，靠应用+了解用户的需求才能产生money</p><p>核心竞争力，自己的优势到底是什么<br>应用者 利用信息差快速上位<br>了解客户，而不仅仅是了解技术；少去炫技，多去用感情+满足需求来挣钱<br>技术是应用者 利用信息差快速上位</p><p>应用者中赚到钱的一定是最理解用户的人，手里有客户 ，ai只是一个工具，所面对的用户仍然没有变过<br>铁打的用户，流水的技术</p><h3 id="孩子教育-是否要学ai">孩子教育 是否要学ai</h3><p>为什么要学扑克？ 象棋 围棋  人还要跑步？而不是打车呢</p><p>会让使用和学习的门槛回归0</p><p>提升审美和能力</p><p>从音乐 ，商业 到编程 都会一点的东西</p><p>培养孩子的内心力量，他们真的需求是什么</p><p>孩子的创造力和想象力很珍贵，AI影响不了</p><p>训练孩子的思维</p><p>情绪力 和创造力 ，人天然是一个大模型，</p><p>人要跟生活 跟很多元认知来产生很多碰撞</p><p>锻炼思考能力、逻辑能力、创造力</p><p>==遇到人生巨大困惑，需要使用创造力来进行利用解决，知识自己一只忽略了他的价值==</p><h1>中国基层的基础框架</h1><blockquote><p>政治经济学 组织经济学</p></blockquote><p>北上广深苏浙杭 VS 乡村 （90%人口）</p><p><img src="..%5Cassets%5Cimage-20240222101953924.png" alt="image-20240222101953924"></p><p>知之深才能爱之切</p><p><img src="..%5Cassets%5Cimage-20240222103121120.png" alt="image-20240222103121120"></p><blockquote><p>理解基层才能理解中国</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222103214019.png" alt="image-20240222103214019"></p><p>循序渐进，逐渐深入的关系</p><blockquote><p>主要研究政企关系，营商环境和制度经济学</p><p>三农问题和基层治理 并不强擅</p></blockquote><h3 id="怎么学？">怎么学？</h3><p><img src="..%5Cassets%5Cimage-20240222103441214.png" alt="image-20240222103441214"></p><blockquote><p>底层框架+冷冰冰的手术刀</p></blockquote><ol><li><p>构建思维框架，增强因果推断；</p></li><li><p>完成思维的培养和锻炼，在沟通中成长，在交流中进步</p></li></ol><p>==如切如磋如琢如磨==</p><h1>第一讲——理解中国的治理架构</h1><blockquote><p>治理架构也就是根本制度</p><p>掌握</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222103729013.png" alt="image-20240222103729013"></p><blockquote><p>元规则 支撑中国文明运行几千年的基本制度</p></blockquote><p>为什么要从制度或者底层治理框架上开始？</p><blockquote><p>只有先从制度角度理解制度背景，才能从宏观角度理解中国的基层治理，才能高屋建瓴的看透中国基层治理的现状、问题 和对策</p></blockquote><p>从制度角度看问题才能看的更透，更深 更远</p><p><img src="..%5Cassets%5Cimage-20240222104937019.png" alt="image-20240222104937019"></p><p>不是就事论事 形成头痛医头 脚痛医脚的毛病</p><p>主要研究三个问题：</p><ol><li>为什么要演研究中国的治理架构？</li><li>什么是中国治理的基本架构</li><li>如何在中央—地方关系下，理解基层与上层的互动<ol><li>中央和地方之间到底是兄弟关系还是父子关系呢？</li></ol></li></ol><p><strong>主要目的是形成理解基层治理的底层框架和系统思维</strong></p><p><img src="..%5Cassets%5Cimage-20240222105109455.png" alt="image-20240222105109455"></p><blockquote><p>万变不离其宗，掌握底层框架才算是掌握了知识的金钥匙，也就是武林秘籍中的 独孤九剑</p></blockquote><blockquote><p>条块结合，以块为主的中央集权体制</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222105215896.png" alt="image-20240222105215896"></p><blockquote><p>纪委为什么成为强势单位？ 县委书记为什么不好当?</p><p>怎么就形成了上边千条线，下边一根针的基层治理困境</p></blockquote><blockquote><p>大A  制度经济学</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222110039821.png" alt="image-20240222110039821"></p><p><img src="..%5Cassets%5Cimage-20240222110129262.png" alt="image-20240222110129262"></p><blockquote><p>制度经济学 包括 政治制度 经济制度 法律 文化 以及政府政策</p></blockquote><h2 id="需求">需求</h2><ol><li>实时，需要有<strong>较快</strong>的推理速度（高优先级）</li><li>便捷，需要该模型便于部署，特别是部署到Unity平台</li><li>精确，至少要让用户觉得数字人的口型可以和实际口型对应</li><li>风格化，无需针对某个特定的音色/角色微调模型，即<strong>one model for all user</strong></li><li>口型、avatar面部表情、动作同步；其中口型、表情都是用blendershape驱动，动作暂时是循环播放固定动作</li></ol><h2 id="实验计划">实验计划</h2><p>优先参考现有的代码：</p><p><a href="https://github.com/yunik1004/SAiD.git">https://github.com/yunik1004/SAiD.git</a></p><p><a href="https://github.com/huailiang/LipSync">https://github.com/huailiang/LipSync</a></p><p><a href="https://github.com/zhongshaoyy/Audio2Face">https://github.com/zhongshaoyy/Audio2Face</a></p><p><a href="https://github.com/leventt/surat.git">https://github.com/leventt/surat.git</a></p><p><a href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p><p><a href="https://github.com/Rtyper/LipSync-Pro">https://github.com/Rtyper/LipSync-Pro</a></p><p><a href="https://github.com/guanjz20/StyleSync">https://github.com/guanjz20/StyleSync</a></p><h2 id="后续需要补的知识">后续需要补的知识</h2><p>@李润一</p><p>Unity开发、部署相关知识Sentis的使用：<a href="https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4">https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4</a></p><h2 id="Audio2Face-expression更新">Audio2Face+expression更新</h2><p>可以参考下列工作：</p><p><a href="https://github.com/FACEGOOD/FACEGOOD-Audio2Face/">https://github.com/FACEGOOD/FACEGOOD-Audio2Face/</a></p><p>更新：FaceGood无法在linux平台上测试，只能在windows上运行</p><p>（其余工作未使用blendershape的输出，可在下https://github.com/FACEGOOD/FACEGOOD-Audio2Face/列链接中查看：）</p><p><a href="https://xie.infoq.cn/article/2f414deaa57ca29783d6f873f">语音驱动嘴型与面部动画生成的现状和趋势_算法_行者AI_InfoQ写作社区</a></p><p><a href="https://www.gameres.com/895985.html">语音生成口型与表情技术的演进与未来 - GameRes游资网</a></p><p><a href="https://github.com/nowickam/facial-animation/tree/production">nowickam/facial-animation: Audio-driven facial animation generator with BiLSTM used for transcribing the speech and web interface displaying the avatar and the animation (github.com)</a></p><p>[<a href="https://www.jianshu.com/p/fbc5743152f0">Paper Reading] Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion - 简书 (jianshu.com)</a>（这篇工作就是nvidia开发的omnivetrse audio2face的基础）</p><h1>个人选择</h1><p>相对竞争优势 &amp;&amp; 个人的兴趣爱好</p><blockquote><p>根据对应的时代特征，来进行选择</p><p>有一个选择4象限</p></blockquote><p><img src="..%5Cassets%5Cimage-20240301094224506.png" alt="image-20240301094224506"></p><p><strong>政府部门进行招商引资</strong></p><p>PPT 造车  <a href="https://zhuanlan.zhihu.com/p/642907927">PPT造车鼻祖倒下？烧光174亿，九年也没憋出一辆车 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT%e9%80%a0%e8%bd%a6&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1">https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT造车&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1</a></p><h2 id="工具ChatGPT">工具ChatGPT</h2><p>ChatGPT4的归档功能</p><p><a href="https://www.landiannews.com/archives/101487.html">OPENAI宣布ChatGPT现在可以归档会话喽 不删除会话的同时保持简洁 - 蓝点网 (landiannews.com)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/project/posts/4a17b156.html"/>
      <url>/project/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>思考的原点1.1</title>
      <link href="/project/posts/6c9e281a.html"/>
      <url>/project/posts/6c9e281a.html</url>
      
        <content type="html"><![CDATA[<p><img src="../assets/image-20240319200003990.png" alt="image-20240319200003990"></p><p><img src="../assets/image-20240319200138262.png" alt="image-20240319200138262"></p><p>具体影响:</p><p><img src="../assets/image-20240319200442568.png" alt="image-20240319200442568"></p><blockquote><p>民国时期的张爱玲 胡适  不超过10%</p><p>国民党的留学博士  20%  高智商的人 但是不一定明白</p></blockquote><p><img src="../assets/image-20240319200959489.png" alt="image-20240319200959489"></p><p>思考框架背后</p><p><img src="../assets/image-20240319222056735.png" alt="image-20240319222056735"></p><blockquote><p>哈耶克 说英美 两种经验主义？？</p><p><strong>14-15分钟左右 没听懂</strong></p></blockquote><p><img src="../assets/image-20240319223401990.png" alt="image-20240319223401990"></p><p><img src="../assets/image-20240319223752011.png" alt="image-20240319223752011"></p><p>人类积累了上万年的常识智慧 我们需要借鉴吗？</p><p>哈耶克认为 英国自由的保守主义 包括解决一战 和法国大革命 是以经验主义演变来的，马老师定义为经验的共识？</p><p><img src="../assets/image-20240319223950464.png" alt="image-20240319223950464"></p><blockquote><p>不言而喻 的东西 + 假设 形成自然科学</p></blockquote><p>社会科学 基于很多公理 在进行理性推理+假设</p><blockquote><p>先聊基本常识有哪些【多年积淀】</p><p>在聊到良知【希特勒 ，斯大林的大清洗 法国资产阶级大革命】</p></blockquote><p><img src="../assets/image-20240319225127785.png" alt="image-20240319225127785"></p><blockquote><p>人是目的还是工具？</p><p>与人相处的时候什么是平等和公平</p><p>个人权利和公众权力</p><p>平等和自由的时候哪个更重要 本质是左派和右派</p><p>权力和道德的关系 良知和正义的关系</p></blockquote><p>读写是最好的自我投资</p><p>经济持续放缓（5年之内）悲观的2%的商铺不会盈利，疫情导致大家消费欲望变低</p><blockquote><p>最难的反而是28-40 的中层危机，持续危机</p></blockquote><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240409221345478.png" alt="image-20240409221345478"></p><blockquote><p>读书是</p></blockquote><p>35岁 经验为导向的职业，努力不在撬动发展，需要持续做对选择来撬动个人发展</p><blockquote><p>生存问题变成发展问题</p></blockquote><p>π型人才，</p><blockquote><p>所有的变化和需求都需要持续的阅读，来保证个人的读写能力</p></blockquote><p>每个人都最容易触及的，都可以获得的，收益最大的都是读写能力，</p><p>读写只是自己的一种阅读输入输出的方式</p><blockquote><p>读写 在信息时代把数据吃进去再用更好的方式把数据吐出来，就永远都不会饿死</p></blockquote><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240409221646881.png" alt="image-20240409221646881"></p><blockquote><p>消费型的阅读或者时间投入</p></blockquote><blockquote><p>仅仅是方便体验和知道了一些知道的</p></blockquote><p>投资型的阅读和输入</p><blockquote><p>咖啡豆的示例</p></blockquote><p><strong>自己的投资型和消费型的比例是怎样的呢？是否需要控制呢（包括视频和读书方面，甚至是任何各种输入自己眼中、头脑中的数据和信息）</strong></p><p>10分的话，消费占多少？投资占多少？</p><p><strong>思想变得浅薄</strong></p><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240409224042781.png" alt="image-20240409224042781"></p><p>底层思维的书  怎样应用呢？【】</p><blockquote><p>复用率 ？ 有短期收益或者长期收益？？？</p></blockquote><blockquote><p>遇到任何书的时候，都要去重新去思考，对症下药去分析</p></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240409224331895.png" alt="image-20240409224331895"></p><p>当你把自己价值能完整的输出的时候，就是一件非常有价值的东西</p><ol><li>你比别人想得深</li><li>你能够宣传，有品牌</li></ol><p>拍卖效应 + 手电思维？灯塔效应</p><blockquote><p>持续放光</p></blockquote><blockquote><p>德国人的灵魂？？丁元英  灵魂<br>开启一个一个的小镇</p></blockquote><p>穷人/普通人最好的投资、最好的职业规划、最好的学习方法 、是最便宜最舒适让内心能够更宁静的一种旅游小镇的方式</p><p><img src="..%5Cassets%5Cimage-20240409224953036.png" alt="image-20240409224953036"></p><h2 id="写卡感触">写卡感触</h2><p>我最有感悟的古典⽼师的⼀句话是：古典老师关于阅读读书时候的考量因素（如何选书或者评价书是否适合自己去阅读）+对于书复用率的不同分类（经典的底层逻辑的书  实现某种功能的书  畅销书 ）+古典老师如何论证阅读的效益的递进方式（先背景介绍-&gt;环境要求-&gt;个人发展需要(投资数据支持、职业规划)-&gt;心灵上读书是必要条件）+读写是一个持续的东西，很多公众号断更/没坚持到最后，只有持续的读+写，形成闭环才能够好的坚持走下去【读写是最好的个人投资，而不仅仅是只读/只写（写一篇没用，读一篇也没用）】</p><p>阅读不是要做的唯一一件事情，写作一样，并不是孤立的，而应该两者形成整体的一个系统，作为输入和输出，同时去构建他，维护他们，并作为长期主义，持续的做下去。</p><p>我过去踩的坑是：没坚持下来阅读+写卡（也有各种原因，也一直没有开始去做）;阅读读书没有将该原则应用，还是老套路选书；自己是第二次看这个视频，之前也看过这个视频好几遍，甚至都写过关于这个视频的卡（第一次参与共读会+选书开始写卡的时候，都是看的这个视频），但自己只是知道并没有做到，这两者中间真的差了一个巨大的鸿沟。</p><p>我以后可以更好的⽅式是：</p><ol><li><p>尽量减少完美主义的影响，有时候一想起要把事情做得很好会碰到一些苦难，所以就会害怕或者懒得去开启行动；尽可能快的开始，言出努力去行，用行动来进行反馈，尽量只要收获一点点东西就enough。</p></li><li><p>昨天听了师兄师姐的会议分享，感触颇多，要多跟大家去同频共振，减少自己对于现实世界中困难的想象，联机学习，一起进步</p></li></ol><h1>4.10</h1><ol><li>整理时间安排</li></ol><h2 id="读书读不懂？记不住？用不上？">读书读不懂？记不住？用不上？</h2><p>读不懂</p><ol><li>读深入 读透作为自己的读书指标</li><li>读书 缺少体验，<ol><li>听话vs阅读理解？ 人的大脑临时组件是不能对阅读理解来实现天生组件完成该功能的，所以需要将听觉、视觉、大脑打猎的部分临时组合来实现阅读理解</li><li>缺少人生经历和体验（任何的抽象概念都无法不通过具体的体验去理解）——没有见过圆的人永远想象不到圆有多完整（没见过趋近于直线的人无法想象到无限延长的直线）</li></ol></li></ol><p>记不住</p><p>存难取易（偷钱挨打这件事 一辈子记住（一辈子记住自己不要丢三落四））</p><p>大脑 easy come easy go 5</p><p>一天听4个小时 什么都记不住</p><blockquote><p>给别人讲一遍</p></blockquote><p>最好的记录员？？（跟他们做朋友）</p><p>利于记忆，但不利于提取</p><p>缺少必要的知识管理工具</p><blockquote><p>缺少实践【】</p><p>缺少系统【老板上课】</p></blockquote><h3 id="个人脑子里记录的东西">个人脑子里记录的东西</h3><blockquote><p>思维导图—— 笔记整理软件——写卡片</p></blockquote><blockquote><p>白天听课，晚上写</p><p>？？难度</p></blockquote><p>整体的系统框架？？</p><p>读写镜像，关于知识树的构建和拆分是利用读写来完成的</p><p>读的话 就是输入，将个人读的信息从一维提取主题后形成二维大纲，然后放入到大脑的知识网中</p><p>写的话，是输出，将大脑中的网状知识结构，先提取出一个主题形成大纲，然后大纲每一个主题往外写，形成一维输出</p><blockquote><p>既是一讲写作课，也是一讲阅读课</p></blockquote><p>写卡营第二讲体悟分享</p><p>【内容来源】写卡营第二讲课程古典老师分享（下）</p><p>【本卡主题】关于建立体系以及写卡的四个重要知识点</p><p>【原文摘录】<br>SS卡的实现体系是从①选书开始 ② 关注理解体验 ③深刻记忆内容④付出行动实践（各种训练营 打卡营 读书营） ⑤坚持30天不断积累卡片 ⑥形成卡片体系 ⑦卡片成文营训练 ⑧文章/作品 大纲⑨创造价值，不断发展<br>SS卡的四个重要知识点：①存难取易②最小单元（卡片） ③乐高式写作（搭建） ④读写镜像（读写同构）</p><p>【概念转述】<br>————————————————————<br>选书——选的是底层的，比较基础的且复用率比较高的书<br>理解——转述出来之后，用自己的话说一遍辅助加深理解<br>体验——记录那一瞬间的感触/体验/思考，并不断积累汇总<br>行动——行动与体验并行，才能帮助更好的融会贯通<br>其他的点是在上边的基础上不断的践行+积累，慢慢创造价值</p><p>————————————————————<br>存难取易——既能帮助深入理解，方便快速提取体验卡，也能实现系统搭建<br>最小单元——借用卡片的方式，每天复习，学习+联系<br>乐高式写作——反对模板化的方式，以阅读输入为起点，存下每块乐高，最终搭建出体系<br>读写镜像——读写不分离，读书—写作应该是同步进行，并且是一个相互可逆的过程<br>————————————————————<br>【个人体验/思考】</p><ol><li>形成整个读写体系应该是战略性的设定，然后再按照整个流程详细的展开，按照古典老师讲述的过程，不断积累，一点点去主动阅读并写下体验，深刻体会后并用行动去加深理解，想起了之前阅读《七个习惯》好像是一样的思路</li></ol><p>2.在自己一个月从158变成142的过程中，深刻体会到自己实践——体验——继续付出努力增强信心的整个闭环过程，现在想来应该也是得益于系统的搭建</p><p>3.读写同构这一点，让自己印象深刻；在读书的时候，是从1维度获取零散的知识，然后在笔记中进行罗列整理，形成2维度大纲，最后在脑中形成3维度网状知识体系；<br>书写的时候，是利用脑中的3维度网络知识图，将信息提取后，在纸面上形成2维度大纲，最后按照每个要点详细展开细节也即1维度的内容加工。</p><p>【行动指南】<br>写卡先，首先认识到，到知道，再到做到，感谢老师给了一个可行性比较强的抓手——写卡</p><p><a href="https://doc.weixin.qq.com/doc/w3_AdgAaQawAMceNWSNgcxRSWAS0TYZ0?scode=AFgAiwfiAGI7oggms0AdgAaQawAMc">软件打包 (qq.com)</a></p><p><a href="https://www.zhihu.com/column/c_1245860717607686144">Nuitka-Python打包exe - 知乎 (zhihu.com)</a></p><p><a href="https://github.com/erdengk">erdengk (尔等同学) · GitHub</a></p><p><a href="https://book.douban.com/subject/35050614/">我的二本学生 (豆瓣) (douban.com)</a></p><p><a href="https://avuucupcq6.feishu.cn/docx/EWYYdkYIeoL4XIxw97xcLqQnnRc">一册通.《锻炼》共读营 - Feishu Docs</a></p><p>请结合下边给出的5个Skeleton-based Action Recognition 介绍的内容，生成一个新的关于Skeleton-based Action Recognition的详细介绍，内容尽量新颖独特一点，用词合理且通顺，大概在500单词左右<br>1 Skeleton-based Action Recognition<br>With the huge advances of deep learning, recurrent neural net-<br>work (RNN)-based, convolutional neural network (CNN)-based,<br>graph convolutional network (GCN)-based, and transformer-based<br>methods are studied for skeleton-based action recognition. RNNs<br>have been widely used to model temporal dependencies and cap-<br>ture the motion features for skeleton-based action recognition. The<br>work in [9] uses RNN to tackle the skeleton as sequence data. Sub-<br>sequently, Song et al. [37, 38] proposed to utilize the attention<br>mechanism and multi-modal information to enhance the feature<br>representations. Some other works [15, 24] transform each skeleton<br>sequence into image-like representations and apply the CNN model<br>to extract spatial-temporal information. Recently, GCN-based meth-<br>ods have attracted more attention due to the natural topology struc-<br>ture of the human body. Many works [6, 35, 47] apply GCN to<br>the spatial and temporal dimension [47] and achieves remarkable<br>results in the supervised skeleton-based action recognition. Mean-<br>while, transformer models [29, 36] also show promising results,<br>owing to long-range temporal dependency learning by attention.</p><p>2 Skeleton-based action recognition<br>Generally, there are two ways to obtain skeleton data from<br>videos. Firstly, the position information of human joints is ob-<br>tained from RGB video frames by pose estimation methods, such<br>as Openpose [26]. Secondly, the 3D skeleton data of the human<br>body can be obtained directly through the depth sensor, e.g., Mi-<br>crosoft Kinect v.2. In this paper, the datasets we use includes two<br>forms, RGB videos and skeleton sequences. These two forms of<br>data are shown in Fig. 1.<br>Most early research on skeleton-based action recognition usu-<br>ally used handcrafted features to represent the human body.<br>Hussein et al. [10] used the covariance matrix for joint posi-<br>tions over time. In [11], rotations and translations were used to<br>model 3D geometric relationships between body parts. However,<br>these methods were complex in design processes and ignored the<br>correlations between specific human body parts.<br>With the further development of deep learning, CNN-based<br>and RNN-based methods were proposed to process the grid<br>data reconstructed from human skeleton data. CNN-based meth-<br>ods [27–29] convert the skeleton data into pseudo-images, then<br>attempt to learn action features. In [30], a skeleton-based trans-<br>former module was designed for action classification and recog-<br>nition. Kim et al. [31] proposed a 3D human action recognition<br>model named Temporal Convolutional Neural Networks. In [32],<br>an enhanced skeleton visualization method was presented to<br>deal with view variations and noisy data. Liu et al. [33] first<br>applied 3D convolution for the skeleton-based recognition task to<br>simultaneously capture motion correlations of spatial–temporal<br>dimensions. Huynh et al. [34] proposed a new encoding method,<br>Pose-Transition Feature to Image (PoT2I), which transforms high-<br>level features of skeleton data into color pixels. In recent works,<br>the skeleton-based data was formulated as sequences of grid-<br>shaped structures and then modeled with RNN-based methods [8,<br>35,36]. Shahroudy et al. [37] proposed an RNN-based structure<br>to model the long-term temporal correlation of motion features.<br>In [38], RNN was used to analyze the hidden sources of motion<br>relationships in both spatial and temporal dimensions. In [39],<br>an end-to-end spatial–temporal attention model was built with<br>Long Short-Term Memory (LSTM) to focus on discriminative<br>joints. In [40], a novel contrastive action learning paradigm called<br>AS-CAL with a momentum LSTM module was proposed to unsu-<br>pervised action recognition. However, the skeleton-based data are<br>embedded not in the form of vector sequences or 2D grids but<br>in the graph structure, so both RNNs and CNNs based methods<br>cannot represent the structure of skeleton-based data well.<br>GCN-based methods utilize graph convolution modules to ex-<br>tract motion dependencies from the graph structure of natural<br>human skeleton data, which has been demonstrated to be the<br>most expressive. Yan et al. [4] innovatively proposed the first<br>model with GCN, namely ST-GCN, to capture the balance between<br>spatial and temporal dependencies. In this model, the skeleton<br>is expanded into a graph structure, and the spatial–temporal<br>patterns are automatically extracted from skeleton-based graph<br>data. Subsequently, many studies [41–43] were proposed based<br>on ST-GCN. In [35], 2s-AGCN was presented with adaptive ad-<br>jacency matrices to capture the motion information from high-<br>order skeleton data. Liu et al. [44] proposed MS-G3D to conduct<br>multi-scale long-range modeling of spatial–temporal graphs. Sim-<br>ilarly, AS-GCN [21] inferred A-links from input data to capture<br>multi-scale actional relationships. Zhang et al. [45] investigated<br>high-level semantics information from skeleton-based data and<br>designed two modules to exploit the high-order correlation of<br>skeleton data.</p><p>3 Skeleton-based models: To find a more effective rep-<br>resentation of the dynamics of human actions, Johansson [24]<br>utilizes 3D skeleton sequences for action recognition, making<br>an obvious decrease of computational cost as well as a good<br>performance boost. Recently, with the rapid development of<br>deep learning techniques, skeleton-based action recognition<br>methods have attracted increasing attentions. Researchers have<br>proposed various models to improve the performance of action<br>recognition, which can be divided into three major categories.<br>The first category builds the models with convolutional net-<br>works. For example, Li et al. [16] propose a CNN-based<br>co-occurrence feature learning framework, which gradually<br>aggregates various levels of contextual information. Kim et<br>al. [15] build a temporal convolutional network to explicitly<br>learn readily interpretable spatio-temporal representations for<br>3D human action recognition.<br>Besides, for the second category, researchers concatenate all<br>joints in one frame into a single vector, then use sequential<br>models such as long short-term memory (LSTM) to explore<br>the temporal dynamics. Du et al. [25] design a hierarchical<br>bidirectional RNN to capture rich dependencies between dif-<br>ferent human body parts. The study in [9] employs a view<br>adaptive LSTM, which enables the network itself adaptive to<br>the most suitable observation viewpoints. Additionally, Song<br>et al. [26] firstly introduce attention modules into skeleton-<br>based action recognition.<br>Both CNN-based and RNN-based methods are still limited<br>to extract the spatial structure information among skeleton<br>joints, where the joints of different body parts are connected<br>as a skeleton graph. Instead, in the third category, graph-<br>based methods can be naturally utilized to deal with the<br>skeleton graph, which successfully captures the most infor-<br>mative features for various actions. Si et al. [13] use GNN to<br>model the relationships among five body parts. Yan et al. [10]<br>initially introduce GCN into skeleton-based action recognition,<br>and produce a baseline named ST-GCN for future research.<br>Based on the ST-GCN, many studies achieve continuous<br>improvements on skeleton-based action recognition [27], [28],<br>[29].<br>4Skeleton-based action recognition<br>The progress of deep learning [30–32] and the advancement of<br>human pose estimation algorithms have facilitated human action<br>research. Basically, skeleton-based action recognition deals with<br>temporal series, where the dynamics of human body poses over<br>time characterize human actions. Early methods generally used a<br>manual feature approach to capture human action [33–35] . How-<br>ever, this relies mainly on 3D rotations and translations between<br>joint points, so feature design is more complex and has average<br>performance.<br>Recently, deep learning methods have achieved favorable out-<br>comes in skeleton-based action recognition, which can be basi-<br>cally classified according to the network architecture: Convolu-<br>tional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),<br>and Graph Convolutional Networks (GCNs). CNN-based methods<br>convert human skeleton sequences into pseudo-images, which are<br>trained and then classified. Structurally, CNNs are constructed by<br>stacking convolutional layers, pooling layers, and activation func-<br>tions, etc. The CNN achieves network optimization by adjusting the<br>number of convolutional and pooling layers, the size of convolu-<br>tional kernel, and the step size of convolution. Li et al. [36] pro-<br>posed an end-to-end convolutional co-occurrence feature learn-<br>ing framework. The co-occurrence features are learned using a hi-<br>erarchical method with different levels of contextual information<br>gradually aggregated. Kim and Reiter [37] proposed a new model<br>called Temporal Convolutional Neural Network (TCN) for 3D hu-<br>man action recognition, it used one-dimensional residual CNNs and<br>based on directly connected joint coordinates to identify skeleton<br>sequences. Liu et al. [38] proposed an enhanced skeleton visual-<br>ization method, which visualizes the skeleton as a series of color<br>images and then implicitly encodes the spatio-temporal informa-<br>tion of the skeleton joints. Since CNN can only process regular grid<br>data in non-Euclidean space, it is not as good as other networks in<br>skeleton-based action recognition.<br>RNN-based approaches typically model skeleton sequences as<br>a series of coordinate vectors along spatial and temporal dimen-<br>sions, where the vectors represent the body’s joint points. Liu et al.<br>[39] proposed a tree-structure-based traversal method that uses<br>an RNN-based approach to model the spatio-temporal domain and<br>introduces a new gating mechanism in LSTM to learn the relia-<br>bility of sequential input data. Liu et al. [40] designed a Global<br>Context-Aware Attention L STM (GCA-L STM), which is able to se-<br>lectively focus on information joints in action sequences with the<br>assistance of global contextual information. Zhang et al. [41] de-<br>signed an attentional mechanism, it embeds a recurrent attentional<br>network that can explore the spatiotemporal relationships between<br>different local regions to focus on important regions. Zhu et al.<br>[42] proposed an end-to-end fully connected deep LSTM network<br>that takes the skeleton as input at each time point and introduces<br>a new regularization method to learn the co-occurrence features of<br>skeleton joints.<br>However, both CNNs and RNNs have difficulty in capturing the<br>natural graph structure of skeleton topological features. To better<br>capture human action features [16,43] , recent work has utilized<br>GCNs [44,45] for spatial and temporal modeling of actions</p><p>5 Skeleton-Based Action Recognition<br>Skeleton-based action recognition is a fundamental yet<br>challenging field in computer vision research. Previous<br>skeleton-based motion recognition methods are usually re-<br>alized with the geometric relationship of skeleton joints [7,<br>36, 37]. The latest methods pay more attention to deep net-<br>works. Du et al. [6] applied a hierarchical RNN to process<br>body keypoints. Attention-based methods are proposed to<br>automatically select important skeleton joints [28–30, 47]<br>and video frames [29, 30] to learn more adaptively about<br>the simultaneous appearance of skeleton joints. However,<br>recurrent neural networks often suffer from gradient van-<br>ishing [11], which may cause optimization problems. Re-<br>cently, graph convolution networks attract more attention<br>for skeleton-based action recognition. To extract both the<br>spatial and temporal structural features from skeleton data,<br>Yan et al. [40] proposed spatial-temporal graph convolution<br>networks.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>今日一心</title>
      <link href="/project/posts/3ccff96b.html"/>
      <url>/project/posts/3ccff96b.html</url>
      
        <content type="html"><![CDATA[<h1>聂辉华番外课程1该不该走选调生</h1><h2 id="细节展开1">细节展开1.</h2><p><img src="../assets/image-20240315191357916.png" alt="image-20240315191357916"></p><p><img src="../assets/image-20240315191542319.png" alt="image-20240315191542319"></p><p><img src="../assets/image-20240315191701845.png" alt="image-20240315191701845"></p><p><img src="../assets/image-20240315191804417.png" alt="image-20240315191804417"></p><p><img src="../assets/image-20240315191829363.png" alt="image-20240315191829363"></p><p>区别</p><p><img src="../assets/image-20240315191931043.png" alt="image-20240315191931043"></p><p><img src="../assets/image-20240315192049444.png" alt="image-20240315192049444"></p><p><img src="../assets/image-20240315192057670.png" alt="image-20240315192057670"></p><p>选调生 vs 统考生</p><p><img src="../assets/image-20240315192337006.png" alt="image-20240315192337006"></p><p><img src="../assets/image-20240315192406696.png" alt="image-20240315192406696"></p><h2 id="选调生、国考、省考">选调生、国考、省考</h2><blockquote><p>资料汇总</p><p><a href="http://www.sdgwy.org/html/kszc/202403/2_81743.html">2025年山东公务员考试报名信息「提前收藏」 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/">山东公务员考试最新消息 -2024年山东公务员考试网上报名时间、考试大纲、历年真题 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/html/zkgg/xds/index.html">招考公告/选调生 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><img src="../assets/image-20240324125043891.png" alt="image-20240324125043891"></p><p>时政学习</p><p><a href="https://www.chinagwy.org/html/ggjczs/mszt/202401/52_615293.html">公务员考试备考必看：2024年时事政治合集「持续更新」 - 国家公务员考试最新消息 (chinagwy.org)</a></p></blockquote><h3 id="选调生">选调生</h3><p>23年毕业生有一个青岛的选调  但是24年毕业生没有，只面向了东南大学</p><p><strong>25年未知</strong></p><p><a href="http://www.sdgwy.org/html/gdzk/qd/202211/67_73792.html">2023年青岛市“青选计划”选调公告（山东师范大学） - 山东公务员考试最新消息 (sdgwy.org)</a></p><p>都是只能报考一个单位或者岗位。</p><blockquote><p>需要选调生的历年数据</p><p><a href="https://zhuanlan.zhihu.com/p/645424990">山东定向&amp;常规选调最全报录比数据（2023年最新） - 知乎 (zhihu.com)</a></p></blockquote><blockquote><p>详细政策和选择分析</p><p><a href="https://zhuanlan.zhihu.com/p/601525199">2023山东定向选调第一批——山东专额选调全面解读（附岗位指导） - 知乎 (zhihu.com)</a></p><p>专额选调 是指的第一批选调</p></blockquote><blockquote><p><a href="https://ah.huatu.com/zt/sdxdszwb/">2024年山东招录选调生考试职位表下载_岗位查询-山东公务员考试网 (huatu.com)</a> x</p><p>选调岗位  【下载下来的是22年的】</p><p>在济南或者说山东 是普通选调的县直 或者是乡镇</p></blockquote><h3 id="证监会-VS-金管局">证监会 VS 金管局</h3><p><a href="https://mp.weixin.qq.com/s/_XW4D0jRiy-g0Q6ldQWGVA">获取国考证监会历年真题资料方式 (qq.com)</a></p><h1>载脑体 爱情第一讲</h1><blockquote><p>系统性，本源性 去剖析男女关系的本质</p><p>从可实践的角度分析如何遇到一个对的人</p></blockquote><h2 id="摆正观念">摆正观念</h2><p>对于大多数人而言，人生中最重要的决定是跟什么样的人相伴一生</p><p><img src="../assets/image-20240320155836104.png" alt="image-20240320155836104"></p><p>其次才是选择行业，再差的行业都会给与自己一个安身立命之所，职业的周期性会贯穿个人的前半生，但在未来的AI时代，自己可能无法保证未来行业的连续性，且很多时候决定行业的趋势的往往是风口和周期率，而非自身所能左右</p><blockquote><p>选对一个好的伴侣不仅仅影响自己的金钱、时间，也会影响自己人生漫漫长路的质量，也是自己为数不多的能够抵抗世界周期率和波动的抉择之一</p></blockquote><p>好男儿不在乎儿女情长，但对于大多数人，尤其是男生，除了谋生赚钱的知识以及专业技能基础课之外，优先掌握如下知识：</p><p><img src="../assets/image-20240320160615680.png" alt="image-20240320160615680"></p><blockquote><p>每一条都事关个人一生真正的幸福</p></blockquote><p>其他事情可以感兴趣，但不能对于以上事情无知的情况下，去琢磨其他事情</p><h3 id="摆正对于婚姻的理解">摆正对于婚姻的理解</h3><blockquote><p>职场雇佣关系，朋友关系，商业上的价值互换关系</p></blockquote><p><img src="../assets/image-20240320160800430.png" alt="image-20240320160800430"></p><p>婚姻的本质 是找一个战场上的一个战友关系，是一个相互爱慕的战友</p><p>首先是同目标，共进退。不抛弃，不放弃；</p><p>其次才是对方的技能有多强</p><p>==慕强的本质关系==</p><h3 id="亲密关系的无知">亲密关系的无知</h3><p>大学教育里边却没有教育，自己家庭里边的空白，大多数事情里边都有一个专业，</p><p>而对于两性关系方面，大多数人是面对的确是自己没有自知之明的无知</p><h2 id="先进行自身建设">先进行自身建设</h2><blockquote><p>爱人之前，先爱自己</p></blockquote><p><img src="../assets/image-20240320161516105.png" alt="image-20240320161516105"></p><p>==沉默成本解释外，更深层的理解==</p><blockquote><p>自我认可度不高的人，很容易会把恋爱中的甜蜜时刻，理解为对方是命运给与自己的恩惠和赏赐，从而会给对方添加滤镜。即使往往对方对自己并不好，也很难结束一段有毒的关系。</p><blockquote><p>能做到这一步很难，很多人要用一生来去治愈童年</p></blockquote><p>自我认可度不高的人很容易为了人与人之间当下的短暂快乐，而自愿去做明知没有结果的事情，去维持明知没有结果的关系，甚至会让自己主动去付出巨额代价。</p></blockquote><blockquote><p>会遇到人渣并让自己在一段感情中自我献祭</p></blockquote><p><img src="../assets/image-20240320161612688.png" alt="image-20240320161612688"></p><blockquote><p>异地女朋友 单独和男生吃饭后才主动告知，下次买票钱我来出？？</p></blockquote><h3 id="如何把握严于律己，宽以待人的分寸？"><strong>如何把握严于律己，宽以待人的分寸？</strong></h3><p>严于律己，宽于待人 要当做适用于对待特殊情况下的策略，而不是适用于任何情况下的美德</p><p>对方和你没有任何的利益关系，且对方的行为在长远期看，不会损伤你的个人利益</p><p>朋友关系，朋友圈点赞关系，甚至某些情况下 要采用“宽于律己，宽以待人”的策略</p><p><img src="../assets/image-20240320163117338.png" alt="image-20240320163117338"></p><p>对于对方如果跟你有直接利益关系，并且它的行为会长远期看，会影响你自己的个人利益的情况下，==尤其是涉及到人品这个特殊变量的情况下==那么就要去采取“严于律己，同以待人”的策略</p><p><img src="../assets/image-20240320163603054.png" alt="image-20240320163603054"></p><p>学会善用反问句，要让那些让你自己不舒服的人同样难看</p><p><img src="../assets/image-20240321102350839.png" alt="image-20240321102350839"></p><blockquote><p>不管是亲密关系，还是社交关系，要清楚别人时刻是在筛选自己而自己也要时刻学会去主动筛选别人</p></blockquote><p><img src="../assets/image-20240320163636818.png" alt="image-20240320163636818"></p><blockquote><p>不是所有人都会与你同行，在一段亲密关系里边，在你为对方去付出并且对方很享受在你这里的获得时候，对方也应该为你而建立一份契约，你自己也有筛选别人的资格，而非被筛选的资格。</p><p>等你学会如何不惧直面冲突，再去学习如何高情商。</p></blockquote><p><img src="../assets/image-20240320164821898.png" alt="image-20240320164821898"></p><blockquote><p>宜人和温和是强者能量溢出的特权，再身弱之人那里，只会演变成懦弱和委曲求全而不自知，并最终反向筛选出一批人渣主动留在自己的身边。</p></blockquote><blockquote><p>要做到以上，内核是自尊自爱；而不管自己有没有做成一些事情，现在自己要做的就是在所有的关系中，去无条件的提高自我认可度。</p></blockquote><blockquote><p>重要的不是对自己高看或者低看，而重要的是在爱人之前，先让自己变得足够强</p></blockquote><h3 id="爱人之前先变强的两个心法">爱人之前先变强的两个心法</h3><ol><li><p>改变自己的关键是创造价值</p><ol><li>学会用价值规律看问题，让自己变成一个有价值的人</li><li>改变自己的关键是发掘自己并不断的创造价值（创造视频，写文案）<ol><li>帮助别人的同时不断的成就自己</li><li>创造的价值必须是长久的</li></ol></li></ol><blockquote><p>价值够长久，才能够价值够大。同时价值足够大时候，才能够在任何关系中得到对等的回馈</p><p>当把视角“我想要”变成“我能给”的时候，很多浮躁妄念，就会马上消失。</p></blockquote></li><li><p>建立自己的&quot;身份感&quot;,不是指的是社会世俗意义上的身份，而是从心底建设开始，<strong>弄清楚自己我要成为一个怎样的人</strong></p></li></ol><p>无论想要成为什么样的人，都要建立自己的身份感，是一种从上到下的全方面塑造，远比以结果为导向从下至上的塑造更具力量。</p><p>如果认为自己强大，那就要有自己发自内心的驱动力，让自己去承受相应的痛苦和困难，不断蜕变。</p><blockquote><p><strong>男性在两性关系中必须接受的事实</strong></p><p>男女而言，通用的是对一个人有没有情欲是爱情能不能发生的一个大前提，而体面感是情欲重要的催化剂。表面上是爱情，其实真正在乎的是体面感。</p><p>体面表面上的表现形式是外貌，财富，而==真正的体面感会在后边展现==</p><blockquote><p>生物进化领域的常识：</p><ol><li>长得太丑的男性，几乎不会获得女性来自情欲所产生的爱情；但当长得丑的男性，如果获得良好的供养能力的时候，比如有钱、有地位，女性会不介意去和其发展长期关系；<strong>但</strong>依旧很难因为情欲而产生爱情。同时这个阶段如果出现一个长得帅的男性来追求这个女生，即便是这个男生没有钱，也没有社会地位，多数女生也不会抗拒和其发展短期关系</li><li>男性在生物学上也并不是多高尚，让人舒服的话，如果说的太多就会让人丧失接收真相的能力</li><li>剔除掉男女两性中很善良的人，也剔除掉很坏的人，剩下的广大7成男女中就基本都是这样子</li><li>进化心理学的观点也就是对于大多数人的进行一个特征的描述</li></ol></blockquote><ol><li><p>财富在两性关系中很重要,但是不要过于高估</p><ol><li>有钱则低调，没钱尽量找一个没钱时爱你的女人，比如爱你的性格，爱你的才学等等、</li></ol></li><li><p>平时保持一个基本的整洁也算是一个必须要做的事情，保持一个基本的精气神（长得好看（生气时候看到就不会生气）vs精神干净，男性好看的包容性）</p></li><li><p>遇到人的“运气和概率”</p><p>烟花0.5s的幸运，会让20多年的努力显得渺小。</p><p>每个人的人生的关键节点，都充满着这0.5s幸运的随机性。如果你碰到的人足够好，不一定是你自己有多么的优秀，而是因为你自己遇到了这0.5妙的随机性，<strong>0.5妙的概率幸运，让自己的所有的人品，包括20多年的努力都会显得相形见绌</strong></p></li></ol></blockquote><h4 id="人唯一能做的">人唯一能做的</h4><p><img src="../assets/image-20240320170004561.png" alt="image-20240320170004561"></p><blockquote><p>而不是遇到对的人和爱自己的人时候，发现不了，识别不了，甚至将别人对自己的爱轻视甚至浪费掉，这样的人注定不会承接住好东西。</p></blockquote><p>提高自己在亲密关系里识别和抓住好东西能力重点在于<strong>祛魅</strong> ==第二节重点讲祛魅==</p><h2 id="总结">总结</h2><p><img src="../assets/image-20240321102137401.png" alt="image-20240321102137401"></p><p><img src="../assets/image-20240321102225633.png" alt="image-20240321102225633"></p><blockquote><p>唯有自知，方有自爱；唯有自爱，方有人爱。</p></blockquote><p><img src="../assets/image-20240321105014986.png" alt="image-20240321105014986"></p><p><strong>一切从认识自我开始</strong></p><blockquote><p>关于幸运的思考:  自己高考 524  差最后一名 ；大学里边成绩搞得挺好，但是能够让自己去兼职+成功大二；考研扩招+读研</p></blockquote><blockquote><p>中文互联网越来越趋向于封闭，获取有效信息的成本越来越大；想要看到真正有价值的思考，或者“隐学”正变得越来越难，很多真正的好回答将再次变成“家学”。</p></blockquote><blockquote><p>不再花费时间去回答很多热点内容，摒弃掉所有没有长期价值的内容，执着于长期价值的回答和做事（未来两三年或者长期十几年后的仍然有用的东西），力求每一件事情都能够解决实际问题或者改变一个观念，消除一切浮躁的动机。</p></blockquote><p>推荐电影《背靠背 脸对脸》，B站就有很多介绍</p><p><img src="../assets/image-20240322204210917.png" alt="image-20240322204210917"></p><p><img src="../assets/image-20240322211957586.png" alt="image-20240322211957586"></p><blockquote><p>纪律 服从 权威<br>1.恰当微笑。不得罪人，给人留下好印象，仪容仪表，把领导同事当客户，端庄微笑，和蔼可亲<br>2.夸别人。忌讳说别人坏话，夸领导表示服从，夸自己，夸同事打下群众基础<br>3.为领导分担。错事错办，但不能不办<br>4.忍。忍批评，受委屈，吃小亏<br>5.谨慎。慎之又慎，不背后说人坏话，不犯原则性错误；事无巨细，谨小慎微，反复检查<br>6.混。不鼓励个人英雄主义，强调集体主义，和光同尘。<br>7.熬。延迟满足，曲折前进，自制力强。熬资历，熬时间（人生低潮时期），熬领导。你年轻，时间站在你这边，终究会熬出头的</p></blockquote><p><a href="https://mp.weixin.qq.com/s/B2eBdYGzXEJkq0woTGWKAQ">冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究 (qq.com)</a></p><h1>冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究</h1><p>#善思会写更快乐219</p><p>节后复工第一天，感觉怎么样？</p><p>有一种说法我很喜欢，职业就像游戏打怪通关，倦怠意味着你在这一关地图都展开了，打到头了，而新一关的钥匙，就在某个你曾经视而不见的熟悉地方。</p><p>而古典老师说过的这4条职场认知，我觉得帮助很大，也分享给你。</p><p>1️⃣结果思维，过程不重要，结果才重要。</p><p>2️⃣贡献思维，怀才不遇是个伪命题。<br>现在的社会分工很细，就像一条流水线拆出来的内容。所以，专业人士有义务告诉上游和下游，你能干什么。换言之，有义务把自己的才华“卖出去”，变成“贡献”。</p><p>3️⃣得失思维，职场不谈对错，谈得失。</p><p>4️⃣灰度思维，所有的职业环境，都是多元视角妥协而成的产物。在职业世界中，不是非黑即白，任何事情都是灰色的、混色的。</p><p>以上这4点，你想清楚了，职业世界的事，无往而不胜。工作即道场，记得要升级哦！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LLm_learn</title>
      <link href="/project/posts/1908cf59.html"/>
      <url>/project/posts/1908cf59.html</url>
      
        <content type="html"><![CDATA[<h1>Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs？</h1><blockquote><p>相比于封装好的相关的事实知识信息，利用外部数据能够扩展大模型的性能，并减少其对于训练数据依赖性；</p><blockquote><p>如何更好的使用外部数据集来合并新信息，或根据之前看到的信息改进llm的功能，是一个重大的挑战</p></blockquote></blockquote><p>比较了两种常见的扩展方法:<strong>无监督微调</strong>和<strong>检索增强生成(RAG)（retrieval-augmented generation (RAG)）</strong>。我们在不同主题的知识密集型任务上评估这两种方法。我们的发现表明，==尽管无监督的微调提供了一些改进，但RAG的性能始终优于它==，无论是在训练中遇到的现有知识还是全新的知识。此外，我们发现，llm很难通过无监督的微调来学习新的事实信息，而在训练期间让他们接触同一事实的众多变体可以缓解这个问题。</p><h2 id="1-Introduction">1. Introduction</h2><ol><li>大型语言模型（LLMs）能够捕获大量的事实信息（Petroni等，2019年；Cohen等，2023年；Hu等，2023年）。由于它们庞大的预训练数据集，LLMs在各个领域展示出了显著的知识水平。然而，这种知识存在两个显著的局限性。<strong>首先，它是静态的，不随时间更新。其次，它不具体，因此可能在特定领域缺乏细致的专业知识。虽然这是两个不同的问题，但它们深深相关，因为它们的解决方案是相同的：增强模型的知识。</strong></li><li>最近，将LLMs适应特定领域并更新其知识的想法变得越来越普遍（Yu等，2022年）。已经提出了各种模型来改进不同领域的事实知识和能力，例如医疗保健（Singhal等，2023a;b；Wu等，2023a）、金融（Wu等，2023b；Yang等，2023）和法律（Huang等，2023年；Nguyen，2023年）。</li><li>在这项工作中，我们**专注于评估模型的知识及其记忆、理解和检索事实数据的能力。**我们的目标是理解知识注入的概念（Wang等，2020年；Chen等，2022年；Liu等，2020年；Lauscher等，2020年）。<strong>在给定文本语料库形式相关某些知识库的情况下，找到如何教一个预训练模型这些知识的最佳方法</strong></li><li>向预训练模型添加知识的一种方法是通过微调。通过微调，我们继续模型的训练过程，并使用特定于任务的数据进行适应。通过让模型接触特定的知识库，我们期望模型的权重相应地适应。这个过程旨在优化模型以用于目标应用，提高其在专业领域的性能和上下文相关性。</li><li>另一种增强模型知识库的方法是通过使用上下文学习（ICL）（Chen等，2021年；Radford等，2019年；Min等，2021年；Lampinen等，2022年）。ICL背后的主要思想是通过修改模型的输入query而不直接改变模型的权重来提高预训练LLMs在新任务上的性能。<strong>ICL的一种形式是检索增强生成（RAG）</strong>（Lewis等，2020年；Neelakantan等，2022年）。<strong>RAG利用信息检索技术使LLMs能够从知识源中获取相关信息并将其合并到生成的文本中。</strong></li><li>本研究旨在通过比较微调和RAG评估LLMs的知识注入能力。为了说明原理，让我们使用一个类比。考虑三名大学生在特定主题的考试上。所有人都可以获取课堂材料，但事先不知道主题。第一名学生只在考试时有教材，第二名学生在考试之前有准备，并学习了，而第三名学生在考试通知发布后失去了准备时间。谁可能表现得更好？</li></ol><h2 id="2-Background">2. Background</h2><ol><li>背景：为了评估知识注入，我们必须首先了解对于LLMs而言知识意味着什么。知识和语言模型：定义知识是一个复杂的哲学任务，远远超出了本研究的范围。然而，我们可以考察在语言模型的背景下事实知识意味着什么。如果一个模型知道一个事实，它可以准确而一致地回答与之相关的问题。此外，它可以可靠地区分与该事实相关的真假陈述。然后，我们可以将这个定义扩展到整个知识库，而不仅仅是单个事实。数学上，设Q = {qn}N为N个多项选择事实问题的集合，其中每个问题有L个可能的答案和一个正确答案。设A = {(a1n，…，aLn)}N为相应的可能答案集合，C = {cn}N为正确的答案。设M是一个语言模型。我们用M(qn) ∈ {a1n, …，aLn}表示模型对第n个问题的预测答案。我们定义M关于Q的知识分数L为标准的准确度分数：LM,Q := #{qn| M(qn) = cn} / N。如果以下条件成立，我们说模型M在Q集合方面拥有任何知识：LM,Q &gt; 1 / L。简单地说，模型可以一致地给出正确答案，超过简单的随机猜测基线。自然地，如果模型M相对于另一个模型的知识得分LM,Q更高，那么我们断言前者相对于后者在Q方面更有知识。</li><li>先前见过的知识：一个重要的区分是模型在预训练期间曾接触过的知识与全新事实之间的区别。考虑到现代LLM训练集的规模，它们涵盖了通过网络文本获得的大量信息。因此，即使在利基领域，知识注入的目标不一定是教会模型全新的事实，而是通过向特定领域施加偏好来“刷新”其记忆。</li><li>知识和推理：我们强调，LLMs的这种知识评估框架是不完美的。重要的是，它并没有解决影响模型响应的其他质量指标。创建一个纯粹以知识为基础的数据集而不涉及某种程度的推理是具有挑战性的。因此，具有强大推理能力的模型可能通过在多项选择考试中进行“有根据的猜测”而在陌生的知识密集型任务上表现出色。因此，对LLMs中的知识的任何评估都应考虑到这一点，并将结果视为推理（Sakaguchi等，2021年）、阅读理解（Dua等，2019年）和一般语言能力（Srivastava等，2022年）的更广泛基准的一部分。然而，这个评估框架仍然强调事实信息高于其他一切。</li><li>事实错误的原因：模型无法准确回答事实问题有很多可能的原因。在Wang等人的研究中，Wang等人介绍了五个主要的模型级错误原因的分类：<ul><li>领域知识不足：语言模型可能在特定领域缺乏全面的专业知识，因为它没有接触过这个领域。例如，一个只在威廉·莎士比亚的文本上训练的模型在被问及马克·吐温的作品时表现会很差。</li><li>过时信息：LLMs的截止日期由它们的训练数据集确定。因此，任何在最后一次训练更新之后发生的事件、发现或变化，如果没有外部来源的访问，将不会在模型的知识范围内。</li><li>遗忘：有时，模型在训练过程中接触到知识，但没有保留下来。对于在训练数据集中很少出现的罕见事实，这一点尤为真实。</li><li>遗忘：语言模型在预训练阶段之后通常会进行额外的训练（微调）。在某些情况下，这可能会导致一种被称为灾难性遗忘的现象，其中模型会失去在微调过程之前具有的一些知识。</li><li>推理失败：在某些情况下，语言模型可能具有与事实相关的相关知识，但未能正确利用它。这在复杂的多步推理任务（Tan等，2023年）或当对同一事实提出不同问题时特别明显，导致不同的结果。</li></ul></li></ol><h2 id="3-Injecting-Knowledge-to-Language-Models">3. Injecting Knowledge to Language Models</h2><p>根据第2节中给出的背景，很明显，对于许多知识密集型任务来说，一般的预先培训是不够的。为了解决这个问题，额外的后处理步骤对于增强预训练模型的知识至关重要。这一步通常被称为知识注入（Wang et al.，2020年; Chen等人，2022; Liu等人，2020; Lauscher等人，2020年）。<strong>在本节中，我们将研究两种广泛使用的知识注入框架：微调（FT）和检索增强生成（RAG）。我们开始制定知识注入问题，旨在解释这两种方法使用一致的术语。</strong></p><p>现在使用相同的术语将这个公式扩展到知识注入的问题。给定一组事实问题，存在一些包含与这些问题相关的信息的文本语料库。**知识注入的核心假设是，如果能够完全访问该语料库，它可以作为辅助知识库，并提高模型在这组问题上的性能。**在数学上，假设M是一个预先训练好的模型，Q是一组事实问题。现在，假设我们有一个相关的辅助知识库BQ。我们的目标是发现一个变换，记为F，当应用时，将增强关于Q：M′：= F（M，BQ）s. t的知识。LM′，Q &gt; LM，Q.(3)</p><h3 id="3-2-微调">3.2. 微调</h3><p>3.2. 微调：微调是调整预训练模型以增强其在特定、通常更狭窄的数据集或任务上性能的过程。在这里，重要的是要区分不同类型的微调。FT技术通常被分类为监督、无监督和<strong>基于强化学习（RL）的方法</strong>。我们接下来简要回顾这些方法及其与知识注入问题的关系。</p><ul><li>监督微调：监督微调（SFT）需要标记的输入-输出对。其中一种最常见的SFT方法是指导微调，它已成为提高模型性能的最强大方法之一。使用指导微调，输入是自然语言任务描述，输出是所需行为的示例。许多当前最先进的LLMs在预训练阶段之后经历了指导微调。</li><li>强化学习：FT的另一种形式依赖于RL或<strong>RL启发的优化策略来更好地调整模型</strong>。其中一些突出的例子是来自人类反馈的强化学习（RLHF）、直接偏好优化（DPO）和近端策略优化（PPO）。这些技术已被证明在与指导微调结合使用时非常有用。然而，与指导微调类似，这些方法<strong>侧重于响应的整体质量和其预期行为，而不一定关注其知识的广度。</strong></li><li>无监督微调：我们讨论的最后一种FT策略是无监督的，这意味着模型没有可用的标签可供学习。一个常见的无监督FT技术通常被称为持续预训练或非结构化FT。在这种方法中，FT过程被视为预训练阶段的直接延续。</li></ul><blockquote><p>启发式离散数据微调【基于强化学习的方法】</p></blockquote><h3 id="3-3-检索增强生成：">3.3. 检索增强生成：</h3><p>3.3. 检索增强生成：检索增强生成（RAG）**是一种通过使用外部知识源扩展LLMs能力的技术，尤其是在知识密集型任务中。**原始的制定涉及每个任务的额外训练，但已经证明，**一个预训练的嵌入模型可以在没有额外训练的情况下获得改进的性能。**该想法是在给定一个辅助知识库和一个输入查询的情况下，使用RAG架构找到与输入查询相似的文档。这些文档随后被添加到输入查询中，从而使模型对查询的主题有进一步的上下文理解。实践中，实现建议的架构相当简单：<strong>给定一个辅助知识库BQ和一个预训练的嵌入模型Me，我们为每个文档b ∈ BQ创建一个密集向量表示（嵌入），并将这些存储在向量存储器中。收到新查询q后，我们使用其嵌入Me(q)来检索与q的前K个最近邻，bq = {bk}K1，根据点积排名。然后，我们更新q为˜q = bq∥q，其中∥表示字符串连接。最后，我们返回M(˜q)作为模型的输出。</strong></p><h2 id="4-Knowledge-Base-Creation">4. Knowledge Base Creation</h2><h3 id="4-1-任务选择和原理">4.1. 任务选择和原理</h3><p>MMLU基准：为了正确评估LLMs在知识密集型任务上的能力，我们从大规模多语言语言理解评估（MMLU）基准中选择了四个不同的任务，涵盖解剖学、天文学、大学生物学、大学化学和史前等主题。所选任务是基于它们对事实知识的重视以及对推理的最小依赖而选择的。作为<strong>一种启发式方法</strong>，我们<strong>选择了问题简短且不涉及上下文的任务</strong>。在实践中，我们选择了四个STEM学科以及一个人文学科，以确保评估不局限于某些领域。值得注意的是，史前涉及涵盖所有非现代历史的问题。这种方法旨在使我们能够测试LLM在理解和操纵信息方面的熟练程度，与其推理过程隔离开来。</p><p>时事任务：为了进一步孤立LLM学习新知识的能力，我们<strong>创建了一个包含关于当前事件的多项选择题的任务</strong>。该任务包括关于发生在各种模型训练数据截止日期之后的事件的多项选择题。具体来说，我们侧重于美国的“当前事件”，时间跨度为2023年8月至11月，这些事件包含在相关的维基百科索引中。这种方法使我们几乎可以保证模型没有接触过这些事实，因此能够直接测试知识注入的能力。</p><p>总结：在本节中，作者选择了四个来自MMLU基准的不同任务，涵盖了解剖学、天文学、大学生物学、大学化学和史前等主题，以评估LLMs在知识密集型任务上的能力。此外，为了测试LLM学习新知识的能力，还创建了一个关于当前事件的任务。这些任务的选择旨在确保测试能够涵盖各种领域，并在不同背景下评估LLM的能力。</p><h3 id="4-2-数据收集与预处理">4.2. 数据收集与预处理</h3><p>4.2. 数据收集与预处理 为了有效评估LLMs在这些知识密集型任务上的表现，通过从维基百科中针对每个主题抓取相关文章，收集了一个全面的辅助数据集。选择维基百科作为主要知识来源的原因是它对相关主题的广泛覆盖以及作为群众验证知识库的可靠性。通过识别每个主题的相关中心页面，通过官方维基百科API检索了所有与任务相关的文章。 随后，采用了严格的清理过程将数据从原始子部分转换为干净的块。此步骤使用了“wikiextractor”工具（Attardi，2015）。将数据分成小的、干净的块（例如，删除HTML、URL等）旨在增强LLMs在各种知识领域的理解，并帮助LLMs进行微调过程。</p><h3 id="4-3-当前事件任务创建">4.3. 当前事件任务创建</h3><p>4.3. 当前事件任务创建 在从维基百科收集到相关块之后，我们借助GPT-4（OpenAI，2023）创建了一个新的多项选择数据集。首先，我们移除了任何小块。对于语料库中的每个剩余块，要求GPT-4创建四个高度具体、高质量的只有一个正确答案的多项选择题。所谓具体，是指问题可以在不了解问题所指的上下文的情况下回答，并且具有最小的歧义性。接下来，要求GPT-4选择其中最具体的两个。然后进行手动评估和验证步骤。总共，这产生了910个新问题。</p><h3 id="4-4-释义生成">4.4. 释义生成</h3><p>4.4. 释义生成 在创建数据集之后，我们利用GPT-4生成数据集的增强版本。我们要求GPT-4提供输入数据的释义版本，这些版本完全保留信息但用不同措辞。为了确保多样性，每次释义迭代都使用不同的种子。我们随机选择了每个任务的240个块，并为每个块创建两个释义。这些被保留用于用于超参数调整的验证集。对于描述在第6节中的微调过程中使用的当前事件数据集，我们为每个块创建了十个释义。</p><h2 id="5-实验和结果">5.实验和结果</h2><p>实验框架 我们使用了流行的LM-Evaluation-Harness（Gao等人，2021）存储库来评估LLMs在所选知识密集型任务上的性能。LM-Evaluation-Harness是一个强大的基准测试工具，目前是模型评估的行业标准，并且是HuggingFace排行榜的基础。利用这个平台确保了标准化的评估框架，并允许跨模型、方法和数据集进行一致的比较。更重要的是，通过使用行业标准进行评估，我们可以避免由提示工程和格式问题引起的任何差异，并复制每个模型的报告基准结果。</p><p>模型选择 我们选择了三个模型进行推理评估：Llama2-7B（Touvron等人，2023）、Mistral-7B（Jiang等人，2023）和Orca2-7B（Mitra等人，2023）。选择这些模型旨在代表最流行的开源基础模型和跨各种基线能力的经过指令调整的模型。此外，我们选择了bge-large-en（Xiao等人，2023）作为RAG组件的嵌入模型，并使用FAISS（Johnson等人，2019）作为其向量存储。根据HuggingFace MTEB排行榜，这个嵌入模型目前是开源嵌入模型的SOTA。</p><p>配置变化 我们的评估包括多个配置，并对它们进行了网格搜索，以便进行更全面的基准测试。首先，我们比较了基线和微调模型以及它们与RAG组件的性能。其次，我们探索了在RAG中添加到上下文中的文本块的最佳数量。具体来说，采用了不同的K值 ∈ {0, . . . , 5}来分析对模型性能的影响。最后，我们探讨了5-shot性能与0-shot性能之间的比较。</p><p>训练设置 我们使用了第3.2节中描述的无监督训练过程对所有模型进行训练。对于每个数据集，我们将辅助知识库分成大小为256的相等块，通过连接或分割原始块来实现。我们还添加了两个特殊标记，<BOS>和<EOS>，来标记原始块的开头和结尾，以保留文档的结构。模型的训练使用了介于1 × 10^−6和5 × 10^−5之间的学习率，这是通过超参数搜索找到的。所有模型都在4个NVIDIA A-100 GPU上进行了最多5个时期的训练，并且批量大小为64。</p><p>评估方法 所有评估都是通过将每个多项选择选项附加到问题后，然后通过模型传递连接以获得每个选项的对数概率分数来完成的。最高分被解释为模型的选择，并用于准确性计算。更正式地说，在方程（1）中，我们说如果M(qn) = cn： cn = arg max l {M(qn∥a1 n), . . . ,M(qn∥aL n)}, 其中M(qn∥al n) = log PM(qn∥al n)。</p><p>MMLU结果 对于每个任务和模型，我们比较了四种方法：仅使用基本模型、RAG、FT以及通过使用微调模型作为生成器来合并FT和RAG。此外，我们使用了0-shot和5-shot场景来测试MMLU任务。完整的结果显示在表1中。相对准确性增益的聚合，即 (LM′,Q − LM,Q)/LM,Q， 其中M是基本模型，M′是注入知识的模型，显示在图2中。在所有情况下，RAG的表现明显优于基本模型。此外，将RAG与基本模型作为生成器一起使用一直比仅进行微调更好。在某些情况下，将微调模型而不是基本模型用作RAG管道中的生成器进一步提高了结果。然而，这并不一致，因此显示了微调的固有不稳定性。此外，我们发现5-shot方法在大多数情况下都略微提高了结果，在所有不同方法中观察到了类似的趋势。</p><p>当前事件结果 当前事件任务的评估显示在表2中。由于问题和辅助数据集之间的一对一对应（请参阅第4.3节），RAG证明了其特别有效性。微调与RAG相比不具竞争力。然而，微调与多个释义仍然比基线提供了显著的改进。我们注意到，与仅使用RAG相比，将RAG与微调相结合的性能较差。</p><p>值得注意的是，虽然问题是基于模型在训练过程中未接触到的信息，但基本模型的结果超过了1/L = 0.25。这部分可以通过模型在回答不独立于过去信息的问题时使用推理和/或现有知识来解释。一些这方面的例子可以在附录C中找到。</p><p>微调与RAG： **在MMLU和当前事件任务的结果中，RAG相对于微调的显着优势是明显的。**虽然微调在大多数情况下改善了结果，但与RAG方法相比并不具有竞争力。可能有几个因素会导致这种行为。首先，**RAG不仅向模型添加知识，还包含与问题相关的上下文，这是微调所缺乏的功能。**此外，**微调可能会影响模型的其他功能，因为微调程度可能导致灾难性遗忘。**最后，未监督微调模型可能会通过监督或基于RL的微调进一步对齐，正如Orca2相对于基本的Llama2的性能大大提高所示。</p><h2 id="6-重复的重要性">6.重复的重要性</h2><p>与其他任务不同，在其他任务中，模型已经在预训练期间暴露于与主题相关的方面，当前事件包括新信息。在这种情况下，标准的常规微调不仅没有提高Llama2的性能，而且还显着降低了它。为了改善微调结果，我们探索了使用释义的数据增强。</p><h2 id="7-结论和未来的工作">7.结论和未来的工作</h2><p>大型语言模型拥有各种主题的大量知识。在这项工作中，我们测试了他们适应新知识的能力：包括专业知识和完全看不见的知识。这是第一个研究比较两个突出的方法在这一领域，即微调和检索增强生成。虽然微调对许多用例都很有用，但我们发现RAG是知识注入的更可靠选择。这项工作的某些方面仍需要进一步研究。例如，我们专注于无监督训练作为我们的主要微调方法，而不是预防性调整或基于RL的方法。研究各种技术与各种辅助知识库的组合可能会产生更好的结果。这种方法，结合我们在第6节中的假设，可以进一步增强我们对通过FT进行知识注入的理解。虽然我们相信这项工作进一步提高了我们对LLM知识的理解，但在这一领域还有很多工作要做。具体而言，需要更多的研究，特别是从理论的角度来看，关于在LLM知识表示的问题。最后，需要进一步努力衡量法学硕士的知识。虽然我们采用了公式（2）中描述的经验方法，但探索其他关于知识的定义和观点并扩展这项工作也很重要。</p><h2 id="8-Limitations">8. Limitations</h2><p>在所有机器学习应用中，超参数的选择会显著影响结果。因此，我们强烈建议针对特定情况优化所有相关的超参数。我们通过在三个不同的模型上进行实验来支持我们的主张。然而，对其他LLM的推广应该彻底测试。例如，GPT-4对于某些MMLU任务实现了近乎完美的准确性（Nori等人，2023），因此不适用进一步的改进。最后，当我们为知识库选择各种主题时，我们所有的来源都来自维基百科。其他数据集可能会产生不同的结果，必须仔细评估。</p><blockquote><p>Knowledge Base Creation   创建启发式方法</p></blockquote><p>优化prompt</p><blockquote><p>基于公开的数据库 大家去做测试</p><p>能不能做两个 一个获取更好的数据，另一个去做</p></blockquote><blockquote><p>带着地理信息 图片信息 怎样放到prompt 中</p></blockquote><h1>RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning</h1><blockquote><p>强化学习+模型微调</p></blockquote><blockquote><p>自动为每个任务找到最佳prompt</p></blockquote><p>==<strong>现状</strong>==</p><p>现有的大多数工作都依赖于调优<strong>软提示</strong>(例如，嵌入式提示)，这些<strong>提示缺乏可解释性</strong>、跨模型之间的的可重用性以及当梯度不可访问时的适用性。另一方面，离散提示很难优化，通常是通过“枚举(例如，解释)-选择”启发式创建的，这种<strong>启发式不会系统地探索提示空间</strong>。</p><p>==提出了一种<strong>基于强化学习的离散提示优化方法RLPROMPT</strong>。RLPROMPT构建了一个参数高效的策略网络，在奖励训练后生成优化的离散提示。==  为了利用来自大LM环境的复杂和随机奖励信号，我们结合了有效的稳定奖励，大大提高了训练效率。RLPROMPT可以灵活地应用于不同类型的大模型，如mask(如BERT)和left-to-right模型(如GPTs)，用于分类和生成任务。在少镜头分类和无监督文本样式转换上的实验表明，在现有的各种微调或提示方法中，该方法具有优越的性能。有趣的是**，优化后的提示往往是不合语法的乱码;令人惊讶的是，这些胡言乱语提示在不同的LM之间是可转移的，以保持显著的性能，这表明LM提示可能不遵循人类的语言模式。**</p><h3 id="introduction">introduction</h3><p>Prompting已经成为使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，包括从左到右的模型（例如GPTs）（Radford等人，2019年；Brown等人，2020年）和掩码LMs（例如BERT）（Devlin等人，2019年）、RoBERTa（Liu等人，2019年）等。与昂贵地为每个下游任务更新大量LM参数的传统微调相比，<strong>Prompting在输入中连接附加文本，指导LM生成所需的输出，从而降低了成本</strong>。Prompting的一个关键问题是如何找到最佳提示来提高LM在各种任务上的性能，通常只有很少的训练样本。</p><ul><li>Prompting是使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，与传统的微调相比，它在输入中连接额外的文本以指导LM生成所需的输出，从而降低了成本。</li><li>关于Prompting的一个关键问题是如何找到最佳提示以提高LM在各种任务上的性能，通常只有很少的训练样本。</li></ul><p>其中，最流行的方案之一是调整软提示（即连续嵌入向量），因为它们易于梯度下降。然而，由于其本质上来说，由于人类难以理解且无法与其他LMs一起使用，生成的提示对人类来说很难理解，并且不兼容。此外，LM内部梯度通常难以计算，或者仅对仅具有推理API（例如GPT-3）的LMs不可用。因此，<strong>使用由词汇表中的具体标记组成的离散提示通常是可取的</strong>。然而，它们的离散性质使优化变得非常困难。以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择。AutoPrompt使用梯度信息编辑提示标记，但受到训练不稳定性和与基于梯度的软提示相同的适用性问题的影响，在实践中的效果有限。</p><ul><li>一个流行的方案是调整软提示，因为它们易于梯度下降，但生成的提示难以理解，而且不兼容其他LMs。</li><li>离散提示通常由具体标记组成，但它们的离散性质使得优化变得非常困难。</li><li>以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择，但这些方法在实践中的效果有限。</li></ul><p>本文提出了一种新的基于强化学习（RL）的离散提示优化方法RLPROMPT，该方法汇集了在不同任务和LMs上高效使用的广泛理想属性。与直接编辑离散标记不同，<strong>RLPROMPT训练一个生成所需提示的策略网络</strong>。离散提示优化实际上就是学习少量策略参数，我们将其设置为<strong>插入到冻结的紧凑模型中的MLP层</strong>，例如distilGPT-2。该公式还使我们能够<strong>使用现成的RL算法</strong>，这些算法<strong>可以使用任意奖励函数来学习策略，无论是使用可用数据（例如在少样本分类中）定义的还是在没有监督数据可用时（例如在可控文本生成中）使用的其他弱信号。</strong></p><ul><li>RLPROMPT是一种新的基于强化学习（RL）的离散提示优化方法，它训练一个生成所需提示的策略网络，并在不同任务和LMs上高效使用。</li><li>与直接编辑离散标记不同，RLPROMPT训练一个生成所需提示的策略网络，这使得离散提示的优化更加高效。</li></ul><p>然而，RL用于提示优化对学习效率提出了新的挑战：大型黑箱LM呈现出高度复杂的环境，给定提示（即操作）后，经过一系列复杂的转换（例如读取输入和推断输出）才能计算奖励。这使得奖励信号非常不稳定，很难学习。为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</p><ul><li>RL用于提示优化对学习效率提出了新的挑战，因为大型黑箱LM呈现出高度复杂的环境，奖励信号非常不稳定且难以学习。</li><li>为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</li></ul><p>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法（例如表1中描述的方法）上都有所改进，并且对不同的建模选择（例如分类中的verbalizers）都具有鲁棒性。所得到的离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。尤其是，尽管优化后的提示能够引起强大的任务性能，但倾向于是没有明确人类可理解含义的无意义文本，与最近的研究相呼应，即利用提示的LMs并不一定遵循人类语言模式。也许令人惊讶的是，学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</p><ul><li>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法上都有所改进，并且对不同的建模选择都具有鲁棒性。</li><li>离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。</li><li>学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</li></ul><p><strong>我们将给出离散prompt optimization的RL公式(§2.1-2.2)。然后我们讨论我们的策略网络的设计(§2.3)。最后，我们描述了我们的奖励工程技术来提高RL培训(§2.4)。</strong></p><blockquote><p>第一篇文章的knowledge base 搭建</p></blockquote><blockquote><p>学习了初步的在大模型上以及大模型之间的应用的微调策略以及增强式生成策略，并对比了相应方法的不同</p></blockquote><blockquote><p>第二篇文章 是对应与微调策略里边的强化学习提示网路模型，生成对应的网络提示</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>参与开源社区</title>
      <link href="/project/posts/48379df9.html"/>
      <url>/project/posts/48379df9.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>尝试参与开源活动，整理一下自己的github主页</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AI_Agent调研</title>
      <link href="/project/posts/7641cfbd.html"/>
      <url>/project/posts/7641cfbd.html</url>
      
        <content type="html"><![CDATA[<h1>24.1.9日WorkShop活动准备</h1><blockquote><p>需要看的3D产品 链接是官网</p></blockquote><p><a href="https://www.tripo3d.ai/">Tripo AI (tripo3d.ai)</a></p><h2 id="活动前置须知">活动前置须知</h2><h3 id="AGI领域">AGI领域</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/607125649">知识科普：什么是AGI？ - 知乎 (zhihu.com)</a></p></blockquote><p>AGI最近经常被提到，主要是因为ChatGPT的开发公司OpenAI将其写在了自己的企业使命中了，只要在介绍OpenAI的场景都会介绍到企业使命，而且在此时此刻，AGI作为OpenAI的企业使命中重要的关键词，也显得那样伟大。</p><p>AGI是<strong>Artificial General Intelligence</strong>的缩写，中文应该是“通用人工智能”，是指一种能够像人类一样思考、学习和执行多种任务的人工智能系统。我觉得这个词就是为了有别于“特定领域人工智能”而创造出来的。</p><blockquote><p>个人感觉有点像使用了AI赋予能力之后的智能机器人，当然这种智能机器人是主要应用在企业领域</p></blockquote><p>AGI可以视为一种更高级别的人工智能，是当前人工智能技术发展的一个重要方向和目标。但由于其在技术和理论方面的挑战，它仍然是一个较为遥远的目标。Chatgpt也还不能被称为AGI，至少它还不会画画。</p><blockquote><p>几个AGI的实例：</p><ol><li>王一快的写作猫【帮助生成写作内容和文档】</li><li>AIGC的相关产品【AI 生成内容】</li><li>最近刚刚开始用的微信智能日程提醒</li></ol></blockquote><h3 id="AGI的发展方向">AGI的发展方向</h3><p><a href="https://m.huxiu.com/article/968593.html">AGI的大时代，我们可以做些什么？-虎嗅网 (huxiu.com)</a></p><ol><li><p><strong>通用的AGI平台</strong></p><ol><li>【商业公司】Google的Bard、百度的文心一言、Facebook的LLaMa、阿里、腾讯和字节的大语言模型</li><li>【个人或者社区可以自己搭建自己的AI平台】清华大学开源的GLM-130B 、Facebook开源出来的Alpaca<ol><li>可以从GitHub上pull代码，在自己的机器上快速地搭建起一个模型，提供服务</li><li><strong>大语言模型的平台已经白菜价</strong></li></ol></li><li>先从商业或者科研领域研发并推广，再到向企业和军方中广泛使用，最后在资金和风向的推导下为大众所熟知，再到个人领域的应用和不断完善【这其中估计要有3-5个月的延迟，类比于ChatGPT【自己12月份就已经开始用，但是到明年的3月底才推广】】</li><li><strong>现在的工程师投身通用AGI平台的建设，亲自去参与搭建平台，设计架构，做一位AI平台架构师和算法工程师，将毫无疑问成为最抢手的人才</strong></li></ol></li><li><p><strong>定制化专业垂直领域的AGI平台</strong></p><ol><li>金融行业、医疗行业、教育行业、汽车行业、制造行业等各种行业对应的，与专业知识结合的平台</li><li>赶紧在Github上pull GLM-130B，或者骆驼的代码，先学习起来</li><li>除了可以直接参与AGI平台的构建。其实也可以做平台之上的应用。<ol><li>**现有的软件通过AGI的能力进行提升，为用户或者客户提供增值的服务。**最典型的就是微软的Office 365 集成GPT的能力以后，可以极大提高办公人员的工作效率；Adobe的Firefly集成AGI的能力以后，可以帮助设计人员快速设计更多的创意产品。</li><li><strong>基于GPT的插件体系</strong>**，在平台上根据自己的需要进行插件开发，完成自己细分领域的创新型产品。**</li></ol></li></ol></li><li><p><strong>软件服务商升级AI的能力</strong></p><ol><li><strong>在传统软件行业的各个垂直领域，实际上都存在着用AI去提升软件生产力的机会</strong><ol><li>英伟达的CEO黄仁勋在GTC大会上，就介绍了在芯片制造领域，利用AI的能力提升芯片制程和生产效率的案例。所以，在未来十几年，很可能AI能够极大地加速人类的科技和生产能力</li></ol></li><li>微软的<strong>Office 365</strong>。最近微软也发布了接入GPT-4模型能力的Office 365 Copilot。有了AI能力的加持，我们通过简单的自然语言对话，告诉Office我们的文档需要什么，Office Copilot就会自动帮助我们生成结果，例如：微软的office365 自动生成需要的文档和对应文档内容；【本质是利用的ai自动生成office所识别的VBA代码】</li><li>Adobe也演示了Firefly，利用AI的能力进行自动绘画功能，增强了Adobe软件的设计效率</li></ol></li><li><p><strong>基于通用大平台，开发细分领域的创新型应用</strong></p><ol><li>基于小数量的用户创新或者开发面向有特殊需求的专业化产品</li><li><strong>基于类似GPT-4这样的通用AGI平台，一定可以在各个细分行业涌现无数的创新应用出来</strong><ol><li>过去的各种插件开发 例如谷歌或者edge 的插件开发</li><li>苹果 或者 安卓的应用市场中各种应用开发</li><li>微信平台中的某些特殊功能的产品开发</li></ol></li></ol></li></ol><h2 id="活动内容">活动内容</h2><blockquote><p>介绍产品</p><p>了解非3D从业者怎样上手玩转3D</p><p>Tripo产品和3D内容生成的未来发展趋势</p></blockquote><p><img src="../assets/image-20240109190152774.png" alt="image-20240109190152774"></p><h2 id="活动准则">活动准则</h2><blockquote><p>准备自己的问题，询问AGI领域内的超集个体怎样应对</p><p>学习或者了解现在大家都在做的AGI产品或者用的技术</p></blockquote><p><img src="../assets/image-20240109190322701.png" alt="image-20240109190322701"></p><h1>开始主题</h1><p>Founder Park创业者社区</p><ol><li>两个嘉宾分享</li><li>大家有问题 评论区提问</li></ol><p>索尼  gta5  到gta6 的跨越</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图卷积算法应用落地——骨架数据行为识别系统</title>
      <link href="/project/posts/99a90e7a.html"/>
      <url>/project/posts/99a90e7a.html</url>
      
        <content type="html"><![CDATA[<h1>图卷积算法应用——视频行为识别系统落地方案</h1><h2 id="1-识别系统介绍">1.识别系统介绍</h2><p>随着AI、web3、短视频的不断发展，现代生活对于图片的注意力开始转向视频数据，AI智能生成视频和各种主题短视频爆火等都是针对于视频数据，不断推动这视频内容的感知和创作。人作为视频中的最重要的一个主体，对视频内容的创作和生成有着至关重要的作用，所以对于视频中人体的动作进行识别变得越来越重要。近几年视频中人体动作识别领域已经成为热门领域，同时也有越来越多的产品开始应用该技术。</p><p>针对于视频中人体行为，最直观是将RGB视频直接作为视频数据或者间接裁剪为图片数据进行处理。利用CV中的算法将图片数据扩充上一个维度——时间帧维度，对于视频中人体的行为数据进行处理分析并最终输出一个类别概率数据。另外受限与视频数据处理量较大、视频数据噪音较多等因素，利用从视频中提取出的人体骨架信息能够更好的去除背景影响和其他噪音信息，重点关注与骨架动作和内容，提高识别准确率；因此针对于骨架数据的动作识别领域也不断兴起新技术，针对于从视频中提取出的骨架数据，不断提高对于骨架数据的模型性能和完善骨架数据动作识别技术生态有了至关重大的意义。</p><p>本篇文章针对于最近时间做的一项工作，针对于行为识别方面，尤其是视频和骨架数据方面，从视频采集到骨架数据库划定，再到算法设定和模型训练，完成整个动作视频实时处理和动作分类展示，完成了一项落地方案的实施，将内容进行输出和分享，希望跟大家讨论交流，不断改进方案的细节，精益求精。大家如果对于整套方案中所涉及到的某个技术或者实现过程感兴趣，或者能够提供相应的改进，欢迎在后台留言，与作者一起交流</p><h2 id="2-系统需求设定">2.系统需求设定</h2><h3 id="1-骨架数据标准库">1.骨架数据标准库</h3><p>对于骨架数据先进行动作识别，然后针对与完成的动作依据对应的动作标准进行评分，是非常常见的操作。根据日常的动作进行动作标准库选择，选择执行动作比较标准的数据，完成数据的采集、清洗、入库后，完成对应的数据标准库设定。</p><h3 id="2-数据存储格式指定">2.数据存储格式指定</h3><p>对应于所处理的骨架数据，在完成数据采集的时候，应该设定好按照什么样的格式进行存储，同时将数据标准进行设定，方便后边其他操作应用数据的时候，使用相应的标准进行数据解析，方便下游任务的实施。</p><h3 id="学习与评价算法">学习与评价算法</h3><p>针对于相应的视频或骨架数据，需要选择相应的学习算法和评价算法：</p><ol><li>学习算法完成对于数据集中视频的学习，并在推理阶段完成动作类别划定，完成动作分类</li><li>评价算法需要将测试的数据与标准库中的数据，进行动作比对，完成对于测试数据的分数划定</li></ol><h3 id="4-标准数据生产与模型训练流程">4.标准数据生产与模型训练流程</h3><p>下边图片是设定的整个项目的流程和操作过程，从数据源的采集到标准库的设定，在宝库模型训练系统的完善。</p><p><img src="../assets/image-20240109153021375.png" alt="image-20240109153021375"></p><h2 id="3-数据和算法调研">3.数据和算法调研</h2><h3 id="1-视频or骨架">1.视频or骨架</h3><p>针对于视频数据和骨架数据进行深入的分析，将两种数据进行详细的优劣对比，发现有如下对比：</p><p><img src="../assets/image-20240109160548890.png" alt="image-20240109160548890"></p><p>视频相对更加的方便采集，可以直接从摄像头拍摄后完成数据获取；但是因为视频数据中包含内容比较丰富，同时数据量比较大，所以导致数据中会包含一些对于动作识别分析过程中不可用的噪音数据，对于后续的识别过程会带来一些麻烦。</p><p>相对于视频数据，骨架数据的优势便是主要针对的就是动作进行获取的骨架数据，能够完整的表示动作表示的同时不受到噪音数据的影响，同时能够很好的减少像光照、视频背景等信息的干扰。但对于骨架数据的获取可能会需要一些相应的设备，并需要提前做好数据转换。</p><p><img src="../assets/image-20240109160022176.png" alt="image-20240109160022176"></p><blockquote><p>视频数据骨架数据转换图</p></blockquote><h3 id="2-算法分析">2.算法分析</h3><p>目前在基于视频动作识别和基于骨架数据动作识别方面主流方法准确率都能达到80%以上，主要基于深度学习方法包括但不限于（1）CNN，（2）GNN,（3）Transformer等主流方法</p><p><img src="../assets/image-20240109161059481.png" alt="image-20240109161059481"></p><p>​视频动作识别算法发展脉络图</p><p><img src="../assets/image-20240109161202559.png" alt="image-20240109161202559"></p><p>​骨架动作识别算法发展脉络图</p><h3 id="3-数据集选定">3.数据集选定</h3><p>针对于所要完成的整套方案，需要从视频中完成数据采集后，在处理成骨架数据。相较而言，骨骼数据相比于RGB视频数据不易受背景影响，避免遮挡、光照等噪音影响搜集到数据集包括太极拳数据、Ntu数据集及相关研究。这里我们直接选择了对应的NTU60骨架数据集来作为我们的预定义数据集，并在后续的实现方案中完成所需要的全部操作。</p><p>在这里我们也对于其他的一些数据集和开源算法以及可以直接进行实操的项目进行了实操和测试，下图是我们进行过测试的项目罗列，如果对图中兴趣感兴趣，欢迎在后台留言和交流</p><p><img src="../assets/image-20240109161839748.png" alt="image-20240109161839748"></p><p>​方案调研技术罗列</p><h2 id="4-NTU60标准库构建">4.NTU60标准库构建</h2><p>在这里，我们会对与我们使用到的标准库进行介绍，并完成我们方案所需要的标准库的数据格式解析：</p><p><img src="../assets/image-20240109162158671.png" alt="image-20240109162158671"></p><h3 id="1-骨架数据格式设定">1. 骨架数据格式设定</h3><p>NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p>文件命名方式如下：</p><p><img src="../assets/image-20240109162328756.png" alt="image-20240109162328756"></p><p>命名格式和相应实例</p><p><img src="../assets/image-20240109162353641.png" alt="image-20240109162353641"></p><p>这里为相机和不同的人物设定id 是为了下边在执行相同动作时候，能够划定不同的变量，保证数据集能够更加全面的展示相应的数据，并能够帮助模型学习同一个动作的不同表现格式。在模型完成动作识别后，会对于动作类别进行划定，输出的动作分类结果就是A后边相应的数字借以表示动作所属的类别</p><h3 id="2-骨架数据获取标准">2.骨架数据获取标准</h3><p>这里介绍一下根据不同的相机角度拍摄结果来完成动作跨视角的获取和解析，以及根据不同人物id完成相应动作来区别跨主题分类</p><p><img src="../assets/image-20240109162916550.png" alt="image-20240109162916550"></p><p>​<strong>原始数据图</strong></p><h4 id="1-跨视角">1.跨视角</h4><p>将所有的3个摄像头获取的数据，根据摄像头的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同视角的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162951421.png" alt="image-20240109162951421"></p><p>​处理后的可解析跨视角数据</p><h4 id="2-跨主题">2.跨主题</h4><p>将所有的不同id的人物执行的相同动作获取的数据，根据人物的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同主题的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162957586.png" alt="image-20240109162957586"></p><p>​<strong>处理后的可解析跨视角数据</strong></p><h3 id="3-动作类别介绍">3.动作类别介绍</h3><p>NTU-RGB+D60数据集包含60个种类的动作。数据集由Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括3D骨架信息、RGB视频等。</p><p><img src="../assets/image-20240109163500174.png" alt="image-20240109163500174"></p><h3 id="4-骨架数据内容标准">4.骨架数据内容标准</h3><p>这里解析一个.skeleton数据内容:一个动作包含104帧 ,25是关节个数,紧接着的25行12列表示,25个关节点的12类别信息,后边还有类似的其他103组表征103帧的关节特征数据</p><p><img src="../assets/image-20240109163607368.png" alt="image-20240109163607368"></p><p>​                                                                                                 <strong>.skeleton数据格式解析</strong></p><p>其中的‘x’, ‘y’, ‘z’, 构建成骨架关节点特征信息，用来表示动作发生过程中的空间坐标信息，构建X∈[25,3] 维特征向量标识一个动作的特征信息，点线数据结构表征空间信息。</p><p><img src="../assets/image-20240109163944894.png" alt="image-20240109163944894"></p><p>​                                                                                           <strong>X Y Z三维数据格式解析</strong></p><h3 id="5-骨架数据构图">5.骨架数据构图</h3><p>每个骨架都有25个骨骼点,使用自连接节点构建成数值为0/1 的[25，25]维邻接矩阵,数值为1表示节点之间有邻接关系,数值为0 表示节点间没有邻接关系</p><p>25个节点自身与自身之间具有自连接关系,利用25个节点选择不同的构图方式构建成为人体骨架模型</p><p><img src="../assets/image-20240109163803430.png" alt="image-20240109163803430"></p><h3 id="6-骨架数据可视化">6.骨架数据可视化</h3><p>下边是一些骨架执行动作的2d可视化</p><p><img src="../assets/image-20240109164258750.png" alt="image-20240109164258750"></p><p>​                                                                                                              <strong>抢劫</strong></p><p><img src="../assets/image-20240109164438223.png" alt="image-20240109164438223"></p><p>​                                                                                                              <strong>坐下</strong></p><p><img src="../assets/image-20240109164332785.png" alt="image-20240109164332785"></p><p>​                                                                                                             <strong>扔东西</strong></p><p>​</p><p><img src="../assets/image-20240109164455067.png" alt="image-20240109164455067"></p><p>​                                                                                                             <strong>单手捡东西</strong></p><h2 id="5-落地方案展示">5.落地方案展示</h2><h3 id="1-标准数据生产与模型训练流程">1.标准数据生产与模型训练流程</h3><p>下边图片展示的是我们本套方案所计划应用的全部技术，以及已经完成的相应展示，在后边视频中已经展示了相应的实现过程</p><p><img src="../assets/image-20240109165738475.png" alt="image-20240109165738475"></p><h3 id="2-动作学习方案展示">2.动作学习方案展示</h3><blockquote><p>视频插入</p></blockquote><p>视频中展示的是我们利用我们预训练后的模型学习完成整个NTU60动作后，对于骨架数据——扔东西这个动作先进行可视化，然后将测试数据输入给模型后，模型对于数据进行识别后将识别到的动作进行可视化并将测试结果写回到本地文件中并将模型的准确率进行计算并同步到数据文件中</p><h3 id="3-评价算法">3.评价算法</h3><h4 id="1评价算法的模型图和实现过程">1评价算法的模型图和实现过程</h4><ol><li><p>将两个样本分别送入到两个相同的深度神经网络中，两个特征提取网络的结构和参数相同;</p></li><li><p>使用损失函数计算两个输出结果的距离差，误差越小相似度越高;</p></li><li><p>损失函数根据输入数据和任务的不同可以使用对比损失、余弦损失、交叉熵损失等;</p></li></ol><p><img src="../assets/image-20240109171444116.png" alt="image-20240109171444116"></p><p><strong>评价算法的模型图</strong></p><h4 id="2-评价算法改进">2.评价算法改进</h4><p>我们对于评价算法重新划定评价等级，设定了下边的改进计划</p><p><img src="../assets/image-20240109171817932.png" alt="image-20240109171817932"></p><p>​<strong>改进方案设定</strong></p><h4 id="3-方案前端界面">3.方案前端界面</h4><p>方案设定的前端展示如下，可以对于某个视频中动作进行骨架展示并将对应的数据骨架评分进行展示</p><p><img src="../assets/image-20240109171929541.png" alt="image-20240109171929541"></p><p>​<strong>前端展示图</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解7骨架序列的目标表示自监督学习框架</title>
      <link href="/project/posts/f1740f63.html"/>
      <url>/project/posts/f1740f63.html</url>
      
        <content type="html"><![CDATA[<h1>Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence骨架序列的目标表示自监督学习框架</h1><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下Abstract段落</p><h2 id="Abstract">Abstract</h2><p>Abstract:<br>Self-supervised pre-training paradigms have been exten-<br>sively explored in the field of skeleton-based action recog-<br>nition. In particular, methods based on masked predic-<br>tion have pushed the performance of pre-training to a new<br>height. However, these methods take low-level features,<br>such as raw joint coordinates or temporal motion, as pre-<br>diction targets for the masked regions, which is subopti-<br>mal. In this paper, we show that using high-level contex-<br>tualized features as prediction targets can achieve supe-<br>rior performance. Specifically, we propose Skeleton2vec,<br>a simple and efficient self-supervised 3D action represen-<br>tation learning framework, which utilizes a transformer-<br>based teacher encoder taking unmasked training samples<br>as input to create latent contextualized representations as<br>prediction targets. Benefiting from the self-attention mech-<br>anism, the latent representations generated by the teacher<br>encoder can incorporate the global context of the entire<br>training samples, leading to a richer training task. Ad-<br>ditionally, considering the high temporal correlations in<br>skeleton sequences, we propose a motion-aware tube mask-<br>ing strategy which divides the skeleton sequence into sev-<br>eral tubes and performs persistent masking within each<br>tube based on motion priors, thus forcing the model to<br>build long-range spatio-temporal connections and focus on<br>action-semantic richer regions. Extensive experiments on<br>NTU-60, NTU-120, and PKU-MMD datasets demonstrate<br>that our proposed Skeleton2vec outperforms previous meth-<br>ods and achieves state-of-the-art results. The source code<br>ofSkeleton2vec is available at <a href="https://github.com/Ruizhuo-Xu/Skeleton2vec">https://github.com/Ruizhuo-Xu/Skeleton2vec</a>.</p><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下1. Introduction段落</p><h2 id="1-Introduction">1. Introduction</h2><p>Introduction:<br>Human action recognition has significant applications in the<br>real world, such as security, human-robot interaction, and<br>virtual reality. The development of depth sensors and ad-<br>vancements in pose estimation algorithms [4, 12, 41] have</p><p>propelled skeleton-based action recognition into a popular<br>research topic, owing to its computational efficiency, back-<br>ground robustness, and privacy preservation. A series of<br>fully-supervised skeleton-based human action recognition<br>methods have been developed using CNNs [10, 19], RNNs<br>[24, 46], and GCNs [5, 43]. Despite their promising per-<br>formance, these methods rely on large amounts of manu-<br>ally annotated data, which is expensive, labor-intensive, and<br>time-consuming to obtain. This circumstance motivates us<br>to explore self-supervised representation learning for 3D ac-<br>tions.<br>Earlier works [21, 29, 33, 47] have employed various<br>pretext tasks, such as motion prediction, jigsaw puzzle<br>recognition, and masked reconstruction, to learn 3D ac-<br>tion representations. Recently, contrastive learning meth-<br>ods [15, 22, 28, 30] have gained prominence. However,<br>these methods often require carefully designed data aug-<br>mentations and tend to encourage the encoder to learn more</p><p>global representations, thereby neglecting local spatiotem-<br>poral information. With the rise of transformer models [37],<br>self-supervised pre-training methods based on masked pre-<br>diction tasks have become mainstream in visual represen-<br>tation learning [15, 22, 28, 30]. Works like SkeletonMAE<br>[39, 42] and MAMP [27] have attempted to transfer MAE<br>[17] methods to the field of 3D action representation learn-<br>ing, achieving promising results. However, these MAE-like<br>methods inefficiently utilize model capacity by focusing on<br>low-level high-frequency details with raw joint coordinates<br>or temporal motion as learning targets, which is subopti-<br>mal for modeling high-level spatiotemporal structures. We<br>believe that using higher-level prediction targets will guide<br>the model to learn better representations and improve pre-<br>training performance.<br>Motivated by this idea, we propose Skeleton2vec, a sim-<br>ple and efficient self-supervised framework for 3D action<br>representation learning. Addressing the limitations of ex-<br>isting MAE-like methods, as illustrated in Fig. 1, Skele-<br>ton2vec leverages contextualized prediction targets. Fol-<br>lowing the work of data2vec [1, 2], we employ a teacher<br>encoder that takes unmasked training samples to generate<br>latent contextualized representations as targets. We then<br>use a student encoder, taking a masked version of the sam-<br>ple as input, combined with an asymmetric decoder to pre-<br>dict data representations at the masked positions. The entire<br>model is based on the vanilla transformer architecture. The<br>self-attention mechanism ensures that the constructed tar-<br>gets are contextualized, incorporating information from the<br>entire sample, making them richer than isolated targets (e.g.<br>raw joint coordinates) or targets based on local context (e.g.<br>temporal motion).<br>Additionally, considering the strong spatiotemporal cor-<br>relations in 3D skeleton sequences, we propose a motion-<br>aware tube masking strategy. Initially, we divide the in-<br>put skeleton sequence along the temporal axis into multiple<br>tubes, where frames within each tube share a masking map<br>to avoid information leakage from neighboring frames. This<br>forces the model to extract information from distant time<br>steps for better prediction. We then guide the sampling of<br>masked joints based on the spatial motion intensity of body<br>joints within each tube. Joints with higher motion inten-<br>sity will be masked with higher probability, allowing the<br>model to focus more on spatiotemporal regions with rich ac-<br>tion semantics. Compared to random masking, our method<br>better utilizes the spatiotemporal characteristics and motion<br>priors of 3D skeleton sequences, effectively improving pre-<br>training performance.<br>In summary, the main contributions of this work are<br>three-fold:<br>• We propose the Skeleton2vec framework, which uses<br>contextualized representations from a teacher encoder as<br>prediction targets, enabling the learned representations to<br>have stronger semantic associations.<br>• We introduce a motion-aware tube masking strategy that<br>performs persistent masking of joints within tubes based<br>on spatial motion intensity, forcing the model to build bet-<br>ter long-range spatiotemporal connections and focus on<br>more semantic-rich regions.<br>• We validate the effectiveness of our method on three<br>large-scale 3D skeleton-based action recognition datasets<br>and achieve state-of-the-art results.</p><h2 id="2-Related-Work">2.Related Work</h2><h3 id="2-1-Self-supervised-Skeleton-based-Action-Recognition">2.1. Self-supervised Skeleton-based Action Recognition</h3><p>Previous studies [21, 33, 47] on self-supervised representa-<br>tion learning for skeleton-based action recognition utilize<br>various pretext tasks to capture motion context. For in-<br>stance, LongTGAN [47] leverages sequence reconstruction<br>to learn 3D action representations. P&amp;C [33] employs a<br>weak decoder to enhance representation learning. MS2L<br>[21] employs motion prediction and jigsaw puzzle tasks.<br>Yang et al. [44] introduce a skeleton cloud colorization<br>task. Contrastive learning methods have gained prominence<br>in 3D action representation learning [14–16, 22, 28, 30].<br>AS-CAL [30] and SkeletonCLR [20] utilize momentum<br>encoder and propose various data augmentation strategies.<br>AimCLR [15] introduces extreme augmentations. ActCLR<br>[22] performs adaptive action modeling on different body<br>parts. Despite their remarkable results, contrastive learning<br>methods often overlook local spatio-temporal information,<br>a crucial aspect for 3D action modeling.<br>The surge in popularity of transformers has led to the<br>mainstream adoption of self-supervised pretraining based<br>on masked visual modeling for visual representation learn-<br>ing [3, 17]. SkeletonMAE [39] and MAMP [27] apply the<br>Masked Autoencoder (MAE) approach to 3D action repre-<br>sentation learning. SkeletonMAE employs a skeleton-based<br>encoder-decoder transformer for spatial coordinate recon-<br>struction, while MAMP introduces Masked Motion Predic-<br>tion to explicitly model temporal motion. In this study, we<br>demonstrate that utilizing higher-level contextualized rep-<br>resentations as prediction targets for masked regions yields<br>superior performance compared to directly predicting raw<br>joint coordinates or temporal motion.</p><h3 id="2-2-Masked-Image-Modeling">2.2. Masked Image Modeling</h3><p>BEiT [3] pioneered masked image modeling (MIM) for<br>self-supervised pretraining of visual models, aiming to re-<br>cover discrete visual tokens from masked patches. Subse-<br>quently, various prediction targets for MIM have been ex-<br>plored. MAE [17] and SimMIM [40] treat MIM as a de-<br>noising self-reconstruction task, utilizing raw pixels as the<br>prediction target. MaskFeat [38] replaces pixels with HOG</p><p>descriptors to enable more efficient training and achieve<br>superior results. PeCo [8] introduces a perceptual loss<br>during dVAE training to generate semantically richer dis-<br>crete visual tokens, surpassing BEiT. These works demon-<br>strate superior performance by utilizing higher-level and se-<br>mantically richer prediction targets in MIM. To further en-<br>hance performance, data2vec [1, 2] employs self-distillation<br>to leverage latent target representations from the teacher<br>model output at masked positions. Compared to isolated<br>targets like visual tokens or pixels, these contextualized rep-<br>resentations encompass relevant features from the entire im-<br>age, enabling improved performance.<br>In this research, we introduce the data2vec framework<br>into self-supervised pretraining of skeleton sequences, uti-<br>lizing latent contextualized target representations from the<br>teacher model to guide the student model in learning more<br>effective 3D action representations.</p><h2 id="3-Method">3.Method</h2><h3 id="3-1-Overview">3.1. Overview</h3><p>3.1. Overview<br>The overall framework of Skeleton2vec is shown in Fig. 2.<br>It takes a skeleton sequence I ∈ RTs×V×Cs<br>as input, where<br>Ts is the the number of frames, V is the number of joints,<br>and Cs is the the coordinates of joints. Similar to most vi-<br>sual transformers [9], the skeleton sequence is first divided<br>into fixed-size patches and then linearly transformed into<br>patch embedding E ∈ RTe×V×Ce<br>. After that, we employ<br>the motion-aware tube masking strategy to guide the mask-<br>ing of joints. The teacher model constructs the full con-<br>textualized prediction targets using unmasked training sam-<br>ples, while the student model receives the masked version<br>of the samples and predicts corresponding representations<br>at the masked positions.<br>As our student model, we adopt an asymmetric encoder-<br>decoder architecture, where the encoder operates solely<br>on non-masked tokens. The lightweight decoder inserts<br>masked tokens into the latent representations outputted by<br>the encoder, forming a full set for predicting the targets. The<br>teacher encoder shares the same model structure as the stu-<br>dent. After accomplishing the aforementioned pre-training<br>task, the teacher encoder is retained for downstream task<br>fine-tuning.</p><h3 id="3-2-Model-Architecture">3.2. Model Architecture</h3><p>3.2. Model Architecture<br>Encoder: Following MAMP [27], we first divide the raw<br>skeleton sequence I ∈ RTs×V×Cs<br>into non-overlapping<br>segments I′ ∈ RTe×V×(l·Cs), where Te = Ts/l and l is<br>the length of each segment. A trainable linear projection is<br>then applied to each joint to obtain the embedding:</p><p>where Ce represents the dimension of the embedding. Tem-<br>poral positional embedding Et ∈ RTe×1×Ce<br>and spatial po-<br>sitional embedding Es ∈ R1×V×Ce<br>are then added to the<br>joint embedding to yield the final input:</p><p>For the teacher encoder, the entire set is flattened as input<br>ET ∈ RNT×Ce<br>, where NT = Te × V represents the total<br>number of tokens in the skeleton sequence. For the student<br>encoder, most tokens are masked, and only the unmasked<br>tokens are utilized as input, flattened as ES ∈ RNS×Ce<br>,<br>where NS = Te × V × (1 − m) denotes the number of<br>visible tokens, and m is the masking ratio. Subsequently,<br>Le layers of vanilla transformer blocks are applied to extract<br>latent representations. Each block comprises a multi-head<br>self-attention (MSA) module and a feed-forward network<br>(FFN) module. Residual connections are employed within<br>each module, followed by layer normalization (LN).<br>Decoder: The decoder input D ∈ RTe×V×Ce<br>contains the<br>full set of tokens, including the latent representations of vis-<br>ible encoded tokens ZS<br>e and the inserted masked tokens.<br>Each masked token is represented by a shared learnable<br>vector EM ∈ RCe<br>, indicating missing information to be<br>predicted at that position. Similar to the encoder, spatial<br>positional embedding E′<br>s and temporal positional embed-<br>ding E′<br>t are added to all tokens to assist masked tokens in<br>locating their positions. The decoder employs an additional<br>Ld layers of transformer blocks for masked prediction.</p><h3 id="3-3-Contextualized-Target-Prediction">3.3. Contextualized Target Prediction</h3><p>Rather than relying on isolated raw joints or temporal mo-<br>tion with limited local context, we employ a transformer-<br>based teacher encoder to construct globally contextualized<br>prediction targets, thereby introducing a diverse training<br>task.<br>Contextualized Target Representations: We extract fea-<br>tures from the output of each FFN block in every layer of<br>the teacher encoder and average them to form our training<br>targets. Following data2vec 2.0 [2], the features from each<br>layer are normalized with instance normalization [36] be-<br>fore averaging. Finally, the averaged features are normal-<br>ized by layer normalization to serve as the prediction tar-<br>gets. Normalizing the targets helps prevent the model from<br>collapsing to a trivial solution, and also prevents any sin-<br>gle layer’s features from dominating. The generation of the<br>target representations can be formulated as:</p><p>where IN and LN refer to instance normalization and layer<br>normalization, respectively. ZT<br>l denotes the output of the<br>FFN block in the lth layer of the teacher encoder.</p><p>Target Prediction: Given the output Hd of the student<br>decoder, we employ an additional linear prediction head<br>to regress the contextualized target representations of the<br>teacher:</p><p>Finally, we adopt L2 loss as our learning objective, cal-<br>culating loss only for the masked positions:</p><p>whereM denotes the set of masked positions.<br>Teacher Parameterization: The student model weights θ<br>are updated through backpropagation on the loss gradients.<br>The teacher model weights ∆ are initialized to be the same<br>as the student weights and parameterized during training by<br>taking an exponentially moving average (EMA) of the stu-<br>dent weights:<br>∆ ← τ∆+ (1 − τ)θ, (6)<br>where τ is a hyperparameter controlling the update fre-<br>quency of the teacher weights using a linearly increasing<br>schedule, gradually increasing from an initial value τ0 to 1<br>throughout training.</p><h3 id="3-4-Motion-Aware-Tube-Masking">3.4. Motion-Aware Tube Masking</h3><p>3.4. Motion-Aware Tube Masking<br>We propose the motion-aware tube masking strategy to ad-<br>dress the issue of high spatiotemporal correlations in skele-<br>ton sequences.<br>Tube Division: The tube masking strategy, initially intro-<br>duced by VideoMAE [35], considers the entire video se-<br>quence along the temporal axis as a single tube, sharing the<br>same masking map across different frames. This mitigates<br>the information leakage issue between adjacent frames. Al-<br>though the skeleton sequence is derived from the video, di-<br>rectly applying this single-tube masking strategy to skeleton<br>data is suboptimal due to the inherent structural differences.<br>In video data, the basic units for masking are image patches<br>in each frame. Due to scene motion or camera viewpoint<br>changes, a masked body part like the hand in the first frame<br>may find its correspondence in unmasked regions in later<br>frames far apart, which facilitates long-range dependency<br>modeling. In contrast, the basic units for masking in skele-<br>ton sequences are the joints in each skeleton frame, where<br>the same-order joints have explicit correspondence across<br>frames. As a result, a body part masked in the first skeleton<br>frame will remain masked in all frames, causing a complete<br>loss of information for that part, which makes the masked<br>prediction task overly difficult and harms the model’s learn-<br>ing capability. To address this, as illustrated in Fig. 2a, we<br>empirically divide the skeleton sequence along the time axis<br>into multiple tubes instead of one tube. Each tube shares the<br>same masking map to force the model to extract informa-<br>tion from farther time steps, while different tubes use differ-<br>ent masking maps to avoid joints being masked throughout.<br>The tube division can be represented as:<br>E′ = Reshape(E) ∈ RN×α×V×Ce  ,(7)<br>where α is tube length and N = Te α is number of tubes.<br>Motion-Aware Sampling: Regions with larger motion in-<br>tensity intuitively contain richer semantic information about<br>actions. Therefore, we utilize the spatial motion intensity of<br>each human body joint within a tube as empirical guidance<br>to generate the masking map.<br>Specifically, we first extract the corresponding motion</p><p>sequence M ∈ RTs×V×Cs<br>from the input skeleton se-<br>quence I ∈ RTs×V×Cs<br>by calculating temporal differ-<br>ences of corresponding joint coordinates between adjacent<br>frames:</p><p>Similar to joint embedding in the encoder, we reshape<br>M into non-overlapping segments M′ ∈ RTe×V×(l·Cs) to<br>match the shape of input sequence I′. We then calculate the<br>motion intensity of each joint within a segment as:</p><p>Afterwards, we compute the spatial motion intensity of<br>each body joint within a tube, normalizing it along the spa-<br>tial dimension:</p><p>Finally, we utilize the normalized spatial motion inten-<br>sity to generate a unique masking map for each tube:</p><p>where η is random noise drawn from a uniform distribu-<br>tion between 0 and 1, β is a hyperparameter controlling the<br>influence of spatial motion intensity on sampling, Mi is<br>the masking map for ith tube, K = V × (1 − m) is the<br>number of joints to be masked, and m is the masking ra-<br>tio. By customizing motion-aware masking maps for each<br>tube, the model is encouraged to focus more on seman-<br>tically richer regions, leading to improved spatiotemporal<br>representations.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解6利用三维热图量改善小规模人体动作识别性能</title>
      <link href="/project/posts/ba38a36f.html"/>
      <url>/project/posts/ba38a36f.html</url>
      
        <content type="html"><![CDATA[<h1>Improving Small-Scale Human Action Recognition Performance Using a 3D Heatmap Volume利用三维热图量改善小规模人体动作识别性能</h1><h2 id="摘要">摘要</h2><p>摘要：近年来，基于骨架的人体动作识别引起了广泛的研究关注，提出的识别或分割方法通常在大规模粗粒度动作数据集上进行验证。然而，在使用深度学习方法识别小规模细粒度人体动作方面仍存在研究不足，而这在实际应用中具有更大的实际意义。为填补这一空白，我们提出了一种基于热图伪视频的新方法，并采用适用于所有模态数据集的统一通用模型。利用人体测量学运动学作为先验信息，通过一个专门预训练的模型提取数据集之间的共同人体运动特征。为了克服关节不匹配问题，我们将人体骨架分为五个部分，这是一种简单而有效的信息共享技术。我们的方法在两个数据集上进行了评估，包括公共的护理活动数据集和我们自建的太极动作数据集。通过线性评估协议和微调评估，结果表明我们的预训练模型有效地捕捉了人体动作之间的共同运动特征，并在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。值得注意的是，我们的模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><p>近年来，骨架基础的人体动作识别备受关注，但主要集中在大规模的粗粒度动作数据集上。</p><p>尚缺乏关于使用深度学习方法识别小规模细粒度人体动作的研究，而这在实际应用中更为实际。</p><p>提出了一种新方法，基于热图伪视频和适用于所有模态数据集的统一通用模型。</p><p>利用人体测量学运动学作为先验信息，通过专门的预训练模型提取数据集之间的共同人体运动特征。</p><p>为了解决关节不匹配问题，将人体骨架分为五个部分，采用简单而有效的信息共享技术。</p><p>在两个数据集上进行评估，包括公共的护理活动数据集和自建的太极动作数据集。</p><p>通过线性评估协议和微调评估，结果表明预训练模型有效捕捉了人体动作之间的共同运动特征。</p><p>在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。</p><p>模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><h2 id="1-Introduction">1. Introduction</h2><p><strong>引言</strong></p><p>人体动作识别领域经历了广泛的研究，导致建立了几个包含各种数据模态的人体动作数据集，包括RGB视频、深度、雷达[1]、红外辐射（IR）和骨架坐标[2]。目前的研究主要集中在利用深度和复杂的神经网络来取得令人印象深刻的结果，这得益于构建了大规模的人体动作数据集，如包含13,000个样本的UCF-101 [3]和包含392,000个样本的Kinetics-600 [4]。然而，这些广泛的多模态人体动作数据集的潜力不应仅限于验证这些算法的性能。相反，必须广泛探索它们的应用价值，例如用于辅助小规模细粒度数据集的识别。</p><p>目前的人体动作数据集通常侧重于粗粒度的日常活动，显示出明显的类间差异，并且仅涉及某些身体部位的主要运动。相反，对特定细粒度动作的研究具有显著的实际价值，因为它们可以被视为粗粒度环境中的一般动作。例如，舞蹈可以包括芭蕾舞、萨尔萨舞和探戈等。一个高效的模型可以识别微妙的差异，并在互动任务中提供适当的反馈，同时在动作评估场景中评估运动表现。然而，由于取样条件有限或取样成本高昂，这类数据集中的样本数量明显少于主流数据集。因此，使用深度学习方法对具有有限训练样本的细粒度动作进行识别仍然是一项具有挑战性且值得关注的努力，这构成了本文的主要贡献。</p><p>在创建这些数据集之后，已经提出了许多与人体动作识别任务相关的方法。早期的方法依赖于手工制作的特征来区分不同的动作类别。然而，随着深度学习的出现，各种方法，如2D卷积神经网络（2D-CNN）、长短时记忆（LSTM）、3D卷积神经网络（3D-CNN）、图卷积网络（GCN）和Transformer，被用于识别样本，并且分类准确性稳步提高。目前，在基于骨架的人体动作识别中，研究人员将GCN或Transformer视为必不可少的工具，而在视频人体动作识别中，3D-CNN和TimeSformer [5]很受欢迎。</p><p>上述方法在特定样本模态方面都有优势和局限性。视频模态样本无法通过GCN的图节点表示有效地，而骨架数据不能直接用于CNN的输入。这种差异在视频和骨架动作之间形成了自然的边界。在骨架模态动作样本的情况下，局部关节可能不存在，并且可能发生多人互动动作。之前使用基于GCN的方法通过在剩余关节上执行线性插值来完成骨架结构 [6]，并将每个执行者的输出的平均值作为动作特征 [7]来解决这些情况。然而，补充的骨架结构可能包含不可靠的信息并引入干扰信息。此外，多人特征的平均化忽略了字符之间的互动信息。如果仅使用小规模的细粒度数据集来从头开始训练深度神经网络，网络容易过拟合。先前的研究在图像处理中采用了自适应锐度感知最小化[8]（ASAM）或使用ImageNet数据集 [9]中的预训练模型来解决这些挑战。基于预训练策略，我们的研究致力于利用公共动作数据集对网络进行预训练，并解决不同数据集之间的关节和模态不匹配问题。</p><p>在使用深度神经网络进行小规模人体动作识别时，一些问题，如缺失的骨架数据、互动样本和过拟合，尚未解决。我们提出了一个统一且通用的模型SSCLS用于小规模动作分类。图1显示了我们识别框架的网络结构。与主流基于骨架的人体动作识别任务不同，我们引入了3D热图体，也称为基于热图的伪视频，作为网络输入，而不是视频图像或骨架序列。来自两种不同格式的代表性热图帧在图2中显示。对于视频输入的任何帧，我们使用高分辨率网络（HRNet）[10]估计人体2D姿势。对于原始的3D骨架坐标，我们将（x，y）2D坐标在每个时间步转换成热图。通过这种方式，我们可以获得一个热图h ∈ RC×H×W，其中C、H和W分别指通道、高度和宽度。在计算每个单帧的热图之后，我们沿着时间维度连接这些热图以形成3D热图体H ∈ RC×T×H×W。如图1所示，公共的NTU RGB+D样本 [11]有17通道的输入，对应于17个骨架关节，而我们自建的太极动作 [12]有72个关节。为了解决在使用完整关节信息的样本时不同数据集之间的骨架关节不匹配问题，我们将骨架关节分为五个部分，包括四肢和一个躯干，并将样本均匀分成5通道输入，如图1中标记为红色、绿色、黑色、紫色和橙色，同时确保每个通道都保持其物理含义。当以3D热图体作为网络输入时，我们使用主流的PoseConv3D模型 [13]作为我们的模型骨干。在这项工作中，我们使用NTU RGB+D数据集进行网络预训练，小规模数据集仅用于最终分类头训练或微调预训练网络。</p><p>我们总结本文的贡献如下： • 一个统一的动作表示。我们引入基于热图的伪视频来统一动作输入的格式。这种格式可以将基于视频和基于骨架的样本转换为统一的输入，消除了这些格式之间的障碍。 • 人体测量学运动先验。我们提出并证明人体动作具有共同的运动特征，因此预训练模型骨干可以帮助提取人体特征，以克服网络过拟合并提高在小规模数据集上的识别性能。 • 一个统一的关节分割方法。我们根据人体结构将骨架关节分为五个部分，以充分利用不同数据集之间的数据信息，而不考虑动作数据的格式和采样器数量。 • 最先进的性能。我们在公共的Nursing Activities和我们自建的太极数据集上分别进行各种配置的实验。识别性能和提取特征的t-SNE [14]可视化表明，与先前的SOTA方法相比，我们的识别准确性得到了很大程度的提高，网络的过拟合现象得到了显著缓解。当融合从关节和肢体提取的特征时，识别准确性进一步提高，证明了我们提出的模型的普遍性。我们的实验记录和结果可在网站https://github.com/eunseo-v/SSCLS（于2023年5月10日访问）上找到。</p><h2 id="2-Related-Work">2. Related Work</h2><p>本节旨在概述基于骨骼的人类动作识别研究的最新进展，包括相关数据集、识别算法、小尺度细粒度动作数据集、训练前策略和评估协议。我们还对我们提出的方法与其他相关工作进行了比较分析，包括先前对太极拳动作识别的研究。</p><h3 id="2-1-基于骨架的人体动作数据集">2.1. 基于骨架的人体动作数据集</h3><p>2.1. 基于骨架的人体动作数据集 本节重点介绍基于骨架的人体动作数据集及其获取方法，可以分为三组：基于深度的、基于可穿戴的和基于提取的。其中，Microsoft Kinect系统[15]广泛用于基于深度传感器的3D人体动作提取。两个大规模的人体日常活动数据集，即NTU RGB+D数据集 [11,16]和PKU-MMD数据集 [17]，包含由Kinect V2传感器收集的骨架坐标，并可以使用姿势估计算法提取多个人体动作。然而，这些数据集的准确性受到光强和背景等因素的影响。另一种类型的数据集是使用可穿戴运动捕捉系统获取的，例如Perception Neuron [18]，它利用惯性传感器测量穿戴者的方向和加速度。我们自建的太极动作数据集 [19] 使用Perception Neuron系统收集，仅包含3D骨架坐标。尽管这种方法可以精确捕捉骨架坐标，但仅限于单个采样器的动作，并且测量精度可能受到惯性传感器漂移的影响。</p><p>在基于提取的数据集领域，姿势估计算法已被广泛用于从视频中检测和估计2D人体姿势，实现从动作视频剪辑直接获取骨架坐标。与3D姿势估计器相比，估计的2D骨架更准确且更稳健。值得注意的是，高分辨率网络（HRNet）[10]被提出作为一种预测更精确和空间准确的关键点热图的手段，而MediaPipe [20]则作为一个跨平台的机器学习（ML）框架，可以从视频中提取人体姿势，如MediaPipe Hands [21]，一个实时的设备上手部跟踪解决方案，可以预测手的骨架。这些方法为调整网络输入格式提供了一种方便的方式，也有效地丰富了人体动作数据集。</p><p>在我们的研究中，我们利用NTU RGB+D数据集的骨架坐标作为我们的预训练数据集，该数据集包含60个跨足日常、相互和健康相关动作的动作类别，采集了三个摄像头视角和众多主体。鉴于我们的假设，即人类活动共享共同特征，我们预计预训练数据将包含丰富的动作信息，并因此有助于我们的小规模动作识别任务。</p><p><strong>要点总结：</strong></p><ol><li>介绍了基于骨架的人体动作数据集，分为深度传感器、可穿戴系统和提取方法三类。</li><li>提到Microsoft Kinect系统广泛用于3D人体动作提取，NTU RGB+D和PKU-MMD数据集是两个使用该系统的大规模数据集。</li><li>另一类数据集使用可穿戴运动捕捉系统，如Perception Neuron，但受到单一采样器和传感器漂移的限制。</li><li>在基于提取的数据集中，介绍了姿势估计算法在从视频中获取2D人体姿势方面的应用，以及使用高分辨率网络（HRNet）和MediaPipe的方法。</li><li>本研究使用NTU RGB+D数据集的骨架坐标作为预训练数据，认为预训练数据将有助于小规模动作识别任务。</li></ol><h3 id="2-2-人体动作识别方法">2.2. 人体动作识别方法</h3><p>2.2. 人体动作识别方法</p><p>由于方法部分不是本文的主要焦点，我们将简要介绍人体动作识别。先前的方法依赖于手工制作的描述符来表示人体动作，成功程度各异。例如，Wang等人[22]尝试通过将摄像机运动纳入其方法来改进基于运动的描述符。与此同时，Oreifej和Liu [23]开发了一种称为4D定向梯度直方图（HON4D）的新型描述符，它创建了时间和空间的4D投影仪。这些描述符是专门设计的，对所有数据集的泛化性较差。近年来，基于深度学习的方法在动作识别研究中变得流行[24]。这些方法主要集中在将基于骨架的序列作为序列问题使用，提出了基于LSTM的模型[25–28]，用于构建序列的不同空间对齐的时间关系。 研究人员[29–32]还探讨了使用伪图像表示3D骨架序列的方法，这允许使用预训练的主流CNN模型进行特征提取，而无需从头开始训练。例如，Wang和Li [31]提出将3D骨架序列转换为关节轨迹图，然后使用CNN核进行分类。然而，这些方法因分解骨架序列的物理结构且缺乏时空动态而受到批评。 为了解决这些局限性，Kipf等人[33]开发了一种有效的卷积神经网络（CNNs）变体，可直接在图上运行，用于处理图结构化数据。该方法将CNNs推广到任意结构的图上，使其非常适用于表示3D骨架序列，因为这可以自然地看作是图数据。Yan等人[34]随后提出了一种称为时空图卷积网络（ST-GCN）的新模型，该模型基于Kipf的方法，并可以自动学习关节的空间配置和时间动态。该模型已成为后续动作识别研究的基准比较。后续基于GCN的工作主要包括自适应空间图卷积核或双向骨架序列[35–38]，以及论文[35]中的Ck核计算每帧中任意两个关节的相似度，类似于注意机制。Qin和Liu [39]在原始输入的通道维度中引入了角度编码，以融合高阶特征。尽管具有自注意模块的Transformer架构[40]在自然语言处理（NLP）任务中已成为事实上的标准，但其结构已在包含计算机视觉[42]和长序列时间序列预测[43]的各种研究领域中得到广泛应用。在基于骨架的人体动作识别领域，研究人员提出了一些方法，例如时空Transformer网络（ST-TR）[7]和群体活动识别网络GroupFormer [44]。这些方法利用Transformer的自注意机制来模拟关节之间的依赖关系，并共同捕获时空上下文信息。观察到与使用骨架坐标作为输入的基于GCN的方法相比，基于Transformer的方法在性能上表现出色。 视频理解是另一个使用视频剪辑作为输入的人体动作识别领域[45]。这些视频数据集中的剪辑通常来自涵盖广泛人体动作相关类别的互联网视频。Karpathy等人[46]收集了Sports-1M数据集，并提出了一种具有上下文流的多分辨率CNN架构，该流模型低分辨率图像，而视觉流模型高分辨率中心裁剪图像。Simonyan和Zisserman [47]提取了光流并提出了一种两流ConvNet，用于使用单帧图像和多帧光流捕获空间和时间信息。Jing等人[48]试图在复杂场景中识别动作。具体而言，他们提出了一个联合损失的时空神经网络模型，用于在视频中识别人体动作，并使用提取光流和外观的两流网络来捕获空间特征，以及使用LSTM网络来建模时态动态。Tran等人[49]将3×3 2D扩展到3×3×3 3D卷积核，并学习由简单线性分类器对齐的C3D特征用于视频理解。由于视频稀缺，3D CNN很难训练。Carreira和Zisserman [4]扩展了2D核并提出了一种两流扩张3D ConvNet（I3D）。I3D模型可以使用在大图像数据集上预训练的相应2D CNN网络初始化，有助于网络训练。由于从视频中提取光流耗时，一些工作[50,51]尝试应用基于幻觉的方法来学习扩充网络输入。例如，Wang等人[50]试图通过一种幻觉步骤将I3D特征图的输出转换为Fisher向量表示。唐等人[51]提出了一种从外观输入中想象光流特征的网络，以节省计算成本。Feichtenhofer等人[52]提出了一个SlowFast网络框架，其中慢路径具有低帧速率和更高通道以捕获空间特征，而快路径可以学习具有高帧速率和更轻通道的时态嵌入。3D CNN核，如C3D和I3D，可以填充到该框架中以进行识别任务。随着注意机制的流行，工作[53]将时空非局部操作应用于视频识别任务，而论文[5,54]的作者则将标准Transformer架构调整到不同技能的大规模视频数据集识别中。 在我们之前的工作[12]中，我们使用3D骨架坐标作为太极动作识别的输入。然而，由于预训练的NTU RGB+D数据集与太极数据集之间的数据格式和收集系统差异，即使在网络管道中应用了批归一化，识别性能仍然相对较差。为了解决这个问题，Duan等人[13]提出使用3D热图体代替传统的图序列来表示人体骨架，这比3D骨架坐标具有几个优势。例如，由于测量可能在不同数据集之间有所不同，使用3D骨架坐标可能导致相同动作的显著差异，需要在数据集之间进行归一化。使用3D热图体可以直接解决这种归一化问题，同时允许应用视频处理技巧，如居中和裁剪，以丰富训练数据集。此外，多人动作骨架可以投影到单个热图上，我们可以利用视频理解领域的主流方法，以在不增加额外计算成本的情况下实现稳定和更好的性能。然而，GCN的计算随着一个动作中采样器数量的增加而呈线性扩展。在这项工作中，我们的目标是将基于图的骨架坐标转换为3D热图体，以实现在不同数据集之间共享人体动作特征，并缓解数据格式不匹配的问题。对于特征提取，我们将使用PoseConv3D网络作为我们的模型骨干，该网络在性能上表现优越，超过了先前的动作识别方法。</p><p><strong>要点总结：</strong></p><ol><li>介绍了人体动作识别的方法，强调了先前依赖手工制作描述符的方法和近年来基于深度学习的方法。</li><li>概述了基于骨架的序列的传统方法，如使用LSTM模型，以及对3D骨架序列使用伪图像的方法，以便利用预训练的CNN模型进行特征提取。</li><li>引入了基于图的CNN变体，特别是ST-GCN模型，以及后续工作中基于GCN和Transformer的方法，这些方法在处理骨架坐标作为输入时表现出优越性能。</li><li>概述了基于视频的动作识别方法，包括对视频理解领域的关键工作，如I3D模型和SlowFast网络。</li><li>提到先前的工作中使用3D骨架坐标的问题，引出了使用3D热图体的解决方案，以便在不同数据集之间共享人体动作特征，并介绍了PoseConv3D网络作为特征提取的模型骨干。</li></ol><h3 id="2-3-小规模细粒度数据集研究">2.3. 小规模细粒度数据集研究</h3><p>2.3. 小规模细粒度数据集研究 研究人员致力于通过新颖的神经网络提高在流行的大规模粗粒度数据集上的识别性能。创建特定的小规模细粒度数据集可能更为有益。Weinland等人[55]创建了一个包含来自多摄像头的11个动作的IXMAS数据集。Nicora等人[56]建立了MoCA数据集，该数据集包含每个活动的3个相机视图中的20个细粒度烹饪动作。此外，MoCA是一个双模态数据集，收集了烹饪场景中的运动捕捉数据和视频序列。这些特定数据集可以证明所提出模型的细粒度识别能力，有助于在特定场景中的应用。Gu等人[57]发布了一个包含注释篮球比赛视频的细粒度篮球动作数据集。他们提出了一个集成了NTS-Net的两流网络，用于提取其细粒度数据集的判别特征。 Wu和Shao [58]提出了一种多最大间隔支持向量机（MMM-SVM），以提高IXMAS数据集的准确性，采用了多视图系统。Wang等人[59]使用内部迁移学习策略增强了小规模数据集的性能。这种策略未使用其他数据集，需要从候选模型列表中选择最佳模型，这在应用中可能需要更复杂。Shen等人[60]提出了一种自动数据增强模型，称为Imaginative Generative Adversarial Network，它可以从学习的数据集分布中采样新数据。增强的数据集可以提高使用相同神经网络的分类准确性。这种方法需要相对较大规模的数据集，而由于样本不足，小规模数据集仍然难以训练。Ijaz等人[6]提出了一种多模态基于Transformer的网络，用于提取和融合骨骼关节和加速度数据的特征信息，以提高小规模细粒度护理活动[61]的识别性能。然而，他们没有使用NTU-RGB+D数据集对网络进行预训练，只引入了自适应锐度感知最小化（ASAM）[8]来收敛他们的Transformer模型。他们还在结论中推断，可以探索预训练骨架分支以进一步提高模型的收敛性，这在我们的工作中已经实施，并陈述了预训练管道的优越性。Goyal等人[62]试图利用大规模预训练表示，假设它们隐含地包含小规模数据集任务的相关线索。该论文还应用了视图智能批量归一化以最小化跨视图动作识别任务的内部协变量转移。这种方法对可以提取为运动特征的哪一层输出进行了实验性分析，并没有详细描述人体数据集之间的隐含相关线索，这些都将在我们的论文中解决。 与普遍的粗粒度动作不同，太极动作可以视为专业的动作类别，每个太极动作由多个与身体运动协调的元动作组成。与日常人体动作相比，需要关注不同人体部位在不同阶段的运动特征。在太极动作中，由于太极动作类别之间的相似性，识别是具有挑战性的。由于太极已成为亚洲运动会的项目，有效的识别网络可以帮助初学者评估他们的活动。在太极相关的动作识别工作中，Lin等人[63]提出了一个包含17个类别的大深度包含人体动作（DHA）视频数据集。他们将所有太极动作视为相同的类别。Sun等人[64]提出了一个包含58个太极动作的细粒度太极数据集，总共有2772个样本。所有视频样本都来自具有动态背景的网站。他们还应用了改进的密集轨迹特征和Fisher向量表示进行识别，并取得了51.39%的识别准确度。Dong等人[65]还提出了一个名为“Sub-Tai chi”的太极数据集，包含15个动作，并应用了带有注意模块的结构LSTM进行识别；他们在自己的数据集上达到了79%的识别准确度。Liu等人[66]在他们自己的太极数据集上应用了ST-GCN模型，取得了89.22%的识别准确度。所有这些研究都创建了自己的太极数据集，并且不能在网站上进行比较。他们提出了具体的或应用了流行的深度学习方法进行识别，对于细粒度动作识别而言并不通用。 我们之前的工作在太极动作上进行了初步研究。我们首先创建了太极动作数据集，并提出了一种使用节点轨迹特征的太极动作识别算法[19]。我们提取了手工制作的特征，并且仅使用了一个单一节点进行特征提取，在小规模训练集下表现不佳，通用性可能更好。我们引入了深度学习方法，并提出了一种用于太极动作识别的空间变换网络[12]。我们使用了NTU RGB+D数据集对模型进行预训练，以解决过拟合网络问题。太极训练样本仅负责使用冻结的模型骨干参数进行最终分类头培训。该算法使用了人体骨架的24个关节点，并相对于第一个传统方法取得了改进。然而，它仅部分利用了太极数据集的72个骨架关节，导致了我们的细粒度太极动作缺失信息。网络识别还需要进行复杂的数据预处理。此外，小规模训练集下的性能仍需要改进，并且结果的分析较少呈现给读者。我们的论文包含准确性提高和消融研究，将解决这些弱点。 我们打算在我们的工作中为这些小规模细粒度动作数据集提供一个通用框架。我们假设样本在人体动作数据集之间共享共同的运动特征，并提出了一个简单但有效的模型框架来解决小规模细粒度动作识别问题，该问题已在公共护理活动数据集和我们自建的太极数据集上进行了评估。该模型框架不引入对抗网络，可以识别小规模细粒度动作数据集。</p><p><strong>要点总结:</strong></p><ol><li>研究人员致力于改善大规模粗粒度数据集上的识别性能，特别关注了小规模细粒度数据集的建立。</li><li>提到了不同研究中创建的小规模细粒度数据集，如IXMAS、MoCA、篮球动作数据集等，以及它们的用途和特点。</li><li>讨论了先前研究采用的一些方法，包括支持向量机、迁移学习、数据增强、多模态Transformer等，以提高小规模细粒度数据集的性能。</li><li>强调太极动作作为细粒度动作类别的挑战性质，以及太极数据集的创建和其他研究。</li><li>总结了先前关于太极动作的研究，包括使用节点轨迹特征和空间变换网络进行识别，指出了它们的局限性和需要改进的地方。</li><li>提出了在工作中旨在为小规模细粒度动作数据集提供通用框架，假设样本在不同数据集之间共享运动特征。</li></ol><h3 id="2-4-预训练策略和评估协议">2.4. 预训练策略和评估协议</h3><p>2.4. 预训练策略和评估协议 在计算机视觉和视频理解领域，许多研究人员通过线性评估协议或微调评估验证他们的方法。在线性评估协议中，我们冻结预训练的网络参数，仅对目标数据集训练最终分类器。此外，在微调评估中，所有网络参数都与新数据集进行微调。例如，MoCo-V1 [67]首先在ImageNet-1M [9]数据集上进行无监督预训练，然后冻结参数并重新训练一个监督线性分类器。这种无监督学习的主要目标是学习可转移的特征。这种策略可以证明提取特征的有效性。Simonyan和Zisserman [47]使用了三种评估方法来衡量其空间流ConvNet的性能，包括在目标UCF-101数据集上从头开始训练，基于ILSVRC-2012数据集的预训练线性评估协议，以及在UCF-101上训练分类器并进行微调。I3D模型 [4]提出了充气的3D CNN核，可以在2D ImageNet模型上进行预训练，并将相关的2D CNN核引导到3D CNN核以进行初始化。I3D模型还通过首先在Kinetics上进行预训练，然后在HMDB-51 [68]和UCF-101数据集上进行微调来评估其出色的迁移学习能力。 如上所述，预训练策略总是与评估协议相配。在计算机视觉领域的下游任务中，如目标检测 [69]、语义分割 [70] 和人体姿态估计 [71]，研究人员将在ImageNet数据集上预训练的网络模型用于微调其下游任务。在视频理解领域，I3D模型使用在ImageNet数据集上预训练的2D卷积网络的参数来初始化其3D卷积网络的权重参数，从而加速网络的收敛。我们观察到，这些下游任务的数据集都是自然图像或视频样本。此外，在ImageNet上预训练的模型具有相似的现象，即从层次网络的第一层到最后一层的提取特征逐渐从通用性变化为任务特定性 [72]。具体而言，前几层可以捕获样本的低级特征，例如图像的颜色斑点特征，而最后一层的输出则是高级和任务特定的。因此，在ImageNet上预训练的模型骨干可以提取自然图像的低级边缘和纹理特征，并加速在其他自然图像或视频数据集的下游任务中的模型收敛，以实现与从头开始训练相比更好的性能。 在本文中，我们打算使用SlowFast构建一个通用且准确的模型框架。与基于骨架的人体动作识别方法不同，我们将基于骨架的动作序列转换为基于热图的伪视频。我们利用大规模数据集提取热图的常见低级运动特征，这有助于改善我们的小规模细粒度动作数据集上的识别性能。以前的基于骨架的动作识别任务只是使用大规模基于骨架的动作数据集NTU RGB+D从头开始训练其提出的模型。对于足够大规模的数据集，从头开始训练的模型也可以展现出令人印象深刻的性能 [7,13]。</p><p>然而，在小规模数据集的下游任务中，预训练策略是提高识别性能的有效方法 [73]。因此，我们基于预训练策略的模型框架对于小规模动作识别任务是必要的，这在先前的工作中从未得到解决 [6,57,59,60]。我们还通过直观的t-SNE [14]可视化给出了完整的分析，以展示预训练策略的有效性。 我们的动作研究使用了两种不同目的的评估。线性评估协议表明我们的预训练模型可以通过基于热图的伪视频样本学习共同的人体运动特征，而微调评估实验证明了我们框架的迁移学习能力，并与从头开始训练网络相比展现出卓越的动作识别性能。</p><ol><li><strong>评估协议：</strong><ul><li>计算机视觉和视频理解领域的研究者使用线性评估协议或微调评估来验证方法。</li><li>线性评估协议中，冻结预训练网络参数，只训练目标数据集的最终分类器。</li><li>微调评估中，对新数据集进行所有网络参数的微调。</li><li>以MoCo-V1为例，首先在ImageNet-1M数据集上进行无监督预训练，然后通过线性评估协议验证特征可转移性。</li></ul></li><li><strong>预训练策略的有效性：</strong><ul><li>在计算机视觉领域的下游任务中，如目标检测、语义分割和人体姿态估计，使用ImageNet上预训练的模型可加速收敛。</li><li>模型在ImageNet上的预训练使其能够在其他自然图像或视频数据集的下游任务中表现更好。</li></ul></li><li><strong>本文模型框架：</strong><ul><li>使用SlowFast构建通用准确的模型框架。</li><li>不同于骨架方法，将基于骨架的动作序列转换为基于热图的伪视频。</li><li>利用大规模数据集提取热图的共同低级运动特征，以改善小规模细粒度动作数据集上的性能。</li></ul></li><li><strong>小规模数据集的挑战：</strong><ul><li>在小规模数据集的下游任务中，预训练策略对提高识别性能是有效的。</li><li>本文模型框架基于预训练策略，专注于解决小规模动作识别任务，这是以往工作未解决的问题。</li></ul></li><li><strong>两种评估方法：</strong><ul><li>线性评估协议展示预训练模型学到共同人体运动特征的能力。</li><li>微调评估实验展示框架的迁移学习能力，相较于从头开始训练，表现出更优越的动作识别性能。</li></ul></li></ol><h2 id="3-Methods">3. Methods</h2><p>我们在3.1节中详细描述了从三维骨骼关节坐标到三维热图体积的动作序列转换。然后我们在第3.2和3.3节中介绍了我们的模型框架和培训策略。</p><h3 id="3-1-从关节坐标到3D热图体积">3.1. 从关节坐标到3D热图体积</h3><ol><li><strong>PoseConv3D方法：</strong><ul><li>对于PoseConv3D方法，动作数据应转换为3D热图体积。</li><li>在PoseConv3D方法[13]中，使用在COCO-keypoints上训练的2D姿势估计器HRNet [10]直接从动作视频剪辑中提取2D人体姿势。</li><li>Nursing Activities和Tai Chi动作数据集仅包含帧内的3D骨架坐标，需要转换为3D热图体积。</li></ul></li><li><strong>坐标转换及数据格式处理：</strong><ul><li>不采用PoseConv3D建议的将3D骨架（x，y，z）分成三个2D骨架的建议，而是使用（x，y）、（y，z）和（x，z）分别，因为有些数据集可能直接由相机收集，3D坐标提取不可行或噪声干扰，影响预训练或微调模型。</li><li>数据格式（y，z）和（x，z）因未固定采样器到相机的距离而缺乏物理含义，因此仅使用2D坐标（x，y）形成3D热图体积，z轴坐标可用于通过沿y轴旋转进行数据增强。</li></ul></li><li><strong>多视角和数据增强：</strong><ul><li>每个动作从三个固定相机视角同时捕捉，可获取相同动作的多视角样本。</li><li>还可以通过沿y轴旋转进行透视变换访问多视角样本，如图3b所示。</li><li>3D骨架坐标序列X ∈ RT×V×C，其中T，V，C表示帧数、关节数和坐标数，包含运动捕捉系统收集的V个骨架关节的三个坐标。</li><li>为了充分利用收集到的3D骨架序列，对每个小规模训练样本执行旋转和剪切操作。</li></ul></li><li><strong>2D热图构建：</strong><ul><li>对于每个2D关节坐标（xk，yk），在方程（3）中，Hkij是像素（i，j）处的关键点热图值，ck是（xk，yk）的置信度分数，σ控制了高斯图的方差；Hvij是肢状热图中像素（i，j）的值。</li><li>利用方程（4）计算所有这些像素的热图值，其中d2_ab是从像素到骨ab的投影距离，因此当像素位于肢上时，该值始终为1。</li><li>计算所有关节和骨的热图值后，像素（i，j）的最终热图值是这些V个热图值的最大值，如方程（5）所示。</li><li>图4说明了原始3D关节坐标和太极动作样本在一个帧上的最终2D关键点和肢状热图。</li></ul></li><li><strong>3D热图体积的处理：</strong><ul><li>完成2D热图构建后，将它们组合成跨所有帧的3D热图体积。</li><li>由于网络的输入是两个具有不同关节数的数据集，需要重新构建热图通道。</li><li>采用两种策略来关联两个数据集的信息：一种是从72个太极关节中选择与NTU RGB+D数据集匹配的17个关节；第二种是将整个骨架分为五个部分 - 四肢和一个主干。</li><li>将所有2D热图沿时间维度连接，构建3D热图体积H ∈ RC×T×H×W。</li><li>进行一系列的数据处理策略，将体积固定为48，采用均匀采样。</li><li>遵循中心裁剪策略[13]，找到包围所有帧的2D姿势的最小框，进行随机裁剪，这是视频理解中的一种流行处理方法。</li><li>热图体积将调整为56×56的分辨率，并有50％的概率翻转骨架的右侧和左侧以丰富数据集。</li><li>与其他识别方法相比，我们的预处理数据方法更为简洁，没有零填充[34]或数据插值策略[12]的限制，对采样器数量也没有限制。</li></ul></li></ol><h3 id="3-2-模型架构">3.2. 模型架构</h3><p>我们的3D热图体积可视为一种特殊的视频格式。我们将关节分割后的3D热图体积作为网络输入，如图1所示。我们的网络骨干显示在图1的中间模型骨干部分，遵循PoseConv3D [13]的配置，详细信息见表1。他们采用了SlowOnly [52]方法，将ResNet层直接从2D扩展到3D的最后两个阶段作为其模型骨干。中间列出了每个残差块的组成部分，右侧说明了相应块之后的输出特征大小。核的尺寸用{T× S2, C}表示，分别代表时间、空间和通道的大小。非退化的时间滤波器（时间核尺寸 &gt; 1）用下划线标示。残差块用括号表示，实例化的骨干是ResNet-50 [76]。从图1中可以看出，在时间维度上没有应用下采样操作。最后，动作嵌入将通过线性分类头传递，其中包含全局平均池化（GAP）层和全连接（FC）层，以生成预测的动作类别。</p><p>相对于ResNet-50模型，我们从模型骨干中移除了ResNet2块，因为我们已经使用骨架坐标从视频中提取了姿势特征。我们在第2.4节详细说明了我们的分层网络逐渐从通用特征提取到任务特定特征。由于基于热图的伪视频样本缺乏背景信息，不包含任何低层特征。因为我们的输入相对于自然图像已经是中层特征，所以具有浅层和窄通道的网络模型更加适用。与X3D [77]和I3D模型 [4]相比，我们的模型骨干更轻便，提高了基于热图的伪视频输入的特征提取效率。我们计划使用每个输入样本中均匀采样的48帧更密集的时间输入，使我们能够捕捉更多的时间动态。在模型框架中，沿时间维度没有下采样操作，其他操作遵循ResNet-50结构以提高模型骨干的鲁棒性。</p><h3 id="3-3-训练策略">3.3. 训练策略</h3><p>在NTU RGB+D动作识别研究中，段等人[13]在基于骨架的跨视角（CV）和跨主体（CS）设置中取得了最先进的准确性。然而，他们使用从视频中提取的2D骨架坐标数据，并丢弃了原始数据集提供的3D骨架坐标，而这些坐标在其他识别方法中被使用。图7a显示了原始3D骨架坐标的可视化，图7b是从RGB样本视频中提取的2D坐标的热图。对于这个喝水的动作，采样环境中有两个人，只有一个人在喝水，另一个是干扰采样者。原始3D骨架坐标有两个采样器，数据过于模糊，难以直观识别。先前的工作筛选了原始数据，并利用去噪后的数据进行动作识别。这可以从采样器中提取正确的采样器，但采样环境总是影响数据的一致性。当他们无法从原始样本中提取正确的采样器时，他们必须将两个采样器的数据输入到网络中，并计算提取特征的平均分数。NTU RGB+D数据集中有交互样本，因此采样器的数量根据动作类别而异。先前的工作必须将单人动作样本的第二个人填充为零值，这对于批量计算很方便。图7b反映了一个更准确的喝水动作，视频格式没有零填充问题，这对应用来说是简洁的。在我们的预训练阶段，我们继续使用从NTU RGB+D视频数据集中提取的2D坐标，这有助于更好地学习普通人类动作特征，并减少网络的过拟合。在训练过程中，如图1所示，我们首先使用NTU RGB+D数据集对模型骨干进行预训练。对于小规模细粒度动作识别，我们分别尝试线性评估协议和冻结所有预训练骨干参数或微调整个网络的微调评估，以验证提出的模型框架的有效性。由于我们可以生成关节和肢体格式的3D热图体积，我们还尝试了伪热图及其融合策略，详细内容在第4节讨论。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>备战1月</title>
      <link href="/project/posts/7a7d65fb.html"/>
      <url>/project/posts/7a7d65fb.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>如何快速精通一个领域</p><blockquote><p>尝试更新一下自己的个人学习系统</p><p>自己之前都是过多关注于刷了多少课，却没有关注学习效果 同时导致学的太多 累了之后懒得去复习和反馈应用，应该关注学到的知识和应用</p></blockquote></blockquote><p><img src="../assets/image-20240102105700646.png" alt="image-20240102105700646"></p><h1>备赛</h1><blockquote><p>先练手 开始刷题</p><p>在深入学习对应的不会的知识点</p></blockquote><p>java 公开课<a href="https://www.bilibili.com/video/BV1MA4116793?p=28&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">试题一：指数计算_哔哩哔哩_bilibili</a></p><p><a href="https://pan.baidu.com/s/1lELNX9iYDW6FN5b1pOX4Gg?pwd=6fo6">2020年蓝桥杯JavaC组省考题目.pdf_免费高速下载|百度网盘-分享无限制 (baidu.com)</a></p><ul><li>[ ] 19年老视频<a href="https://www.bilibili.com/video/BV1Lb4y1k7K3?p=23&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">22-动态规划入门视频讲解_哔哩哔哩_bilibili</a></li></ul><p>java备考方法论</p><blockquote><p>主要是关于对应的算法题目的一些拿分技巧 建议后期掌握一下</p><p><a href="https://www.bilibili.com/video/BV1NZ421z7WV/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">Java组经验分享| 蓝桥杯拿分技巧_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p>下边方法来源于<a href="https://www.bilibili.com/video/BV1Hb4y1W7Pn/?spm_id_from=333.880.my_history.page.click">【蓝桥杯】蓝桥杯备赛攻略/算法学习路线/经验分享_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p>排序算法 查找算法</p><blockquote><p>八大排序 + 主要练习归并+ 快速排序</p></blockquote><p>难度低的通过率高的题目</p><p>二分查找+双指针+ 练习</p><p>递归+暴力学习+练习【蓝桥杯的基础】</p><p>贪心算法</p></blockquote><blockquote><p>最后学习完算法之后在学习一边数据结构 加强能理解</p><p>搜索【深度搜索【暴力基础 打蓝桥杯最重要的算法，可以骗到很多的分数】+ 广度搜索（练习几道题即可）】</p><p>动态规划杯</p></blockquote><p>暴力 递归 暴力搜索这些练好刷题之后 直接刷蓝桥杯真题</p><p>学习路线<a href="https://www.bilibili.com/video/BV1u84y1w7xt/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">（新生必看）小白或新手如何准备蓝桥杯Java组，以及如何避坑_哔哩哔哩_bilibili</a></p><blockquote><p>java 可以减少一些底层的大佬，好好备赛，应该能拿奖</p><blockquote></blockquote></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100118769.png" alt="image-20240218100118769"></p><blockquote><p>先用洛谷刷题  去练习基础题目</p><p>再用对应真题刷题系统来刷真题</p></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100129245.png" alt="image-20240218100129245"></p><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100103648.png" alt="，"></p><blockquote><p>必须要用Main 函数 来实现所有的代码提交，不能写package 包</p></blockquote><p>OA 赛制   有步骤分，也有技巧</p><p>日期计算器  excel 表格 写代码</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2024.1月</title>
      <link href="/project/posts/668a32f0.html"/>
      <url>/project/posts/668a32f0.html</url>
      
        <content type="html"><![CDATA[<h2 id="12月末尾">12月末尾</h2><p>🗓️2023.12.30【日知日进】日历<br>📚日知书目：《人是如何学习的》</p><p>✨日进思考：“普通人和高手的认知差距在哪里?”</p><pre><code> 🔹普通人和高手之间的差距在于知识迁移能力的高下，高手总是能够更有效地迁移知识。是否了解原理决定了能否进行知识的迁移，而高手总是乐于探究事物运行背后的原理。高手往往也具备“元认知”的能力，元认知即“对自己认知的认知”，这种能力相当重要。</code></pre><p>💫共读推荐：<br>🔹《知识大迁移》<br>🔷学习者必须掌握足够多的事实，才能对自己掌握的知识及欠缺部分心中有数。</p><p><img src="../assets/image-20240102220712895.png" alt="image-20240102220701767"></p><p>今天是2023年最后一天啦！<br>年初的目标是否已经完成？未来的日子，又有哪些梦想要实现？</p><p>凡是过往，皆为序章。<br>每个终点也是新的起点，只要拼搏过，时光会看见。<br>2023最后一天，早安！</p><p>今日简报</p><p>12月31日   星期日</p><p>1、阿根廷正式拒绝加入金砖国家。</p><p>2、董宇辉宣布由东方甄选100%控股的新账号1月9日开播。</p><p>3、直播PK输了给对方洗脚？线下约架签“生死状”？长沙警方整治网红乱象。</p><p>4、央行同意支付宝变更为无实际控制人。</p><p>5、元旦假期首日跨省游占55%，中国香港成最火跨年境外目的地。</p><p>6、北京城市轨道交通运营总里程达836公里，超越上海跃居全国第一。</p><p>7、因假冒大凉山农特产品，凉山包括赵灵儿、曲布等多名百万级网红被判刑！</p><p>8、易建联宏远9号球衣正式退役！传奇生涯不会落幕。</p><p>9、一眼望不到头！也门首都大规模集会声援巴勒斯坦。</p><p>10、2024年起USB-C将成为欧盟电子设备通用标准。</p><p>11、马斯克强势回归领跑2023年全球富豪榜。</p><p>12、哈马斯官员称停火前不再交换被扣押人员。</p><p>✨此心不动，随机而动。<br>来帆书APP 听《心学的诞生》</p><p>🎈今天是三末:<br>周末，月末，年末。<br>把过去所有的遗憾、烦恼、病痛通通抹去！</p><p>明天是三新:<br>日新，月新，年新。<br>让喜乐、梦想、健康都新新启程！<br>让2023的一切不顺都变成2024惊喜的铺垫！</p><h1>2024年1月</h1><h2 id="1-1日周一">1.1日周一</h2><h3 id="英语角">英语角</h3><p>无</p><h3 id="1-1晨读">1.1晨读</h3><p>无</p><h3 id="今日简报">今日简报</h3><p>今日简报</p><p>1月1日，星期一，元旦快乐</p><p>1、国家主席习近平发表二〇二四年新年贺词。</p><p>2、家用神车桑塔纳月销仅1辆，又一代经典车型将退出历史舞台。</p><p>3、爱国主义教育法2024元旦起施行：将爱国主义教育贯穿学校教育全过程。</p><p>4、我国调整部分商品进出口关税，对部分抗癌药实施零关税。</p><p>5、国家统计局：2022年全国女性就业人员3.2亿人，占全部就业人员的比重为43.2%。</p><p>6、段永平向浙大再捐10亿+，支持母校“双一流”建设。</p><p>7、经济日报：摘牌倒逼景区提质升级。</p><p>8、7500米深，日本发现大地震海底断层悬崖，此次发现的断层有助于提高地震、海啸等灾害预测的准确度。</p><p>9、印度法院下令释放vivo两高管。</p><p>10、国家统计局：2022年中小学生《国家学生体质健康标准》达到优良的比例为55.1%。</p><p>11、以总理称加沙地带军事行动将持续数月。</p><p>12、美军研发无线能量“网”为无人机空中充电。</p><p>✨以自己的想法安排人生，即便在别人眼里有些离经叛道，至少还有机会活出自我。——《不安的哲学》</p><h3 id="名人语录">名人语录</h3><p>🗓️2024.01.01漫画国学👣<br>『元旦佳节』</p><p>💫【诗歌欣赏】</p><p>《今日歌》<br>【明】文嘉<br>今日复今日，今日何其少。<br>今日又不为，此事何时了。<br>人生百年几今日，<br>今日不为真可惜。<br>若言姑待明朝至，<br>明朝又有明朝事。<br>为君聊赋今日诗，<br>努力请从今日始。</p><p>💫【诗歌鉴赏】<br>这是一首劝勉诗。语言朴实无华，道理浅显易懂，就是叫人珍惜光阴，努力奋进。因为我们整个人生就是由每一个今日组成，每一天都做好今日事，“明日”才不会因为虚度了“昨日”而感到遗憾，那么，由无数个“今日”组成的今生，才会有所作为。</p><h3 id="日知日进">日知日进</h3><p><img src="../assets/image-20240102220932012.png" alt="image-20240102220932012"></p><h2 id="1-2日周二">1.2日周二</h2><p>每个人的花期不同，不必因为有人提前拥有而焦虑。重要的是，当你选择了你要的方式，那就坚定地走下去。</p><h3 id="英语角-2">英语角</h3><h3 id="今日晨读">今日晨读</h3><p>🍁2024.1.2晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。<br>——来自帆书《十分钟冥想》</p><p>👉【精彩选段】<br>冥想不是思考！它所做的不过是将一道明亮的光照耀在你的心灵上，所以你才更清楚地看到一切。</p><p>👉【作者之语】<br>无论我们感受如何，心灵的深层本质就像蓝色的天空一样，并没有发生变化。</p><p>👉【樊老师之语】<br>本书作者用生动故事和精妙比喻帮你破除误区，理解冥想精髓，带领你学习一些简单易学且非常有效的冥想方法，将其融入你的生活、工作和内心。每天给自己十分钟，拥抱清醒、平和与快乐，让我们的生活变得更加丰富而细腻。</p><h3 id="今日简报-2">今日简报</h3><p>今日简报</p><p>1月2日，星期二</p><p>1、百城二手住宅平均价格去年累计下跌3.53%，今年市场有望逐渐筑底企稳。</p><p>2、各地推动开展根治欠薪冬季专项行动，让农民工安“薪”过节。</p><p>3、新疆三文鱼火出圈，尼勒克县年产6000吨三文鱼。</p><p>4、中美两国领导人互致贺信庆祝两国建交45周年。</p><p>5、南方人涌入哈市洗浴店爆满，有游客排队搓澡要等两三小时。</p><p>6、雪乡住宿2床1炕一晚3000元？价格被指“高攀不起”，当地回应雪乡住宿价格被指过高。</p><p>7、多地降温降雪低温津贴引关注，多名户外劳动者称“没听说过”。</p><p>8、2023全国电影票房超549亿元。</p><p>9、国家移民局：去年12月6国超11万人次免签入境中国，观光休闲占七成。</p><p>10、日本本州西岸近海发生7.4级地震。</p><p>11、2023年泰勒巡演带动50亿美元，消费者支出泰勒巡演经济效益或超50个国家GDP。</p><p>12、丹麦女王玛格丽特二世宣布退位，王位将传给儿子。</p><p>✨最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。——《十分钟冥想》</p><h3 id="名人语录-2">名人语录</h3><p>💞拉卡拉创始人孙陶然曾说：<br>“我所有的成就，<br>一半来源于天资，<br>一半来源于复盘。”<br>可见复盘对人生的重要性。</p><p>人生没有白走的路，每一步走过的路，每一件经历过的事，都是人生的财富。拉卡拉创始人孙陶然曾说：“我所有的成就，一半来源于天资，一半来源于复盘。”</p><p>反思自己的不足之处，总结出过往的经验。好的方面继续保持，坏的方面及时改正。</p><p><img src="../assets/image-20240102221129250.png" alt="image-20240102221129250"></p><p><img src="../assets/image-20240102221119189.png" alt="image-20240102221119189"></p><blockquote><p>✍🏻如果用五到十年坚持做一件事，也许生活会发生巨变。</p></blockquote><h3 id="日知日进-2">日知日进</h3><p>🗓️2024.01.02漫画国学👣</p><p>🔹生活小困扰：朋友误会我了，该怎么办?</p><p>💫国学大智慧：人不知，而不愠，不亦君子乎?</p><p>🔸译文诠释：别人不了解我，我不怨恨(恼怒)，不也是君子吗?</p><p>✨迁移阅读：参差多态，乃幸福之源。（《不抱怨的世界》）</p><p><img src="../assets/image-20240102221307687.png" alt="image-20240102221307687"></p><p>农夫种地以前有一位农夫，做事总爱瞻前顾后。春天的时候，有人问农夫：“春天到了，你种麦子了吗？”农夫回道：“没，我担心天不下雨。”那人又问：“那你种棉花没？”农夫又回道：“没，我担心虫子吃了棉花。”那人再问：“那你种了什么？”农夫说：“什么也没种，我要确保安全。”</p><p>感悟：我们总是想的太多而做的太少。顾虑重重只会束手束脚，一事无成。生活永远充满不确定，有时放手一搏，大胆尝试，方能有所收获。</p><h2 id="1-3日周三">1.3日周三</h2><h3 id="英语角-3">英语角</h3><p>Life is too short for long-term grudges.<br>人生短暂，何必长期心怀怨念。</p><h3 id="今日晨读-2">今日晨读</h3><p>🍁2024.1.3晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生中最大的发现就是发现自己。<br>——来自帆书《心态》</p><p>👉【精彩选段】<br>我们无须思考我们是否“有成功特质”和“无成功特质”，那是固定型心态所关注的。一个更健康、有效的成长型观念是，“如果成功没有如期而至，那只是因为我没有付出足够的努力”。</p><p>👉【作者之语】<br>解决问题的方法永远都在问题之外。</p><p>👉【樊老师之语】<br>只有真正了解自己的心态，你才能更好地驾驶人生航船。听了这本书，你将从4个维度了解自己的心态，并收获一套科学改善心态的行动方案，从而更积极地面对工作与生活。</p><h3 id="今日简报-3">今日简报</h3><p>今日简报</p><p>1月3日星期三</p><p>1、中泰将从3月起永久互免对方公民签证。</p><p>2、日本石川县能登地区7.6级地震已致48人死亡。</p><p>3、印度人口76年增长近11亿！印度成世界人口第一大国。</p><p>4、东航第四架C919大型客机交付入列。</p><p>5、日媒：客机与海上保安厅飞机相撞，保安厅飞机死亡人数增至5人。</p><p>6、韩媒：袭击李在明嫌疑人为60多岁男性，供述中承认有谋杀意图。</p><p>7、乌称基辅等地遭袭，俄方对此无回应。</p><p>8、因反垄断未获批，百度宣布终止收购YY直播。</p><p>9、驻美大使馆回应中国留学生疑似遭绑架：系遭遇“虚拟绑架”电信诈骗。</p><p>10、强震造成日本多个核电站燃料池水溢出。</p><p>11、广东一地公示8名躺平休闲人员，当事人称不便回应，专家称躺平者标准并无法律支撑：应谨慎。</p><p>12、古茗向港交所提交上市申请。</p><p>✨人生中最大的发现就是发现自己。——《心态》</p><h3 id="名人语录-3">名人语录</h3><p>生活就像一面镜子，你对它微笑，它便会为你绽放欢颜。<br>从今天开始，用乐观代替沮丧，用行动代替懒散。<br>调节好自己的心态，成功和幸福才会不请自来。</p><p>你人生的起点并不是那么重要<br>重要的是你最后抵达了哪里<br>———— 巴菲特</p><h3 id="漫画国学">漫画国学</h3><p>🗓️2024.01.03漫画国学👣</p><p>🔹生活小困扰：对新事物，我总是不敢尝试，怎么办?</p><p>💫国学大智慧：司马牛问君子。子曰：“君子不忧不惧。”</p><p>🔸译文诠释：司马牛问怎样才是君子。孔子说：“君子不忧愁，不恐惧。”</p><p>✨迁移阅读：儒家学说的终极目标不是建立道德规范来束缚人，而是提供一种让人美好、艺术地生活的指南。</p><p>📔推荐书目：《孔子：人能弘道》</p><p><img src="../assets/image-20240103143404860.png" alt="image-20240103143404860"></p><hr><h2 id="1-4日周四">1.4日周四</h2><h3 id="英语角-4">英语角</h3><p>It’s no use going back to yesterday, because I was a different person then.<br>回到昨天毫无用处，因为今天的我已和过去有所不同。</p><h3 id="今日晨读-3">今日晨读</h3><p>❄️2024.01.04晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>只要人活得高兴，穷也不怕。<br>——来自帆书·李蕾讲经典《活着（上）》</p><p>👉【精彩选段】<br>我看着那条弯曲着通向城里的小路，听不到我儿子赤脚跑来的声音，月光照在路上，像是撒满了盐。</p><p>👉【作者之语】<br>人是为活着本身而活着，而不是为了活着之外的任何事物所活着。</p><p>👉【李蕾老师之语】<br>《活着》是一本真实的书，同时也很坚韧，它讲了一个人和他命运的复杂友情。不管遭遇什么，活着是不该也不能放弃的事情。</p><p><img src="../assets/image-20240105081848787.png" alt="image-20240105081848787"></p><h3 id="今日简报-4">今日简报</h3><p>今日简报</p><p>1月4日星期四</p><p>1、日本地震死亡升至73人。</p><p>2、成品油价迎来2024年首次上调，加满一箱92号汽油将多花8元。</p><p>3、科技部发文规范AI使用，禁用生成式人工智能直接生成申报材料。</p><p>4、12306客户端推出购票需求预填及起售提醒订阅。</p><p>5、日本撞机前通话记录公布，日本海上保安厅飞机未被允许进跑道。</p><p>6、俄罗斯钻石不再被允许进入欧盟。</p><p>7、美国国债总额首次达到34万亿美元。</p><p>8、伊朗克尔曼市发生爆炸致多人死伤。</p><p>9、无人机未经实名登记飞行最高罚2万。</p><p>10、哈佛历史上首位黑人校长辞职，卷入抄袭丑闻和校园反犹争议。</p><p>11、303名抖音主播被关闭收礼物权限，抖音直播新版健康分处罚正式生效。</p><p>12、哈尔滨市政府提醒宾馆酒店珍惜“出圈”机遇：不盲目调整价格。</p><p>✨只要人活得高兴，穷也不怕。——《活着（上）》</p><h3 id="名人语录-4">名人语录</h3><p>越是优秀的人，越是知道这世间没有任何捷径可走。<br>把每一件事情坚持做、用心做，才可能厚积薄发，收获意料之外的惊喜！</p><h3 id="漫画国学-2">漫画国学</h3><p>🗓️2024.01.04漫画国学👣</p><p>🔸生活小困扰：能说会道和踏实肯干，究竟哪个比较重要？</p><p>💫国学大智慧：子贡问君子。子曰：“先行其言而后从之。”</p><p>🔅译文诠释：子贡问怎样才能做一个君子。孔子说：“对于自己要说的话，要先实行了，然后再说出来，这样就够算是一个君子了。”</p><p>✨迁移阅读：曾国藩之于后人最大的意义是，他以自己的实践证明，一个中人，通过“陶冶变化”，可以成为超人。</p><p>📔推荐书目：《曾国藩的正面和侧面》</p><p><img src="../assets/image-20240105082045243.png" alt="image-20240105082045243"></p><h3 id="日知日进-3">日知日进</h3><p>📯你是否也有这样的情况？<br>想和家人好好说话，<br>可一开口就管不住脾气。<br>想换一份工作，<br>但一直没有勇气辞职。<br>在一段感情里内耗许久，<br>却又舍不得放手。</p><p>🌟为什么我们都清楚需要改变，却没法做到呢？<br>如果你也有类似这样的困境，<br>想要改变却不知道如何开始。<br>不妨读读这本《5%的改变》。<br>试着从5%的改变里，<br>去感受自己的变化，<br>去发现生活的不同。</p><p>萧伯纳曾说：很多时候，完成比完美更重要，在一次次完成中迭代，就是进步。</p><p>苛求完美，像是一场与自己的无穷斗争，只会搞得精疲力竭。而真正的蜕变，是以轻松的心态去接纳原来的我，以5%的改变去迎接崭新的我</p><h2 id="1-5日周五">1.5日周五</h2><h3 id="英语角-5">英语角</h3><p>Nothing is impossible, the word itself says “I’m possible”!<br>没有不可能，连“不可能”这个词自己都说：“不，可能！”</p><h3 id="今日晨读-4">今日晨读</h3><p>❄️2024.1.5晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>我们不能仅仅依靠善心和意志力就成为更好的人。<br>——来自帆书·非凡精读《善恶之源》</p><p>👉【精彩选段】<br>由共情直接激发出的助人行为，往往会有“只见树木不见森林”的效果，它把我们的注意力引导到一个个案上，但往往没法兼顾公平。</p><p>👉【作者之语】<br>我们创造出来的社会环境，可以把一个只拥有部分道德意识的婴儿，变成一个拥有强烈道德感的成年人。</p><p>👉【主讲老师之语】<br>这本书帮助我们认识到天生的道德直觉并不完美。道德本能很强大，我们需要好好理解它。个人的道德观、社会的道德规范都是从这些道德本能里发芽生长出来的。</p><h3 id="今日简报-5">今日简报</h3><p>今日简报</p><p>1月5日星期五</p><p>1、2024年购买新能源车免征购置税。</p><p>2、我国自研电动飞机成功首飞：最大平飞速度218km/h，航程1100km。</p><p>3、外交部回应中国留学生被美方强制遣返：敦促美方停止打压限制中国留学生。</p><p>4、我国将开展“信号升格”专项行动。</p><p>5、多家日媒：日本石川县震后失联人数已增至179人。</p><p>6、首个石墨烯制成的功能半导体问世，天津大学团队承担主要研究与攻关。</p><p>7、圆通、韵达和京东物流违规寄递烟花被约谈。</p><p>8、伊朗克尔曼省发生两起爆炸已致84人死亡。</p><p>9、四部门：大力推广智能有序充电设施，原则上新建充电桩统一采用智能有序充电桩。</p><p>10、美国多个州议会大厦收到炸弹威胁。</p><p>11、高校严控重复设置过热专业，2035年全国或有近200万教师过剩。</p><p>12、湖南将保障支付农民工工资切实纳入对市州人民政府考核。</p><p>✨我们不能仅仅依靠善心和意志力就成为更好的人。——《善恶之源》</p><h3 id="名人语录-5">名人语录</h3><p>总是充满期待的人，每一天都是全新的开始~</p><p>山不却垒土之功，故能成其高；海不避涓涓细流，故能成其大。这世上从来就没有一步登天的神话，有的只是日积月累、滴水穿石。</p><h3 id="漫画国学-3">漫画国学</h3><p>🗓️2024.01.05漫画国学👣</p><p>🔸生活小困扰：我们如何获得他人的尊敬?</p><p>💫国学大智慧：君子以仁存心，以礼存心。</p><p>🔅译文诠释：君子心里存在着仁爱，存在着礼义。</p><p>✨迁移阅读：我们希望对方以何种方式对待我们，我们也就必须以同样的方式对待他们——己所不欲，亦施于人。</p><p>📔推荐书目：《人性中的善良天使》</p><p><img src="../assets/image-20240105111556361.png" alt="image-20240105111556361"></p><h3 id="日知日进-4">日知日进</h3><blockquote><p>糟糕的坚持 也是一种坚持，也要比轻易的放弃要好</p><p>两次放弃坚持的东西   就是一种坏习惯的开始了</p></blockquote><blockquote><p>糟糕的坚持  也能够帮你保持一种身份，帮助你对抗这种世界外部的无序</p><blockquote><p>跑5分钟 走一圈 因为今天实在是不想动，那就溜一圈，也比放弃要好，你要对自己宽容一点，</p><p>至少你现在所做的一切还能帮助你保持一种身份</p><p>我们宁肯要糟糕的坚持，也要对抗这种轻易的放弃</p></blockquote></blockquote><p>🌻你的孩子在外遇到难题了，<br>你会是他第一个求助的人吗？<br>你的孩子犯错了，<br>他会有勇气跟你说真话吗？<br>你的孩子会主动跟你袒露自己的脆弱吗？</p><p>:sun_with_face:前几天，看到这样一则故事：<br>“有个7岁的小男孩和朋友打篮球时摔倒了，<br>把手给摔折了。他害怕被父母骂，<br>生生地忍着疼痛，没跟大人说。”<br>直到后面忍不了了，被父母发现了，<br>才送往医院治疗。</p><p>:full_moon:到底是什么原因，<br>让孩子有事宁愿自己扛，<br>也不愿告诉父母？</p><p>📚分享内容｜简单人生<br>📆分享递增｜Day251<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.5</p><p>禅宗思想里，有一个“看山理论”。它将人生分为了三重境界：看山是山，看山不是山，看山还是山。第一重境界的人，心思单纯，没啥城府，看见什么便是什么；第二种境界的人，经历了世事的摧残，觉得生活复杂，看什么都是雾里看花；而最高境界的人，饱经沧桑后，依旧能淡看人生起落，把一切看得简单而自然。人生最好的状态，是在复杂的世界里，做一个简单的人。的确，人生短短三万天，学会看轻得失，看淡纷扰，将复杂的生活简单过，就是最好的处世之道！</p><h2 id="1-6日周六">1.6日周六</h2><h3 id="英语角-6">英语角</h3><p>would die for you.<br>But I won’t live for you.<br>我愿意为你赴死，但不会指望你而活。</p><h3 id="今日晨读-5">今日晨读</h3><p>☃️2024.1.6晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>除了吃饭，生命还有更重要的事!<br>——来自帆书·李蕾讲经典《海鸥乔纳森》</p><p>👉【精彩选段】<br>天堂不是一个地点，也不是一段时间。天堂是一种完美的状态。</p><p>👉【作者之语】<br>不要只相信你眼睛看到的东西。要用你的悟性去看，理解你已经知道的东西，然后你会发现飞翔的真理。</p><p>👉【李蕾老师之语】<br>《海鸥乔纳森》是一部关于梦想与追求的寓言故事，故事里的海鸥乔纳森是个“异类”。对于他来说，飞翔远比吃饭更重要。因此在命运转折的时候，大多数海鸥选择安于现状，而他选择成为想要成为的自己。</p><h3 id="今日简报-6">今日简报</h3><p>今日简报</p><p>1月6日，星期六</p><p>1、市场监管总局：特斯拉召回超160万辆存在安全隐患电动汽车。</p><p>2、央视曝河南距今2300年的赵长城被拦腰截断。</p><p>3、IS宣布对伊朗克尔曼市的连环爆炸案负责。</p><p>4、无锡全面施行经常居住地登记户口制度。</p><p>5、快递新规禁止未经同意擅用快递柜，未经同意代收货最高罚3万元。</p><p>6、韩方称朝鲜发射200多发海岸炮，外交部：正密切关注形势发展。</p><p>7、美媒：美军一架B-1轰炸机试图降落时坠毁，事发时正执行训练任务。</p><p>8、深圳拟出新规推进城中村改造，涉全市约40%建面。</p><p>9、阿根廷法院发布禁制令，暂停阿根廷总统米莱提出的涉及300多项经济改革中的劳工法令。</p><p>10、浙江将组织千名科学家任中小学校科学副校长。</p><p>11、福彩回应网曝福彩主持人提前播报中奖球号：视频系拼凑嫁接，已报警。</p><p>12、广西警方通报“女大学生摆摊卖糖葫芦被壮汉威胁”：行拘3人。</p><p>✨人生是一场无人可替代的修行，生活则是修炼心性的真经。——《给孩子一生的安全感》</p><h3 id="名人语录-6">名人语录</h3><p>有时候，成长就如种子发芽一般，需要积蓄足够多的力量，才能冲破土壤。在人生的跑道上，一旦认准目标，就只管努力，总有一天会惊艳所有人。</p><h3 id="漫画国学-4">漫画国学</h3><p>🗓️2024.01.06❄️漫画国学👣</p><p>『小寒已至』</p><p>❄️小寒，是踏雪赏梅的好时节，也标志着一年中最寒冷的日子就要来了。出门游玩的时候，小朋友们一定要做好保暖工作，日常也要多食用一些温热的食物来滋补身体。相传，古时的南京人对小寒颇为重视，有吃菜饭的习俗。但每家每户的菜饭并不相同，比如有用青菜、咸肉片、生姜粒和糯米一起煮的，咸香可口。大家可以在家里邀请爸爸妈妈一起来制作哦!</p><p>📔小寒谚语:<br>🔸小寒暖，立春雪。<br>🔸小寒不寒，清明泥潭。<br>🔸小寒蒙蒙雨，雨水还冻秧。<br>🔸小寒一场白，来年收小麦。<br>🔸小寒胜大寒，常见不稀罕。<br>🔸小寒暖立春雪，小寒不寒立春雪。<br>🔸小寒时处二三九，天寒地冻冷到抖。<br>🔸小寒不寒大寒寒，大寒不寒倒春寒。</p><p><img src="../assets/image-20240106210118345.png" alt="image-20240106210118345"></p><h2 id="1-7日周天">1.7日周天</h2><h3 id="英语角-7">英语角</h3><p>Years fly by, but the heart stays in the same place.<br>时光飞逝，我心依旧。</p><h3 id="今日晨读-6">今日晨读</h3><p>❄️2024.1.7晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>生活本有千万可能，别让过去束缚了你。<br>——来自帆书·非凡精读《拥抱可能》</p><p>👉【精彩选段】<br>被动就是让别人替你做决定，争强好胜就是为别人做决定，坚定自信就是为自己做决定。</p><p>👉【作者之语】<br>生命中，所有能让你惊喜若狂的事情都来自于内心。没有人能把你心里的东西拿走。</p><p>👉【推荐之语】<br>《拥抱可能》是埃格尔对自己一生的回望，记录了坚韧、治愈、有尊严地生存、充满勇气地拥抱自由和幸福。</p><h3 id="今日简报-7">今日简报</h3><p>今日简报</p><p>1月7日，星期日</p><p>1、首张房票已发放！广州打响一线城市房票安置“第一枪”。</p><p>2、境外机构免费送航空爱好者设备窃取我国信息。</p><p>3、江苏新发现1例小p血型，目前我国仅有9人。</p><p>4、世界最大冰雪主题乐园！哈尔滨冰雪大世界拿下吉尼斯世界纪录。</p><p>5、不分节假日、不限身份，山西21所高校向社会开放。</p><p>6、学历入户放宽至大专！成都户籍新政来了，2月20日起施行。</p><p>7、中美联合研制全球首个由石墨烯材料制成的功能性半导体。</p><p>8、上海与嘉兴实现公共交通乘车码互联互通，后续将扩大至长三角11城。</p><p>9、美国阿拉斯加航空公司宣布：暂时停飞65架波音737 MAX 9飞机。</p><p>10、日本羽田机场撞机事故影响波及逾15万人，跑道预计8日重启。</p><p>11、美邦服饰“二代”接任7年亏损超32亿，创始人重新被提名董事。</p><p>12、英国多地洪水，气象局连发三百多个预警。</p><h3 id="名人语录-7">名人语录</h3><p>✨大脑的疲劳和压力都来自过去和未来：对过去的事情心有不甘，对未来的事情充满不安。——《高效休息法》</p><p>生命是一棵长满可能的树 ————米兰·昆德拉</p><p>每一天都是新的旅程，<br>不需要去改变任何人，<br>只需要不断的提升自己，<br>自然有同频人与你同行。</p><h3 id="漫画国学-5">漫画国学</h3><p>🗓️2024.01.07漫画国学👣</p><p>📖一周复习：</p><pre><code>🔸人不知，而不愠，不亦君子乎?     ——《论语·学而》🔸司马牛问君子。子曰:“君子不忧不惧。”     ——《论语·颜渊》🔸子贡问君子。子曰:“先行其言而后从之。”    ——《论语·为政》🔸君子以仁存心，以礼存心。     ——《孟子·离娄下》</code></pre><p><img src="../assets/image-20240108193200012.png" alt="image-20240108193200012"></p><p>💞美好的亲子共读时光开始啦<br>🎤今日共读内容<br>大禹治水</p><p>很久很久以前，<br>洪水经常泛滥。<br>大水淹没了田地，<br>冲毁了房屋，<br>毒蛇猛兽到处伤害百姓和牲畜，<br>人们的生活痛苦极了。<br>洪水给百姓带来了无数的灾难，<br>必须治好它。</p><p>当时，<br>一个名叫鲧(gǔn)的人领着大家治水。<br>他只知道筑坝挡水，<br>九年过去了，<br>洪水仍然没有消退。</p><p>他的儿子禹继续治水。<br>禹离开了家乡，<br>一去就是十三年。<br>这十三年里，<br>他到处奔走，<br>曾经多次路过自己家门口。<br>可是他认为治水要紧，<br>一次也没有走进家门看一看。</p><p>禹吸取了鲧(gǔn)治水失败的教训，<br>采用疏导的办法治水。<br>他和大家一起，<br>疏通了很多河道，<br>让洪水通过河道，<br>最后流到大海里去。<br>洪水终于退了，<br>毒蛇猛兽被驱赶走了，<br>人们把家搬了回来。</p><p>大家在被水淹过的土地上耕种，<br>农业生产渐渐恢复了，<br>百姓安居乐业，<br>重新过上了幸福的生活。<br>由于禹为人们作出了很大的贡献，<br>大家便把他称为大禹。</p><h2 id="1-8日周一">1.8日周一</h2><h3 id="英语角-8">英语角</h3><p>May we be together till forever.<br>朝暮与岁月并往，愿你我共至光年。</p><h3 id="今日晨读-7">今日晨读</h3><p>❄️2024.1.8晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生在世，做事只求尽心，而成败不必强求。<br>——来自帆书《战安庆》</p><p>👉【精彩选段】<br>所谓中年危机，无非就是人到中年因事业无成而产生的焦虑和惶恐，一旦跳出功名利禄的圈子，从宇宙的维度来看待世事，以恬淡冲融的态度来重新看待人生，危机带来的抑郁也就消泯了。</p><p>👉【作者之语】<br>乱极时站得定，才是有用之学。</p><p>👉【樊老师之语】<br>打开本期新书，为你还原曾国藩那段最艰难的时光，看他如何用谦虚自省的心态，战胜中年危机，走向完满的人生境界。</p><h3 id="今日简报-8">今日简报</h3><p>今日简报</p><p>1月8日   星期一</p><p>1、教育部：网传义务教育教学改革实验区“取消中考”等说法不实。</p><p>2、上海税务：个人通过网络买卖虚拟货币需要缴纳个人所得税。</p><p>3、外交部：中方决定对5家美国军工企业实施制裁。</p><p>4、广西河池回礼东北老铁：16个景区免首道门票。</p><p>5、今年首批银行业罚单开出，3家银行合计被罚千万！去年千万以上罚单超20张。</p><p>6、陕西省定边县一住户发生煤炭炉取暖中毒，致4人死亡。</p><p>7、预计最快10天可找到马航MH370！最新研究指向未经搜索区域。</p><p>8、日本气象厅：1日以来能登地区已发生1000余次地震。</p><p>9、腾讯微信团队致歉：私密朋友圈bug已彻底修复。</p><p>10、流感病例数激增，西班牙多地医疗系统超负荷！部分地区重启口罩强制令。</p><p>11、OpenAI和微软遭集体诉讼，被指控“窃取”他人作品训练AI模型。</p><p>12、中国在极地布放首个生态潜标。</p><p>✨人生在世，做事只求尽心，而成败不必强求。</p><h3 id="名人语录-8">名人语录</h3><h3 id="漫画国学-6">漫画国学</h3><p>🗓️2024.01.08漫画国学👣</p><p>🔸生活小困扰：别人总是看不到我的优点，我该怎么办?</p><p>💫国学大智慧：君子病无能焉，不病人之不己知也。</p><p>🔅译文诠释：君子只会惭愧自己没有能力，不怨恨别人不了解自己。</p><p>✨迁移阅读：问题不能在产生问题的维度得到解决，升一个维度，才能解决问题。</p><p>📔推荐书目：《升维》</p><p><img src="../assets/image-20240108193443684.png" alt="image-20240108193443684"></p><h2 id="1-9日周二">1.9日周二</h2><h3 id="英语角-9">英语角</h3><p>Through the crowd to you.<br>穿越人群，奔赴于你。</p><h3 id="今日晨读-8">今日晨读</h3><p>❄️2024.1.9晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>——来自帆书《减压生活》</p><p>👉【精彩选段】<br>无论是我们自己，还是身边的人，在改变自己的时候，都要学会借助外力和改变认知。改变，并没有使我们失去什么，而是获得了一个更好的自己。</p><p>👉【作者之语】<br>健康的四大基石：乐观的心情、均衡的饮食、充足的睡眠、适度的运动。</p><p>👉【樊老师之语】<br>本书作者从神经外科医生的视角提出减压的根本是通过改变生活习惯，来干预和调节自主神经，给神经“松松绑”，让你由内而外，击退压力。更从四大维度提出十余种减压实操法，让你告别疲劳、焦虑，重新回归愉悦的生活。</p><h3 id="今日简报-9">今日简报</h3><p>今日简报</p><p>1月9日   星期二</p><p>1、多国停飞波音737MAX9型客机。</p><p>2、广西龙门大桥顺利合龙。</p><p>3、贵州村超新赛季打响！村超带动旅游综合收入近60亿元。</p><p>4、东航C919机型9日起开始执飞京沪航线。</p><p>5、深圳欢乐谷过山车碰撞事故调查报告公布，9人被追责问责。</p><p>6、字节与腾讯谈判出售多款游戏。</p><p>7、日本一核电站因地震漏约1.98万升油入海。</p><p>8、黎巴嫩真主党一高级指挥官在黎南部被以方打死。</p><p>9、我国破获一起英国秘密情报局（MI6）间谍案，MI6利用第三国人员从事对华间谍活动。</p><p>10、巴基斯坦发生爆炸袭击事件，致5名警察死亡。</p><p>11、广州“房票安置”政策细化：广州房票全市通用可转让。</p><p>12、北京新增设集成电路专业职称，放宽个人企业参评限制。</p><p>✨整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>——来帆书APP 听《减压生活》</p><h3 id="名人语录-9">名人语录</h3><p>无论年纪，只有不断提升自己，眼界才会愈加开阔，也才能更有底气、更加从容地面对人生。<br>保持学习、持续向上，就是我们给生活最好的回馈</p><p>与其千方百计去改变别人，让自己陷入麻烦，不如专注于对方的优点，放过别人，也放过自己。</p><p>德国哲学家康德说：请接受任何一个独立灵魂的存在，哪怕有些你并不认可，但也尽可能试着去理解。</p><p>允许对方有不同于我们的喜恶，生活才不会只有一种颜色。</p><h3 id="漫画国学-7">漫画国学</h3><p>🗓️2024.01.09漫画国学👣</p><p>🔸生活小困扰：我有些偏科，只喜欢学数学，这样不可以吗?</p><p>💫国学大智慧：君子不器。</p><p>🔅译文诠释：君子不能像器皿一样(只有一种用途)。</p><p>✨迁移阅读：风会熄灭蜡烛，却会使火越燃越旺。对随机性、不定性和混沌也是一样：你要利用它们，而不是躲避它们。你要成为风，渴望得到风的吹拂。</p><p>📔推荐书目：《反脆弱》</p><p><img src="../assets/image-20240109111933885.png" alt="image-20240109111933885"></p><h2 id="1-10日周二">1.10日周二</h2><h3 id="英语角-10">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-9">今日晨读</h3><h3 id="今日简报-10">今日简报</h3><h3 id="名人语录-10">名人语录</h3><h3 id="漫画国学-8">漫画国学</h3><h2 id="1-11日周二">1.11日周二</h2><h3 id="英语角-11">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-10">今日晨读</h3><h3 id="今日简报-11">今日简报</h3><h3 id="名人语录-11">名人语录</h3><h3 id="漫画国学-9">漫画国学</h3><h2 id="1-12日周二">1.12日周二</h2><h3 id="英语角-12">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-11">今日晨读</h3><h3 id="今日简报-12">今日简报</h3><h3 id="名人语录-12">名人语录</h3><h3 id="漫画国学-10">漫画国学</h3><h2 id="1-13日周二">1.13日周二</h2><h3 id="英语角-13">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-12">今日晨读</h3><h3 id="今日简报-13">今日简报</h3><h3 id="名人语录-13">名人语录</h3><h3 id="漫画国学-11">漫画国学</h3><h2 id="1-14日周二">1.14日周二</h2><h3 id="英语角-14">英语角</h3><p>Live up to the tender years<br>不负人间温柔岁月。</p><h3 id="今日晨读-13">今日晨读</h3><p>❄️2024.1.14晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>四大运动指南 ，教你如何正确锻炼。<br>——来自帆书·非凡精读《锻炼》</p><p>👉【精彩选段】<br>人体哲学与人生哲学同样重要。在健康面前，每个人都不是一座孤岛，我们良好的健康状况是可以相互影响的。</p><p>👉【作者之语】<br>不管是快走、短跑，还是跳舞，挑一个适合自己、自己喜欢的动起来，我们的身体将会因此受益无穷。</p><p>👉【馆长推荐】<br>本书从人类学和进化学的角度探讨我们的身体，为我们解答关于锻炼的种种问题。</p><h3 id="今日简报-14">今日简报</h3><p>今日简报</p><p>1月14日    星期日</p><p>1、发改委推出第七批重大外资项目，计划总投资超150亿美元。</p><p>2、六部门明确：进一步加强适老化无障碍出行服务，优先给老年人安排下铺。</p><p>3、气象台：北方大部地区将迎来大范围雨雪天气，局地降温可达15℃。</p><p>4、多地探索教师退出机制，以转变岗位、待岗培训、解聘为主要途径。</p><p>5、全球新冠加速蔓延！感染科专家：春运将至护好“脆弱人群”。</p><p>6、2024年台湾地区领导人和民意代表选举结果揭晓：民进党候选人赖清德、萧美琴当选台湾地区正副领导人。</p><p>7、上海放松限购：青浦、奉贤两区取消单身限购，社保3年即可买房。</p><p>8、河南平顶山煤矿事故已确认10人遇难，6人失联，其余人员全部脱险。</p><p>9、2023中国城市海外网络传播力前三名为上海、北京、杭州。</p><p>10、2.89万亿美元！微软超越苹果重新成为全球市值最高的公司。</p><p>11、加沙地带逾2.3万名巴勒斯坦人死亡，190万平民流离失所。</p><p>12、美军再次空袭胡塞武装，拜登政府考虑重新将也门胡塞武装列为恐怖组织。</p><p>✨审视自己的想法，而不是根据自己的想法来看问题。<br>——来帆书APP 听《跳出头脑，融入生活》</p><h3 id="名人语录-14">名人语录</h3><h3 id="漫画国学-12">漫画国学</h3><p>🗓️2024.01.14☀漫画国学👣</p><p>🔸国学小课堂：孔子推崇的同时代君子有哪些?</p><pre><code>在周朝是老子，在卫国是蘧伯玉，在齐国是晏子，在楚国是老莱子，在郑国是子产，在鲁国是孟公绰。</code></pre><p>✨迁移阅读：看2000年前的圣人如何练成自我。</p><p>📔推荐书目：《孔子如来》</p><p><img src="../assets/image-20240114172254529.png" alt="image-20240114172254529"></p><h3 id="故事角">故事角</h3><p>三国时期，<br>孙权送给曹操一只大象。<br>曹操带领大臣们和小儿子曹冲，<br>一同去看。</p><p>大象又高又大，<br>人站在它跟前，<br>只能够到它的肚子，<br>曹操对大臣们说：<br>“这只大象真是大，<br>可是到底有多重呢？<br>你们哪个有办法称一称？”<br>大臣们纷纷议论起来，<br>一个大臣说：<br>“只有造一杆很大很大的秤来称。”<br>而另一个大臣说：<br>“这可要造多大的一杆秤呀！<br>我看只有把它宰了，切成块儿称。”<br>大臣们想了许多办法,<br>但都行不通。</p><p>这时，<br>曹冲对曹操说:“我有一个办法。”<br>曹操就让他说来听听。<br>曹冲把办法说了，<br>曹操一听连连叫好。<br>对大臣们说:“走！咱们到河边看称象去!</p><p>众大臣跟随曹操来到河边，<br>河里停着一只大船，<br>曹冲叫人把大象牵到船上，<br>等船身稳定了，<br>在船舷上齐水面的地方，<br>刻了一条线。</p><p>他再叫人把大象牵到岸上来，<br>把石头往船上装，<br>船身就一点儿一点儿往下沉。<br>等船身沉到刚才刻线的地方，<br>曹冲就叫人停止装石头。<br>大臣们起先还摸不清是怎么回事，<br>看到这里不由得赞叹不已：&quot;好办法!好办法！”<br>现在谁都明白了，<br>只要把船里的石头分别称一下，<br>再把重量加起来，<br>就知道大象有多重了。</p><h2 id="1-13日周二-2">1.13日周二</h2><h3 id="英语角-15">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-14">今日晨读</h3><h3 id="今日简报-15">今日简报</h3><h3 id="名人语录-15">名人语录</h3><h3 id="漫画国学-13">漫画国学</h3><h1>独白记录</h1><h3 id="1-1日独白">1.1日独白</h3><blockquote><p>从家里回来</p><p>累并快乐着</p><p>从姨家里吃晚饭 回家 回济南</p></blockquote><h3 id="1-2日独白">1.2日独白</h3><p>成大事不在于力量的大小而在于能坚持多久？ 五到十年之内如果你坚持做一些事情 你的生活会发生巨变！ 就是我们日常生活当中的一点点 微小的改善</p><p>这个让我想到了坚持晨读和夜读的力量</p><blockquote><p>早上果然不能看 各种的  会想一天</p><p>炮</p></blockquote><p>正念冥想应对胡思乱想</p><p>把想法当成天空的云，看着它，来来去去，时而乌云密布，时而蓬松轻柔</p><h3 id="1-3日独白">1.3日独白</h3><blockquote><p>关于习惯</p></blockquote><p><img src="../assets/image-20240103190120380.png" alt="image-20240103190120380"></p><blockquote><p>关于产品价值</p></blockquote><p>情绪，是下一个赛道。<br>【原文引用】<br>一个产品有三种价值。一是资产价值，二是功能价值，三是情绪价值。<br>【核心概念转述】<br>每一个产品都可以挖掘它三个方面的价值。第一方面是资产价值，资产价值也就是这个产品的天然属性。二是功能价值，也就是这个产品所带来的实用性。三是情绪价值，也就是为了获得某种情绪或感受而愿意支付的价值。这个产品的增值价值、附加价值。<br>【个人体验】<br>我想用几个例子来进一步理解如何去看待一个产品的三种价值，比如说一杯奶茶。它的售价是15块钱，这就是它的资产价值，它的价值是由它的人工成本，它的配料来确定的。同时一杯奶茶，它可以解渴，也可以饱腹，可以当早餐，也可以做下午茶，这就是它的功能价值。那么如果一杯奶茶是在冬天的清晨，由你的男朋友专门为你点送的，他就具有了情绪价值，是温暖，是爱，是无价的。<br>这样我们就比较好理解，为什么所有迪士尼的产品都会有溢价，再比如说像去年的冰墩墩大火大卖，溢价不止十倍以上。这也是我们常说的，要讲好一个故事，把历史，把文化，把情感带进去，也是现在IP打造的核心内涵。<br>我们经常会强调客户的满意度，客户的满意度也分几种。第一种不达预期的，那一定是不满意，第二种符合预期。其实只能算接受、认可。只有超出预期给客户带来惊喜才能是真正的满意。海底捞的服务为什么被奉为教科书式的经典，同样是因为被赋予了超值的情绪价值。<br>从银行的角度来看，产品的价格越来越低。各种的减费让利尽可能多的惠及客户。 同时也通过各种活动，各种权益来吸引客户。价格战已经卷到不能再卷。而产品的功能属性不断的在开发迭代，基本上是属于同质化的。谁能为客户带来超预期的情感体验，才是能够吸引客户，稳定客户，获取最大份额的利器。<br>【行动指南】<br>直营经理作为线上经营的主力。从产品的价格、产品的功能上和线下的营业机构都是同质化的。但是又缺乏和客户面对面的交流，客户的信任感天然是有差距的。<br>直营经理的人设打造，不断注入有情感，有温度的服务。将是需要我们不断去思考，去尝试，改进，迭代的方向。<br><a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=255360&amp;tag_name=%E5%88%98%E6%B6%A6&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#刘润 </a>/ <a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=240257&amp;tag_name=%E8%90%A5%E9%94%80&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#营销 </a></p><blockquote><p>关于怎样选择 看书 和选择一个是否要做的东西</p><blockquote><p>跃迁知识体系+ 读书投资</p></blockquote></blockquote><blockquote><p>关于笔记</p><blockquote><p>探索下浮墨卡片 + 成段的笔记</p></blockquote></blockquote><p><img src="../assets/image-20240103190109567.png" alt="image-20240103190109567"></p><blockquote><p>精选出所有的点赞量大的文章 进行写文+ 应用+ 反馈【社区里边的】</p></blockquote><p>认同共读的三大原则：<br>1、成年人原则<br>2、自助餐原则<br>3、以自己为圆心<br>先让自己快乐起来，再去体验：善思会写的快乐！</p><p>长期主义，乐观主义</p><blockquote><p>关于金钱</p></blockquote><p>金钱心理学：财富的转机和钱给你带来什么？<a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=254525&amp;tag_name=%E9%87%91%E9%92%B1%E5%BF%83%E7%90%86%E5%AD%A6&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#金钱心理学 </a><br>个人转述：什么时候你会感觉到财富转机的出现，不一定是说你需要拥有多少钱？而是你开始拥有的能力或技能（不是体力或时间），能轻松地换取到满足你生存所需的钱，这代表财富转机的出现。而钱最终能给人带来什么？一是允许你能自由控制自己的时间，二是可以不用为了生存做讨厌的事，接触不喜欢的事和人。概括两字就是“选择”的底气。<br>个人体验：刚从体制内辞职的一两年，我会有一种失重感，这种失重是彷徨和迷茫，满腔热血但迷茫，深耕哪个行业哪个岗位？广告行业、商务拓展、政务公关？每个我都细细思量过，适不适合自己，有没有前途，容不容易被替代？直到在财富管理领域保险行业获取一批客户，以及自己的公司进入平稳后，知道自己的财富转机已经到来，我不再需要再找工作，不再面临中年危机。我的转型第一步目标已经实现。感谢过去支持信任的朋友们，感恩！<br>行动指南：基础的架构已经搭建，那就要去做有意义和扩大规模的事。要再持续输入和输出，在亲子教育方面，不断积累，输出一本客观、理性，分享当父母的书和课。在财务规划团队建设方面，2024要新上台阶，更加珍惜每个合作伙伴，当个好教练，而不仅仅是好人！</p><p>📚分享内容｜活出心花怒放的人生<br>📆分享递增｜Day249<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.3</p><p>活出心花怒放的人生的有几点收获：<br>1 人需要从多个维度来认识自己的生活。健康、愉悦、良好的人际关系、一定程度的社会认可，这是很重要的。对于人来讲，这样的需要才是我们真正的需要。<br>2 你的行动反过来会影响你的想法。多则惑，简单的生活更容易快乐；闻香和深呼吸是调整情绪的最快方法！<br>3 找到工作的意义，除了收入之外，它的价值，对自己的帮助，对社会的帮助，好的团队关系。工作的幸福感，很重要。<br>4 爱需要你去感受才能够有。感谢亲人的付出，望着他们的眼睛，主动拥抱他们！<br>5 孩子是爱的结晶，爱的寄托。不是自我超越的工具。培养孩子的审美、创造和同理心。千万别和别的孩子比。<br>6 少而精才是好，比如孩子的玩具等。现在结婚恋爱的欲望下降，也是看到的美女太多了。<br>7 不开心的时候要立马做点事。嗅觉是让自己开心最快的方法，立竿见影。母亲闻孩子的味道也能感受幸福。副交感神经调动会让人开心，如深呼吸。<br>8 最后借用彭凯平教授的话“再好的方法你不学，你更不信，一点作用都没有的。”共勉！</p><h3 id="1-4日独白">1.4日独白</h3><blockquote><p>跟广阔合作 完成项目可视化 和 对应的代码框架 【不得不说 跟人合作 做项目确实会比较高效】</p></blockquote><h3 id="1-5日独白">1.5日独白</h3><blockquote><p>项目推进 做完ppt</p><p>零零散散 做了一天</p><p>也是不知道咋做，也是不知道怎样面对</p></blockquote><h3 id="1-8日独白">1.8日独白</h3><p><img src="../assets/image-20240108221205689.png" alt="image-20240108221205689"></p><h3 id="1-12日独白">1.12日独白</h3><h4 id="积极主动，操作在我">积极主动，操作在我</h4><p>在自己的选择和回应之间 不断抓住自己的自由度和空间，给与主动的、建设性的回应，并不断的为所有的选择做出积极的行动，不断的把自己的掌控权来进行扩大【方法论包括三个圈的应对方法】来不断的实现自己的人生。</p><p>同时观察并察觉自己的负面言论，不断的去停下来思考这些言论背后的原因和解决这些负面语言的可能性，并问自己能为这些可能性去做些什么，一旦问出这些话就能够不断的扩宽自己人生的可能性。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解5基于图扩散卷积网络的二人动作语义识别</title>
      <link href="/project/posts/d937b92.html"/>
      <url>/project/posts/d937b92.html</url>
      
        <content type="html"><![CDATA[<h1>Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition of Two-Person Actions基于图扩散卷积网络的二人动作语义识别</h1><h2 id="Abstract">Abstract</h2><p>​         Graph Convolutional Networks (GCNs) have successfully boosted skeleton-based human action recognition. However, existing GCN-based methods mostly cast the problem as separated person’s action recognition while ignoring the interaction between the action initiator and the action responder, especially for the fundamental two-person interactive action recognition. It is still challenging to effectively take into account the intrinsic local-global clues of the two-person activity. Additionally, message passing in GCN depends on adjacency matrix, but skeleton-based human action recognition methods tend to calculate the adjacency matrix with the fixed natural skeleton connectivity. It means that messages can only travel along a fixed path at different layers of the network or in different actions, which greatly reduces the flexibility of the network. To this end, we propose a novel graph diffusion convolutional network for skeleton based semantic recognition of two-person actions by embedding the graph diffusion into GCNs. At technical fronts, we dynamically construct the adjacency matrix based on practical action information, so that we can guide the message propagation in a more meaningful way. Simultaneously, we introduce the frame importance calculation module to conduct dynamic convolution, so that we can avoid the negative effect caused by the traditional convolution, wherein the shared weights may fail to capture key frames or be affected by noisy frames. Besides, we comprehensively leverage the multidimensional features related to joints’ local visual appearances, global spatial relationship and temporal coherency, and for different features, different metrics are designed to measure the similarity underlying the corresponding real physical law of the motions. Moreover, extensive experiments and comprehensive evaluations on four public large-scale datasets (NTU-RGB+D 60, NTU-RGB+D 120, Kinetics-Skeleton 400,SBU-Interaction) demonstrate that our method outperforms the state-of-the-art methods.</p><p><strong>摘要:</strong></p><p>图卷积网络（GCNs）已成功提升基于骨架的人体动作识别。然而，现有的基于GCN的方法大多将问题视为单独的人物动作识别，忽视了动作发起者和动作响应者之间的相互作用，尤其是对于基本的双人互动动作识别。有效考虑双人活动的固有局部-全局线索仍然具有挑战性。此外，GCN中的消息传递依赖于邻接矩阵，但基于骨架的人体动作识别方法往往使用固定的自然骨架连接性计算邻接矩阵。这意味着消息只能沿着网络的不同层或不同动作的固定路径传递，这极大地降低了网络的灵活性。为此，我们提出了一种新颖的图扩散卷积网络，通过将图扩散嵌入GCN，用于双人动作的语义识别。在技术上，我们基于实际动作信息动态构建邻接矩阵，以便以更有意义的方式引导消息传播。同时，我们引入了帧重要性计算模块进行动态卷积，以避免传统卷积引起的负面影响，其中共享权重可能无法捕捉关键帧或受到嘈杂帧的影响。此外，我们全面利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并针对不同特征设计不同的度量标准，以衡量相应运动的真实物理法则的相似性。此外，对四个公共大规模数据集（NTU-RGB+D 60、NTU-RGB+D 120、Kinetics-Skeleton 400、SBU-Interaction）进行了广泛的实验证明和全面的评估，证明我们的方法优于现有方法。</p><p><strong>总结:</strong></p><p>本研究针对基于骨架的人体动作识别中双人互动的问题，提出了一种新颖的图扩散卷积网络（GCNs）方法。与现有方法不同的是，该方法考虑了动作发起者和动作响应者之间的相互作用，并通过在GCNs中嵌入图扩散来更有效地利用双人活动的局部-全局线索。在技术上，研究动态构建邻接矩阵以引导有意义的消息传播，并引入帧重要性计算模块进行动态卷积，以避免传统卷积的负面影响。综合利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并设计不同的度量标准以测量相应运动的相似性。实验证明，在四个大规模数据集上，该方法在双人互动动作识别方面优于现有方法。</p><h2 id="I-INTRODUCTION">I. INTRODUCTION</h2><p><strong>I. 引言</strong></p><p>近年来，随着计算机视觉的快速发展，人体动作识别引起了越来越多的关注，这对于自然人机交互、动态行为分析和安全监控至关重要。由于动作理解依赖于信息丰富的表示，视频中的人体动作识别可以利用各种特征，如颜色、深度和骨架。许多方法尝试通过从原始视频帧中提取时间上的视觉运动线索来分析动作。然而，这些方法很容易受到复杂的背景、间歇性遮挡、自遮挡、视角变化和计算成本的挑战。</p><p>随着3D深度摄像机和先进的人体姿势估计算法的快速发展，获取动态3D骨架序列变得方便。骨架基动作识别方法在过去十年中取得了显著的进展，具有对复杂背景和动态环境的灵活适应性，以及计算和存储的高效性。然而，现有的基于骨架的人体动作识别方法通常侧重于单个人的动作识别。在现实生活中，群体活动和多人互动对于理解人类行为更为重要。作为群体活动的基础，双人互动识别成为本文的技术焦点。</p><p>近年来，基于深度学习的方法取得了显著的性能提升。通常，循环神经网络（RNN）通常学习骨架序列的时间上下文，而卷积神经网络（CNN）更倾向于空间线索。然而，同时考虑时间和空间信息仍然很难。因此，提出了在骨架结构上进行深度学习，通过这种方式，网络可以同时集成时间和空间信息。例如，时空图卷积网络（ST-GCN）已经成为骨架识别的最先进骨干。ST-GCN连接具有自然空间连接性的人体关节，并在连续帧中建立相同关节之间的时间连接。其邻接矩阵是通过构建的图形建立的，然后分为三部分以捕获三种运动形式：向心、静态和离心。自那时以来，基于各种ST-GCN的工作提供了一种新的语义动作识别方向，并取得了令人印象深刻的成果，但仍存在一些局限性。</p><p>首先，ST-GCN及其变体的骨架结构是固定的，仅源自身体的自然连接性，在某些需要关节之间进行长距离消息传递的动作（例如“拍手”）时，信息将大大减弱。其次，对于双人行为，ST-GCN及其变体将分成两个独立的样本进行处理，忽略了人体关节之间的连接。第三，在时间上，上述方法中每一帧的关键性相同，这使得难以处理现实中多个连续动作和“小动作”的影响。第四，ST-GCN仅使用关节作为特征，其变体添加了骨骼和运动特征，但所有上述方法都使用了距离导出的高斯相似性作为相似性测量。这很难捕捉物理运动中的相对位置关系。第五，对于ST-GCN的改进往往引入了注意机制以提取用于识别的关键区域，或者设计复杂的网络以捕获关键信息。这些可学习的方法意味着更复杂的网络结构。</p><p>为了缓解上述问题，如图1所示，我们提出了一种用于基于骨架的双人动作语义识别的图扩散卷积网络。在本文中，引入图扩散的思想以建立任意两个关节之间的连接，使得图结构不仅连接同一身体的远距离关节，还连接两个互动人的关节。使用固定的图结构作为先验知识来引导网络学习，并使用通过图扩散获得的图结构作为学到的知识来引导网络快速收敛。通过将关键关节的轨迹叠加来计算帧的重要性，使得时间维度上的卷积具有动态性，并且动态搜索时间扩散的源帧可以促进识别关键动作并屏蔽小动作。对于连续动作，此操作可以自适应地对不同动作进行识别而无需预先分割，这接近于现实生活中对视频的语义理解。</p><p>同时，受2s-AGCN的启发，我们引入“骨”特征，为了与真实物理运动保持一致，我们考虑3D坐标的更高阶导数，如速度、加速度。通过大量实验证明，更高阶导数会引入噪声，而平滑的轨迹会大大降低区分度。因此，我们使用3D坐标、骨骼、速度作为三个特征，并使用余弦相似性以及高斯相似性来测量肢体的相对位置，以获得更具物理意义的结果。</p><p>值得一提的是，空间和时间均以手工方式设计，也就是说，我们没有引入复杂的网络，而是通过统计分析以数学方式提取关键信息。特别是，本文的显著贡献可总结如下：</p><ul><li>我们通过在基于自然人体骨架连接性构建的先验骨架图上进行扩散来提取扩散图结构。先验骨架图用于使网络收敛到全局最优解，而扩散图用于加速收敛和提高灵活性。</li><li>我们提出了基于图扩散的动态卷积，通过在远距离关节和不同身体之间建立直接连接，使GCN的消息传递超过直接邻居。</li><li>我们为时间扩散的源帧引入了动态搜索方案，既可以屏蔽由小动作引起的噪声，又可以促进对连续多个动作的自适应分割，这与现实生活中对视频的语义理解接近。</li><li>我们设计了不同的相似性度量来挖掘与不同特征相关的不同潜在空间，使特征融合更符合动作的相关物理规律。</li></ul><p><strong>引言总结：</strong></p><p>本文介绍了对于实际骨架动作识别的一个新的子领域，并提出了基于信息理论的原则，引入了实际图卷积网络（RW-GCNs）来在NTU-RGB-D-120数据集上实现94.16%的最新准确度，相较于ST-GCN基线具有3.02倍的低延迟。此外，RW-GCNs可以在基线实现的基础上实现10倍的低延迟，并且准确度仅降低了3.8%。对Northwestern UCLA数据集的评估显示，RW-GCNs可以实现90.4%的准确度，比基线ST-GCNs低32.5倍的延迟。这是尽管在验证和训练中存在空间关键点噪声。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，其在Northwestern UCLA数据集上比基线ST-GCNs低32.5倍的延迟，并在保持71.8%准确度的同时。这一切都是通过隐私感知和可扩展的边缘计算为中心的方法实现的，在这种方法中，每个节点的系统成本可以降低10倍，同时仍然保持在场景复杂性（15.6至5.5 ApS）范围内的吞吐量。这项工作标志着实际骨架动作识别子领域的开始。通过设计RW-GCNs，本文希望促进以前不可行的新边缘计算应用的设计和创建。然而，我们认为这个新兴领域仍然存在许多挑战，如时间变化、环境特定场景动态和进一步的应用特定约束。</p><h2 id="II-RELATED-WORKS">II. RELATED WORKS</h2><h3 id="A-Skeleton-Based-Action-Recognition">A. Skeleton-Based Action Recognition</h3><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别</strong></p><p>早期的基于骨架的人体动作识别方法通常采用手工设计的特征来捕捉关节的运动规律[12]，然而，这种手工设计的特征往往不够丰富，因此结果通常令人不满意。实际上，动作识别强烈依赖于人体的表示和分析[13]。随着深度神经网络的发展[14]，数据驱动的方法引起了广泛关注。例如，基于RNN的方法使用坐标作为序列数据来表示人体关节[15]。长短时记忆网络由于其在建模序列数据的依赖性和动态性方面的优势，展现出良好的性能[16][17]，用于提取长期语义信息[18]。将3D坐标映射到基于CNN的伪图像上[19]，而伪图像的第三维用于表示时间动态。</p><p>图卷积的引入使得动作识别更加专注于基于骨架的方法，从而不受复杂背景的干扰，并且极大地提高了识别准确性。2s-AGCN[8]提出了一种两流自适应图卷积网络，并通过注意模块以数据驱动的方式学习新的拓扑结构，增强了网络的灵活性。AS-GCN[7]采用编码器-解码器结构捕获特定于动作的潜在依赖性，建立了无自然连接性的关节之间的连接，形成广义骨架图以获取更丰富的信息。ST-TR[5]引入Transformer[20]处理长时依赖关系，同时在时空中引入自注意机制以提取关键信息。FDGCN[21]设计了一个聚焦扩散图卷积网络，充分探索时空上下文。MS-G3D[22]提出了一个强大的特征提取器，捕获了先前未被注意到的多尺度时空特征。DC-GCN[10]构建了一个解耦图卷积以增强骨架动作识别的图建模能力，并引入了注意引导的DropGraph模块，有效缓解了图卷积网络中的过拟合问题。AGC-LSTM[23]提出了一个增强型注意图卷积LSTM网络，不仅可以捕获空间配置和时间动态中的判别特征，还可以探索时空域之间的共同关系。STGR[24]设计了一种新颖的时空图路由方案，用于骨架动作识别，自适应地学习物理上相距较远的骨架关节的高阶连接关系。</p><p>人与人之间的相互动作识别是人体活动分析的重要研究分支[25]，但由于相互遮挡的原因仍然具有挑战性。在人体动作识别中，如果将动作划分为单个人进行识别，将缺乏对人体各关节之间关联的关注。许多文献已经证明，单个人体的远端关节对于不同动作具有不同的重要性。同样，在两人互动中，需要更多关注两者关节之间的关系。关节的时间运动轨迹应是区分相似动作的关键因素，而大多数先前的方法未能充分利用视频段之间的时空关系。基于图的方法可以以灵活的方式处理骨架数据，以探索骨架关节之间的关系。时空图卷积网络（ST-GCN）[4]是第一个使用图卷积进行动作识别的模型，由于骨架数据的简单性和高效性，ST-GCN实现了最先进的准确性。因此，在本文中，我们采用ST-GCN作为基准模型。</p><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别总结</strong></p><p>在骨架基础动作识别领域，传统方法采用手工设计的特征，但这些特征往往信息不足，效果不佳。随着深度神经网络的兴起，数据驱动的方法备受关注，如RNN和CNN等。引入图卷积后，骨架方法更专注于动作识别，避免了复杂背景的干扰，并大幅提高了准确性。</p><p>具体模型包括2s-AGCN、AS-GCN、ST-TR、FDGCN、MS-G3D、DC-GCN等，它们通过自适应卷积、注意机制、编码器-解码器结构等方式增强网络灵活性，捕获更丰富的时空特征。这些模型在骨架动作识别中取得显著成果，但也存在一些局限性，如固定的骨架结构、对两人互动关系的忽略等。</p><p>为解决这些问题，文章提出了基于图扩散的动态卷积网络，引入图扩散建立关节连接，通过动态卷积适应不同距离和不同身体之间的连接，提高信息传递效率。同时，通过关键关节轨迹的叠加计算帧的重要性，使时间维度的卷积具有动态性，能够识别关键动作并抑制小动作的噪音。该方法综合利用了多维特征，通过不同相似度度量捕捉特征之间的物理关系。总体而言，该方法通过图扩散和动态卷积有效地应对了骨架动作识别中的挑战。</p><h3 id="B-Graph-Neural-Network">B. Graph Neural Network</h3><p><strong>B. 图神经网络</strong></p><p>图神经网络[31]使用节点表示对象，使用边表示对象之间的关系，并在图上应用图卷积。图卷积网络的构建通常遵循两个原则：空间视角[32]和频谱视角[33]。基于频谱理论的方法计算图拉普拉斯矩阵[34]的特征值和特征向量，并借助图傅里叶变换在频域进行图卷积，而无需从图中提取局部区域的连接。与频谱理论相反，基于空间理论的方法需要知道节点及其邻居以执行卷积操作[34]。在本文中，采用空间理论构建图卷积核，同时引入图扩散使我们的模型在频谱视角上更具可解释性。</p><p>图卷积是图神经网络的核心，通常采用直接邻居进行消息传播[37]。动作识别任务的最关键因素在于关节共存的帧内表示和骨架时间演变的帧间表示[38]。关节之间的连接不仅限于直接邻居，还包括更本质相关的关节。例如，在拍手动作中，两只手没有自然连接，但却密切相关[5]。因此，直接邻居的传播将削弱远离但密切相关关节之间的消息[6]。图扩散卷积（GDC）[37]支持超出直接邻居的消息传递，可以有效提取关键区域的不同动作，削弱无用的边。同时，GDC与频谱理论密切相关，可以结合空间理论和频谱理论的优势，突破直接邻居消息传递的局限性。因此，我们将采用图扩散卷积的思想，自适应地构建不同动作的骨架图，以提高动作识别的准确性。</p><p><strong>总结：</strong> 图神经网络是通过节点和边表示对象及其关系，并应用图卷积进行信息处理的模型。构建图卷积网络可基于空间或频谱视角，本文选择空间理论。图卷积是核心，通常使用直接邻居进行消息传播，但对于关节间的复杂关系，引入图扩散卷积（GDC）支持超出直接邻居的消息传递，提高了对不同动作关键区域的识别能力。GDC结合了空间和频谱理论的优势，通过自适应构建不同动作的骨架图，有效突破了传统直接邻居消息传递的限制，提高了动作识别的准确性。</p><h2 id="III-PRIOR-SKELETON-GRAPH">III. PRIOR SKELETON GRAPH</h2><p>我们的先验骨架图以关节为图节点，以人体的自然连通性为空间边，相邻框架中相同的关节连接为时间边。除了关节特征外，我们还使用了骨骼数据的二阶信息(骨骼的长度和方向)，这自然对动作识别[8]更具信息和鉴别性。为了引入物理运动方向的大小，我们添加了速度特征，所以涉及到的特征是关节(3d坐标)，骨骼(骨骼数据)和速度。</p><h3 id="A-Skeleton-Graph-Construction">A. Skeleton Graph Construction</h3><p>构建的两人交互骨架模型分为两部分:一部分是来自人体自然连接的先验知识，另一部分是通过数据驱动的方式获得的个性化模型。先验模型主要用于利用先验知识引导网络学习，而个性化模型产生动态骨架，通过图扩散获得动态骨架，主要用于引导网络更快地收敛，同时增强固定骨架的远距离消息传递。图2为两人动作的先验骨架图，其中棕色实心圆表示人体关节，棕色线表示人体自然连通性，蓝色线表示相同关节的时间轨迹。</p><h3 id="B-Feature-Selection">B. Feature Selection</h3><p><strong>B. 特征选择</strong></p><p>动作特征元素通常包括轨迹、速度、方向等。我们将其分解为关节、速度和骨骼。</p><p>关节特征通常是人体动作识别中最常用的特征之一[4]，[6]，[8]。关节之间的位置关系反映了运动的幅度和轨迹。在空间维度上，关节的相对位置可以有效区分不同的运动，而在时间维度上，关节的轨迹反映了运动的周期和幅度。</p><p>速度特征被引入以反映实际物理运动中力的重要性，它是一个带有方向和大小的矢量。在一些文献中[6]，[39]，位置、速度和加速度经常被用作动作识别的三个特征。基于大量实验，作为二阶导数的加速度可能会受到噪音的影响（例如，小动作），如果在时间上平滑，加速度的区分能力将会减弱，因此我们舍弃了加速度特征。与空间点的位置相比，作为3D坐标的一阶导数，速度更关注力的方向和大小，并且与人体运动学更密切相关。大量实验证明，速度的区分能力不逊于3D坐标，并且通过考虑不同的注意力隐藏空间后，可以显著提高其区分能力。</p><p>骨骼特征。实际上，骨骼和关节将两人互动动作的骨架模型分为两部分，如图3所示，这也是骨架动作识别中通常使用的两个基本特征[6]，[8]，[9]，[11]，[40]。骨骼是人体关节之间的自然连接的反应，是人体运动的最小单位，也是运动中姿势的重要载体。骨骼特征在人体动作识别中具有很强的区分能力。特别是在两人互动中，骨骼的相对关系比其他特征具有更多的物理意义，并且导致更高的识别准确性。</p><p><strong>总结：</strong> 本节介绍了动作特征的选择，主要包括关节、速度和骨骼。关节特征通过反映运动的幅度和轨迹在空间和时间维度上进行区分。速度特征引入了对实际物理运动中力的重要性的考虑，通过矢量表示方向和大小。骨骼特征将骨架模型分为关节和骨骼两个基本部分，对于人体动作识别，尤其是在两人互动中，骨骼的相对关系具有更多的物理意义，导致更高的识别准确性。</p><h3 id="C-Feature-Similarity-Measurement">C. Feature Similarity Measurement</h3><p><strong>C. 特征相似度测量</strong></p><p>不同的特征具有不同的关注特性，使不同的特征在其专业领域发挥出较大的区分能力非常重要。在本文中，为不同的特征引入了不同的相似度度量方法。实验证明，可以使用高斯相似度测量3D位置坐标，而速度更关注余弦相似度。动作识别实际上涉及对人体每个骨骼的力进行分析。这种力不仅具有大小，还具有方向。我们可以使用高斯相似度进行幅度的相似性分析，使用余弦相似度来计算方向的相似性。通过这种方式，我们可以更好地捕捉与不同特征相对应的不同隐藏空间。</p><p>高斯相似度测量多维空间中每个点的绝对距离。为了将欧几里德距离映射到0-1范围，我们使用高斯核函数。当两点之间的欧几里德距离为0时，高斯核函数的值接近1；当距离无穷大时，它接近于0。参数σ用于调整下降速度。σ越小，变化越剧烈。</p><p>余弦相似度计算两个向量之间的夹角的余弦值：余弦值可用于表示两个向量的相似性。夹角越小，余弦值越接近1，它们的方向越匹配，相似性越高。余弦相似度与向量的大小无关，只与向量的方向有关。</p><p><strong>总结：</strong> 本节介绍了针对不同特征引入的不同相似度度量方法。通过使用高斯相似度和余弦相似度分别进行幅度和方向的相似性分析，可以更好地捕捉不同特征对应的隐藏空间，提高动作识别的准确性。</p><h2 id="IV-GRAPH-DIFFUSION-CONVOLUTIONAL-NETWORK">IV. GRAPH DIFFUSION CONVOLUTIONAL NETWORK</h2><p>我们的图扩散卷积神经网络以STGCN为基线。我们首先简要回顾了ST-GCN，然后详细介绍了我们在ST-GCN上的改进。ST-GCN将图形卷积应用于基于骨骼的人体姿态识别。对于单帧，有areNjointsVt, joint对sh = {(i, j)|i, j∈N}，其连接是根据人体关节的自然连度定义的，因此，骨架边集可以定义为Et = {(vti,vtj)|t∈τ， (i, j)∈H}，其中τ为视频帧总数。如图1所示，整个网络采用端到端训练方法，并通过反向传播进行优化。网络的前三层用于自学习，第四层和第五层用于以数据驱动的方式引导网络达到全局最优，后两层用于收敛，最后通过标准的SoftMax分类器对每个类别进行关联</p><h3 id="A-Generalized-Graph-Diffusion-and-Graph-Diffusion-Convolution">A. Generalized Graph Diffusion and Graph Diffusion Convolution</h3><p><strong>A. 广义图扩散和图扩散卷积</strong></p><p>对于单帧，根据上述定义，我们现在有一个无向图 G = (V，E)，其中 V 是点集，E 是边集，广义图扩散的扩散矩阵定义如下：</p><p>其中 θk 是权重系数，T 是广义转移矩阵。为了确保收敛性，限制必须满足 ∑（从 k=0 到 ∞）θk = 1，θk ∈ [0, 1]，且T的特征值被限制在 λ ∈ [0, 1]。</p><p>常用作转移矩阵的随机游走转移矩阵 Trw = AD^(-1) 和对称转移矩阵 Tsym = D^(-1/2)AD^(-1/2)，其中 D 是节点度矩阵，A 代表邻接矩阵。为了确保转移矩阵列随机，我们在转移矩阵中添加了具有权重 wloop 的自环 IN。现在，随机游走的转移矩阵可以定义为：</p><p>实际上，图扩散卷积将原始邻接矩阵 A 替换为广义图扩散矩阵 S，这也可以理解为在图扩散之后，我们构建了一个新图用于后续网络卷积。</p><p><strong>总结：</strong> 本节介绍了广义图扩散和图扩散卷积的定义。对于单帧，通过定义广义图扩散矩阵，构建了一个无向图，并引入了相应的转移矩阵。图扩散卷积通过将原始邻接矩阵替换为广义图扩散矩阵，实际上是在图扩散后为后续网络卷积构建了一个新的图。</p><h3 id="B-Spatial-Graph-Diffusion-Convolution-Neural-Network">B. Spatial Graph Diffusion Convolution Neural Network</h3><p><strong>B. 空间图扩散卷积神经网络</strong></p><p>图卷积的最重要部分是采样函数和权重函数的定义。在ST-GCN中，采样函数获取与关节点vti相隔一跳的直接邻居。邻居集合可以表示为B(vti) = {vtj | d(vti, vtj) ≤ 1}，其中d(vti, vtj)表示vti和vtj之间的最短距离。</p><p>由于图中节点的邻居与图像相比没有顺序，合理的划分策略和权重设置的定义成为图卷积的关键。根据人体运动学，ST-GCN将节点vti的邻居分为三类，如图4所示。节点本身被划分为静态运动集。靠近骨架重心的节点被划分为向心运动集，远离中心的节点被划分为离心运动集。划分策略可表示为：</p><p>其中ri是训练集中所有帧中从重心到关节点i的平均距离。根据划分策略，权重函数定义为：</p><p>因此，ST-GCN中的空间图卷积定义为：</p><p>其中Zti(vtj)=|{vtk | lti(vtk)= lti(vtj)}|是归一化项，表示相应子集的基数。 根据上述公式，我们的图扩散卷积网络需要设计两个部分，采样函数和权重函数。</p><p>我们仍然采用ST-GCN采用的空间配置划分方法。实际上，由于ST-GCN中的固定骨架结构，不同帧的权重函数是固定的，这意味着不同关节具有相同的重要性，与不同部位协同执行不同行为的思想相悖。因此，我们将引入图扩散到空间图卷积，以便不同行为采用不同的权重函数。</p><p>图扩散的示例包括个性化PageRank（PPR），热核等，我们采用了PPR，它使用具有转移矩阵T = Trw和系数θPPR k = α(1 − α)k（我们设置alpha = 0.05）的随机游走转移矩阵。在复杂社交网络挖掘中通常采用PPR，通常数据漫游不是随机的，而是具有用户偏好。因此，我们应考虑如何将PPR应用于两人互动识别模型。</p><p>按照这个思路，我们将节点传输过程转化为基于前述节点相似性计算的数据驱动过程。这意味着节点vi在行走过程中停留在原地的概率是wloop/Di，而权重wloop可以由我们的相似性矩阵定义。</p><p>通过图扩散，我们可以将骨架图转化为任意两个关节之间的关系矩阵。这种关系不是通过相似性计算直接获得的，而是在整个骨架图上使用相似性矩阵进行扩散。消息传递到整个图的每个关节，甚至传递到另一个人的关节。这导致了一个新的骨架结构，包含了在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。图5显示了我们的扩散结果。</p><p><strong>总结：</strong> 本节介绍了空间图扩散卷积神经网络的关键概念，包括采样函数和权重函数的定义。在空间图卷积中，通过引入图扩散，改变了原始的图结构，使不同行为采用不同的权重函数。图扩散通过在整个骨架图上进行相似性矩阵扩散，获得了不同关节之间的关系，包括在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。</p><h3 id="C-Temporal-Dynamic-Convolution-Neural-Network">C. Temporal Dynamic Convolution Neural Network</h3><p><strong>C. 时间动态卷积神经网络</strong></p><p>在一系列视频帧中，通常会存在一些无关的动作[41]。例如，在“拥抱”动作中，“接近”动作首先执行，然后是“拥抱”。因此，我们希望将振幅最大的动作标识为扩散和识别的焦点。同时，在一些动作中，会有一些“小动作”作为插曲，例如，有些人会习惯性地“拿眼镜”、“拿耳朵”或“摇腿”等。这些小动作不应成为识别的重点。而且，有些动作非常相似，振幅略有不同，例如“挥手”和“打排球”等。在实际场景中，动作通常是连续的，因此在分割后识别动作具有重要的实际意义。</p><p>在许多相关研究中，通常通过简单的卷积进行知识融合，并将关节的变化抽象为时间维度的隐藏空间，但忽略了每帧的重要性的差异[42]。由于卷积操作共享卷积核的特性，扩散的源帧与其他帧没有差异。然而，对于一些相似的动作，动作的振幅成为识别的关键。因为大多数动作是由身体部位执行的，而身体部位具有简单的图结构，对于动作来说，主要的区别在于哪些部位适合在一起以及每个部位的振幅。在前一种情况下，我们通过扩散在空间维度的组件之间建立了空间连接。后者是时间维度的问题。因此，我们通过在时间维度中寻找扩散的源帧来放大扩散过程中源帧的权重，使模型收敛更快、更准确，提高相似动作的识别能力。</p><p>在ST-GCN中，时间卷积操作是通过考虑周围9帧来更新当前帧的关节。由于卷积过程的局部感知和参数共享特性，由于对不同帧具有相同的感知强度，很难捕捉关键帧。</p><p>因此，我们研究了关节的时间活动轨迹，以改变不同帧的重要性，这意味着我们的时间卷积是动态的。时间的帧重要性计算可以表示为：</p><p>在这里，我们选择了所有关节进行重要性计算，并采用了原始未处理的特征。在后续的消融实验中，我们将讨论一系列更有意义的堆叠方法。现在，我们的时间卷积可以表述为：</p><p>准确捕捉源帧还将抑制小动作和噪音帧的影响。同时，帧重要性计算可以以细粒度捕捉人体运动趋势的变化，例如从“前进”到“后退”，从“外”到“后”等，这些常常在运动轨迹中呈现为拐点。通过这些拐点，我们可以分割多个连续的动作，以处理现实中复杂而连续的视频。</p><p><strong>C. 时间动态卷积神经网络总结</strong></p><p>在时间动态卷积神经网络中，通过识别动作的振幅最大的源帧来作为扩散和识别的焦点，以区分动作中的“小动作”和相似动作。传统卷积操作的问题在于对不同帧的感知相同，而时间动态卷积通过考虑关节的时间活动轨迹，使模型能够动态地调整每帧的重要性。通过在时间维度中寻找源帧，扩散的源帧在卷积过程中的权重得以放大，提高了模型的收敛速度和准确性，同时消除了小动作和噪音的影响。这种动态的卷积操作能够更好地捕捉人体运动趋势的变化，为处理连续和复杂的实际视频提供了更有效的方法。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解4真实世界的图卷积网络用于智能视频监控中的动作识别</title>
      <link href="/project/posts/69357615.html"/>
      <url>/project/posts/69357615.html</url>
      
        <content type="html"><![CDATA[<h1>Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance</h1><h1>abstract</h1><p>摘要： 动作识别是新兴的边缘智能视频监控和安全系统中的关键算法部分。基于骨架的动作识别是一种吸引人的方法，它不使用RGB像素数据，而是依赖于人体姿势信息来分类适当的动作。==然而，现有的算法通常假设理想条件，这些条件不代表真实世界的限制，如嘈杂的输入、延迟要求和边缘资源约束。==</p><p>为了解决现有方法的局限性，本文提出了Real-World Graph Convolution Networks（RW-GCNs）<strong>，这是一个面向真实世界骨架动作识别域约束的架构级解决方案。<strong>受人类视觉皮层中反馈连接的启发，RW-GCNs利用对现有近最先进（SotA）的时空图卷积网络（ST-GCNs）<strong>进行专注反馈增强</strong>。ST-GCNs的设计选择</strong>源于信息理论中心的原则，以解决端到端实时和边缘智能视频系统中通常遇到的空间和时间噪声</strong>。我们的结果表明，RW-GCNs能够通**过在NTU-RGB-D-120数据集上实现新的SotA准确性（94.1%）**来服务这些应用，并在存在空间关键点噪声的情况下，<strong>在Northwestern UCLA数据集上实现90.4%的准确性</strong>，比基线ST-GCN应用的延迟低32倍。RW-GCNs通过在成本效益高达10倍的NVIDIA Jetson Nano上运行（而不是NVIDIA Xavier NX），同时在资源受限的设备上保持可观的吞吐量范围（15.6到5.5个动作每秒），进一步展示了系统的可扩展性。代码可在此处获取：<a href="https://github.com/TeCSAR-UNCC/RW-GCN%E3%80%82">https://github.com/TeCSAR-UNCC/RW-GCN。</a></p><p>总结： 本文介绍了一种名为Real-World Graph Convolution Networks（RW-GCNs）的架构，用于解决真实世界骨架动作识别中的各种约束。受人类视觉皮层中反馈连接的启发，RW-GCNs通过对现有时空图卷积网络（ST-GCNs）进行专注反馈增强，==以应对噪声、延迟和资源限制等挑战==。实验证明RW-GCNs在NTU-RGB-D-120数据集上==实现了新的最先进准确性==，==同时在较低的延迟下仍保持了高准确性==。RW-GCNs还通过在成本效益高的边缘设备上运行，展示了系统的可扩展性。</p><h1>I. INTRODUCTION</h1><p>动作识别，即在视频片段中识别演员执行的动作，是计算机视觉领域一个具有挑战性且繁荣的领域。在边缘实时应用的约束条件下，这一挑战变得更加严峻。边缘计算是许多旨在维护隐私和可扩展性的应用的要求。深度神经网络（DNNs）的最新进展使该领域迎来了一场复兴，将模型的准确性推动到远远超出以往可能的水平。这在基于骨架的动作识别中尤为显著，该方法不使用RGB像素数据，而是依赖于人体姿态信息来分类适当的动作。特别是，源自图卷积网络（GCNs）[22]，具体来说是时空图卷积网络[64]（ST-GCNs）的方法，将最先进（SotA）的动作识别推向了新的高度。目前，即使对于相对简单的基线ST-GCN算法，也可以实现近SotA的准确性，达到了87%的准确性。有了这么高的准确性，现在可以在理解人们动作的基础上实现依赖于复杂计算机视觉的应用，例如视频监视、自动驾驶车辆、患者监测等。然而，许多这些应用都具有尚未得到适当解决的实际约束条件，例如需要在边缘端到端系统中满足隐私和可扩展性要求以及实时延迟的约束条件。这些现实世界的约束条件阻碍了现有基于骨架的动作识别工作的部署。</p><p>现有致力于基于骨架的动作识别的工作往往假设并非真实世界的理想条件。通常，网络使用的姿势信息被假定为完美的，并且通常基于手动注释的数据。然而，在实际系统中，骨架姿势信息将来自于不完美的人体姿势估计器，导致不完美的姿势数据。即使是最先进的SotA姿势估计器，具有数千万参数和数百亿次操作[4, 37]，也无法产生无瑕疵的姿势数据。这些姿势估计器的准确性受到边缘设备资源约束的进一步限制。此外，这些工作通常忽视了在边缘设备上满足这些现实世界应用的潜在延迟约束，这明显限制了它们在实际世界中的适用性。</p><p>虽然大多数作品主要关注实现高准确性，但也有一些尝试解决实际条件的特定方面，特别是关于噪声数据和端到端边缘系统的实时延迟约束。噪声数据可以看作是由于场景遮挡引起的不完整的人体骨架，或由于ID混淆导致多人设置中缺失的骨架。通过在Kinetics-Skeleton数据集[21]上进行评估，可以看出模拟实际分析的主要方法。这是通过使用OpenPose[4]从原始RGB图像中提取人体姿势信息而导出的流行Kinetics动作数据集的增强。然而，对于该特定数据集，准确性往往较低，SotA低于50%。部分原因可以通过使用不完美的姿势估计器引入的噪声来解释，但主要因素是对于该特定数据集，姿势数据不包括许多动作相关信息。这些原因包括带有移动摄像机的场景、第一人称视角的互动以及依赖于场景背景的信息。</p><p>总体而言，现有方法未能解决包括延迟约束和噪声数据在内的现实世界动作识别约束。这一失败阻碍了在具有边缘和实时要求的实际应用中的实现。这需要一种新的设计思维方式、形式化和领域规范，以应对甚至解决这些挑战，并同时保持强大的准确性。本文提出了实际图卷积网络（RW-GCNs）作为解决方案，以应对实际动作识别的固有约束。通过这样做，我们正在实现在边缘设备上部署实际动作识别。我们关注深度学习的信息瓶颈理论[58]，并受到神经科学的启发[24]，从而提出了关于处理边缘设备上的实时延迟和关键点噪声的注意反馈增强作为解决方案。RW-GCNs的反馈通过存储和重复使用过去的特征以及对重要特征的隐式时间采样，解决了在边缘设备上操作时保持准确性的问题。关注性反馈重新审视过去的特征并增强模型的区分能力。我们进一步推导了RW-GCNs的两个反馈变体：（1）语义反馈和（2）控制反馈。</p><p>为了全面评估与边缘实际约束条件相关的问题，我们为基于骨架的实际动作识别确定了一个现实的边缘系统，该系统满足现有云系统无法满足的隐私和可扩展性约束。此外，我们引入了两个新的计算性能指标：每秒动作（ApS），一种分析在多个延迟约束下的吞吐量的方法，以及动作产品延迟（APD），一种端到端系统延迟度量。然后，我们定量评估了RW-GCNs，包括语义反馈和控制反馈，以应对在实际边缘系统引起的延迟约束和噪声输入条件下的现实世界动作识别挑战。我们的结果显示，RW-GCNs在NTU-RGB-D-120数据集[30]上实现了94.16%的新SotA准确性，其延迟比ST-GCN基线少3.02倍。对Northwestern UCLA数据集[61]的评估显示，RW-GCNs可以在验证和训练中存在空间关键点噪声的情况下，以32.5倍的较低延迟实现90.4%的准确性。总的来说，RW-GCNs能够在完全端到端系统噪声存在的情况下以32.5倍的较低延迟运行，同时在Northwestern UCLA数据集上保持71.8%的准确性。总之，这项提议的工作做出了以下贡献：</p><ul><li>我们定义了领域约束，并引入了针对实际世界基于骨架的动作识别领域的新评估方法。</li><li>引入RW-GCNs作为解决同时处理延迟和噪声数据的实际约束的解决方案，通过基于信息理论的关注反馈架构增强。</li><li>在与现有实际边缘设备上的真实无约束工作的系统性能之间进行系统评估。</li><li>对端到端动作识别系统以及视频监控导向数据集上的紧急系统噪声进行了割离研究。</li></ul><p>接下来，第2节回顾相关工作。第3节介绍我们的领域约束和评估指标，以及与实际端到端边缘系统及其相关的新兴系统噪声。第4节提出了RW-GCNs作为在边缘和实时情况下启用基于骨架的动作识别的解决方案，并解释了语义和控制反馈的RW-GCNs的理论基础和实施细节。第5节呈现了该工作的结果、分析和评估。第6节包含我们的结论性意见。</p><h1>2 Related Works</h1><p>动作识别通常可以通过直接从视频信息中进行 [28]，或者通过时间利用人体姿势信息来完成。前一种方法可以称为基于像素的动作识别，并已在大型数据集上进行了广泛探讨 [21]。基于像素的动作识别方法必须对个体演员的空间外观特征具有鲁棒性，同时捕捉演员的时间动态，正如江等人在 [20] 中所做的那样。由于空间域中的噪声，这变得更加具有挑战性。现有作品在转移到动作数据集之前，利用大型图像数据集进行预训练，正如Tran等人在 [59] 中所示。动作识别的时间动态也需要被考虑，正如Liu等人在视频片段中以多个粒度查看的情况 [32]。</p><p>基于像素的动作识别的另一种选择是基于骨架的动作识别，从直观上来看，它与RGB视频流相比具有更少的噪声输入。正确提取该信息并对特定动作进行分类一直是一项持续的研究挑战。先前的方法依赖于基于循环神经网络（RNN）的模型的表达能力来处理图结构化输入 [25]。图卷积网络（GCNs），如Kipf等人的工作中所见 [22]，利用对非欧几里德图进行卷积，而不是在图像中找到的规则图。这个想法被应用于人体姿势图，用于动作识别，如Yu等人的工作中所见 [64]。最终的结果是ST-GCNs [64]。RW-GCNs可以被看作是RNN和基于ST-GCN的方法之间的中间体，类似于AGC-LSTMs [54]。</p><p>基于骨架的动作识别领域的当前最先进方法主要源自（或至少与）Yang等人的ST-GCN [64] 的工作有关。这些作品在ST-GCNs的基础上进行了多方面的创新。Liu等人的工作 [33] 解决了基线ST-GCN公式的现有偏见。第一个偏见是针对表示远程关节关系的。由于邻接矩阵中的冗余连接在ST-GCN的特征聚合中占主导地位，导致了这种偏见。提出的解决方案是删除现有的冗余连接。第二个偏见是针对复杂的时空特征传播的。这是由于将时空图卷积的因子分解为单独的层。这项工作另外提出了一个统一的运算符，同时处理数据的空间和时间方面。</p><p>一些作品没有将手工设计的设计原则编码到网络中，而是利用自适应图结构，使网络能够学习导致更稳健表示的设计原则。Plizzari等人的工作 [44] 将变压器引入，如自然语言处理领域所见 [60]。变压器的灵活性，最终的核心组件自我关注，使ST-GCN能够动态建模关节连接并学习依赖于数据的重要连接。虽然Plizzari等人 [44] 利用自我关注来克服基线ST-GCN公式的局限性，但仍然利用了图卷积形成（即在不规则图上进行显式卷积）。Shi等人的工作 [52] 超越了这一点，并完全用解耦的自我关注（解耦的自我关注在空间和时间上分解）取代了GCNs。通过利用位置编码，这项工作依赖于自我关注隐式模拟GCNs的能力。与增加图结构的灵活性不同，一些其他方法通过使输入表示更具信息性来改进输入表示 [26, 50, 51]。这些作品通过预处理现有的关节信息，进一步以骨骼和运动的方式明确定义输入。</p><p>尽管这些创新有助于更健壮的泛化和在关键数据集上实现整体更高的准确性，但很少有作品尝试解决在现实世界中部署ST-GCNs时面临的现实约束和困难。因此，大多数现有作品未能实现现实世界的部署，特别是考虑到某些应用所需的边缘设备的计算限制。Song等人的工作 [55] 是一个例外，该工作改进了输入表示，类似于 [26, 50, 51]，然后构建了一个专注于部分注意力的高效架构。Cheng等人的工作 [7] 也通过利用时间移位模块（TSMs）[28]来提高实时性能，作为时序卷积的有效替代方案。Yang等人的工作 [65] 利用反馈机制增加了时序采样策略的灵活性，并进一步减少了分类所需的帧数。最后，Yu等人的工作解决了噪声关键点的真实世界效应，并根据使用人工噪声进行训练的类型提出了解决方案 [67]。</p><p>与考虑现实世界部署要求时的工作方向不同。与Cheng等人和Song等人的工作不同 [7, 55]，我们没有为高效模型做出明确的贡献。相反，我们专注于输入数据依赖性，并在满足延迟约束的同时将我们的模型与大量输入要求解耦，类似于Yang等人 [65]。然而，我们并非利用密集连接层来控制必要的反馈，而是提出了一种基于注意机制的反馈机制，类似于Plizzaris等人 [44] 中发现的机制。此外，我们还考虑与Yu等人 [67] 类似的关键点噪声，尽管我们是在现实端到端动作识别系统的背景下这样做。通过这样做，我们希望实现在实时边缘应用中部署基于骨架的动作检测。</p><h1>3 Real-World Action Recognition: Domain Constraints, Gaps, and Methods</h1><p>虽然基于骨骼的动作识别领域取得了越来越多的成功，但与现实世界的约束有关的算法性能在很大程度上还未被探索。我们的工作希望能够启动现实世界中基于骨骼的动作识别的探索，这将使基于骨骼的动作识别能够部署在边缘上。在本节中，我们将分析诸如患者监控和安全等应用程序的实际部署限制，并定义子领域真实世界的骨骼动作识别。首先，我们简要回顾了现实世界的行动识别约束和边缘计算范式的必要性。然后，我们引入两个新的评估指标来捕获延迟约束。最后，我们将深入研究边缘视频监控应用中完整的端到端视觉处理管道背景下的噪声数据的细节。</p><h2 id="3-1-Real-World-Constraints">3.1 Real-World Constraints</h2><p>嘈杂输入：这个新的子领域的第一个主要约束是现实世界的数据。基于骨架的动作识别高度依赖以关键点形式呈现的人体姿势数据。通常，该领域中发布的作品假设提供了完美的人体姿势数据。这种完美的数据通常是通过耗时的手动标注或在实验室环境中使用昂贵且高度准确的传感器获得的。在实际部署中，姿势数据通常需要由专用的机器学习算法即时生成，导致数据不完美。这被称为噪声数据，在本文中我们假设这种噪声是由基于深度神经网络的多人姿势估计器生成的。我们采用了他们关于空间噪声的定义，即应该在帧中但未提供的人体关键点关节。然而，我们对他们对于时序噪声的定义进行了扩展，将其分为两个不同的类别。第一类是帧级时序噪声。当个体的数据完全缺失时，就会发生这种情况，无论是由于未能检测到一个人，还是由于在多人环境中误认为他们。第二类是感受野级时序噪声。这是一种隐式的噪声，定义为网络在能够查看整个动作剪辑的持续时间方面的能力的限制。</p><p>延迟约束：现实世界基于骨架的动作识别的第二个主要约束是延迟。虽然一些现有的作品关注基于骨架的动作识别的计算复杂性，但许多忽视了执行单个动作识别所需的帧数。通常，现有作品中用于分类单个动作（即剪辑大小或T）的帧数为300。如果我们假设神经姿势估计器以FPSin = 30运行（这取决于硬件，是一项非常庞大的成就），那么300帧的剪辑对应于10秒的延迟。这对于需要实时响应的应用，如患者监测、公共安全和一般视频监控而言，是无法接受的昂贵。</p><p>边缘计算约束：最后，由于考虑到现实世界基于骨架的动作识别需要额外的隐私和可扩展性考虑，边缘计算范式变得必不可少。大多数现有的视频监控实现从边缘的本地摄像头流式传输图像数据到云服务器，云服务器执行大部分计算。这使用户隐私无法得到保障，并限制了可扩展性。隐私对于启用智能边缘视频应用至关重要，其他作品为确保用户隐私而专门致力于使用计算昂贵的深度神经网络 [57]。在我们的工作中，靠近传感器的低功耗边缘设备用于进行姿势估计和动作识别的计算。通过直接在边缘处理视频流，传送到网络的唯一数据是经过预处理的，不包含个人身份信息。</p><p>这从根本上解决了通过云端传输个人数据的不安问题。此外，使用边缘设备来处理大部分计算正面影响系统的可扩展性。在基于云计算的系统中，可扩展性受云服务器的处理能力和网络吞吐量的限制。增加摄像头会导致对服务器的更多请求，这可能导致要么昂贵的服务器升级，要么由于无法满足系统计算需求而增加系统范围的延迟。当系统实现成本太高时，它会限制智能视频应用在边缘的推广，以及整体人工智能的普及。通过减少昂贵的云服务器，采用更便宜的本地边缘计算解决了这个问题，并已经在现有的深度神经网络服务平台中使用 [34, 35]。然而，边缘解决方案通常受到资源限制，这导致其他实现使用混合边缘/云解决方案 [38]，或者优化DNN以提高性能 [40, 45]。</p><p><strong>总结：3.1 现实世界约束</strong></p><p><strong>1. 噪杂输入：</strong></p><ul><li>真实世界的数据对于基于骨架的动作识别构成首要约束，依赖于以关键点形式呈现的人体姿势数据。</li><li>大多数文献中假设提供完美的人体姿势数据，但在实际部署中，姿势数据往往由专用机器学习算法即时生成，导致数据不完美，即噪声数据。</li><li>噪声数据主要由基于深度神经网络的多人姿势估计器生成，包括空间噪声和时序噪声（分为帧级和感受野级）。</li></ul><p><strong>2. 延迟约束：</strong></p><ul><li>实际世界基于骨架的动作识别的第二个主要约束是延迟。</li><li>许多现有作品关注计算复杂性，但忽略了执行单个动作识别所需的帧数。</li><li>典型情况下，现有作品的帧数为300，而这将导致10秒的延迟，对于需要实时响应的应用来说是昂贵的。</li></ul><p><strong>3. 边缘计算约束：</strong></p><ul><li>由于隐私和可扩展性的考虑，现实世界基于骨架的动作识别需要采用边缘计算范式。</li><li>大多数现有视频监控实现通过从边缘传输图像数据到云服务器进行计算，这违反了用户隐私且限制了可扩展性。</li><li>我们的工作使用低功耗边缘设备进行计算，解决了云端传输个人数据的问题，同时提高了系统的可扩展性。</li></ul><h2 id="3-2-Real-World-Evaluation-Metrics">3.2 Real-World Evaluation Metrics</h2><p>为了解决资源受限边缘系统的延迟约束，我们定义了两个新的指标，用于分析和研究动作识别算法在现实世界中的部署能力。第一个指标是每秒动作数（ApS）。</p><p>该指标表示模型在单个秒内能够完成多少工作（即吞吐量）。其中，T是剪辑大小，W是窗口大小，CpSin是每秒剪辑数，或者是完整剪辑被输入到网络的速率。这个指标是为了将受到延迟约束的作品（如我们的作品）与无约束的作品进行比较而需要的，因为它们在单个剪辑中完成了不同数量的工作。虽然模型接受的理论输入吞吐量（FPST）对于像自动驾驶汽车这样的快速应用非常重要，但视频监控领域的要求要宽松得多（对于人体运动为30 FPS）。相反，第二个指标动作产品延迟（APD）更具预测性，特别适用于视频监控等领域的实际部署性能。</p><p>APD是输入帧速率FPSin的倒数与网络窗口大小W之间的乘积。该指标表示处理单个动作所需的延迟量。在监测患者或危险工作环境等应用中，检测事故（例如摔倒、绊倒）的响应时间至关重要，其下限即为APD。</p><p><strong>总结：3.2 现实世界评估指标</strong></p><p><strong>1. 每秒动作数 (ApS)：</strong></p><ul><li>用于衡量模型在单秒内完成的工作量，即吞吐量。</li><li>考虑剪辑大小（T）、窗口大小（W）和每秒剪辑数（CpSin）。</li><li>用于比较延迟受限作品与无约束作品，因为它们在单个剪辑中执行不同数量的工作。</li><li>对于视频监控等领域，与模型接受的理论输入吞吐量（FPST）相比，具有更宽松的要求。</li></ul><p><strong>2. 动作产品延迟 (APD)：</strong></p><ul><li>是输入帧速率（FPSin）的倒数与网络窗口大小（W）之间的乘积。</li><li>表示处理单个动作所需的延迟量。</li><li>在监测患者或危险工作环境等应用中，检测事故的响应时间至关重要，其下限为APD。</li><li>更具预测性，特别适用于视频监控等领域的实际部署性能评估。</li></ul><h2 id="3-3-End-to-End-Video-Surveillance-and-Noisy-Data">3.3 End-to-End Video Surveillance and Noisy Data</h2><p>在现实世界的环境中，基于骨骼的动作识别将成为更大端到端视觉管道的一部分。图1展示了智能视频监控应用的典型视觉管道。该图受到[36]和[23]中提出的现有视觉管道框架的启发。如图1所示，这些框架通常包括三个概念阶段。我们还增加了一个额外的阶段，其中包含我们的RW-GCN网络。 RW-GCN的详细描述可以在第4.2节中找到。第一个阶段是检测阶段。多人体姿势估计器用于检测人体关键点并从输入视频的单个帧中生成边界框。接下来，在提取阶段，使用先前阶段的边界框生成人体图像裁剪。这些人体裁剪通过特征提取网络，为每个人体图像生成嵌入式特征表示。这些边界框还用于计算每个个体在与先前帧相比的帧中的时间位置。在匹配阶段，匹配算法测量当前帧特征与先前帧特征之间的欧几里得外观相似性，并考虑先前计算的时间位置，确定要分配给哪个人的ID。</p><p>这一端到端的视觉管道完全在靠近摄像机的边缘节点上运行，通过从多个节点聚合非敏感信息到边缘服务器，实现了[36]中所做的操作。</p><p><strong>空间噪声：</strong> 基于骨骼的动作识别需要人体姿势信息，以关键点骨架的形式作为输入。在封闭环境中的动作识别中，假定这些骨架是演员场景的无缺陷、完美的表示。然而，在现实世界中，由人体姿势估计器提供给网络的骨架通常存在缺陷。即使是最先进的方法也不是完美的，有时会出现错误，要么是误检测关键点，要么完全未检测到。这些错误在整体上可能是不可避免的，但当应用环境与姿势估计器训练的数据集不同时，错误会进一步加剧，而在现实世界中几乎总是这种情况。对于需要考虑硬件资源的系统，或者存在实时约束的系统，使用大型、高精度的最先进模型可能不是一个选择。在这些情况下，需要做出权衡，使用轻量级的姿势估计器，并经常牺牲准确性以满足应用要求，这可能进一步增加这些错误的频率。</p><p>这些缺失和错误检测的关键点在系统中形成了一种空间噪声，如图2所示。当传播到动作识别时，这种噪声可能会带来很大问题。图2上半部分的演员正在弯下身子将物品放入桶中。这对于人眼来说是清晰的。然而，用于动作识别的关键点骨架在这个场景中存在明显错误，特别是涉及演员右臂的部分。除了在检测方面通常不一致之外，在图中最右边的图像中，演员的右手被错误地检测到了他的脚踝。如果动作识别系统被设计为假定接收到的数据是完美的，这种类型的错误很容易使网络混淆，并导致不正确的动作分类。因此，在现实世界的动作识别中需要考虑这种类型的噪声。</p><p>为了在准确性和实时约束之间取得平衡，我们使用EfficientHRNet [37]来获取人体姿势骨架。</p><p><strong>时间噪声：</strong> 现实世界动作识别中常见的另一类噪声是时间噪声。时间噪声可以通过两种方式之一发生。第一种是演员的骨架在视频片段的一个或多个帧中丢失。通常，这是因为姿势估计器没有检测到骨架，或者因为演员的骨架在这一帧中没有与演员正确匹配。在这两种情况下，这导致动作识别网络在应该有骨架信息的情况下没有该演员的骨架信息。在封闭环境中的动作识别中，其中提供了完美的骨架和ID数据，这不是问题，但在现实世界的动作检测中，使用不完美的算法生成关键点和ID，这种类型的错误相当普遍。</p><p>第二种形式的时间噪声发生在边缘系统没有同时将动作的所有帧馈送到网络中。虽然这对于封闭环境的动作识别不是问题，在现实世界中，在延迟约束 dictate 下，只能一次处理少数帧的应用程序中，这种情况经常会遇到。</p><p>图2说明了第一类时间噪声的一个例子；关键点骨架丢失。虽然图2下半部分的演员在第一帧中被正确识别，但中间两帧的关键点骨架变灰，表明它们与演员没有正确匹配。这是由重新识别算法引起的错误。此外，在最后一帧中，根本没有检测到关键点骨架；这是神经姿势估计器的失败。凭借仅有的一帧姿势数据，动作识别网络将无法理解演员的运动和位置情况。直观地说，这将使得对于场景的动作识别非常具有挑战性，设计现实世界的动作识别时必须考虑这些情况。</p><p>我们选择使用与[36]中相似的Re-ID方法，这是一种不依赖于面部识别的现实解决方案。</p><p><strong>总结：</strong></p><p>这一段描述了基于骨架的动作识别在现实世界环境中的应用，作为更大端到端视觉管道的一部分。该视觉管道包括检测、提取、匹配等阶段，其中使用多人体姿势估计器进行关键点检测，并通过边缘节点进行端到端的实时处理。文章指出了两种主要噪声：空间噪声和时间噪声。空间噪声源于人体姿势估计的不完美，包括关键点缺失和错误检测，可能导致动作识别错误。为了平衡准确性和实时性，文章采用了EfficientHRNet等轻量级姿势估计器。时间噪声涉及骨架在视频中的丢失，以及边缘系统无法同时处理所有帧的情况。这种噪声使得动作识别在现实场景中更具挑战性，需要针对性的解决方案，如采用Re-ID方法。最后，介绍了Real-World GCNs（RW-GCNs）作为一种解决方案，通过端到端处理空间和时间噪声，提高对于现实世界骨架动作识别的适用性，尤其是在边缘计算领域的应用。</p><h1>4 Real-World-GCNs (RW-GCNs) through Attentive Feedback Augmentation</h1><p>为了满足现实世界骨骼动作识别的领域约束，本节介绍了实际世界GCNs（RW-GCNs）。在RW-GCN中，我们利用一个具有预定义结构的静态图，模仿了COCO数据集关键点格式[29]，用于空间图表示。时间组件由每个顶点通过相邻帧之间的一对一连接组成。这个时空人体姿势图被网络视为一个张量X ∈ RN×T×C×M，其中N是节点数，T是帧数，C是通道数，M是整个剪辑中单帧中分开的人体姿势的最大数量。此外，网络使用的邻接矩阵A ∈ RN×N用于定义图上的卷积。</p><p>我们基于ST-GCN的网络的空间组件是通过在输入缓冲区的单个帧上执行1×P标准的2D卷积来实现的，其中P决定了权重矩阵和邻接矩阵的分区数。然后，结果与一组归一化的P邻接矩阵相乘。根据定义的分区策略，权重矩阵和邻接矩阵可能被分割成子集，并按方程（3）中所示相加，该方程在[7]中进一步解释。RW-GCN利用了在[64]中定义的空间分割策略。RW-GCN通过细致的反馈增强了现有的ST-GCN架构。反馈可能增强ST-GCN在处理嘈杂输入方面的能力。馈送回架构的特征将包括更高级别的语义，如果通过时间缺乏变化，则可能是冗余的。如果神经姿势估计器的噪声与其估计相关，这些在时间上冗余的特征可能在空间上产生协同作用，这在定性上是成立的。总体而言，细致的反馈增强使RW-GCN能够应对各种噪声，特别是由现实世界的延迟约束引起的时间噪声。因此，RW-GCN为边缘计算领域的基于骨骼的动作识别应用打开了可能性。对于大多数计算在接近传感器边缘节点的系统，例如REVAMP2T [36]，RW-GCNs添加了一个灵活的网络，可以根据其他网络的计算强度使用不同数量的帧。对于像EdgeEye [31]这样将所有数据发送到本地边缘服务器的系统，RW-GCNs增强了边缘系统在处理网络带宽和传感器存储限制方面的灵活性。最后，对于更分布式的边缘系统，例如VideoPipe [46]，RW-GCNs相对于本地存储容量和网络带宽的灵活性将能够实现许多不同的流水线实现，每个都专门针对特定的应用。</p><p><strong>总结:</strong></p><p>上述段落介绍了Real-World GCNs（RW-GCNs），这是为了满足现实世界骨骼动作识别领域的约束而提出的一种方法。RW-GCN采用静态图和预定义结构，模拟了COCO数据集的关键点格式，用于构建空间图表示。时间组件由相邻帧之间的一对一连接组成。网络将这个时空人体姿势图视为张量，其中包含节点数、帧数、通道数和单帧中人体姿势的最大数量。空间组件通过在输入缓冲区的单个帧上执行标准的2D卷积来实现，并结合了一组归一化的邻接矩阵。RW-GCN采用了一种空间分割策略，并引入了有意义的反馈，以增强网络对嘈杂输入的处理能力。这种反馈包括高级语义特征，有助于处理时间上的冗余。最终，RW-GCN通过处理各种噪声，尤其是由于现实世界延迟引起的时间噪声，为边缘计算领域的基于骨骼的动作识别应用提供了可能性。这对于不同计算强度和网络带宽的系统提供了灵活性，为实时应用和边缘设备上的动作识别提供了解决方案。</p><h2 id="4-1-Theoretical-Aspects">4.1 Theoretical Aspects</h2><p><strong>4.1 理论方面</strong></p><p>我们关注的自注意力反馈增强的理论基础基于深度神经网络的信息瓶颈理论，由Tishby等人提出[58]。</p><p>虽然Saxe等人的工作[48]最初对这一理论提出了质疑，但Noshad等人的工作[41]随后证实了该理论，指出存在对互信息的计算问题，因为使用了较差的互信息估计器，Nosahd等人提出了他们自己的互信息估计器。此外，还有其他尝试验证信息瓶颈理论在更复杂架构上的应用[9]。尽管直接将信息瓶颈应用于深度学习存在困难，但其他研究已经表明，基于信息瓶颈理论的解决方案在开发时在泛化方面取得了明显的改进[19, 43]。我们的工作采用相同的方法，基于信息瓶颈理论的基础，开发了面向现实世界实时动作识别的体系结构级解决方案。</p><p>信息瓶颈理论假设深度神经网络可以被表示为马尔可夫链： X → T1 → … → Tz → Y 每一层的输出在马尔可夫链中由一个随机变量表示。例如，X是输入特征，T1是第一层的隐藏表示，Tz是第Z层的隐藏表示，Y是输出随机变量。</p><p>应用于深度学习的信息瓶颈理论认为学习等效于最小化以下拉格朗日函数：</p><p>我们可以解释方程（4）为学习具有两个明显且独立的阶段，通过最小化−I(T; Y)（我们可以将其视为最大化其倒数）和最小化I(X; T)。拟合阶段是第一阶段，如方程（5）所示： max I(T; Y) = H(T) − H(T|Y) （5） 拟合阶段由β调节，通过最大化隐藏特征的熵H(T)来学习信息性特征，使隐藏表示更具信息性。此外，拟合阶段最小化H(T|Y)，这最小化了当已知标签信息时隐藏表示的信息。从直观上看，这相当于删除与隐藏表示无关的特征。</p><p>学习的第二阶段是压缩阶段，如方程（6）所示： min I(X; T) = H(X) − H(X|T) （6） 压缩阶段最小化H(X)，即源数据的熵，同时最大化隐藏表示对输入的信息量H(X|T)。这导致了一个模型，它对输入信息的依赖较少。我们的工作假设利用具有助于拟合或压缩学习阶段的特性的解决方案，可以导致类似于Jeon等人[19]的强大算法解决方案。</p><p>RW-GCNs的第一个体系结构级解决方案基于自注意力。Bloem等人的工作[2]指出，注意力使网络能够通过捕获数据相关的不变性来学习和建模。了解信息瓶颈可以被看作是查找最小充分统计量的框架[53]，我们假设注意力使模型更有效地捕获数据集的最小充分统计量，这直接有助于模型与信息瓶颈框架相关的方面。这进一步得到了支持，因为注意力可以作为一种形式的预训练压缩[2]。我们此外强调了早停止（仅在验证准确性不再增加时停止训练）通常用于训练深度神经网络。其他研究发现，早停止通常（但不总是）在压缩阶段之前停止训练，暗示压缩阶段可能导致过拟合[43, 63]。鉴于此，可以将压缩阶段解释为学习要关注的信息。我们认为自注意力是一种足够灵活的归纳偏置（由于模仿和扩展归纳偏置的卷积的能力[1, 8]），可以实现非过拟合的压缩阶段。</p><p>反馈还可以协助模型的泛化，不是通过压缩阶段，而是通过拟合阶段。事实上，RW-GCNs的主要前提是保持良好的表示或最大化H(T)，尽管存在感受野级别的时间噪声（由延迟约束引起）。在考虑到这一前提的基础上，我们将目光投向拟合阶段，max I(T; Y)，将其视为一个经典的信道编码问题，其中我们的网络既是编码器又是信道本身，信道容量是互信息I(T; Y)。我们理论上认为，如果反馈可以协助经典通信信道中的信道编码，那么它可以通过改善神经网络训练的拟合阶段来协助深度神经网络。</p><p>已知，当反馈应用于具有不相关高斯噪声信道的传统通信通道时，信道容量不会增加[12]。但是，当噪声与输入相关（即带有记忆的信道）时，反馈可以导致信道容量的小幅提高。此外，反馈还可以显著降低信道编码器和解码器（即神经网络）的复杂性[6]，通过降低传统通信通道中接近容量所需的平均块长度。我们认为这相当于减少深度神经网络在单个时间步中需要看到的特征数量。我们将这与卷积神经网络层具有具有受限感受野的神经元（即更少的空间特征）以及网络深度作为一种模仿反馈能力以用更简化的复杂性实现合理准确性的机制相联系。已知，由于深度神经网络通常具有高度的层间权重共享[3, 47]，深度可以模仿反馈。</p><p>反馈还可以在无记忆多路访问信道（MACs）的容量中提供附加增益，其中多个信号传递到同一信道[15]。这可以看作是神经网络处理多个特征。通过反馈的能力增强信号的合作，可以实现容量的提高，使神经网络更好地共同优化多个竞争特征。当仅希望部分信息的多个信号时，可以实现信道容量的乘法增益[56]。我们将语义反馈视为一种试图通过利用高级语义信息（例如深度神经网络特征）来最大化H(T)或等效地增加特征表示的信息的机制，如在CliqueNets[66]中所见。我们认为这种高级语义信息通过反馈的能力降低编码器（即网络）的复杂性，从而允许我们使用更小的时间感受野。</p><p>此外，我们将控制反馈视为一种使统计信息提取与特定随机变量相关但与其他变量无关的机制。这种类型的反馈专注于最小化H(T|Y)，从而在已知目标分类的某些信息时减少隐藏特征的冗余。这可以看作是在传统多路访问通信信道中协调多个信号的反馈的类似物。此外，已知信息瓶颈在有条件地优化适用于不同输出随机变量的模型时具有交替公式[5]。根据反馈的不同，可能可以学习专注于不同输出随机变量的条件模型。直观地说，如果我们假设一个具有树、狗和人类类别的深度神经网络模型，最佳模型将不会将树的高级特征暴露给人类类别的分类。控制反馈可能有助于实现这种条件信息流。</p><h2 id="4-2-Implementation-details">4.2 Implementation details</h2><p>以下是段落的逐句翻译：</p><p><strong>4.2 实施细节</strong></p><p>为了解决第3节中讨论的领域限制，我们使用注意反馈增强了现有的ST-GCNs，创建了Real-World Graph Convolutional Networks（RW-GCNs）。在所有变种的RW-GCNs中，我们假设存在一种窗口大小W的延迟约束。当W等于动作片段T中的总帧数时，RW-GCN充当通用的ST-GCN。对于RW-GCN的基线实现，将包含T帧的片段分解为W个窗口，每个窗口包含T/W帧，并对每个窗口生成一个动作分类。RW-GCN采用滑动窗口操作，其步幅等于T/W。</p><p>这个有限的窗口大小导致了有限的时间感受野。为了克服这一问题，RW-GCN在处理当前窗口的同时，将先前窗口的信息传递到网络中。为此，我们探讨了两种专门的反馈形式，即（1）语义反馈和（2）控制反馈用于RW-GCNs。每种公式都使用过去的特征作为输入（除了当前特征），并实现两种专门的注意力形式（语义注意力和通道注意力）。接下来，我们详细讨论每种形式。</p><p><strong>总结：</strong></p><p>在这一部分，作者介绍了为了解决第3节中提到的领域限制而引入的Real-World Graph Convolutional Networks（RW-GCNs）的实施细节。为了处理潜在的延迟约束，他们采用注意反馈来增强现有的ST-GCNs。RW-GCNs的不同变体在窗口大小W的情况下操作，可以作为通用ST-GCN，但作者也介绍了基线实现，其中通过滑动窗口操作处理T帧片段，以克服有限的时间感受野。为了更好地处理这一问题，他们引入了两种专门的反馈形式：语义反馈和控制反馈。每个形式都使用过去的特征作为输入，并实现了两种专门的注意力形式。这些细节旨在提高RW-GCNs在处理实际世界骨架动作识别时的性能。</p><h3 id="4-2-1-Semantic-Feedback">4.2.1 Semantic Feedback:</h3><p><strong>4.2.1 语义反馈:</strong></p><p>我们的语义反馈设计有两个目标。第一个目标是在考虑延迟约束的同时解决空间和时间噪声。通过首先仅在输入的一个窗口（或时间切片）上操作，并保留上一个窗口的高级语义特征以供下一个窗口的处理，实现了这一目标。然后，我们将这些特征馈送到我们的语义空间-时间注意块（语义注意块）中，如图3所示。</p><p>语义注意块与原始的自注意机制有显著的不同，后者通常以方程（7）的形式存在。 自注意(<em>Q</em>,<em>K</em>,<em>V</em>)=<em>so<strong>f</strong>t<strong>ma</strong>x</em>(<em>Q<strong>K</strong>T</em>/√<em>d**k</em>)×<em>V</em> (7) 这个实现是通过过去的特征和当前特征之间的点积来实现的，而不仅仅是当前特征。这是一种交叉关注。如果X中的时间信息高度冗余，那么我们的语义关注公式几乎等同于简化的自关注[11]。这可以为模型提供捕获更强大的空间特征的机会，并在理论上处理空间噪声。然而，在存在多样化的时间特征（即运动非常快）的情况下，语义关注块使得可以捕获跨帧的成对交互，从而通过最大化H(T)实现更精细的特征表示。</p><p>此外，我们的语义关注旨在具有计算效率。通过简化方程（7）的公式，我们的语义关注块将键和值的计算合并为X的单一投影，而不是两个[11, 16]。我们还受到[11]中的工作的启发，不对输出进行规范化。当我们将X的输入之一替换为我们的过去特征FB时，这使得我们的语义关注块能够捕获语义反馈。</p><p>为了进一步提高计算效率，我们将过去的特征压缩成1X32张量，然后应用语义关注。尽管关注机制的计算复杂度呈二次扩展，但这限制了计算成本。最后，我们通过1×1卷积加上以零初始化的批归一化，以及在注意特征上带有乘法门的残差，添加了门，有助于网络的收敛。我们反馈增强的语义关注块的最终公式如图4所示，这个公式使我们的ST-GCN能够实现语义反馈。 语义关注块放置在标准ST-GCN之前，如图3所示。在这项工作中，我们保留了Yan等人的相同基础架构[64]，如表1所示。重要的是要注意，ST-GCN块由GCN层和TCN层组成。GCN层的工作与本节前面描述的相同。时态卷积只是通过时间维度进行的1×9卷积，具有可变的步幅。</p><p><strong>总结：</strong></p><p>本节介绍了语义反馈的实现细节。语义反馈旨在解决空间和时间噪声，并考虑了延迟约束。该机制通过在输入的窗口上操作并保留上一个窗口的高级语义特征，然后将这些特征馈送到语义空间-时间注意块中。与原始的自注意机制不同，语义关注块采用了一种交叉关注的实现，其中点积考虑了过去和当前的特征。这允许模型更好地捕获空间特征，并理论上处理时间噪声。此外，语义关注块设计为计算效率高，通过将过去的特征压缩为1x32张量并添加门来进一步提高效率。最终，该机制被放置在标准ST-GCN之前，以实现语义反馈。</p><h3 id="4-2-2-Control-Feedback">4.2.2 Control Feedback</h3><p><strong>4.2.2 控制反馈：</strong></p><p>与实施语义反馈不同，RW-GCN 也可以实施控制反馈。控制反馈仅使用图4中所示的语义关注公式的压缩反馈特征。与通过时间和空间对特征进行成对交互的建模不同，我们的控制反馈实现使用过去的时空特征来主动加权通道，并有效地控制哪些特征传播更强。我们通过利用简单高效的通道关注[62]来实现这一点。这可以解释为特征选择，并允许整个网络在理论上根据过去的信号调整其信息流，以预期未来的信号（即最小化H(T|Y)或等效地减少冗余特征）。</p><p><strong>总结：</strong></p><p>本节介绍了控制反馈的实现。与语义反馈不同，控制反馈仅使用语义关注公式的压缩反馈特征。控制反馈利用过去的时空特征，通过通道关注主动加权通道，从而有效地控制特征的传播。这实际上是一种特征选择，允许整个网络根据过去的信号理论上调整其信息流，以预测未来的信号。</p><h1>5 Experimental Results and Evaluation</h1><p>这项工作评估了我们提出的RW-GCN在现实世界骨架动作识别领域的几个变体，重点是视频监控应用。为此，我们将RW-GCN分为三种变体:(1)基线，(2)受控反馈，(3)语义反馈。RW-GCN的基线实现不使用语义或控制反馈，而是简单地将过去的框架窗口与当前的框架窗口平均起来。我们称这种变化为共识RW-GCN。相反，我们的语义反馈变异被称为RW-GCN-SF，我们的受控反馈被称为RW-GCN-CF。所有实验都在Nvidia-V100 GPU上进行训练，使用Nesterov加速随机梯度下降，动量项为M = .9，权值衰减为WD = 10−4。除非另有说明，否则基础开始学习速率是lr = .01，并有10的衰减。此外，当训练任何反馈增强的RW-GCNs(不是共识)时，我们在特定时期实现了一个不断增长的架构策略。我们取基础RW-GCN的权重，对语义反馈和控制反馈分别“增长”语义注意和有效渠道注意。我们相信，通过更好地捕捉数据的不变性，这些注意力模型可以导致更鲁棒的压缩阶段[2,63]。</p><h2 id="5-1-Latency-Constraint-Evaluation-and-Analysis">5.1 Latency Constraint Evaluation and Analysis</h2><p><strong>5.1 延迟约束评估和分析：</strong></p><p>对于延迟受限准确性实验，我们关注 NTU-RGB-D-120 [30] 数据集。NTU-RGB-D-120 数据集是当前最大的姿势 RGB+D 人体动作识别数据集。该数据集包含 120 种不同的室内动作类别，是评估骨架动作识别作品的当前事实标准。每个样本片段在 30 FPS 下最多有 300 帧。此外，该数据集在单个帧中最多有 2 人。数据集由 106 个独特的主体组成，配有 32 种可能的设置。总体而言，它包含超过 114,000 个视频样本和 8 百万帧。原始 NTU-RGB-D-120 数据集的姿势格式包含 3D 坐标空间中的 25 个节点 (x, y, z)。我们忽略深度维度，并将节点数量减少到 18，以与 REVAMP2T 等端到端系统兼容。 此次训练的具体超参数使用了 N = 32 的批处理大小。基线 RW-GCN 模型在第 30 和第 60 个时期进行了学习速率衰减（10^(-1)）。对于该数据集，我们仅关注第一个骨架序列，即使场景中可能有两个演员。RW-GCN-SF 变体在第 40 个时期采用学习率重新启动，并训练 45 个时期，总共 85 个时期。所有共识模型均在 60 个时期内进行了训练。我们没有对此数据集进行 RW-GCN-CFs 的测试。在验证结果时，我们使用交叉设置和交叉主体验证策略[30]。 我们分析的第一个实验结果是训练基线 RW-GCN 共识模型的准确性曲线与使用语义反馈的 RW-GCN-SF 相比。由于用于训练反馈增强网络的时期较多，有必要将训练超参数的贡献与我们的关注反馈增强的贡献分开。图 5 显示了当共识模型的训练延长到相似数量的时期时，其准确性停滞在其现有准确性。这些学习曲线使用 50 和 90 时期的学习率重新启动，并且没有使用增长的训练策略。我们发现，当使用增长的反馈增强注意力进行训练时，RW-GCN 可以在更少的时期内达到相似的准确性，避免过拟合。 接着，我们将受延迟约束的 RW-GCN 的准确性与第 2 节中先前提到的多个现有作品进行比较。这不包括具有额外模态的作品，如 RGB 补丁。表 2 显示，仅通过利用 COCO 格式的关键点，RW-GCN 就能够实现 87.7% 的跨主体准确性，比常规 NTU-RGB-D 骨架基线高出 16.4%。这是在没有延迟约束的情况下完成的，距离之前的 SotA 作品 DSTA-net [52] 仅有 1.6% 的差距。我们认为性能提高是由于输入的维度减少所致。通过将节点数量从 25 降低到 18 并去除深度通道，我们将输入的维度降低了 48%。直观地说，这些维度被解释为冗余的，使模型能够专注于学习而不是降维和特征选择。 跨主体的结果进一步显示，我们具有延迟约束的 RW-GCN 在 W = 30 的情况下表现为 70.6%，比 SotA 低了 17% 以上。这是在拥有 3 倍较少帧的情况下完成的，因此在动作分类之间有 3 倍较少的延迟。事实上，假设这项工作将基于 NTU-RGB-D-120 数据集的 30 FPS 输入，SotA（以及除 FGCN [65] 外的所有现有作品）将具有相当于 10 秒延迟的动作产品延迟，这在实时视频监控等真实应用中可能是 prohibitively expensive。RW-GCN 具有 1 秒的延迟，我们将其分类为次实时。当使用语义反馈时，RW-GCN-SF 实现了 86.4% 的准确性，距离之前的 SotA 仅有 1.3% 的差距。这显著缩小了与 SotA 之间的差距，同时满足次实时延迟约束，从而使新兴的边缘应用得以实现。当放宽延迟约束以允许 3.33 秒的延迟时，我们成为新的 SotA（就交叉主体验证而言），准确性为 94.16%。这表明，我们的方法不仅满足了实际骨架动作识别的子领域，而且为基于骨架的动作识别作出了贡献。在进行交叉设置验证时，我们看到我们的模型并没有定义新的 SotA，而是仅比 SotA 少 0.4%。交叉主体和交叉设置结果之间的这种差异可能可以通过两个因素来解释。第一个因素是缺乏深度。由于我们依赖于 COCO 关键点，由于训练集和验证集之间的视点不同（以及缺乏特征来通知模型样本视点），我们在域中经历了更多的转变。第二个因素是跨主体的时间方差，这也导致验证和训练集样本分布之间的差异。由于我们的注意力反馈增强具有更大的灵活性来适应时间变化，因此可以学会对个体主体时间变化不变，最小化训练集和验证集之间的分布转移。这种解释在基线 ST-GCN 在交叉设置验证中的更好性能方面受到轻微质疑。这表明在存在时间接收场噪声的情况下，缺乏深度只是一个问题。</p><p>我们相信这种准确性是由于语义关注块在时间信息高度冗余或无关时（即窗口内的帧受到接收场噪声的影响较小）能够编码空间成对特征交互的能力，稀疏的时间特征交互需要对表示进行编码。因此，我们认为在时间噪声较低时，我们的模型编码了更有区别性的空间特征，从而实现了高准确性。我们还分析了其他作品在延迟约束方面与我们的性能匹配的能力，如表 3 所示。对于这个分析，我们没有训练这些现有方法，而是使用基本的共识反馈对它们进行验证。我们发现，如果没有训练，这些作品无法在真实世界的领域中执行，最好的作品仅达到 10.12%。 我们在 NTU-RGB-D-120 数据集上的最后一个延迟约束分析是对专注语义反馈增强到 ST-GCN 上的概括改进的消融研究。这种消融允许我们看到语义反馈执行两种操作模式。在 W = 30 的第一个延迟约束中，语义反馈发挥其预期功能，通过关注成对的特征交互保留模型的过去特征。W = 30 的延迟约束可能缺乏时间冗余，并隐含地强制执行类似于 FGCN [65] 的稀疏采样策略。因此，模型保留了其在长程时间特征交互方面的能力，并且准确性仅比其基线无约束 ST-GCN 实现低 1.3%，如表 2 所示。 当受到 W = 60 的窗口约束时，ST-GCN 的操作模式开始转变。它记录的特征更具有时间冗余性，而模型自然具有存储更长程时间相互作用的能力。因此，该模型改善了无约束基线 +2.3%。我们认为，这种窗口约束平衡了提高其空间判别能力与保持暂时性特征保持能力之间的任务。由于这两项任务的复杂性，与 W = 30 测试中的 16.4% 改进相比，模型在共识模型的准确性改善方面仅有 6.4%。最后，使用 W = 100 的约束，语义关注块可以保持最少量的时间特征表征力量，并专注于更有效地编码空间特征。由于共识模型无法提高 ST-GCN 的空间表征能力，共识模型的准确性停滞不前。</p><p><strong>总结：</strong></p><p>在延迟约束评估和分析章节中，研究侧重于使用 NTU-RGB-D-120 数据集进行实验。该数据集是目前最大的骨架动作识别数据集之一。实验中采用了 RW-GCN 和 RW-GCN-SF 变体，其中 RW-GCN-SF 引入了语义反馈机制。实验结果显示，通过利用 COCO 格式的关键点，RW-GCN 在无延迟约束情况下就能够实现较高准确性，较之前的 SotA 作品有显著提升。在具有延迟约束的情况下，RW-GCN 在较少的帧数下仍能取得较高的准确性，相比于其他作品具有更低的延迟。语义反馈机制的引入进一步提高了模型的准确性，特别是在满足次实时延迟约束的情况下。此外，与其他现有方法相比，该模型在没有进行训练的情况下也能够在真实世界领域表现出色。最后，通过对语义反馈的消融研究，显示了其在不同延迟约束条件下的作用，从而揭示了其对模型性能的影响。</p><h2 id="5-2-End-to-End-System-Error-Propagation-Analysis">5.2 End-to-End System Error Propagation Analysis</h2><p><strong>5.2 端到端系统误差传播分析</strong></p><p>端到端系统噪声分析超越了延迟约束分析，着眼于现实端到端边缘系统的新兴系统噪声。使用的数据集是 Northwestern-UCLA 数据集，也是一个骨架动作识别数据集。它包含 1494 个视频片段，每个片段平均包含 18.5 帧，标准差为 ±13.25。该数据集有 10 个可能的动作类别，非常适用于视频监控应用。采用了 [65] 使用的 Cross-View 验证策略，其中略去最后一个摄像头视图。</p><p>为了真正评估适用于视频监控应用的端到端系统噪声，我们将该数据集的 RGB 图像通过 REVAMP2T 处理，使用 EfficientHRNet-H0 [37] 和 REVAMP2T 论文 [36] 中使用的 MobileNetV2 特征提取器。这导致由于神经姿势估计器引入的关键点的空间噪声，以及由 REVAMP2T 的 Re-ID 模型带来的帧级时间噪声，另外还有极端的延迟约束 W = 10。</p><p>由于 Re-ID 改变了检测到的人数以及数据集的自然时间变化，输入张量 X ∈ RN×T×C×M 具有广泛变化的 T 和 M 维度。我们利用动态批处理步骤来处理这一问题，首先使用重复剪辑填充 T 维度，直到批处理中最大剪辑的长度。张量的 M 维表示多个人。由于 Re-ID 的误差传播到动作识别，我们可能在 M 维度中以不连续和不规则的方式中有单人剪辑。为解决这个问题，动态批处理会自适应地将输入求和到较小的 M = λ。如果 λ = 1，则称为 ID 无关场景，此时我们不再建模来自 Re-ID 的误差传播。在样本的 M &lt; λ 的情况下，我们将张量填充为 M = λ。</p><p>对于训练，所有共识模型最初训练了 30 个 epoch，然后在 epoch 30 时在共识模型的权重上添加了反馈模型，并进行额外的 40 个 epoch 的训练。由于动态批处理引入了时间噪声，我们使用批量大小为 8。验证批处理大小设置为 1，以最小化动态批处理带来的时间噪声。这组实验还利用了附加到每个 FB-Augmented 注意力的可学习乘性门，从零开始初始化。这使得反馈增强模型能够顺利地集成到共识模型中。</p><p>表 4 显示了在 NW-UCLA 动作数据集上的噪声输入下的准确性结果。它还将我们的方法与 FGCN [65] 进行了比较，后者是一个同样引入反馈的现有作品，作为 SotA。在没有空间关键点噪声和 Re-ID 噪声的情况下，我们超越了 SotA 1.3%。此外，结果表明，尽管 RW-GCN-SF 具有 W = 10 的延迟约束、EfficientHRNet-H0 的嘈杂关键点和 REVAMP2T Re-ID 带来的帧级时间噪声，但我们的方法仍能够与 FGCN 架构竞争。我们报告了 90.4% 的准确性，仅比 FGCN 的 SotA 低 4.9%。此外，Yang 等人 [65] 报告了他们的准确性，考虑到一个 64 帧的延迟约束（与我们的 10 帧相比），并随机采样。</p><p>我们相信，尽管存在上述噪声，我们能够在超过 90% 的准确性下进行动作识别，这对于现实世界的骨架动作识别和智能边缘视频监控应用至关重要。这一点在表 3 的结果中得到了进一步的加强，表明现有作品无法适应延迟约束的感受场噪声。此外，我们继续分析控制反馈和语义反馈模型之间准确性的差异。表 2 强调了控制反馈的表现甚至不如基线共识模型，而且无法与语义反馈一起使用。我们的主要假设是，由于 RW-GCN-SF 建模了成对的双线性特征交互，而 RW-GCN-CF 变体仅建模了线性交互，因此控制反馈的效果不如语义反馈。由于特征的极端延迟约束导致缺乏时间冗余，这一情况更加恶化。此外，通过分析动态批处理注入的时间噪声（通过将验证批处理 N &gt; 1），控制反馈方法达到了 84%，而语义反馈为 83.1%。我们的逻辑推测是，动态批处理过程中特征的不规则复制模拟了嘈杂的循环反馈，即通过相同的网络权重反复推送相同的特征，并且这充当了信息处理的形式。这种信息处理方式更类似于传统循环神经网络处理时间信息的方式。可以说，我们的 RW-GCNs 存在于一个模型家族中，其中网络深度和循环处理的依赖因每个变体而异。AGC-LSTM [54] 模型将是与我们的模型相反的变体，它在循环和注意力上具有较大的依赖，但对网络深度的依赖较小。对于像 AGC-LSTM 这样的模型，控制反馈可能比语义反馈更可靠，因为它控制信息传播。</p><p>Northwestern-UCLA 数据集的最后一项分析是关于 Re-ID 误差传播效应的消融研究。我们在两种情况下测试了 RW-GCN 架构。第一种是 ID 无关场景，其中 RW-GCN 试图通过对输入的 M 维度求和来人为地消除时间噪声，将被剪辑分开的时间特征重新连接成一个单一的 1 × T × V × C 张量。尽管这在人多、执行不同动作的拥挤场景中理论上可能效果较差，但 Northwestern-UCLA 数据集 [61] 的这类样本很少。我们看到 RW-GCN-SF 的最佳性能达到了 90.4% 的动作识别准确性。在第二种情况下，我们分析了与 Re-ID 噪声相关的真实动作识别。我们变化了衡量时间 IoU 过滤器与外观特征距离的超参数。我们发现 RW-GCN-SF 在 71.8% 的准确性上最为弹性。我们结果的一个有趣方面显示，以外观为中心的 Noise 1 对 RW-GCNs 的抑制更为明显。虽然这可能只是数据集的一个方面，但我们认为这值得未来进一步探讨。</p><p>总的来说，我们得出结论，通过沿 M 维度对输入特征进行零填充会引入太多噪声，从而降低了所有模型在真实场景中的准确性。预期基于 IoU 的 Re-ID 将优于基于外观的 Re-ID，但从外观噪声测试的顶级网络中仅获得了较小的改进。</p><p><strong>总结：</strong></p><p>本节继续对提出的骨架动作识别模型进行详细分析，特别是在端到端系统的噪声和误差传播方面进行了探讨。使用 Northwestern-UCLA 数据集，作者模拟了真实世界的视频监控应用场景，通过 REVAMP2T 处理 RGB 图像，并引入了空间关键点和 Re-ID 模型的噪声。在不同的延迟约束下进行了实验，并使用动态批处理处理由 Re-ID 引入的时间噪声。</p><p>实验结果表明，在实际场景中，即使存在大量噪声，提出的 RW-GCN-SF 模型仍能实现超过 90% 的动作识别准确性，超越了同类模型 FGCN。此外，作者分析了控制反馈和语义反馈模型之间的性能差异，发现语义反馈相对于控制反馈更为有效，特别是在极端的延迟约束下。最后，通过消融研究，作者进一步验证了模型对 Re-ID 误差传播的鲁棒性，并提出了对动作识别系统设计的有价值见解。</p><p>总体而言，提出的模型在复杂实际场景下表现出色，为骨架动作识别在视频监控等领域的应用提供了强大的性能和鲁棒性。</p><h2 id="5-3-Real-Time-Performance-Analysis">5.3 Real-Time Performance Analysis</h2><p>本节对模型在边缘设备上的实时性能进行了交叉分析，使用的是 Nvidia Jetson Xavier NX 设备。为了公平评估实时性能，引入了两个度量标准：每秒动作数（ApS），由方程（1）计算，以及动作产品延迟（APD），由方程（2）计算。值得注意的是，ApS是特定于设备的，而APD是端到端系统的。</p><p>该模型直观地支持更高的ApS，因为它在一个剪辑中产生多个动作分类。虽然在考虑实际场景时可能会贬低每个剪辑的多个输出的必要性，但要承认动作执行的速度既未知又依赖于数据。此外，动作识别模型通常不知道输入的帧率（除非经过训练，大多数模型没有），这导致了需要支持比理想训练场景中更大的ApS，并为将来训练模型以对不同输入帧率进行不可知处理提供了潜在可能性（如果不加以控制，可能导致准确性下降）。RW-GCN以 41.7 ApS 报告了最高性能，比现有模型高出 1.6 倍。从性能的角度来看，语义反馈的开销在ApS和参数方面都很小。正如在第4节中提到的，语义反馈被设计成轻量级，只在32个通道上工作。Shift-GCN 是参数效率最高的模型，因为它是专门设计用于轻量级和高效推断的，通过利用时间步长之间的数据移动。这种模型设计与 RW-GCNs 不是相斥的，在未来可以进一步探讨。</p><p>值得注意的是，RW-GCN 模型的 ApS 随着帧大小的变化而变化很大。这是由基于 GPU 的系统的执行偏差导致的。当我们减小窗口大小时，RW-GCNs 的数据依赖性被序列化并失去了性能效率。另一方面，当窗口大小增加时，由于需要更多帧来计算单个动作，我们面临与其他模型相同的问题。APD 的计算依赖于 EfficientHRNet 的 H0 FPS 为 22.95。RW-GCN 实现了最低的 APD，为 0.4 秒，这是从 Northwestern-UCLA 数据集得出的极端延迟结果。</p><p>最后，我们分析了 RW-GCNs 在轻量级和廉价边缘设备上的可扩展性。我们关注 Nvidia Jetson Nano，而不是之前在表6中展示的 Nvidia Xavier NX。截至本文撰写时，Jetson Nano 的价格几乎是 Xavier NX 的 10 倍。价格是确定系统成本可扩展性的重要因素，实际上通过负担得起的价格推动了人工智能的民主化。然而，Nano 的资源更为有限，只有一个 128 核的 Maxwell GPU，而 Xavier NX 则有一个 384 核的 Volta GPU 和 48 个 Tensor 核。</p><p>这是在更大的 RAM 带宽（约2倍）和更大的内存容量（约4倍）的基础上。我们使用 ApS 衡量我们的计算如何映射到这样一个受限环境。我们使用剪辑大小为 30 和窗口大小为 10 来计算 ApS。当一个场景中只有两个人时，我们看到 ApS 下降了 2.67 倍。当扩展到 8 人时，这一差距增加到 7.6 倍的 ApS 减少。我们认为这对于实时动作检测来说已经足够了，实际上我们相信可以进一步牺牲 ApS 来支持更多人数的场景。从理论上讲，我们可以顺序地跨越与我们的滑动窗口解决方案相似数量的人，以较大的时间框架处理场景。这可能允许我们以 1 的 ApS 处理包含 36 人的场景。未来的方向可以通过将更深的层级卸载到类似于[39]的服务器中来进一步优化。通过选择卸载深层次，可以保护隐私。另一个方向是利用输入自适应计算，类似于 Fang 等人的工作。【13】</p><p><strong>5.3 实时性能分析总结：</strong></p><p>本节详细分析了模型在边缘设备上的实时性能。使用 Nvidia Jetson Xavier NX 进行测量，并引入了两个度量标准：每秒动作数（ApS）和动作产品延迟（APD）。结果显示，RW-GCN 在 ApS 方面表现最佳，达到 41.7，比现有模型高出 1.6 倍。在考虑语义反馈的开销时，性能仍然相对较高。Shift-GCN 在参数效率上领先，专为轻量级和高效推断设计。值得注意的是，RW-GCN 模型的 ApS 随帧大小变化而变化较大，这与 GPU 执行的特性有关。最终，RW-GCN 在动作产品延迟方面取得了最佳结果，为 0.4 秒，这是在 Northwestern-UCLA 数据集上得出的。</p><p>最后，对于在轻量级和廉价边缘设备上的可扩展性，使用 Nvidia Jetson Nano 进行测试。虽然 Nano 相对于 Xavier NX 更便宜，但其资源更有限。在不同人数的场景下，ApS下降了，但仍然足够支持实时动作检测。此外，对于将模型部署到更大范围的场景，可以通过进一步优化和利用服务器进行深层次卸载来实现。</p><h1>6 Conclusion</h1><p>总的来说，本文定义了真实世界骨架动作识别的子领域，并利用基于信息理论的原理引入了Real-World Graph Convolutional Networks（RW-GCNs），在NTU-RGB-D-120数据集上取得了新的SotA准确率，达到94.16%。相比ST-GCN基准，RW-GCNs的延迟降低了3.02倍。此外，RW-GCNs在仅有3.8%准确率损失的情况下，可以比基准实现降低10倍的延迟。 通过对Northwestern UCLA数据集的评估显示，RW-GCNs在验证和训练中存在空间关键点噪声的情况下，可以达到90.4%的准确率，延迟比基准ST-GCN降低了32.5倍。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，在Northwestern UCLA数据集上的延迟比基准ST-GCN降低了32.5倍，并保持71.8%的准确率。所有这些都是通过一种注重隐私和可扩展的边缘计算方法实现的，其中系统成本每个节点可以降低10倍，同时仍然保持了一定范围的吞吐量，具体取决于场景的复杂性（从15.6到5.5 ApS）。 这项工作标志着真实世界骨架动作识别子领域的开始。通过设计RW-GCNs，本研究希望促进先前不可行的新边缘计算应用程序的设计和创建。然而，我们认为这一新兴领域仍然存在许多挑战，如时间变化、特定环境场景动态和进一步的应用特定约束。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mzl_视频动作识别调研</title>
      <link href="/project/posts/9e598b04.html"/>
      <url>/project/posts/9e598b04.html</url>
      
        <content type="html"><![CDATA[<h1>视频动作识别调研</h1><blockquote><p>视频动作分类技术【视频分类任务】</p></blockquote><blockquote><p>学习视频中时间和空间特征【Temporal feature时间特征 视频中人和物来进行区别 】【Spatial feature 空间特征辨别人怎么运动 】</p></blockquote><blockquote><p>落地过程</p></blockquote><p><img src="../assets/1%20(6).png" alt="1 (6)"></p><h2 id="1-open-pose">1. open pose</h2><p>OpenPose是<strong>开源==人体姿态估计==系统</strong>，它能够从==图像或视频中====准确地检测并估计人体的关键点位置==。 这使得<strong>OpenPose DW</strong>能够更好地捕捉人体姿态中的微妙细节，并对复杂动作进行更准确的分析。关键点识别的精度提升OpenPose DW通过深度和宽度的优化，取得了关键点识别精度方面的显著提升。</p><blockquote><p>输入视频或者图片，输出原有图片+关键点展示，关键点数据存储文件</p></blockquote><p><img src="..%5Cassets%5C200z51apve.gif" alt="img"></p><blockquote><p>参考链接</p><p><a href="https://cloud.tencent.com/developer/article/1373539?from=15425"> Github开源人体姿态识别项目OpenPose中文文档</a></p><p><a href="https://zhuanlan.zhihu.com/p/503396298">人体姿势估计openpose简单理解和应用</a></p><p><a href="https://cloud.tencent.com/developer/article/1775853?from=15425">实现人体姿态估计（人体关键点检测)</a></p></blockquote><p>简要介绍<a href="https://blog.csdn.net/qq_43258953/article/details/104441286">人体姿态估计算法之open pose-CSDN博客</a></p><p>项目源码<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation (github.com)</a></p><h2 id="2-openPose-DW">2.openPose DW</h2><p>OpenPose DW作为<strong>一种精细的人体姿态识别系统</strong>，通过==深度和宽度优化==取得了==关键点识别==精度方面的显著提升。 它具有广泛的应用前景，在人机交互、虚拟现实、体育分析等领域都能发挥重要作用。 然而，仍然需要进一步的研究和改进，以提高OpenPose DW的准确性和实时性，以满足实际应用的需求。</p><p>深度优化：OpenPose DW引入了==更深层次的卷积神经网络==（Convolutional Neural Network, CNN），通过增加网络的深度，可以提取更高级别的特征表示。这使得OpenPose DW能够==更好地捕捉人体姿态中的微妙细节==，并对复杂动作进行更准确的分析。</p><p>宽度优化：OpenPose DW通过增加网络的宽度，即==增加每个卷积层的通道数==，从而增加了==网络的容量和感受野==。这种宽度优化可以提高网络的鲁棒性和泛化能力，使得OpenPose DW在不同场景下都能稳定地进行姿态估计。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">官网：<span class="attr">https</span>:<span class="comment">//github.com/IDEA-Research/DWPose论文地址：https://arxiv.org/abs/2307.15880</span></span><br><span class="line">配套骨骼图下载：<span class="attr">https</span>:<span class="comment">//pan.quark.cn/s/b73b2531675f</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>输入</strong>：图像、视频、网络摄像头、Flir/Point Grey、IP 摄像头，并支持添加您自己的自定义输入源（例如，深度摄像头）。</p><p><strong>输出</strong>：基本图像+关键点显示/保存（PNG、JPG、AVI等），关键点保存（JSON、XML、YML等），KEY点作为数组类，并支持添加自己的自定义输出代码（例如，一些花哨的UI）。</p></blockquote><blockquote><p>参考链接</p><p><a href="https://cloud.tencent.com/developer/article/2358119?areaId=106001">更加精细的OpenPose DW Openpose(23)</a></p><p><a href="https://cloud.tencent.com/developer/article/2335410">更加精细的OpenPose DW Openpose</a></p></blockquote><h2 id="3-model-scope-开源识别模型">3.model scope 开源识别模型</h2><h3 id="1-Patch-Shift-Transformers">1.Patch Shift Transformers</h3><p>魔搭社区中有一个开源的识别模型</p><p>Patch Shift Transformers(PST) 是在2D Swin-Transformer的基础上，增加temporal建模能力，使网络具备视频时空特征学习能力。而这一操作几乎不增加额外参数。具体地，通过shift不同帧之间的patch, 然后在每帧内部分别进行self-attention 运算，这样使用2D的self-attention计算量来进行视频的时空特征建模，论文原文<a href="https://readpaper.com/paper/4650578659522920449">链接</a>。</p><p><img src="../assets/1%20(5).png" alt="1 (5)"></p><blockquote><p><a href="https://www.modelscope.cn/models/damo/cv_pathshift_action-recognition/summary">PST动作识别模型-tiny · 模型库 (modelscope.cn)</a></p></blockquote><blockquote><p><a href="https://github.com/MartinXM/TPS">MartinXM/TPS: A simple but efficient transformer model for video action recognition (github.com)</a></p></blockquote><h3 id="2-日常动作检测">2.日常动作检测</h3><p>输入视频文件，输出该段时间内视频所包含的动作。算法内部每两秒均匀采样4帧输入到动作检测模型中，然后按设定时间步长滑动对整个视频的动作进行检测并返回结果。CUDA和CPU运行环境均支持。</p><p><img src="../assets/1%20(6).png" alt="1 (6)"></p><blockquote><p><a href="https://modelscope.cn/models/damo/cv_ResNetC3D_action-detection_detection2d/summary">日常动作检测 · 模型库 (modelscope.cn)</a></p><p><a href="https://detectron2.readthedocs.io/en/latest/tutorials/install.html">https://detectron2.readthedocs.io/en/latest/tutorials/install.html</a></p></blockquote><p>阿里云社区带的日常动作监测库教程</p><p><a href="https://detectron2.readthedocs.io/en/latest/index.html">Welcome to detectron2’s documentation! — detectron2 0.6 documentation</a></p><h2 id="4-mmaction-2">4.mmaction 2</h2><p>MMAction2 是一款基于 PyTorch 的视频理解开源工具箱，是 <a href="https://gitee.com/link?target=https%3A%2F%2Fopenmmlab.com%2F">OpenMMLab</a> 项目的成员之一</p><blockquote><p>开源的数据仓库，有对应的安装和使用文档，以及对应的数据集测试过程等</p></blockquote><blockquote><p><a href="https://gitee.com/open-mmlab/mmaction2">mmaction2: 基于 PyTorch 和 MMCV 的视频理解工具库，支持动作识别、动作定位、时空动作检测和骨骼动作识别等多种任务。 (gitee.com)</a></p></blockquote><p><img src="../assets/1%20(1).gif" alt="1 (1)"></p><blockquote><p><a href="https://gitee.com/open-mmlab/mmaction2">mmaction2: 基于 PyTorch 和 MMCV 的视频理解工具库，支持动作识别、动作定位、时空动作检测和骨骼动作识别等多种任务。 (gitee.com)</a></p></blockquote><p><img src="../assets/1%20(1)-1703393662178.png" alt="1 (1)"></p><blockquote><p>中文教程<a href="https://mmaction2.readthedocs.io/zh-cn/latest/index.html">MMAction2 中文教程! — MMAction2 1.2.0 文档</a></p><p><a href="https://github.com/open-mmlab/mmaction2">open-mmlab/mmaction2: OpenMMLab’s Next Generation Video Understanding Toolbox and Benchmark (github.com)</a></p></blockquote><p>MMPose 是一款基于 PyTorch 的姿态分析的开源工具箱，是 <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopen-mmlab">OpenMMLab</a> 项目的成员之一。</p><p><img src="https://user-images.githubusercontent.com/26127467/226655503-3cee746e-6e42-40be-82ae-6e7cae2a4c7e.jpg" alt="yolox-pose_intro"></p><blockquote><p><a href="https://gitee.com/open-mmlab/mmpose">mmpose: 基于 PyTorch 的姿态估计算法库，支持人体、人手、人脸、动物、服装等多类物体的 2D/3D 姿态估计。 (gitee.com)</a></p></blockquote><h2 id="5-slow-fast">5.slow fast</h2><blockquote><p>CNN 领域 比较强的一个模型</p><p>使用双流 + 帧内空间特征和帧时间序列特征学习</p></blockquote><blockquote><p><strong>提出了一种新的快慢网络SlowFast架构，来实现两个分支分别对时间与空间维度进行处理分析</strong>。</p><ul><li><strong>Slow分支</strong>：较少的帧数以及较大的通道数学习空间语义信息。</li><li><strong>Fast分支</strong>：较大的帧数以及较少的通道数学习运动信息</li></ul></blockquote><p><img src="../assets/1%20(4).png" alt="1 (4)"></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/457854899">深度学习视频理解（分类识别）算法SlowFast原理解读 </a></p></blockquote><p><img src="../assets/1%20(2).gif" alt="1 (2)"></p><blockquote><p><a href="https://github.com/facebookresearch/slowfast">facebookresearch/SlowFast: PySlowFast: video understanding codebase from FAIR for reproducing state-of-the-art video models. (github.com)</a></p></blockquote><h2 id="6-Multi-person-Real-time-Action-Recognition-Based-on-Human-Skeleton">6.Multi-person Real-time Action Recognition Based-on Human Skeleton</h2><p><img src="../assets/1%20(3).gif" alt="1 (3)"></p><p><a href="https://github.com/felixchenfy/Realtime-Action-Recognition?tab=readme-ov-file">felixchenfy/Realtime-Action-Recognition：将 ML 应用于 OpenPose 中的骨架动作识别（警告：很抱歉，这只适用于课程演示，不适用于现实世界的应用程序!!那些非常困难!!） (github.com)</a></p><p><img src="../assets/1%20(3).png" alt="1 (3)"></p><h2 id="资源汇总">资源汇总</h2><p><a href="https://developer.aliyun.com/live/251465">达摩院-开放视觉智能实验室 | OpenVI Tech Talk No.5：视频动作识别前沿技术介绍-云视频-阿里云开发者社区 (aliyun.com)</a></p><blockquote><p>论文list</p></blockquote><p><a href="https://github.com/jinwchoi/awesome-action-recognition">https://github.com/jinwchoi/awesome-action-recognition</a></p><blockquote><p>阿里云开放平台</p><p><a href="https://help.aliyun.com/zh/viapi/developer-reference/api-action-recognition?spm=a2c4g.11186623.0.0.65cf2ccfIO6Mgu">动作行为识别功能介绍及使用方法 (aliyun.com)</a></p></blockquote><p>视频动作识别分类</p><blockquote><p>cnn 卷积算法</p><p>transformer （vision transformer  video swin transformer  shift transformer ）</p><p>self -supervised方法（video MAE  (Encoder-Decoder 模型)    Masked方法）</p><p>多模态（视频 文本 图片  /clip ）</p><p><a href="https://zhuanlan.zhihu.com/p/639351055">视频动作识别前沿技术介绍)</a></p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/456817021">深度学习视频理解（分类识别）算法梳理</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Mzl_古典写作训练营</title>
      <link href="/project/posts/bf4a1918.html"/>
      <url>/project/posts/bf4a1918.html</url>
      
        <content type="html"><![CDATA[<h1>古典老师共读会</h1><blockquote><p>了解古典老师，老师有一个职业规划相关的课程，感觉很打动自己，自己一直在思考 到底是什么东西的缺失——其中一样是职业规划上没有做好</p><p><a href="https://zhuanlan.zhihu.com/p/129650975">古典老师职业规划公开课（7000字全） - 知乎 (zhihu.com)</a></p><p>首先是没有这个意识，再次是没有这个机会，也丝毫从来没想过</p></blockquote><p>古典老师  带领写文章，完成读书与知识系统搭建的全套流程</p><blockquote><p>读书不是为了读书而读书，而是为了内化知识，让自己变得更加智慧</p></blockquote><ol><li>书中并没有知识，而有的只是信息，只有经过读书人内化之后，与自己的过去经历和思考之后的东西形成链接之后才会叫知识；<code>这一点很打动我</code></li><li>新知与旧概念的关系</li><li><strong>知识就是能够降低生活中所有的不确定性</strong></li></ol><h2 id="day0-开营">day0 开营</h2><blockquote><p>开营汇总链接 ——课程安排和介绍</p><p><a href="https://dxpqyi2j4o.feishu.cn/docx/EFGpdFZfZo1pb8xtxVDckCtXnRb">Day0-共读会体验营开营整理 - Feishu Docs</a></p></blockquote><p>古典老师</p><p>第一部分：3天的课程安排<br>第二部分：【个人发展共读会】是什么？<br>第三部分：【个人发展共读会】包含哪些内容</p><p>学习感悟是将信息内化成自己的知识的过程，很重要</p><p>但是你真的问他，那你简单复述一下，读过的书的要点？你觉得和自己生活对应起来，有什么感悟？你是怎样用到自己生活工作里去的？</p><p>所谓经典，就是在一个领域里，能够把整个知识从头到尾串起来的作品。经典是碎片知识收纳器，是领域知识地图，是牛人思维复刻机。具体可以直接看我之前的文章<br>读经典 且包含整个系列的经典书籍</p><p><img src="../assets/image-20231224124510014.png" alt="image-20231224124510014"></p><p>从高空到地面到边界，点亮「个人发展全景图」。</p><p><img src="../assets/image-20231224124558054.png" alt="image-20231224124558054"></p><p>这是明天的早读链接大家提前预习，期待看到大家的感悟输出</p><h2 id="day1早">day1早</h2><blockquote><p>早7:00 第一节课 <strong>如何真正读透一本书</strong></p></blockquote><p>古典老师02年新东方到23年一直在做的事情</p><p>为什么做个人发展共读会？以及怎样做的 或者说是做的形式是什么？</p><p>08年-23年一直在做 <strong>生涯发展</strong> <strong>个人发展共读</strong></p><blockquote><p><strong>拆掉思维的墙  10年再版</strong></p></blockquote><p><img src="../assets/image-20231224124226255.png" alt="image-20231224124226255"></p><p>为什么读了很多书 没改变？</p><blockquote><ol><li>读了 没理解<ol><li>读了用的时候想不起来</li></ol></li><li>读的书太杂，并不系统，来源不清晰稳定</li></ol></blockquote><p>第一期的小伙伴 在读跃迁   读跃迁书的内容很重要，但是更重要的是跃迁实现的方式 和这个算法的怎样造成影响以及怎样用这个算法到自己生活中 更重要</p><p>怎样跟业内的 高手交流 沟通的 请教的 以及有什么方式 把自己这么多年的积累 去应用</p><p>以及怎样从写作的角度 来进行反推  该怎样读书</p><p>两个部分</p><ol><li>怎样选书<ol><li>个人发展共读会怎样去选书</li></ol></li><li>怎样读书 以及应用<ol><li>怎样读透一本书 怎样用卡片的方式去读书</li></ol></li></ol><p>知识分子  一辈子 做6件事 <strong>输入和输出</strong></p><p><img src="../assets/image-20231224124241336.png" alt="image-20231224124241336"></p><blockquote><p>算上教材编写270w字  2kw收入</p></blockquote><blockquote><p>每天听本书 kindle 微信读书</p></blockquote><ol><li><p>听别人演讲的时候  把自己读的书用上了 说出来 自己却不会用</p></li><li><p>出租车司机知识 —— <strong>小故事</strong>【相对论知识  司机蒙住了】——糟糕的知识，只会转述，只会引述，并不会用它解决问题</p><ol><li>出租车知识的承载者</li></ol></li><li><p>能讲出来 但是写不出来文章 <strong>写自己的感想写不出来</strong></p></li><li><p>大部头书 读不懂，看不完</p><ol><li><strong>社会心理学，发展心理学  追忆似水年华，百年孤独 毛泽东传 原则 人类简史</strong></li></ol></li></ol><blockquote><p>知识 ——&gt; 应用  读书≠知识</p></blockquote><p><img src="../assets/image-20231224124301394.png" alt="image-20231224124301394"></p><p>信息论发明者 克劳德·香农</p><p><img src="../assets/image-20231224124315596.png" alt="image-20231224124315596"></p><blockquote><p>品酒师  喝一口就能品出来  喝酒收钱</p></blockquote><blockquote><p>知识 vs 信息  （瓜）</p><p>越学越乱，并不能说明学到了知识</p></blockquote><h3 id="评估阅读深度">评估阅读深度</h3><p><img src="../assets/image-20231224124330609.png" alt="image-20231224124330609"></p><blockquote><p>data 数据  information 信息【分门别类 图上颜色 分清好坏了 】  knowledge 知识（与过去有链接，并不单纯是一个点）【信息能够互相连接 】</p><p>恍然大悟insight【洞察】 形成了见解  wisdom 面对事情 【洞察形成一片 】能够打通形成智慧    impact 洞见【单点洞穿一片 】击穿 第一性原理，基础原理  底层原理  道</p></blockquote><blockquote><p>好的文章 都是一些洞察  写文</p></blockquote><h3 id="阅读的有多深，输出就有多好">阅读的有多深，输出就有多好</h3><p>发圈—— 刷过</p><p>好奇——读懂了之后，引发好奇</p><p>应用到生活中——学会了某个知识，与过去形成链接</p><p>两个好的知识点以一种独特的形式链接在一起 就是写文——洞见</p><p>有一个很好的打通 —— 写书</p><p>经典——一个点贯穿系统，击穿人类知识体系</p><blockquote><p>反脆弱这个概念 击穿了所有文章</p><p>原则 用人像机器一样进化  击穿了所有的算法</p><p>人类简史 把讲故事 击穿了整个人类过程</p><p>相对论 用光速不变 击穿了所有时间进化</p></blockquote><p><img src="../assets/image-20231224124340520.png" alt="image-20231224124340520"></p><h3 id="读书从来不是要记忆，重要的是要理解和链接">读书从来不是要记忆，重要的是要理解和链接</h3><blockquote><p>新知识永远都是新东西和新概念链接到旧的只是上来获得的</p></blockquote><p><img src="../assets/image-20231224124402340.png" alt="image-20231224124402340"></p><p>书单 是自己个人过去的链接，以及与未来相关的一个发展方向展示</p><blockquote><p>有过去</p><p>写故事 编剧  戏剧的书 用戏剧方式表达出来</p><p>短视频时代，每个人都要学会借助人的视觉化思维，要用故事化的方式进行表达</p></blockquote><blockquote><p>书评 了解 书评是别人嚼过的口香糖</p></blockquote><blockquote><p>自己能把他复述一遍，遇到问题可能还不能用</p></blockquote><h3 id="知识存储方式">知识存储方式</h3><blockquote><p>错误的存储方式</p></blockquote><p><img src="../assets/image-20231224124412324.png" alt="image-20231224124412324"></p><blockquote><p>真实的存储结构</p></blockquote><p><img src="../assets/image-20231224124443032.png" alt="image-20231224124443032"></p><p>多去读增量知识，而不是延展概念</p><blockquote><p>关键知识和原知识，能够吸引很多其他的小知识到一起</p></blockquote><blockquote><p>这种知识应该多去读</p></blockquote><h3 id="回顾">回顾</h3><p><img src="../assets/image-20231224124454161.png" alt="image-20231224124454161"></p><blockquote><p>读书这件事不重要，有链接和理解才是最重要的</p></blockquote><blockquote><p>不断内化+ 不断应用+ 复习深入理解</p></blockquote><h4 id="个人复述">个人复述</h4><blockquote><p>读书 和阅读深度</p><p>数据  信息  知识  洞见  智慧 击穿 一套体系</p><p>复述并不断与旧的只是形成链接</p><p>原知识 扩展成现在的思维知识体系</p></blockquote><blockquote><p>知识的概念是能够不断的降低生活中的不确定性</p></blockquote><h3 id="作业">作业</h3><p>day1作业：</p><ol><li>你之前是如何读一本书的？效果怎么样？<ol><li>这段时间会去图书馆读之前想读的书，或者之前偶然间碰到的书，凭借兴趣和想法，还有一些偶尔读到的书去延伸去读，包括专业方面（计算机专业方面）和非专业方面（社科，小说，传记）</li><li>听书，参加了樊登读书： 买了樊登读书的会员，每天偶尔会在耳边听一些书，或者随缘听一些想读的书</li></ol></li></ol><p>2.听完古典老师的课程，接下来你准备怎么读书？</p><ol><li>整理下自己已经读过或者了解的书目，包括所有已知道的知识，尝试搭建一下个人读书系统</li><li>有目的的去读，思考下现阶段要做的事情以及会碰到的问题，或者未来想要成为什么样的人或者做什么样的事情，去读一些相关的书目并进行复述和理解，增强个人知识的链接并尝试不断应用</li></ol><blockquote><p>晚课汇总链接_——为什么读写是最好的个人投资</p><p><a href="https://dxpqyi2j4o.feishu.cn/docx/JUc4drgTRodfNLxBSs1crdqanbe">共读会体验Day1晚课整理 - Feishu Docs</a></p></blockquote><h2 id="day2-早">day2 早</h2><blockquote><p>卡片是什么 该怎么写 写多长时间 写多少字 400-500字左右</p></blockquote><h3 id="作业-2">作业</h3><p>补day2作业:<br>1.学完今天的课程，卡片格式是什么？你对这几个部分是怎么理解的？</p><p>卡片格式，一共分为三部分，包括核心概念，个人体验和行动指南;</p><ol><li>核心概念:【不是…而是…】首先可以先将原文抄录下来，然后用自己话进行总结复述一遍</li><li>个人体验：【我有一个经验、感悟、发现等，来加深体验】 ；应用实例、联系案例，更深更生动理解和呈现出概念的理解</li><li>行动指南：  【当……时候，要……做，具体落实】把书中对应的概念落实到生活，作为自己的行动指南顺便也可以给SS卡做一个标签和命名。</li></ol><p>核心概念转述——【复述并记忆】</p><p>个人体验——【链接过去，加深理解】从以前的观念升级到另一个层面或是深度的理解</p><p>行动指南——【写下行动计划】作为自己的原则，生活中再碰到问题的时候翻阅卡片去应对</p><p>理解： 通过知识卡片，可以将自己学到的知识存起来，既可加深理解，又能偶尔翻阅，需要时也能及时找到。<br>是一个很好用的学习方法。</p><p>2.你觉得卡片在你接下来的工作和学习中能帮你做些什么？</p><ol start="0"><li><p>思维的改变——转述+链接+ 应用</p></li><li><p>学习方法【各种知识的一个方法改变】</p></li><li><p>读书方法【应对读书时候的一种新的体验】</p></li><li><p>尝试往工作和其他方面去应用</p></li></ol><h3 id="新概念学习">新概念学习</h3><p><strong>集装箱原理，存入的越难，提取就容易</strong><br>【我的理解】卡片可以帮助我们把知识内化成自己的知识体系，用自己的语言转述是建立自己对知识的理解和吸收上，把新知和自己的知识经验勾连起来，情绪体验越深越好，有了深度理解这个知识才记得更深。<br>在阅读时遇到自己感兴趣或者有触动的地方，可以停下来，针对这个部分进行理解用自己的语言转述，并与过去自己的经验和认知感悟进行连接发散，这样就可以记得更牢更深。<br>最后通过行动指引，把知识学以致用，通过行动指引存一个心锚，为未来遇到相关问题时提供行动指南。<br>我好像更明白为什么自己读了很多书，但心里依然觉得很空虚，很迷茫，解决问题的思维和能力没有得到提升。很关键的就是过去的读书只是在扫描信息，而没有把知识与过去勾连，也没有用知识来解决问题，破除不确定性，真的是存入越简单，提取就越难。卡片笔记让学习真正与自己有关，让知识更有深度和宽度</p><p>卡片可以帮助我更好的理解知识，让阅读留下思考、理解、运用的痕迹，通过攒卡片建立有体感的知识宫殿，提高自己的认知深度和运用知识的能力，为知识输出打下坚实的基础。</p><blockquote><p>卡片制作难，难存入但是将知识内化效率高，易输出。真正的能帮助深入链接、思考，形成知识网络。可以帮助我梳理文案，助于书写报告，已经短视频。慢慢的写，持续的写，未来自己的思考深度，写作能力一定会有变化！期待。</p></blockquote><p>输入的越难，提取的越容易。这是一种思维，能让我初步理解看网课和看书没用的原因，都是被动输入且没有思考，在做题和处理事件的时候才会没什么效果<br>（一百个人中就有一百个哈姆雷特，让这一切贴近自己）</p><blockquote><p>能够有助于我建立自己的知识体系，当遇到有用的知识时，将其加上自己的理解记录下来，并做好标签，加深理解的同时也建立起自己的知识库，做到既能输入又能输出。</p></blockquote><p>1.学完今天的课程，卡片格式是什么？你对这几个部分是怎么理解的？<br>卡片格式是：原文摘录、概念转述、个人体验、行动指引。<br>原文摘录：就是在看书、文章、视频等过程中，发现有意思、有触动、有启发、有感想的内容，直接原文摘录下来。<br>概念转述：就是把原文用自己的话重新说一遍，也可以说是“翻译”一下。也可以说用其它的词重新组织一下，把这个意思表达出来。<br>个人体验：就是结合自己的一些经验、感触、也或是一些思考，对这个有意思的知识点再重新组织描述一下。<br>行动指引：有了思考，有了和自己经验的结合，如何再以后的工作学习中“实践”，对自己以后有哪些指引。<br>2.你觉得卡片在你接下来的工作和学习中能帮你做些什么？<br>找到一个领域或问题，进行有目的的选择一些书来阅读，然后用ss卡片法对书进行深度阅读，积累卡片逐步解决问题，加深理解，直至解决问题，成为行家或专家。</p><blockquote><p>晚课汇总链接——为什么普通人要有自己的作品</p><p><a href="https://dxpqyi2j4o.feishu.cn/docx/KqEId690LoUnkex6VzscclLvnqb">共读会体验营Day2晚课整理 - Feishu Docs</a></p></blockquote><h2 id="day3">day3</h2><h3 id="作业-3">作业</h3><p>古典将书分为3类</p><ol><li>修身养性的书【陶冶情操类】——边城</li><li>经典的书【架构性数目】——动机心理学</li><li>功能性数目【技术类】——秒赞</li></ol><p>day3作业：<br>1.你觉得接下来应该读哪些书？<br>我觉得古典老师提到的三类书籍都可以读。经典书籍要保证，其它两类看自己的爱好和需要即可。<br>2.你觉得选书和掌握一套善思会写的方法哪个更帮到你？为什么？<br>两个都有用。选好书读，会少走很多弯路，选好书相当于你可以上高速路，免得在乡间小路奔波。而善思会写的方法会帮我们读得更深甚至完成一部作品，它相当于给了你一个清晰的导航或者地图，否则很难或者永远也来不到目的地。</p><h3 id="直播">直播</h3><blockquote><p>弟弟在直播间中每一个明星 打造了每个ip  需要回答2000 多个问题</p></blockquote><p>读写的好处</p><ol><li>管理情绪 拖延 焦虑 抑郁</li></ol><blockquote><p>在书中 找到关于情绪 的各种东西</p><p>比如抑郁  焦虑  拖延</p><p>将小拖延 整理下来 放到大的拖延前面去</p></blockquote><p>管理拖延</p><p>读书就像串门一样，每一本书 都是一个人，</p><p>读书好比隐身的串门儿，要参见钦佩的老师或拜谒有名的学者，不必事前打招呼求见，也不怕搅扰主人。<br>——杨绛</p><p>正念书写</p><p><img src="../assets/image-20231224124706929.png" alt="image-20231224124706929"></p><p><strong>1，焦虑 恐惧  —— 不确定的恐惧</strong></p><p>写下你的情绪 和 焦虑，写着写着就变好了，书写能够舒缓你的焦虑</p><p>2.情绪 心情 的即刻获得，与身边朋友 老师 家人产生化学反应，能够获得以后有钱后才能获得的那种感觉</p><blockquote><p>比如他说的 下边那一种都能给自己带来回报的（姐姐和妹妹之间表达感情 书写感情）</p></blockquote><p><img src="../assets/image-20231224124717756.png" alt="image-20231224124717756"></p><blockquote><p>所有的东西 都是文字</p><p>鼓励每个人给自己的家庭写一份信，</p><p>记录父母的人生，父母看的时候 会很心安</p></blockquote><p><img src="../assets/image-20231224124730405.png" alt="image-20231224124730405"></p><p>功利化的 对待读书 这件事情</p><ol start="3"><li>改变生活，知识改变命运</li></ol><blockquote><p>怎样摆脱一般人</p><p>读一些一般人不会读的书，总结一些一般人不会总结的</p></blockquote><p><img src="../assets/image-20231224124739418.png" alt="image-20231224124739418"></p><p><img src="../assets/image-20231224124750688.png" alt="image-20231224124750688"></p><blockquote><p>3.6倍溢价</p></blockquote><p><img src="../assets/image-20231224124807078.png" alt="image-20231224124807078"></p><p><img src="../assets/image-20231224124817439.png" alt="image-20231224124817439"></p><h1>参与读写共读会</h1><blockquote><p>目标： 成为推广大使【成为带课班班】+参加线下活动认识 + 认识很多新朋友，感觉好久没有新朋友认识了</p><p>自律就是找到自己的节奏，成长就是找到自己的方式</p></blockquote><p>12.30号  面基</p><p><img src="../assets/image-20231224124828513.png" alt="image-20231224124828513"></p><blockquote><p>家书的力量</p><p>带领孩子 父母 一起写起来</p></blockquote><blockquote><p>写作是跨越时空的</p></blockquote><p>已更新25本书直接听</p><p>1.每个月有8节解读课<br>2.初阶写作营（可延后学）【21天】<br>3.高阶写作营（可延后学）【28天】<br>4.AI写作营（可延后学）</p><p>3门成长课（可以永久回看，慢慢看就行）<br>6.已经更新的24本书的解读课（选修，看不看都行，或挑自己喜欢的看也可以）<img src="../assets/image-20231224124840904.png" alt="image-20231224124840904"></p><p>5个人发展大地图邮寄</p><blockquote><p>樊登老师 每周六更新一节课</p><p>先把上周的看了</p></blockquote><blockquote><p>越早完成21 天读写应 越好 + 进阶的课程</p><p>养成读写思维习惯</p></blockquote><h1>赚回学费</h1><h3 id="1-稍微小赚">1.稍微小赚</h3><blockquote><p>读3-5本书 +践行一个方法，改变自己的生活方式</p></blockquote><blockquote><p>刷视频的时间 转成刷同学的卡片+ 同学联机的行动，获得N多个不同的视角读书体验和收获，杜绝长期刷视频的习惯</p></blockquote><h3 id="2-200-到无穷">2.200%到无穷</h3><blockquote><p>深度读6本书 + 生活工作中真实践行 + 提升自己不同维度的认知</p></blockquote><blockquote><p>完成21 天和28天  一套好的读写思维+掌握好文章的底层方法</p></blockquote><blockquote><p>一本书写5-10张卡片，养成习惯后 积累自己的人生体验，形成自己转述的独特人生素材</p></blockquote><blockquote><p>利他者，影响着 成为志愿者：支持他人，以教带学</p><blockquote><p>内部成为志愿者，组长 助教 带教（有偿） 发起活动</p></blockquote><blockquote><p>外部 分享经历，影响更多人，申请共读大使，获得副业收入</p></blockquote></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>晨读日更</title>
      <link href="/project/posts/3e014ae4.html"/>
      <url>/project/posts/3e014ae4.html</url>
      
        <content type="html"><![CDATA[<h1>12.22日冬至 周五</h1><p><img src="../assets/image-20231224124904345.png" alt="image-20231224124904345"></p><h2 id="英语角">英语角</h2><p>Sometimes <strong>accompanied</strong> ，sometimes alone,stay <strong>awesome</strong> all the time.<br>聚散终有时，潇洒走一回。(有时陪伴，有时独自一人，时刻保持精彩。)</p><blockquote><p>awesome 令人敬畏的;令人惊叹的;令人惊惧的, 很好的；了不起的。</p></blockquote><p>His actions were accompanied by a rambling monologue<br>他的行动伴随着一串冗长含糊的独白。</p><blockquote><p>rambling 杂乱无章的;冗长芜杂的;漫无边际的, 延伸, 向四处延伸的;蔓延到很大区域的;蔓生的, 散步, （尤指在乡间）漫步，漫游，闲逛, 话／文字。</p></blockquote><blockquote><p>monologue 独白 （一个人的）滔滔不绝的话，长篇大论, 独角戏。</p></blockquote><p>She still had a mind of her own, which is awesome.<br>但是她还是有自己的思想的，她其实是个很好的女人。</p><h2 id="2023-12-22晨读">2023.12.22晨读</h2><p>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>俯瞰3000年儒学，读懂中华文明之根。<br>——来自帆书《中国儒学三千年》</p><p>👉【精彩选段】<br>“儒”这一名词最早记载见于《论语·雍也篇》。孔子在这里告诫他的学生子夏说：要当就当“君子儒”，千万不要当“小人儒”。</p><p>👉【作者之语】<br>对儒家精神的理解应从常识入手，不必人为赋予其某种神秘意味，==只要以平常心对人对己==，就能把握儒学的真谛。</p><p>👉【樊老师之语】<br>这是每个中国人都应该补的一堂文化课。作者从大历史的视角，逐一介绍各个时代的儒学大师的主要成就以及儒家流派、思想流派。一部中国史，就在3000年儒学史里，让我们一同走进3000年儒学中，感受中华文化的跳动脉搏。</p><h2 id="名人语录">名人语录</h2><p><strong>能量加毅力可以征服一切 ——— 富兰克林</strong></p><p>在平凡的生活中，只要你肯付出、肯努力，总有一天，你会站在最亮的地方，活成自己渴望的模样。</p><h2 id="12-22日独白">12.22日独白</h2><p>今天是冬至,但是是周五，要面临开组会了，今天早上看到老师985的车，看来组会一定要开了</p><blockquote><p>下边一段是读书会关于冬至的总结</p><blockquote><p>『冬至已至』</p><p>💨【诗歌欣赏】</p><pre><code>      《满江红·冬至》          南宋·范成大    寒谷春生，熏叶气、玉筒吹谷。    新阳后、便占新岁，吉云清穆。</code></pre><p>💨【诗歌译文】</p><p>🔹寒冬季节，山谷里却早一步有了春意，蕙草初生新叶的香气就像袅袅的笛音若有若无地在山谷里弥漫开来。明早的太阳升起后就可占候新一年的年景了，肯定是丽日纤云、天气清和的好年景。</p><p>💨【关于冬至】</p><p>🔹古时有“日短”或“日短至”之称，这一天，北半球各地白昼最短，黑夜最长，越往北白昼越短。古人认为，冬至是指阴气到达极点，此后阳气渐渐回升～</p></blockquote></blockquote><blockquote><p>关于冬至新的理解</p><p><a href="https://zhuanlan.zhihu.com/p/448355529">冬至快乐！一起来了解一下冬至吧 - 知乎 (zhihu.com)</a></p></blockquote><p>冬至过后，来到最寒冷的时候，数九开始了；</p><p>太阳直射南回归线，北半球白昼最短，黑夜最长，出现极夜现象</p><h2 id="日知日进">日知日进</h2><blockquote><p>今天听书 听了樊登老师跟作者 胡老师讲的《恰如其分的孤独》</p><p>这本书有点绕，但是作者跟樊老师的解读比较接地气，需要重新听两遍+好好理解理解</p><p>今年八月份新上的书</p><blockquote><p>恰如其分的孤独 豆瓣读书信息介绍: <a href="https://book.douban.com/subject/36522232/?_dtcc=1">恰如其分的孤独 (豆瓣) (douban.com)</a></p></blockquote></blockquote><blockquote><p>孤独作者给分了3类  最后给了5个生活中常用的原则</p><p>从自己的孤独 推论到个人的自尊水平 再到怎样在人际关系中处理孤独情绪，实现更好的独处</p></blockquote><p><strong>提到的其他讲过的书目《恰如其分的自尊》</strong></p><p>孤独 VS 独处</p><blockquote><p>孤独不是一种情绪，而是一种状态，然而当我们使用这个词的时候，他可能指的是事实意外的一种东西，也就是感觉——通常是伤心或者无聊： 比如你感受到孤独，那就去找朋友去玩，去嗨，去开心，但反而在那种环境下可能会让你更加孤独——这个人可能需要的是 陪伴，是理解，是爱，或者是喜欢这样的东西</p><p>一个人并不意味着孤独或者孤单，因为一个人独处也可以是快乐的</p><p>换言之，孤独和孤单的表达总是意味着缺失或者剥夺感</p></blockquote><p>在关系互动中，当我们的<strong>渴望无法满足</strong>，抑或<strong>关系的发展与期待有所差距时，人就会感到孤独。</strong></p><p>我们都能接纳孤独，享受孤独，学会与自己独处，因为孤独是生命的本色，也是人类最简单最真诚最长久的伴侣，接纳与觉察才是通透。</p><p>冬至 很舒服 当天晚上跟研一的两位小朋友聊期末考试，聊论文 给他们指导意见</p><p>同时也更新一下电子文档</p><h1>12.23日考研日 周六</h1><h2 id="英语角-2">英语角</h2><p>Trees and flowers are what you want</p><p>满树繁花，你是心之所向。</p><h2 id="23-12-23晨读">23.12.23晨读</h2><p>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>面朝大海，春暖花开。<br>——来自帆书·李蕾讲经典《海子的诗》</p><p>👉【精彩选句】<br>✨风后面是风，天空上面是天空，道路前面还是道路。</p><p>✨雨是一生的过错，雨是悲欢离合。</p><p>✨陌生人，我也为你祝福：愿你有一个灿烂的前程 ，愿你有情人终成眷属，愿你在尘世获得幸福，我也愿面朝大海，春暖花开。</p><p>👉【李蕾老师之语】<br>海子是一个传奇，这要靠时间来证明。海子不幸福，做一个幸福的人，是明天才会发生的一件事。海子的诗，每个人一生至少要读一次。悟透诗意的内核是“身在井隅却心向星光”的人生韧劲。</p><h2 id="今日简报">今日简报</h2><p>12月23日星期六</p><p>1、网游不得设置“每日登录、首次充值”等诱导性奖励！国家新闻出版署公开征求意见。</p><p>2、积石山6.2级地震造成甘肃117人遇难，确认无失联人员！现场搜救已结束。</p><p>3、医生手术时捶打患者，爱尔眼科：贵港爱尔CEO免职、院长停职。</p><p>4、公安部：依法严打网红大V造谣传谣等乱象。</p><p>5、北大燕南园获联合国文化遗产保护奖。</p><p>6、红海航线7500亿元货物紧急改道，红海危机危冲击全球贸易。</p><p>7、特斯拉上海储能超级工厂项目正式启动，年产能高达1万台。</p><p>8、明年起，我国对部分抗癌药、罕见病药实施零关税。</p><p>9、游戏整改征求意见引圈内巨震：网易股价跌近 30%，腾讯跌超 15%。</p><p>10、无锡辟出首批150余个避寒场所，热水、空调24小时不间断。</p><p>11、安理会再次==推迟巴以问题==最新决议草案表决。</p><p>12、日本政府决定向美国提供“爱国者”防空导弹。</p><h2 id="名人语录-2">名人语录</h2><p>✨我们应该遵循自己内心的良知，而不是遵循来自外部的赞扬或谴责。——《身份的焦虑》</p><p>你人生的起点并不是那么重要<br>重要的是你最后抵达了哪里</p><p>— 巴菲特</p><h2 id="12-24日独白">12.24日独白</h2><p>永远都不要给自己的人生设限，要时刻相信你的潜力。要对所有美好事物充满好奇心，要不断学习去充盈自己，要相信你的潜能无限。只要用心去做，一切皆有可能。新的一天，早安！</p><blockquote><p>lyq 去了重庆，自己早上跟德玉商量30号的读书会线下活动</p><p>独享实验室——略感孤独的同时，最近信息太杂乱了，有点摸不清头绪</p></blockquote><blockquote><p>周六上午 做调研 研究视频动作相关的开源工具，同时也为周天晚上开会做准备</p><p>下午被龟给咬了哈哈哈</p></blockquote><p>晚上去槐荫区吃饭 真的以后不能去吃自助了，会把胃给吃坏的</p><p>平时控制太严苛，一下子吃这么多 真的会把胃给搞坏</p><blockquote><p>不过确实很值，自己一个人吃这么多，然后还轻轻松松吃回本</p><p>就是唯一一个缺点就是太远，来回跑太冷 太远了，晚上等车这么久 去的时候 4:30 出发 一开始导航还导错了  到了6.30了</p><p>吃完饭7点多 到学校 9点了 正好买完鸡蛋 回来</p></blockquote><p>==找地方 还蠢蠢的去问别人 哎找老大爷 大妈去问路，却不自己质疑自己是错误的==</p><h2 id="日知日进-2">日知日进</h2><p>🗓️2023.12.23【日知日进】日历<br>📚日知书目：《跨越不可能》</p><p>✨日进思考：“想要快速入门一个领域，该如何选择合适的图书?”</p><pre><code> 🔹尝试按照以下标准，通读五本书。第一本书：关于这个话题最流行、最畅销的书，目的是激发兴趣。第二本书：一本同样很受欢迎，但通常更专业、与主题相关性更强的书。第三本书：关于这个主题的半技术性读物——这本书仍然颇具可读性，也很有趣，但可能略有些枯燥。第四本书：关于这个主题的第一本真正深奥的专业书，开始正式学习。第五本书：关于这个主题发展方向、前沿信息的书，目的是了解未来趋势。</code></pre><p>💫共读推荐：<br>🔹《意志力》<br>🔷我们大部分人的问题不是没有目标，而是目标太多。</p><h4 id="2023年12月23日星期六">2023年12月23日星期六</h4><p>读书分享：《说文解字十二讲》<br>分享书友：会淇<br>距离新年还有8</p><p>我们“姓氏”姓名离我们最近，谁都关心与自己最亲近的东西。尽管每个人的姓名跟了自己多年，却很少有人知道它们原本是什么意思。名太多，不便一一指认，暂且置之不论。而姓是有限的，可以一一道来，如<strong>百家姓里的“赵钱孙李周吴郑王”</strong>，等等。那么这些姓是从哪来的？它们原始的造字意图又是什么？要回答这些问题，还得从“姓氏”二字说起。<br>“姓者，统于上者也；氏者，别于下者也。”“姓”比“氏”的涵盖范围大，且出现得早一些。《左传·隐公八年》：“天子建德，因生以赐姓，胙之土而命之氏。”后来合称“姓氏”。殷商时代，帝王还无姓氏。甲骨文中，帝王、后妃是用天干（甲乙丙丁等）作称谓，又男称祖，女称妣，或别以文武、大小，如祖乙、祖辛、祖丁、大甲、中丁、小乙、武丁、文丁、妣甲、妣丙、妣庚等。**周秦时代有了“姓”，**开始主要是氏族、部落的徽号标记，与后世的姓氏多有不同。<br><strong>姓，原本指的是母亲这一系的血缘关系；氏，指的是父亲这一系的血缘关系。所以在先秦时期的同姓不婚，是有着科学依据的</strong>现在我们基本都知道了，姓，又称姓氏，即名字中的第一个字，各个家族都有自己特定的姓氏，这是更改不了的。名就是名称，用来区分每一个个体。相比于姓氏，名的范围要大的多，这样就容易区分每一个个体</p><h3 id="故事会">故事会</h3><p>相传， 仓颉“始作书契， 以代结绳。”<br>在此以前，人们结绳记事，<br>即大事打一大结，<br>小事打一小结，<br>相连的事打一连环结。<br>后又发展到用刀子在木竹上刻以符号作为记事。</p><p>随着历史的发展，<br>文明渐进， 事情繁杂，<br>名物繁多， 用结和刻木的方法，<br>远不能适应需要，<br>这就有创造文字的迫切要求。</p><p>有一年，<br>仓颉到南方巡狩，<br>以“羊马蹄印”为源灵感。</p><p>仓颉日思夜想，到处观察，<br>看尽了天上星宿的分布情况、<br>地上山川脉络的样子、<br>鸟兽虫鱼的痕迹、<br>草木器具的形状，</p><p>描幕绘写，<br>造出种种不同的符号，<br>并且定下了每个符号所代表的意义。</p><p>他按自己的心意用符号拼凑成几段，<br>拿给人看，经他解说，<br>倒也看得明白。<br>仓颉把这种符号叫做“字”。</p><p><img src="../assets/image-20231225175319418.png" alt="image-20231225175319418"></p><p><img src="../assets/image-20231225175324818.png" alt="image-20231225175324818"></p><h1>12.24日考研第二日 周天</h1><p>📚12/18—12/24一周书目单</p><p>1.《不抱怨的世界》<br>2.《列奥纳多·达·芬奇传》<br>3.《你就是孩子最好的玩具》<br>4.《鸢屋经营哲学》<br>5.《魔术师时代：哲学的黄金十年》<br>6.《沃顿商学院最受欢迎的谈判课》<br>7.《帮孩子摆脱焦虑》<br>8.《为未知而教，为未来而学》<br>9.《中国儒学三千年》<br>10.《海子的诗》<br>11.《跨越不可能》<br>12.《苦乐参半》<br>13.《不被父母控制的人生》</p><h2 id="英语角-3">英语角</h2><p>Meet you like the wind<br>遇见你，温澜如风。</p><h2 id="23-12-24晨读">23.12.24晨读</h2><p>☃️2023.12.24晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>生活中既有喜剧，也有悲剧。<br>——来自帆书·非凡精读《苦乐参半》</p><p>👉【精彩选段】<br>苦乐参半其实是无处不在的，而且我们作为人，无一幸免。</p><p>有痛苦、有快乐、有获得、有失去、有爱、有悲伤才是生活的本来面目。</p><p>👉【作者之语】<br>不是有了痛苦就能创作艺术，而是有了创造力才能够正视痛苦。</p><p>👉【推荐之语】<br>生活中有甜蜜的日子，也有悲痛落泪的日子，苦乐参半其实是人生的常态。有时，痛苦会让我们的人生更完整，也更富有创造力。</p><h2 id="今日简报-2">今日简报</h2><p>今日简报</p><p>12月24日，星期日</p><p>1、<strong>人民币超过日元成为国际支付第四大货币！创最高纪录。</strong></p><p><strong>2、联合国通过决议将春节作为联合国假日。</strong></p><p>3、清华“铊中毒案”受害者朱令去世，刚过50岁生日!</p><p>4、甘肃省积石山县中小学开始有序复课。</p><p>5、2024年考研昨日开考，438万考生奔赴考场。</p><p>6、国有大行年内第三轮下调存款利率，助力消费和需求回升。</p><p>7、冰冻周即将收场！本周末中东部大部气温陆续回升。</p><p>8、公安机关依法行政处罚编造发布地震谣言网民11人、批评教育105人。</p><p>9、泰国宣布控枪措施，停发私人持枪证一年。</p><p>10、我国未成年网民突破1.93亿，九成未成年人拥有属于自己的上网设备。</p><p>11、国产首艘大型邮轮启航，开启试运营航次。</p><p>12、美国商业地产雷声滚滚，洛杉矶第三高楼折价近半卖出。</p><h2 id="名人语录-3">名人语录</h2><p>✨人的天性就是这样不完美的！就连最明亮的星球上也会有黑点。——《简˙爱（上）》</p><p>人有了物质才能生存<br>有了理想才谈得上生活<br>——— 雨果</p><h2 id="12-24日独白-2">12.24日独白</h2><p>有人给予你帮助，那是幸运；<br>没人给予你帮助，那是命运。<br>在幸运青睐自己的时候学会感恩，<br>在命运磨练自己的时候学会坚韧。<br>新的一天，早安！</p><blockquote><p>早上弄龟  佛系 睡到自然醒</p><p>为晚上会议做准备</p></blockquote><p>博客崩溃的一天，重新整理  原来最后是不知道为什么，就只有那两个文件出错误，真懵逼了</p><blockquote><p>视频理解动作识别调研， 古典老师阅读整理</p></blockquote><blockquote><p>晚上八点会议</p><p>在龟身上画了太长时间了，弄得自己周报也没好好写，到周一早上早起来写的，结果周一早上来了实验室 还是弄龟 真的下头</p></blockquote><h2 id="日知日进-3">日知日进</h2><p>🗓️2023.12.24【日知日进】日历<br>📚日知书目：《不被父母控制的人生》</p><p>✨日进思考：“和父母同住，如何和谐相处?”</p><pre><code> 🔹与父母同住是把“双刃剑”，和谐相处的关键是建立恰当的边界感。慢下来，控制好自己的情绪，与其说“你必须”“你赶紧”，不如说“给我点儿时间，让我考虑一下”，让自己的情绪慢下来。“圆滑”一些，避其锋芒。可以不认同父母让你做的事，但需要认同父母的感受。一句“妈，我知道你很生气”，就能表达亲人间的关怀。用提问的方法，引导互动走向。与父母争执时，不要局限在问题表面。试试用提问的方式掌控局面，与父母深入探讨问题的本质。</code></pre><p>💫共读推荐：<br>🔹《父母的羁绊》<br>🔷把原谅换成爱，才是家庭变好的开始。</p><h3 id="故事会-2">故事会</h3><p>💞美好的亲子共读时光开始啦<br>今日共读内容<br>神农尝百草</p><p>上古时候，<br>五谷和杂草长在一起，<br>药物和百花开在一起，<br>哪些粮食可以吃，<br>哪些药草可以治病，<br>没人分得清。</p><p>黎民百姓靠打猎过日子，<br>然而天上的飞禽越来越少，<br>地上的走兽越来越稀，<br>人们就只好饿肚子。</p><p>老百姓的疾苦，<br>神农氏看在眼里，<br>决定走遍天下，<br>分辨出五谷和草药。</p><p>神农翻山越岭寻找各种<br>可以吃的种子果实，<br>和能够治病的药草。</p><p>山上长满奇花异草，<br>远远就闻到香气，<br>神农亲自采摘下来，<br>放到嘴里尝。</p><p>白天,<br>他到山上尝百草，<br>晚上映着火光，<br>把尝百草的经验详细记载下来：<br>哪些草是苦的，<br>哪些草是甜的，<br>哪些热，哪些凉，<br>哪些能充饥，<br>哪些能医病，<br>都记载得清清楚楚。</p><p>尝完一山花草，<br>又到另一山去尝，<br>踏遍了山山岭岭，<br>尝遍了每一种草。</p><p>他尝出了能充饥的果实、种子，<br>就叫人把种子带回去，<br>让百姓种植，<br>于是有了麦、 稻、黍（shǔ)、稷（jì)、菽(shū）五谷。</p><p>在路上遇到生病的百姓，<br>神农就用自己知道的药草为百姓治疗，<br>并把药方记录下来。</p><p>最后神农尝出了三百六十五种草药，<br>并写成 《神农本草》，<br>从此人们有了粮食吃，<br>病了也有药医治了。</p><p><img src="../assets/image-20231225175731589.png" alt="image-20231225175731589"></p><p><img src="../assets/image-20231225175736124.png" alt="image-20231225175736124"></p><h1>12.25日周一</h1><p>📚12/18—12/24一周书目单</p><p>1.《不抱怨的世界》<br>2.《列奥纳多·达·芬奇传》<br>3.《你就是孩子最好的玩具》<br>4.《鸢屋经营哲学》<br>5.《魔术师时代：哲学的黄金十年》<br>6.《沃顿商学院最受欢迎的谈判课》<br>7.《帮孩子摆脱焦虑》<br>8.《为未知而教，为未来而学》<br>9.《中国儒学三千年》<br>10.《海子的诗》<br>11.《跨越不可能》<br>12.《苦乐参半》<br>13.《不被父母控制的人生》</p><p>12.25-12.30日组队书单</p><p>思考输出，齐头并进</p><p>1⃣《理解未来的7个原则》</p><p>2⃣《目标感》</p><p>3⃣《常识工作法》</p><p>4⃣《为未知而教，为未来而学》</p><p>5⃣️《作文六要》</p><p>新的一年，读书明智，思考修心</p><h2 id="英语角-4">英语角</h2><p>A glance at her eyebrows and eyes<br>可叹惊鸿一瞥，误入眉眼，欢喜多年。</p><h2 id="23-12-25晨读">23.12.25晨读</h2><p>🍁2023.12.25晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>许多问题的解决方法都很显眼。简单地看待问题，是解决问题的第一步。<br>——来自帆书《如何成为更聪明的人》</p><p>👉【精彩选段】<br><strong>思考远远不止推理那么简单。<strong>思考</strong>既涵盖了推理，也包含了非理性的心理加工过程，诸如创新、解决问题、分析、综合和评估。</strong></p><p>👉【作者之语】<br><strong>敏捷思维就是教会我们“怎样在不确定的世界中生活而不会感到迟疑和不知所措”。</strong></p><p>👉【樊老师之语】<br>天才就是能够摆脱所受训练，做出有效反应的人。本书作者提出了一个新概念**“敏捷思维”<strong>，目的就是梳理思考技巧，释放大脑智慧的潜力。让人们</strong>不仅不做大众观念的回收者，还能源源不断地想出好点子，在生活和工作中做出好决定。**</p><p>❤著名党史、军史专家<br>刘统先生的最新纪实作品<br>——《转折》</p><p>🎀本书采用<strong>图文结合的编排方式。</strong><br>根据实地考察成果，<br>作者绘制了转战陕北的详细路线图，<br>并辅以 80 余幅，<br>局部战斗形势图和历史现场照片，<br>将专业性与通俗性相结合，<br>为我们再现那个艰苦卓绝而充满希望的年代。</p><p>💟现在让我们一同打开《转折》，<br>走近为中国人民解放奋斗的先辈们。</p><h2 id="今日简报-3">今日简报</h2><p>今日简报</p><p>12月25日   星期一</p><p>1、中疾控：目前<strong>南北方省份流感活动基本是同步的，都处于高发状态。</strong></p><p>2、<strong>我国已培养研究生超1100万，北京6岁以上人口每百人有9个读过研。</strong></p><p>3、国家卫健委：积石山地震伤员均得到及时有效救治，灾区正常诊疗服务已恢复。</p><p>4、河南安阳6名村民吃火锅时3死3伤！镇政府：系一氧化碳中毒。</p><p>5、搜索量暴增980%！65万人争抢1万个课程名额！“夜校”火爆出圈。</p><p>6、中国首批孤独症辅助犬“毕业”，将温柔陪伴“星星的孩子”。</p><p>7、云南农村5桌以上聚餐需报备，按规模大小实行分级指导。</p><p>8、印度公司两名高管被曝在印度遭逮捕！vivo回应：深感震惊，将采取法律措施。</p><p>9、浙江衢州200米街道开15家金店，三四线城市黄金消费火爆。</p><p>10、日本核污水排海前将不再确认氚浓度。</p><p>11、中国化妆品在韩国火了！对韩出口额暴涨190%。</p><p>12、豫园灯会点亮巴黎！700多只上海南翔小笼馒头不到一小时售罄。</p><h2 id="名人语录-4">名人语录</h2><p>✨许多问题的解决方法都很显眼。简单地看待问题，是解决方法的第一步。<br>来帆书APP 听《如何成为更聪明的人》</p><p>正念的核心是专注于当下的状态，不要被过去或未来的事情所干扰。<strong>无论是做事情还是与人交往</strong>，都应该全神贯注地投入其中，体验当下的感受。</p><p>当我们专注于当下时，就能更好地掌握自己的情绪和思维，避免因过去或未来的事情而感到焦虑和烦恼。</p><h2 id="12-25日独白">12.25日独白</h2><p><strong>越是艰难，越要勇于攀爬，因为每一步不好走的路都是上坡路。当保持耐心，专注投入某件事时，就会发现心态变强了，困难也就变弱了。</strong><br><strong>新的一周，早安！</strong></p><p>晚上 吃饭伴音是多线程安全+ 调度</p><p>整理web 网页+ 听会议</p><p>安排论文任务+整理文章草稿，明天试着再发一篇文章</p><h2 id="日知日进-4">日知日进</h2><p>🗓️2023.12.25【日知日进】日历<br>📚日知书目:《曾国藩的正面与侧面》</p><p>✨日进思考：“行事过于方刚的人，看似是强者，实则是弱者。”</p><pre><code> 🔹一个针尖如果特别长，长得都能从麻袋里扎出来，那么过不了几天它就会断了。太锋利的东西，保持不了太长时间。太锋芒毕露的人，也会平白给自己增添许多障碍。真正的强者能够在必要时选择圆滑柔软，做到和光同尘，不必事事刚强，也能为人称道。</code></pre><p>💫迁移阅读：<br>🔹子曰：“君子和而不同，小人同而不和。”<br>🔷孔子说：“君子追求与人和谐而不是完全相同、盲目附和，小人追求与人相同、盲目附和而不能和人相处。”</p><p>go学习过程中了解到的新概念</p><blockquote><p>ebay公司</p><p>ebay是最早进入国内的国际电商平台之一，早期阶段外贸产品销售中一直有优势，随着后期阿里巴巴崛起，ebay在国际产品销售舞台中所具有的作用也越来越小，不过总体来说在欧美等国家依然有很多用户购买中国产品时，会关注到在ebay平台上购买，ebay所涉及的外贸产品类别是异常多样化的，例如国内所生产的艺术品，化工产品，手工艺品以及各种类型的家电产品等，在ebay平台上都可以销售出去。那么ebay平台销售产品需要关注哪些问题呢?</p><p>作者：跨境电商钦雨论坛<br>链接：<a href="https://www.zhihu.com/question/287008109/answer/1130378964">https://www.zhihu.com/question/287008109/answer/1130378964</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-3c8bfd2b59a5335b2fc54ccc0bfa7963_720w.webp?source=1def8aca" alt="img"></p><blockquote><p>讲课老师孟老师从ebay 跳槽到 腾讯T4</p></blockquote><p>CS144 课程介绍和学习</p><p><a href="https://kangyupl.gitee.io/cs144.github.io/">CS 144: Introduction to Computer Networking (gitee.io)</a></p><blockquote><p>agent 概念<a href="https://blog.csdn.net/VucNdnrzk8iwX/article/details/79434572">什么是agent?-CSDN博客</a></p></blockquote><blockquote><p>ddd 概念</p><p>Domain-Driven Design 领域驱动设计 ，是一种通过将实现连接到持续进化的模型来满足复杂需求的软件开发方法。</p><p><a href="https://zhuanlan.zhihu.com/p/411866735">最近爆火的DDD到底是什么？一文带你落地DDD - 知乎 (zhihu.com)</a></p></blockquote><blockquote><p>go学习过程中的 一些可以参考的库源码 <strong><a href="https://github.com/kubernetes/sample-controller">sample-controller</a></strong></p><p><a href="https://github.com/kubernetes/sample-controller">kubernetes/sample-controller: Repository for sample controller. Complements sample-apiserver (github.com)</a></p></blockquote><blockquote><p>csig  腾讯6大部门之一，一个小分支</p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/164882387">腾讯六大事业群介绍（下） - 知乎 (zhihu.com)</a></p><blockquote><p>腾讯t4</p><p><a href="https://www.zhihu.com/question/29910703">腾讯T4专家工程师是什么水平？ - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/gu-yu">游戏开发随笔 - 知乎 (zhihu.com)</a>   曾经的腾讯t4专家的一个知乎专栏，2019年更新</p></blockquote><h2 id="故事会-3">故事会</h2><p>《沈园的故事》<br>作者：夏雨晴</p><p>一个宋朝的园林，能够一代代传下来，到今天还依然有名，也许只有绍兴的沈园了。<br>沈园的出名却是由一曲爱情悲剧引起的。诗人陆游和表妹唐琬在园壁上题写的两阙《钗头凤》是其中的热点。<br>陆游也许是宋朝最好的一个诗人，但肯定不是一个值得唐琬为他而死的人。<br>表妹唐琬是在一个秋天忧郁而逝的，临终前，她还在念着表哥那阙被后人传唱的《钗头凤》。自从这个春天，和陆游在沈园不期而遇后，病榻之上的唐琬就在低吟这阙伤感的宋词。<br>一枝梅花落在了诗人的眼里，这是南宋的春天，年迈的陆游再次踏进了沈园。在斑驳的园壁前，诗人看到了自己四十八年前题写的一阙旧词：红酥手，黄藤酒，满城春色宫墙柳。东风恶，欢情薄，一怀愁绪，几年离索。错，错，错。　春如旧，人空瘦，泪痕红浥鲛绡透。桃花落，闲池阁。山盟虽在，锦书难托。莫，莫，莫!<br>唐琬在临终的日子里，一遍遍回想自己和表哥那段幸福的岁月。<br>陆游二十岁时初娶表妹唐琬，两人诗书唱和，绣花扑蝶，就像旧小说中才子佳人的典型故事。<br>可惜这样的日子太短了，唐琬只记得有一天，婆婆对她说，他们两个太相爱了，这会荒废儿子的学业，妨碍功名的。<br>唐琬至死都没有想通，相爱也会是一种罪名。不过她更没相通的是，那个据说在大风雨之夜出生在淮河一条船上的诗人，后来又横戈跃马抗击金兵的表哥，竟然违不了父母之命，在一纸休书上签下了羞答答的大名。<br>陆游四十八年后重游沈园，发现了园壁间一阙褪色的旧词，也叫《钗头凤》，这是唐琬的词迹：<br>世情薄，人情恶，雨送黄昏花易落。晓风干，泪痕残。欲笺心事，独语斜阑。难，难，难。人成各，今非昨，病魂常似秋千索。角声寒，夜阑珊。怕人寻问。咽泪装欢。瞒，瞒，瞒!<br>在南宋的春天，一枝q梅花斜在了诗人的眼里，隔着梅花，陆游没能握住风中的一双红酥手。</p><blockquote></blockquote><h1>12.26日 周二</h1><blockquote><p>早上8:55</p></blockquote><blockquote><p>做志愿者  写作业 免费听课</p><p>关注读写力</p><p>一个社群，一个社区</p></blockquote><h2 id="英语角-5">英语角</h2><p>It’s late in the world and it’s autumn.<br>人间忽晚，山河已秋。</p><h2 id="23-12-26晨读">23.12.26晨读</h2><p>☃️2023.12.26晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>不要让他人随意评判你，只有你能做自己的最终评判者。<br>——来自帆书·非凡精读《强势》</p><p>👉【精彩选段】<br>沟通中我们有权说“我不明白”，并不是因为我们笨，而是对方有责任解释清楚他们的逻辑和意图。</p><p>👉【作者之语】<br>当人际关系的矛盾层出不穷时，“强势”是一种以不变应万变的处理方式。</p><p>👉【主讲老师之语】<br>人际关系中，矛盾无处不在，每个人都可能遭受他人的“隐形操控”。这些隐形操控会让你怀疑自己、产生社交焦虑，甚至导致心理问题。这本书提供了<strong>三种强势做法、七类强势话术和十大强势原则</strong>，帮你在人际关系中快速识别操控、摆脱操控，成为“社牛”，强势做自己。</p><p>❄️2023.12.26晨读拓展</p><p>👉三种强势做法：<br>1.做决策<br>2.回应别人<br>3.帮助别人</p><p>👉七类强势话术：<br>01 “我是一张坏唱片”法<br>02 可行折中法<br>03 自由信息法<br>04 自我表露法<br>05 迷惑法<br>06 否定决断法<br>07 否定询问法</p><p>👉十大强势原则：<br>1️⃣也是基本的原则：你有权坚持自己的行为、想法和情感，并对产生的一切后果负责；<br>2️⃣坚持你要做的，不必解释；<br>3️⃣帮不到别人，也不用内疚；<br>4️⃣你有权改变想法；<br>5️⃣犯错不可怕，但要承担后果；<br>6️⃣你有权说“我不知道”；<br>7️⃣要与人交往，但不要刻意讨好；<br>8️⃣你有权做出“不合逻辑”的决定；<br>9️⃣你有权说“我不明白”；<br>🔟你有权说“我不在乎”。</p><h2 id="今日简报-4">今日简报</h2><p>今日简报</p><p>12月26日   星期二</p><p>1、中办、国办：<strong>鼓励落实带薪年假安排职工除夕休息</strong>。</p><p>2、甘肃获捐款和中央部委下拨资金超18亿元。</p><p>3、杭州湾跨海铁路桥主体控制性工程开工。</p><p>4、教育部启动首批全国中小学科学教育实验区实验校建设。</p><p>5、四部门发文：严禁公立医院举债购置大型医用设备。</p><p>6、印尼一中资工厂爆炸，外交部：已致包括4名中方员工在内13人死亡。</p><p>7、美国政府5天逮捕5万名非法移民。</p><p>8、土耳其在伊拉克和叙利亚北部实施空袭。</p><p>9、<strong>腾讯调整微信支付和视频号组织架构，后续加大直播带货投入。</strong></p><p>10、多地辟谣医保统筹额度12月底清零。</p><p>11、中国或成为全球<strong>第一大汽车出口国</strong>：全球<strong>超6成新能源汽车销量来自中国</strong>。</p><p>12、一批侵犯企业权益的营销号被查处，营销号发企业不实消息只为索要公关费。</p><h2 id="名人语录-5">名人语录</h2><p>✨不要让他人随意评判你，只有你能做自己的最终评判者。<br>来帆书APP 听《强势》</p><p>有人说“我们不快乐的原因，是不知如何安静地待在房间里心平气和地和自己相处。”和自己相处好了，才会最舒服！步入中年，最好的活法，是让心安静下来，学着和自己相处。减少不必要的社交 ；降低过火的欲望；平复不安的情绪。过低温的生活，保持低温的心态，修好这颗心，余生皆好运</p><p>不要等待<br>时机永远不会恰到好处<br>—— — 拿破仑·希尔</p><h2 id="12-26日独白">12.26日独白</h2><p>不必纠结过去，更不必担忧未来，因为明天的答案，由你度过的每一个今天写就。新的一天，把握当下，努力向上，早安！</p><blockquote><p>重启后要做的事情</p><p>安装 caj软件  安装fumo软件</p></blockquote><p>大家平时的输出要有意识的分享哈<br>不管是问题 还是你的经验输出<br>我这次参加新精英的超级个体IP营 真的领悟到了那句</p><p>先发声，才有好事发生【持续分享，黄金万两 也是一个厉害的人物说的话】</p><p>真正的含义<br>利他分享不仅是他人认识你的被动社交名片</p><p>还可以帮你快速拿到反馈的<br>真的两周内我听完了二三十节课 感觉自己有准备的出发，结果真的做的时候发现还有很多未知的问题等着我</p><p>这时候又只有就问题找到能够解决的问题的同学老师咨询 再往前走~</p><p>我这次最大的收获就是打破了完美主义 先做起来才知道结果</p><p>我记得古典老师出了一个自制力的课 他说时间规划是从自知之明开始,因为我们平时做的这些规划，很多时候都不贴合自己的实际情况，那你真的就很难坚持下去,坚持不下去你就会啊，觉得自己不行啊，low逼啊</p><p>然后你就自我放弃 啊，过段时间好了伤疤忘了疼，又开始做计划</p><p>周而复始【焦虑 归根结底都是因为能力达不到目标造成的】【我明年的核心任务就是学会做规划</p><p>那我今年花了一年时间来做复盘———那时间放的足够的长 我就能看到自己的进步】</p><p>这就是我明年重点要解决的问题——当我忙的时候，我也要把读写放上</p><p>另外成年人的学习一定要主动 知道点什么就做什么 不要像学生一样 老师没说就不做</p><p>真的害死人</p><p>这是我曾经踩过的坑🕳️🕳️🕳️</p><p>超体营的时候 古典老师也是很绝啊</p><p>说世界只会奖励那些行动的人</p><p>我们每周提问都是：</p><p>你的问题是什么？<br>为此你做了什么？</p><p>就像我这次过去参加最终路演的产品</p><p>都不是我最初的产品 是最后一周临时做出来</p><p>那我们组的老师说 丽雯的产品都是吃百家饭做出来的</p><p>所以说大家一定要去打破这个思维上的禁锢~</p><p>不要怕麻烦 你得到哪怕一点点的东西，那个才真的是你~</p><p>否则和你有什么关系呢？</p><blockquote><p>想都是问题，做才是答案，完成比完美更重要</p><p>行动才是打败焦虑最好的办法，大脑吸收了营养才不至于因空洞而胡思乱想</p></blockquote><p>其实就是要资源为我服务~</p><p>我是这么理解的</p><p>我自己在这里带了这么久 我今年上半年很大一个转变就是：==从书从第一页开始读==</p><p>==到为我所有 思维转变==</p><p>==其实想想作者怎么谋篇布局和我有什么关系？==</p><p>==我只需要带走对我有帮助的一点就好呀==</p><p>其他的说再多再好都是虚的</p><p>当我明白这个理的时候 我就开始聚焦</p><p>==让身边的资源为我的成长服务啦==</p><p>读→理解→消化→输出，缺一不可</p><h2 id="日知日进-5">日知日进</h2><p>🗓️2023.12.26【日知日进】日历<br>📚日知书目:《哲学的慰藉》</p><p>✨日进思考：“家徒四壁，不必汗颜；黄金万两，无可炫耀。”</p><pre><code> 🔹我们常常以为赚取更多钱财、买车、买包，就会更快乐；谈一场恋爱，就会更快乐……然而物质带来的快乐转瞬即逝。古希腊哲学家伊壁鸠鲁认为自由是能够真正快乐的重要条件，拿时间、精力换取金钱时，我们恰恰失去了自由。《瓦尔登湖》的作者梭罗说，他只要赚够一年所需的“热量”，工作六周就够了，剩下的时间谁也不伺候，就坐在湖边看书、走路、玩、观察自然，这才叫真正的自由。</code></pre><p>💫迁移阅读：<br>🔹子曰：“贤哉回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！”<br>🔷孔子说：“真是个大贤人啊，颜回！用一个竹筐盛饭，用一只瓢喝水，住在简陋的巷子里。别人都忍受不了那穷困的忧愁，颜回却能照样快活。真是个大贤人啊，颜回！”</p><p><img src="../assets/image-20231226115145223.png" alt="image-20231226115145223"></p><h1>12.27日 周三</h1><h2 id="英语角-6">英语角</h2><p>My journey is the stars and the sea.<br>我的征途是星辰大海。</p><h2 id="23-12-27晨读">23.12.27晨读</h2><p>🍁2023.12.27晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>获得他人力量支持的人，拥有更强的免疫系统，更少患病。<br>——来自帆书《他人的力量》</p><p>👉【精彩选段】<br>迷失在负面的自我评价之后，没有人还能具有很好的能力表现。第二层次通过自我怀疑和自我贬低来毁灭高能力表现。</p><p>👉【作者之语】<br>你的表现有多好，不仅仅取决于你的技能如何，还取决于谁同你一起做了什么。</p><p>👉【樊老师之语】<br>作为群居动物，我们很难摆脱他人对我们的影响。本书为我们详细梳理了四个层次的人际相处模式，其中几乎涵盖人际交往中所有可能的情况，教给我们如何构建受益一生的人际关系。</p><p>🍁2023.12.27晨读拓展</p><p>👉【四种人际关系】</p><p>第一种人际关系叫作孤立状态，就是一个完全没有连接的wifi，你是独自的一个人。<br>第二种关系叫作坏的连接关系。就是这个人跟你关系很亲密，经常在一起交流，但问题是他让你特别不舒服，就是他总是责怪你，总是指出你的问题，总是要求你。<br>第三种关系，叫作看似美好的连接关系。<br>第四种连接关系最主要的特点就是互相关心，说实话，并且富有建设性。</p><h2 id="今日简报-5">今日简报</h2><p>今日简报</p><p>12月27日    星期三</p><p>1、外交部：中方将对美国1家机构和2名个人采取反制措施。</p><p>2、甘肃震区已有序复商复市复课。</p><p>3、中国全面暂停进口，日本：将努力扩大扇贝对韩国及欧盟出口。</p><p>4、俄国家杜马主席：俄国家杜马网站将推出中文版本。</p><p>5、伊朗一高级顾问在叙利亚遭以军空袭死亡，伊朗总统莱希：以色列将“付出代价”。</p><p>6、步入全球第一方阵！我国自研空中出租车亮相。</p><p>7、伊朗准备与埃及复交，结束40余载恩怨。</p><p>8、沪深交易所公布除夕当日休市。</p><p>9、657个药品说明书试点大字版等改造。</p><p>10、广西市场监管局发布典型案例：病死猪被处理后卖出货值超600万。</p><p>11、电信诈骗从缅北转移到了印度、迪拜，印度约120万人口从事电信诈骗。</p><p>12、央媒：本溪花28亿除不净雪令人质疑。</p><p>✨获得他人力量支持的人，总有更强的免疫系统，更少患病。<br>来帆书APP 听《他人的力量》</p><h2 id="名人语录-6">名人语录</h2><p>很多时候，完成比完美更重要，在一次次完成中迭代，就是进步！</p><h2 id="12-27日独白">12.27日独白</h2><blockquote><p>不是…… 而是…… 的结构 和理解（深入读书 深入理解）</p><ol><li>主要内容是 信息时代 成功方式 分析时间 做好判断  抓住机会 形成跃迁</li><li>成功不是线性的，而是非线性的  跳跃式的</li><li>失败的人分类（1. 管不住专注力的  2.不能分辨信息的 3.不能联机学习的）</li><li>看书的方法论（目录  序言（内容序 流量序  自序）附录  出版页信息）</li><li>高手成功是借力  （怎样借力  PEST ）</li><li>长周期 短周期看问题的不同之处</li><li>读书人的四个分类</li></ol></blockquote><p>路线 go &amp;&amp;java 确定</p><p>java 笔记 整理对比</p><blockquote></blockquote><h2 id="日知日进-6">日知日进</h2><p>🗓️2023.12.27【日知日进】日历<br>📚日知书目:《危机领导力》</p><p>✨日进思考：“不愿意驶入风暴的人，永远无法取得胜利。”</p><pre><code> 🔹只有愿意驶进风暴，愿意在最激烈的战况中尝试，迎接未知的挑战，才有可能取得胜利。面对一个新兴的市场，没有人知道它能不能做得起来，但如果这时你敢于冲进去，你就拥有了先发优势。当然，勇敢不等于冒进，在驶入风暴前要做好充分的预估，对涉及的人、事、物进行评判，了解做这件事所需的能力、资金、人员的极限。</code></pre><p>💫迁移阅读：<br>🔹子曰：“工欲善其事，必先利其器。居是邦也，事其大夫之贤者，友其士之仁者。”<br>🔷孔子说：“工匠要想做好工，必须先把器具打磨锋利。住在这个国家，就要侍奉大夫中的贤人，结交士中的仁人。”</p><p><img src="../assets/image-20231227121322638.png" alt="image-20231227121322638"></p><h1>12.28日周四</h1><h2 id="英语角-7">英语角</h2><p>you are as warm as the sunset glow.<br>你与晚霞同样温暖。</p><h2 id="23-12-晨读">23.12.  晨读</h2><p>🍁2023.12.28晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>火是各处可烧的，水是各处可流的，日月是各处可照的，爱情是各处可到的。<br>——来自帆书·李蕾讲经典《边城》</p><p>👉【精彩选段】<br>月光如银子，无处不可照及，山上篁竹在月光下皆成为黑色。身边草丛中虫声繁密如落雨。</p><p>👉【作者之语】<br>“一切充满了善，然而到处是不凑巧。”既然是不凑巧，因之素朴的善，终究难免产生悲剧。</p><p>👉【李蕾老师之语】<br>20世纪中国文学的无冕之王《边城》，让我们沉浸式体验湘西边城的风景与风情，体味人性的善良美好，给漂泊的心灵找到一个宁静的栖息之所。《边城》之后，每个人都想寻找翠翠，为什么呢？因为这个故事充满了爱与美。</p><h2 id="今日简报-6">今日简报</h2><p>今日简报</p><p>12月28日   星期四</p><p>1、全国人大法工委：全面禁燃烟花爆竹不合法，部分地方需修改禁燃令。</p><p>2、5个严禁！四部门进一步规范义务教育课后服务。</p><p>3、我国“八纵八横”高铁网主通道已建成3.61万公里。</p><p>4、北京1月1日起禁止老头乐上路。</p><p>5、各项指标全部合格，我国首艘大洋钻探船完成首次试航。</p><p>6、京东也将支持仅退款，三大电商平台均将支持仅退款。</p><p>7、杭州至南昌高铁全线贯通运营，串起世界级黄金旅游线。</p><p>8、麦当劳中国部分产品今起涨价：运营成本变化，平均涨幅为3%。</p><p>9、围炉煮茶要小心！浙江一家医院一周接诊13名一氧化碳中毒者。</p><p>10、韩国首尔明年将启用无人机监控交通状况。</p><p>11、美国数百万人收到极端天气警告，超2000架次航班延误。</p><p>12、弥补战时预算缺口，以色列财政部建议关停10个政府部门。</p><h2 id="名人语录-7">名人语录</h2><p>心里装着太阳，才能闪亮发光，幸福不在别处，就在我们自己</p><p>幸福，既不在过去也不在未来，而是在当下。</p><p>就好像不要接到工作就开始做一样，要思考工作的结果、意义：和上级澄清目标，理清和兄弟团队的关系，这些都要考虑到。</p><h2 id="12-28日独白">12.28日独白</h2><blockquote><p>任务安排</p></blockquote><ol><li>路线 java go 指定 ⭐⭐⭐⭐⭐</li><li>动作识别任务⭐⭐⭐⭐</li><li>笔记系统 三习⭐⭐⭐⭐</li><li>周报准备⭐⭐⭐⭐⭐</li><li>日读安排⭐⭐⭐</li></ol><h2 id="日知日进-7">日知日进</h2><p>🗓️2023.12.28【日知日进】日历<br>📚日知书目:《当良知沉睡》</p><p>✨日进思考：“最大的伤害不是别人插在我们身上的刀，而是我们经常把刀拔出来看一看，再插回去。”</p><pre><code> 🔹好好地生活是对伤害我们的人最好的报复。不要对他们耿耿于怀，对他们最好的报复是我们自己过得更愉快，我们的生活蒸蒸日上，不断向前走。我们的眼睛要向前看，而不是每天生活在过去的伤害中。对我们造成最大伤害的，不是别人插在我们身上的刀，而是我们经常把那把刀拔出来看一看，然后很生气，再插回去，这种反复的伤害才是影响最大的。所以要向前走，与伤害擦肩而过。</code></pre><p>💫迁移阅读：<br>🔹子贡曰：“伯夷、叔齐何人也？”曰：“古之贤人也。”曰：“怨乎？”曰：“求仁而得仁，又何怨？”<br>🔷自贡问道：“伯夷和叔齐是怎样的人呢？”孔子说：“他们是古代贤人啊。”子贡说：“他们会有怨悔吗？”孔子说：“他们追求仁德，便得到了仁德，又怎么会有怨悔呢？”</p><p><img src="../assets/image-20231228100910051.png" alt="image-20231228100910051"></p><h1>12.29日周五</h1><h2 id="英语角-8">英语角</h2><p>The wind stops at autumn water I stop at you.<br>风，止于秋水，我，止于你。</p><h2 id="23-12-29晨读">23.12.29晨读</h2><p>❄️2023.12.29晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>与有趣的灵魂相遇，是我们认知这个世界最美好的方式。<br>——来自帆书·非凡精读《如何结交比你更优秀的人》</p><p>👉【精彩选段】<br>孔子说“远而不疏，近而不狎”，讲的就是分寸感。人和人要有距离，再亲近也不能狎昵戏弄。</p><blockquote><p>狎昵”，汉语词语，拼音是xia(2) ni(4)，意思是过于亲近而态度不庄重。解释<br>亦作“狎曜”<br>1.亲近;亲昵<br>2.指男女淫猥苟合<br>3.过于亲近而态度不庄重。<br>出处<br>生就视,容华若仙,惊喜拥入,穷极狎昵《聊斋志异“·胡四姐》</p></blockquote><p>👉【作者之语】<br>结交更优秀的人是一个帮助我的学业、事业成功的非常重要的点。如何构建人和人之间的关系，重点是人和人，不是你的标签、我的标签。</p><p>👉【主讲老师之语】<br>《如何结交比你更优秀的人》这本书，就是帮你构建你的人脉力，使你成为真正的人脉达人，能够在身边织成一个有效的人脉网络，能够让这个人脉网络帮助你的事业更加成功，使你的生活更加幸福。</p><h2 id="今日简报-7">今日简报</h2><p>今日简报</p><p>12月29日   星期五</p><p>1、元旦假期成都、北京、三亚成热门租车目的地，港澳游客入境数量大增。</p><p>2、多地限制涉电诈人员子女上学就业，人大法工委：废止地方涉罪人员近亲连坐规定。</p><p>3、印度庭审vivo高管更多细节曝光，为打压中企印检方竟当庭提交密信。</p><p>4、纽约时报起诉微软和OpenAI侵权：擅用报道训练大模型。</p><p>5、调查数据显示30岁人群未婚率超25%，25岁人群有七成未婚。</p><p>6、山东潍坊冷冻薯条出口量猛增，国产冷冻薯条热销海外。</p><p>7、诈骗团伙虚构2500多场竞赛收益超1400万，人民网评：警惕烧钱的野鸡竞赛。</p><p>8、小米首款电动车SU7亮相，雷军：已经小批量量产，目标用户是时代精英。</p><p>9、拖欠数千名员工年终奖，马斯克的社交平台“X”遭起诉。</p><p>10、沈阳地铁回应2天6次延误：向乘客致歉，已做好应急预案迎接跨年夜客流高峰。</p><p>11、京东零售公布加薪细节：固定薪资涨幅近100%，业绩激励上不封顶。</p><p>12、苹果手表发力健康监测功能，或持续面临法律挑战。</p><h2 id="名人语录-8">名人语录</h2><p>与有趣的灵魂相遇，是我们认知这个世界最美好的方式。<br>来帆书APP 听《如何结交比你更优秀的人》</p><h2 id="12-29日独白">12.29日独白</h2><p>相信努力会带来收获，<br>相信用心会收获真情，<br>相信善良是行走世间最好的武器。<br>新的一天，早安！</p><p>今天是2023年的最后一个周五啦！<br>为这一年的晨读画一个完美的句号吧！</p><blockquote><p>如愿以偿的，感恩；力所不及的，释怀。好坏都是经历，得失皆有因果。感谢自己这一年的辛苦努力，感谢伙伴们的陪伴扶助！</p></blockquote><h2 id="日知日进-8">日知日进</h2><p>🗓️2023.12.29【日知日进】日历<br>📚日知书目:《感受爱》</p><p>✨日进思考：“聪明人要学会与不确定性共舞。”</p><pre><code> 🔹我们每天担忧的事是什么？就是怎样能够让所有的事都确定。我们希望一切东西都按照我们的想法按部就班地落实，这就叫作确定。但生活如果事事都确定是一件可怕的事，因为人生的乐趣就是来自未知，来自无常。庄子告诉人们要学会和不确定性共舞，无论天地怎么改变，我们要像飘絮一样随风而定。当我们不需要让自己安稳地站在地面，但依然感觉安稳的时候，便是天地合一。所以一个聪明人，要学会保持不确定性，不要过度担忧，这是对抗想得太多的一个最有效的方法。</code></pre><p>💫迁移阅读：<br>🔹子曰：“直哉史鱼！邦有道如矢，邦无道如矢。君子哉蘧伯玉！邦有道则仕，邦无道则可卷而怀之。”<br>🔷孔子说：“史鱼正直啊！国家政治清明时，他像箭一样直；国家政治黑暗，他也像箭一样直。蘧伯玉是君子啊！国家政治清明时，他就出来做官；国家政治黑暗时，就把自己的才能收藏起来（不做官）。”</p><p><img src="../assets/image-20231229162315569.png" alt="image-20231229162315569"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>java学习笔记整理</title>
      <link href="/project/posts/d5874840.html"/>
      <url>/project/posts/d5874840.html</url>
      
        <content type="html"><![CDATA[<h1>java学习现状与计划调整</h1><h1>java基础面试笔记整理</h1><p>12.16号 学习整理go的学习路线</p><blockquote><p>一边是极客时间的开源实践教学视频</p><p>另一边是马士兵和珊珊老师的学习视频和基础学习笔记</p><p>以及一些现有的网站和博客</p></blockquote><blockquote><p>综上所述，可以以开源项目为目标，争取自己扩展自己造轮子的这个思路，尝试下第一次造轮子</p><p>然后用一些辅助的笔记和博客促进go的学习，努力刷开源课程</p></blockquote><blockquote><p>同时借助学习go语言基础的过程，复习起来java的基础知识</p></blockquote><p>12.17号 周天</p><blockquote><p>学习完成马士兵讲解javaer转变成goer的第一讲视频后，发现要学习java 的多线程和jvm基础【比如线程池 一些内存算法等等】</p><p>整理了现有的多线程和jvm学习的视频</p><p>要求是 有完整的笔记 以及对应的详细视频讲解，如果看笔记看不懂再去看视频</p></blockquote><p>12.18日 混沌+ 焦虑的一天+晚上焦虑的睡眠</p><blockquote><p>上午 泄欲</p><p>下午没有睡午觉，整理那个尚硅谷的一个安装notecase软件的bug后，1.9版本一直有错误，换成其他版本并尝试，花费1.5h 解决后，开始刷剧</p><p>晚上去吃饭+ 刷剧 +  8.15困得不行 一觉睡到9.25</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解3用于不完整骨架动作识别的丰富的激活图卷积网络</title>
      <link href="/project/posts/f874c016.html"/>
      <url>/project/posts/f874c016.html</url>
      
        <content type="html"><![CDATA[<h1>两篇文献总结</h1><blockquote><p>第一篇文章是一篇会议短文，只是做了一下简单介绍</p><p>第一篇代码</p><p><a href="https://gitee.com/yfsong0709/RA-GCNv1/tree/master">RA-GCNv1: RA-GCNv1: implemented by PyTorch - Gitee.com</a></p><p>第二篇是详细介绍，并且在内容上，扩展了数据集，并将内容进行详细介绍</p><p>第二篇代码链接<a href="https://gitee.com/yfsong0709/RA-GCNv2">RA-GCNv2: RA-GCNv2: implemented by PyTorch (gitee.com)</a></p></blockquote><blockquote><p>下边是模型的主要介绍</p></blockquote><p><img src="../assets/image-20231229173601091.png" alt="image-20231229173601091"></p><p>RA-GCN的管道有三个流网络。每个流包含一个ST-GCN。ST-GCN层下的两个数字分别为输入通道数和输出通道数。其他层包含相同的输入和输出通道。第五层和第八层都使用时间跨度来减少序列长度。GAP和Concat是全局平均池和连接操作，S是流的数量，⊗和Θ分别表示元素的乘法和减法。该模型共包含三个步骤。首先通过数据预处理模块将输入序列x转换为x` 。  然后将x ``经过相应的掩码矩阵过滤后发送到每个流。最后，将每个流的输出连接起来，以获得x的最后一个类。</p><blockquote><p>两篇文章在思路上是相同的，都是处理的不完整骨架数据。网络模型以经过时序和空间处理的X特征为输入，在初始流的时候，所有的mask矩阵设置为1，x经过处理后变成x`，然后经过STGCN网络，在经过处理后，分别获得对应的特征矩阵以及经过GAP(全局平均池化和归一化后)后得到的权重矩阵，来作为激活模块的输入。激活模块使用了一种微妙的技术，==类激活映射（CAM），来区分由每个流激活的判别性骨架关节。通过累积前面流获取的激活图，形成一个掩码矩阵，通知新的流关于哪些关节已经被激活。====然后，新的流将被迫从未激活的关节中探索判别性特征。</p></blockquote><p>相比于第一篇文章，第二篇文章在第一篇文章的基础上，改进激活模块，原来的激活模块中的激活掩码是通过Softmax函数获得的，该函数仅激活每个流程的少数关节。==相比之下，第二篇提出使用归一化激活函数来扩大激活范围==，====同时在每个单独的网络流上添加许多额外的交叉熵正则化来扩展原始损失函数==，以便更有效地学习特征。 创造了更多的数据集来进行实验，合成数据集通过==更多的退化算子进行了扩展，其中遮挡退化进一步分为四种类型，包括帧、部分、块和随机，以及两个合成的抖动数据集==。在不同退化条件下进行了更多实验，以验证所提方法的有效性和鲁棒性。</p><p><strong>a)</strong>   <strong>两篇文章共同关注点：</strong></p><p><strong>①</strong>  <strong>从输入特征映射中的所有位置提取非局部特征的方法</strong></p><p><strong>②</strong>  <strong>提出了一种多流图卷积网络（Rich_GCN），以探索足够的判别性特征，以实现鲁棒的动作识别</strong></p><p><strong>③</strong>  <strong>两篇文章都是从类激活映射（CAM）技术出发，借以区分多流网络中每个流激活的判别性骨架关节，并利用mask掩码矩阵来进行记录哪些骨架关节已经被激活，来促进网络对未激活的关节进一步学习。</strong></p><p><strong>b)</strong>   <strong>两篇文章关系</strong></p><p><strong>①</strong>  <strong>第二篇文章以第一篇文章为基础，进行方法提升和数据集扩充</strong></p><p><strong>②</strong>  <strong>相比之下，第二篇提出使用归一化激活函数来扩大激活范围，相应的流程将获得更好且更可解释的激活图make矩阵</strong></p><p><strong>③</strong>  <strong>在每个单独的网络流上添加许多额外的交叉熵正则化来扩展原始损失函数，以便更有效地学习特征</strong></p><h1>RICHLY ACTIVATED GRAPH CONVOLUTIONAL ETWORK FOR ACTION RECOGNITION WITH INCOMPLETE SKELETONS 用于不完整骨架动作识别的丰富的激活图卷积网络</h1><h2 id="ABSTRACT">ABSTRACT</h2><p>目前用于基于骨架的人体动作识别的方法通常==处理完全观察到的骨架==。然而，在实际场景中，很容易==捕捉到不完整和带有噪音的骨架==，这将降低传统模型的性能。为了==增强动作识别模型对不完整骨架的鲁棒性，我们提出了一种多流图卷积网络==（GCN），用于==探索分布在所有骨架关节上的充分判别特征==。在这里，网络的每个流仅负责从<strong>当前未激活的关节中学习特征</strong>，这些关节由前面的流通过**类激活图（CAM）**区分开，因此所提出的方法的==激活关节明显多于传统方法==。因此，所提出的方法被称为丰富激活的GCN（RA-GCN），其中==丰富发现的特征将提高模型的鲁棒性==。与最先进的方法相比，RA-GCN在NTU RGB+D数据集上取得了可比较的性能。此外，在一个合成的遮挡数据集上，RA-GCN显著减轻了性能的恶化。</p><p>摘要章节要点总结：</p><ul><li>目前的骨架人体动作识别方法通常处理完全观察到的骨架。</li><li>在实际场景中，很容易捕捉到不完整和带有噪音的骨架，这会降低传统模型的性能。</li><li>为了增强模型对不完整骨架的鲁棒性，提出了一种多流图卷积网络（GCN）。</li><li>每个网络流仅负责从当前未激活的关节学习特征，通过类激活图（CAM）区分关节，提高了激活关节的数量。</li><li>提出的方法称为丰富激活的GCN（RA-GCN），其中丰富发现的特征将提高模型的鲁棒性。</li><li>与最先进的方法相比，RA-GCN在NTU RGB+D数据集上取得了可比较的性能。</li><li>在合成的遮挡数据集上，RA-GCN显著减轻了性能的恶化。</li></ul><h2 id="1-INTRODUCTION">1.INTRODUCTION</h2><p><strong>引言：</strong></p><p>基于骨架的人体动作识别方法在许多应用中变得日益重要，并取得了巨大进展，这归功于其在适应背景、对光照强度的鲁棒性以及较低计算成本方面的优越性。<strong>骨架数据由多个时空骨架关节的3D坐标组成</strong>，可以通过Kinect等多模态传感器收集，也可以通过姿态估计方法直接从2D图像中估计得到。传统方法通常以两种方式处理骨架数据。一种方式是将这些关节连接成一个整体矢量，然后使用基于RNN的方法对时间信息进行建模。另一种方式是将关节的时间序列视为图像，然后利用基于CNN的方法识别动作。然而，骨架关节之间的空间结构信息在基于RNN和CNN的方法中都难以有效利用，尽管研究人员提出了一些额外的约束或专用网络结构来强烈编码骨架关节的空间结构。最近，<strong>Yan等人首次将基于图的方法应用于基于骨架的动作识别，并提出了空时图卷积网络（ST-GCN）来提取嵌入在空间配置和时间动态中的特征</strong>。此外，还有一些使用图技术的动作识别方法，如[12, 13, 14]，但这些方法主要基于RGB视频而不是骨架数据。</p><p>所有上述方法都假设完整的骨架关节可以被很好地捕获，而不考虑不完整的情况。然而，在真实场景中往往难以获得完整的骨架序列。例如，行人可能会被停放的车辆或其他可见的物体遮挡。**与此同时，当面对不完整的骨架时，传统方法的性能会有不同程度的下降。**因此，**如何识别具有不完整骨架的动作是一个具有挑战性的问题。<strong>许多研究人员</strong>正在探索从输入特征映射中的所有位置提取非局部特征的方法，如[15]和[16]。**受此启发，==我们提出了一种多流图卷积网络（GCN），以探索足够的判别性特征，以实现鲁棒的动作识别。==在这里，我们使用了一种微妙的技术，==类激活映射（CAM），来区分由每个流激活的判别性骨架关节。通过累积前面流获取的激活图，形成一个掩码矩阵，通知新的流关于哪些关节已经被激活。====然后，新的流将被迫从未激活的关节中探索判别性特征。==因此，所提出的方法被称为丰富激活的GCN（RA-GCN），其中==发现的丰富判别性特征将提高模型对不完整骨架的鲁棒性。==在NTU RGB+D数据集[17]上的实验结果表明，RA-GCN在性能上达到了与最先进方法相媲美的水平。此外，针对不完整骨架的情况，我们==构建了一个合成的遮挡数据集==，其中NTU数据集的关节在空间和时间维度上被部分遮挡。图1展示了两种类型遮挡的一些示例。在新数据集上，RA-GCN显著改善了性能下降的情况。</p><h2 id="相关研究">相关研究</h2><blockquote><p>Many researchers are exploring to extract non-local fea-<br>tures from all positions in the input feature maps, such as [15]<br>and [16].许多研究人员正在探索从输入特征图的所有位置提取非局部特征，如[15]和[16]。</p><p>[15] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local<br>neural networks,” in CVPR, 2018, pp. 7794–7803.<br>[16] S. Li, S. Bak, P. Carr, and X. Wang, “Diversity regu-<br>larized spatiotemporal attention for video-based person<br>re-identification,” in CVPR, 2018, pp. 369–378.</p></blockquote><p><strong>总结：</strong></p><ol><li><strong>背景介绍：</strong><ul><li>骨架基础的人体动作识别在多个应用领域中变得越来越重要，并取得了显著进展。</li><li>骨架数据由多个空间和时间骨架关节的3D坐标组成，可通过多模态传感器（如Kinect）或2D图像的姿态估计方法获取。</li></ul></li><li><strong>传统方法的局限性：</strong><ul><li>传统方法通常以两种方式处理骨架数据，但RNN和CNN方法对骨架关节之间的空间结构信息利用不足。</li><li>最近，Yan等人首次将基于图的方法应用于骨架动作识别，提出了空间时间图卷积网络（ST-GCN）。</li></ul></li><li><strong>不完整骨架的挑战：</strong><ul><li>现有方法假设可以完整捕捉骨架关节，而实际场景中往往难以获取完整的骨架序列，例如，由于物体遮挡导致部分关节缺失。</li></ul></li><li><strong>提出的方法 - RA-GCN：</strong><ul><li>为提高动作识别模型对不完整骨架的鲁棒性，提出了多流图卷积网络（RA-GCN）。</li><li>RA-GCN利用类激活映射（CAM）识别每个流激活的骨架关节。</li><li>通过累积前一流的激活图形成掩码矩阵，引导新流探索未激活的关节。</li></ul></li><li><strong>实验证明：</strong><ul><li>RA-GCN在NTU RGB+D数据集上表现出与最先进方法相媲美的性能。</li><li>针对不完整骨架，构建了合成遮挡数据集，RA-GCN显著改善了性能下降情况。</li></ul></li></ol><p><strong>结论：</strong> 该方法通过有效利用骨架关节的空间结构信息，特别是在不完整骨架情况下，提出了一种RA-GCN模型，取得了令人满意的实验结果。</p><h2 id="2-RICHLY-ACTIVATED-GCN">2. RICHLY ACTIVATED GCN</h2><p><strong>丰富激活的GCN：</strong></p><p>为了提高动作识别模型的鲁棒性，我们提出了RA-GCN，旨在从训练骨架序列中提取充分的判别性特征。RA-GCN的概述如图2所示。假设V是一个骨架中的关节数量，M是一帧中的骨架数量，T是一个序列中的帧数量。那么，输入数据x的大小为C × T × V × M，其中C = 3表示每个关节的3D坐标。</p><p>该网络包括三个主要步骤：</p><ol><li><strong>预处理模块：</strong> 将输入数据x转换为x，以提取更多信息。预处理模块由两部分组成：第一部分通过==计算时间差分xt = x[t+1]−x[t]提取运动特征==，第二部分计算==所有关节与每帧中心关节的相对坐标xr==。将x、xt和xr连接以获取x。</li><li><strong>流过滤：</strong> 对于每个流，通过与记录当前未激活关节的掩码矩阵的逐元素乘积，对x中的骨架关节进行过滤。掩码矩阵通过累积前一流的激活模块获取的激活图进行初始化。每个流的输入数据仅包含未激活的关节，然后通过ST-GCN网络获取基于部分骨架关节的特征表示。</li><li><strong>输出模块：</strong> 将所有流的特征连接在一起，使用softmax层获取输入x的最终类别。</li></ol><p><strong>总结：</strong></p><ul><li>RA-GCN是为了==应对不完整骨架数据而提出的动作识别模型，通过多个流同时处理未激活的关节，以提取充分的判别性特征==。</li><li>预处理模块用于从输入数据中提取更多信息，包括运动特征和相对坐标。</li><li>流过滤通过==掩码矩阵和激活图==确保每个流处理不同的关节集，增强了模型对不完整骨架的鲁棒性。</li><li>输出模块将所有流的特征连接，并使用softmax层获取最终的动作类别。RA-GCN在NTU RGB+D数据集上达到了与最先进方法相媲美的性能，并在合成遮挡数据集上显著改善了性能。</li></ul><h3 id="2-1-Baseline-Model">2.1. Baseline Model</h3><p><strong>Baseline Model 章节：</strong></p><p>基准模型是ST-GCN [11]，由多个空间卷积块和时间卷积块组成。具体而言，空间图卷积块可以通过以下公式实现：</p><p><em>f<strong>o</strong>u**t</em>=<em>D</em>max∑<em>d</em>=0<em>D</em>max<em>W<strong>d</strong>f</em>in(Λ2d<em>A**d</em>Λ2d⊗<em>M**d</em>),</p><p>其中<em>D</em>max是预定义的最大距离,f<em>in和</em>f<em>out分别是输入和输出特征图，<em>A**d</em>表示图形距离</em>d<em>的邻接矩阵，<em>d</em>=∑</em>k<strong>A</strong>ik<strong>d*+<em>α</em>是规范化对角矩阵，*A</strong>ik<strong>d<em>表示</em>A</strong>d<em>的第i行第k列的元素，<em>α</em>设置为一个小值，例如10−410−4，以避免Λ�Λ</em>d<em>中的空行。对于每个邻接矩阵，我们配备一个可学习的矩阵</em>M<strong>d*，它表示每个边的重要性。在空间图卷积块之后，使用1×L Conv层提取特征图*f</strong>o<strong>u</strong>t*的时间信息，其中L是时间窗口大小。空间和时间卷积块之后都有Batch-Norm层和ReLU层，并且总的ST-GCN层包含一个残差连接。此外，在空间和时间卷积块之间添加了一个自适应的Dropout层，以避免过拟合。关于ST-GCN的更多细节将在[11]中找到。</p><p><strong>总结：</strong></p><ul><li>基准模型采用ST-GCN结构，包括多个空间和时间卷积块。</li><li>空间图卷积块通过图卷积操作处理邻接矩阵，其中包含学习到的重要性矩阵。</li><li>时间信息由1×L Conv层提取，L为时间窗口大小。</li><li>整体结构包括Batch-Norm、ReLU、残差连接和自适应Dropout层。</li></ul><h3 id="2-2-Activation-Module">2.2. Activation Module</h3><p><strong>Activation Module模块：</strong></p><p>在RA-GCN中，==激活模块被构建用于区分每个流的激活关节，====然后通过累积前面流的激活图来引导新流的学习过程==。这个过程主要可以通过CAM技术[18]实现。原始的CAM技术是在单次前向传递中定位特定类别的图像区域，其中<em>M**c</em>被定义为类别c的激活图，其中每个空间点为 <em>M**c</em>(<em>x</em>,<em>y</em>)=∑<em>k<strong>w</strong>c<strong>k</strong>f**k</em>(<em>x</em>,<em>y</em>). 在这个公式中，<em>f**k</em>(<em>x</em>,<em>y</em>)是全局平均池化操作之前的特征图，<em>w<strong>c</strong>k</em>是类别c的第k通道的权重。在本文中，我们用图像的坐标(x, y)替换为==帧编号t和骨架序列中的关节点编号i，==从而可以定位激活的关节点。这些==关节点也可以视为相应流的注意关节点==。在这里，类别c被选为真实类别。然后，流s的掩码矩阵计算如下。 <em>ma<strong>s</strong>k**s</em>=(⨂<em>i</em>=1<em>s</em>−1mask<em>i</em>)⊗(1−Softmax(<em>M**s</em>−1<em>c</em>)), 其中⨂表示所有掩码矩阵的逐元素乘积，特别地，第一个流的掩码矩阵是一个全1矩阵。最后，流s的输入将由以下公式获得。 <em>x**s</em>=<em>x</em>′⊗masks, 其中′<em>x</em>′是预处理后的骨架数据。</p><p>公式(4)说明了==流s的输入仅包含前面流未激活的关节点。==因此，RA-GCN将充分从所有关节点中探索判别特征。图3显示了基准模型、具有2个流和3个流的RA-GCN的所有流的激活关节点的示例，从中我们可以观察到RA-GCN比基准模型发现了更多激活的关节点。</p><p>RA-GCN的代码可在 <a href="https://github.com/yfsong0709/RA-GCNv1">https://github.com/yfsong0709/RA-GCNv1</a> 找到。</p><p><strong>总结：</strong></p><ul><li>RA-GCN中的激活模块通过CAM技术区分每个流的激活关节点。</li><li>使用激活图引导新流的学习过程，累积前面流的激活图以产生掩码矩阵。</li><li>输入流经过逐元素乘积操作，仅包含前面流未激活的关节点。</li><li>RA-GCN通过充分探索所有关节点，发现了更多激活的关节点，提高了模型的鲁棒性。</li></ul><h2 id="相关研究-2">相关研究</h2><p>This procedure can be<br>implemented mainly by the CAM technique [18].</p><p>【18】B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-<br>ralba, “Learning deep features for discriminative local-<br>ization,” in CVPR, 2016, pp. 2921–2929.</p><h2 id="3-EXPERIMENTS">3. EXPERIMENTS</h2><h3 id="3-1-Dataset-and-Implemental-Details">3.1. Dataset and Implemental Details</h3><p><strong>Dataset and Implemental Details模块：</strong></p><p>在这一部分，我们评估RA-GCN在大规模数据集NTU RGB+D [17]上的性能，这是目前最大的室内动作识别数据集。该数据集包含由Microsoft Kinect v2收集的56880个视频样本，由40名受试</p><p>在训练RA-GCN之前，我们需要对一个带有转换骨架数据的ST-GCN网络进行预训练，以获取基线模型，训练设置与[11]相同。然后，使用此基线模型初始化RA-GCN的所有流。最后，对RA-GCN模型进行微调。所有实验均在两个TITAN X GPU上运行。</p><p><strong>总结：</strong></p><ul><li>评估RA-GCN性能使用NTU RGB+D数据集，包含大量视频样本和多个动作类别。</li><li>数据集分为两个基准：==跨主体（CS）和跨视角（CV）==。</li><li>在训练RA-GCN之前，==需要通过预训练ST-GCN网络获取基线模型==。</li><li>所有实验在两个TITAN X GPU上运行。</li></ul><h3 id="3-2-Experimental-results-on-complete-skeletons">3.2. Experimental results on complete skeletons</h3><p><strong>Experimental results on complete skeletons模块：</strong></p><p>我们在NTU RGB+D数据集上将RA-GCN的性能与先前的最先进方法进行了比较。如表1所示，尽管在CS基准上仅比PB-GCN [20]差1.6％，但我们的方法在CV基准上表现优异。与基线相比，我们的方法分别提高了4.4％和5.2％。由于RA-GCN旨在发现更具判别性的关节，因此RA-GCN的性能仅与最先进的方法相媲美，因为大多数动作只需几个主要关节即可识别。然而，当这些主要关节被遮挡时，传统方法的性能将显著下降。</p><p><strong>总结：</strong></p><ul><li>在NTU RGB+D数据集上，RA-GCN在CV基准上表现优越，仅在CS基准上略逊于PB-GCN。</li><li>与基线相比，RA-GCN在两个基准上分别提高了4.4％和5.2％的性能。</li><li>RA-GCN的性能与最先进的方法相当，因为其目标是发现更有判别性的关节。</li><li>传统方法在主要关节被遮挡时性能显著下降。</li></ul><h3 id="3-3-Ablation-studies">3.3. Ablation studies</h3><p><strong>Ablation studies模块：</strong></p><p>在第2.1节中，我们介绍了基线模型的两个超参数==最大距离Dmax和时间窗口大小L==。这两个超参数对我们的模型有很大影响。我们测试了六组参数，实验结果见表2。观察到在CS基准上，当Dmax = 2且L = 5时，我们的模型获得了最佳准确性。至于CV基准，最佳设置为Dmax = 3和L = 9。</p><p><strong>总结：</strong></p><ul><li>基线模型的两个超参数，Dmax和L，对模型性能有显著影响。</li><li>在CS基准上，当Dmax = 2且L = 5时，我们的模型获得了最佳准确性。</li><li>在CV基准上，最佳设置为Dmax = 3和L = 9。</li></ul><h3 id="3-4-Experimental-results-on-incomplete-skeletons">3.4. Experimental results on incomplete skeletons</h3><p><strong>Experimental results on incomplete skeletons模块：</strong></p><p>为验证我们的方法对不完整骨架的鲁棒性，我们基于NTU-RGB+D数据集构建了一个合成遮挡数据集，其中选择了一些关节在空间和时间维度上进行遮挡（设为0）。 ==对于空间遮挡，我们使用完整骨架训练测试模型，然后用遮挡部分1、2、3、4、5的骨架进行评估==，这些部分分别表示左臂、右臂、双手、双腿和躯干。 对于时间遮挡，我们随机遮挡前100帧的一个块，因为许多序列的长度都小于100。遮挡数据集的一些示例在图1中展示。 在合成遮挡数据集上，我们测试了基线模型[11]、SR-TSL [5]、具有2个流和3个流的RA-GCN。实验结果显示在表3中，从中可以看出3s RA-GCN在大多数遮挡实验中明显优于其他模型，除了遮挡第2部分。特别是在时间遮挡实验中，3s RA-GCN相对于其他模型表现出越来越明显的优势。 因此，提出的方法可以减轻由于遮挡导致的性能下降。我们还发现，当一些重要的关节，例如右臂，被遮挡时，某些动作类别（例如握手）无法通过其他关节推断。在这种情况下，提出的方法将失败。</p><p><strong>总结：</strong></p><ul><li>通过在合成遮挡数据集上的实验证明，3s RA-GCN在大多数遮挡实验中表现出较大优势。</li><li>在时间遮挡实验中，3s RA-GCN相对于其他模型表现出越来越明显的优势。</li><li>方法可以减轻由于遮挡导致的性能下降，但当一些重要关节被遮挡时，模型可能失败。</li></ul><h2 id="4-CONCLUSION">4.CONCLUSION</h2><p><strong>CONCLUSION模块：</strong></p><p>在本文中，我们提出了一种名为RA-GCN的新模型，该模型较基线模型表现更好并提高了模型的鲁棒性。通过对NTU RGB+D数据集进行广泛实验证明了我们模型在遮挡场景中的有效性。</p><p><strong>总结：</strong></p><ul><li>RA-GCN相较于基线模型表现更好，提高了模型的鲁棒性。</li><li>通过在NTU RGB+D数据集上的实验证明了模型在遮挡场景中的有效性。</li><li>未来的工作方向包括将注意力模块引入我们的模型，以使每个流更加关注特定的判别性关节。</li></ul><blockquote><p>使用时间遮挡 和 空间遮挡的方式</p><p>利用关节学习激活模块 激活模型对于没有完全学习的特征进行学习</p></blockquote><h1>基于丰富激活图卷积网络的健壮骨架动作识别**</h1><h2 id="Abstract">Abstract</h2><p><strong>Abstract翻译和总结：</strong></p><p>当前的基于骨架的人体动作识别方法通常使用完整的骨架。然而，在实际场景中，捕捉到不完整或带噪音的骨架是不可避免的，尤其当一些关键关节被遮挡或受到干扰时，这可能显著降低当前方法的性能。</p><p>为了提高动作识别模型的鲁棒性，提出了一种多流图卷积网络（GCN）的新方法，用于探索分布在所有骨架关节上的足够判别性特征。这样，分布式冗余表示减少了动作模型对非标准骨架的敏感性。</p><p>具体而言，==通过一系列有序流程扩展了骨干GCN==，这些流程负责==从之前流程中激活较少的关节中学习判别性特征==。每个GCN流的骨架关节的激活程度通过类激活映射（CAM）进行测量，只有未激活的关节信息将传递到下一个流程，从而获取所有激活关节的丰富特征。因此，这种方法被称为富激活GCN（RA-GCN）。</p><p>与最先进方法相比，RA-GCN在标准NTU RGB+D 60和120数据集上取得了可比的性能。更重要的是，在合成的遮挡和抖动数据集上，由于受到遮挡和干扰的关节，利用提出的RA-GCN可以显著减轻性能下降。</p><ul><li><strong>问题陈述：</strong> 传统的基于完整骨架的人体动作识别方法在面对不完整或带噪音的骨架数据时性能下降。</li><li><strong>提出方法：</strong> 引入了一种名为RA-GCN的多流图卷积网络，旨在通过多流程学习所有骨架关节上的判别性特征，从而提高模型的鲁棒性。</li><li><strong>方法细节：</strong> RA-GCN通过测量每个GCN流中骨架关节的激活程度（CAM）来识别并关注少数先前未激活的关节，以获取全局激活关节的丰富特征。</li><li><strong>性能对比：</strong> RA-GCN在标准NTU RGB+D 60和120数据集上实现了与最先进方法（SOTA）可比较的性能。尤其是在合成的遮挡和抖动数据集上，==RA-GCN显著减轻了由于关节遮挡和干扰导致的性能下降==。</li><li><strong>未来工作：</strong> 计划将注意力模块引入模型，使每个流程更专注于某些判别性关节，以进一步提高性能。</li></ul><h2 id="I-INTRODUCTION">I. INTRODUCTION</h2><p><strong>引言：</strong></p><p>人体动作识别在近期的计算机视觉研究中取得了显著进展，并在许多潜在应用中发挥着越来越重要的作用，如视频监视、人机交互、视频检索等[1]，[2]，[3]。动作识别的主要目的是从可以被捕捉为RGB视频[4]，[5]，深度图[6]，红外图像[7]和3D骨架序列[8]，[9]，[10]的运动数据中对人类动作进行分类。</p><p>传统的动作识别主要由基于RGB视频的方法主导。这些方法通常将RGB视频视为图像帧的时间序列，并使用循环神经网络（RNN）等顺序模型从由卷积神经网络（CNN）提取的每帧的所有特征图中获取时间信息[4]。另一方面，许多研究人员利用3D CNN直接从视频中提取有用信息[5]，这与前一种方法的性能相当。然而，这两种类别都是从2D RGB帧中提取空间结构信息，而演员的空间配置是在3D空间中完全呈现的。因此，这些基于RGB的方法将由于固有弱点而失去一些关键信息。此外，RGB视频通常包含复杂的背景和光照变化，这在实践中导致了显著的性能下降。</p><p>与RGB视频相比，基于骨架的人体动作识别方法由于其在适应性、对光照变化的鲁棒性以及较低的计算成本方面的优势而显示出越来越大的前景。骨架数据由运动序列中多个骨架关节的2D/3D坐标组成，可以通过多模态传感器（如Kinect）收集，也可以通过2D图像的姿态估计方法直接估计[11]。目前的方法通常以两种方式处理骨架数据。一种方法是将这些关节连接成一个全局向量，然后通过使用基于RNN的方法[8]，[9]，[12]，[13]来对时间信息进行建模。另一种方法是将关节的时间序列视为2D图像，然后利用基于CNN的方法来识别动作[14]，[15]，[16]，[17]。然而，通过RNN和CNN方法有效地利用骨架关节之间的空间结构信息是困难的，尽管许多研究人员提出了额外的约束或专用网络结构来建模骨架关节的空间结构。最近，图神经网络（GNN）在许多领域取得了巨大的进展，它可以明确地将图形结构信息纳入神经网络的学习中[18]。Yan等人[10]首次提出了一种空时图卷积网络（ST-GCN）来捕捉嵌入在骨架序列的空间配置和时间动态中的模式，这在动作识别中取得了显著的进展。</p><p>然而，==当前基于骨架的动作识别模型仍然缺乏对在真实场景中捕获的嘈杂或不完整骨架数据的鲁棒性。==例如，主体可能会由于姿势变化而被自身遮挡，或者被场景中的其他对象遮挡。图1显示了由于场景中的其他人物导致的一些遮挡动作的示例。嘈杂的数据会严重恶化模型的性能。因此，如何提高基于骨架的动作识别模型的鲁棒性仍然是一项紧迫且具有挑战性的问题。</p><p>为了对抗各种降级，==集成学习==已被证明是一种有效的策略[19]，<strong>该策略基于相同或不同的预测分类器诱导出多个分类器，以便这些个体分类器的集成可以提高识别模型的鲁棒性</strong>。受集成学习成功的启发，本文提出了一种<strong>基于GCN的多流模型</strong>，旨在从骨架运动序列中学习丰富的判别特征，从而提高所提出模型的鲁棒性。==我们方法中每个流的目的是探索由先前流程激活的骨架关节中未激活的一组判别特征==。==学到的冗余但互补的所有骨架关节特征提供了处理嘈杂或不完整骨架数据的有效策略。==例如，当我们识别投掷动作时，在投掷过程中，最具判别力的关节位于两臂的瞬间，而身体摆动以及手的上下文子动作也可以用来推断投掷动作。</p><p>为了区分每个流程中最具信息性的关节，我们引入了一种名==为类激活映射（CAM）的成功技术到我们的模型中==，该技术最初旨在可视化负责视觉分类的最终CNN层中的激活热图。由先前GCN流程获取的激活图累积为一个掩码矩阵，以通知新的流程哪些关节已经激活。然后，新的流程将被强制从未激活的关节中探索新的判别特征。因此，所提出的方法被称为丰富激活的GCN（RA-GCN），其中丰富发现的互补特征将提高模型对非标准骨架的鲁棒性。据我们所知，==这是首次利用CAM技术通过扩展激活的骨架关节来增强模型鲁棒性，从而缓解基于骨架的动作识别中的遮挡和抖动问题。==</p><p>为了验证所提方法的优势，除了传统的骨架动作数据集NTU RGB+D 60 [21]和120 [22]数据集外，我们还==构建了四个合成遮挡数据集，==其中NTU 60和120数据集的关节在空间和时间维度上部分遮挡，以及==两个合成抖动数据集==，其中一些随机选择的关节受到高斯噪声的干扰。有关这些数据集的更多详细信息，请参见第IV-A节。我们对这些新数据集的实验表明，所提出的RA-GCN在骨架数据不完整或嘈杂的情况下显著减轻了性能下降。</p><p>该工作是[23]中早期和初步版本的延伸。与我们先前的工作相比，本文的修改和贡献总结如下： • 在先前的工作中，激活模块中的激活掩码是通过Softmax函数获得的，该函数仅激活每个流程的少数关节。==相比之下，我们提出使用归一化激活函数来扩大激活范围==，==因此相应的流程将获得更好且更可解释的激活图==。 • 与以前的工作相比，我们通过==在每个单独的网络流上添加许多额外的交叉熵正则化来扩展原始损失函数==，以便更有效地学习特征。 • 合成数据集通过==更多的退化算子进行了扩展，其中遮挡退化进一步分为四种类型，包括帧、部分、块和随机，以及两个合成的抖动数据集==。在不同退化条件下进行了更多实验，以验证所提方法的有效性和鲁棒性。</p><p>本文的其余部分组织如下：第II节描述了与我们工作相关的最近研究。第III节介绍了所提出的RA-GCN的若干关键组件。在第IV节中，对标准和非标准数据集的广泛实验结果进行了报告，本文的结论在第V节给出。</p><ul><li>人体动作识别在计算机视觉领域取得显著进展，应用广泛，涵盖视频监视、人机交互、视频检索等多个领域。</li><li>传统的基于RGB视频的动作识别方法存在局限，因为它们主要从2D RGB帧中提取空间结构信息，无法充分利用演员在3D空间中的配置。</li><li>基于骨架的动作识别方法因其适应性、对光照变化的鲁棒性和低计算成本而变得越来越受关注，骨架数据包括骨架关节的2D/3D坐标。</li><li>目前的骨架动作识别模型在真实场景中对嘈杂或不完整的骨架数据缺乏鲁棒性，例如由于姿势变化或其他对象遮挡引起的问题。</li><li>集成学习是对抗各种降级的有效策略，本文提出了一种基于GCN的多流模型，旨在学习骨架运动序列中的丰富判别特征，提高模型鲁棒性。</li><li>提出方法中每个流的目标是探索由先前流程激活的骨架关节中未激活的一组判别特征，通过学习冗余但互补的骨架关节特征处理嘈杂或不完整的骨架数据。</li><li>引入类激活映射（CAM）技术，通过扩展激活的骨架关节来增强模型对非标准骨架的鲁棒性，解决遮挡和抖动问题。</li><li>针对提出方法的优势进行验证，包括传统骨架动作数据集和新构建的合成数据集，实验证明在不完整或嘈杂的骨架数据情况下，提出方法显著减轻了性能下降。</li><li>本工作是先前研究的扩展，改进体现在使用归一化激活函数、扩展损失函数和更多退化算子的合成数据集，实验证明这些改进提高了方法的性能和鲁棒性。</li><li>文章的结构安排包括相关研究（第II节）、所提出方法的关键组件介绍（第III节）、广泛实验结果报告（第IV节）以及总结和结论（第V节）。</li></ul><h2 id="II-RELATED-WORK">II. RELATED WORK</h2><p>帮我只输出中文翻译的形式逐句翻译并根据输入内容分条总结一下 Skeleton-based models:模块</p><h3 id="a-Skeleton-based-models">a) Skeleton-based models:</h3><ul><li><p>为了找到更有效地表示人体动作动态过程的方法，Johansson [24] 使用3D骨架序列进行动作识别，显著降低了计算成本并取得了良好的性能提升。</p><p>随着深度学习技术的迅猛发展，基于骨架的动作识别方法引起了越来越多的关注。研究人员提出了各种模型以提高动作识别的性能，主要分为三大类别。</p><ul><li>第一类别使用卷积网络构建模型。例如，Li等人[16]提出了基于CNN的共现特征学习框架，逐渐聚合各个层次的上下文信息。Kim等人[15]构建了一个时序卷积网络，明确学习用于3D人体动作识别的可解释时空表示。</li><li>第二类别中，研究人员将一个帧中的所有关节连接成一个单一的向量，然后使用循环模型（如长短时记忆网络LSTM）来探索时间动态。Du等人[25]设计了一个分层双向RNN来捕捉不同人体部位之间的丰富依赖关系。[9]中的研究使用了一种视图自适应的LSTM，使网络本身适应于最适合的观察视点。此外，Song等人[26]首次将注意力模块引入基于骨架的动作识别。</li><li>CNN和RNN方法仍然受限于提取骨架关节之间的空间结构信息，其中不同身体部位的关节连接成骨架图。与此相反，第三类别中可以自然地使用基于图的方法来处理骨架图，成功捕获各种动作的最具信息性的特征。Si等人[13]使用图神经网络（GNN）来建模五个身体部位之间的关系。Yan等人[10]最初将图卷积网络（GCN）引入基于骨架的动作识别，并产生一个名为ST-GCN的基线供未来研究使用。基于ST-GCN，许多研究在基于骨架的动作识别方面取得了持续的改进[27]，[28]，[29]。</li></ul></li></ul><h3 id="b-Occlusion-in-human-action-recognition">b) Occlusion in human action recognition:</h3><p><strong>Occlusion in human action recognition:</strong></p><ul><li>遮挡是人体动作识别中的一个突出挑战。如果骨架关节部分遮挡，上述提到的方法将面临相当大的性能下降。</li><li>为了解决这个问题，Wang等人[30]尝试从全局SVM分类器中推断遮挡图，而Weinland等人[31]提出了一种基于3D梯度直方图（HOG）描述符的局部分区和分层分类方法，以提供对遮挡和视角变化的鲁棒性。然而，在解决骨架关节中的嘈杂或不完整数据问题方面，目前研究较少。</li><li>本文提出一种探索所有关节丰富特征的方法，以减轻数据降级的影响。</li></ul><h3 id="c-Salient-Regions-Exploration">c) Salient Regions Exploration:</h3><p><strong>Salient Regions Exploration:</strong></p><ul><li>与我们的动机类似，一些先前的研究提出<strong>探索显著区域或消除它们</strong>，<strong>以利用在指代表达基础设施[32]，[33]或弱监督检测任务[34]中的互补信息</strong>。研究[32]提出了一种简单而有效的网络，以阻止注意力传播到意外的背景区域，以提高目标注意力的质量。Liu等人[33]设计了一种新颖的引导擦除方法，以对齐跨越视觉和文本模态的各种信息。此外，Li等人[34]提供了一个框架，根据在线注意力图动态擦除关注区域。==然而，先前的显著区域探索方法主要集中在图像中的对象检测或定位任务上，而在这项工作中，我们利用了互补的关注骨架关节，以减轻遮挡或抖动问题，这在先前的工作中尚未考虑。==</li></ul><h2 id="III-MODEL-ARCHITECTURE">III. MODEL ARCHITECTURE</h2><p>为了提升动作识别模型的鲁棒性，我们提出了RA-GCN，==旨在从所有骨架关节中提取足够的判别特征。==该模型构建了一个多流网络，==每个流专门从一组激活的关节中提取特征。==   通过这种设计，即使第一个流激活的关节被遮挡，模型仍能从其他流中获取判别性信息。RA-GCN的概览见图2。在输入数据方面，我们假设V为一个骨架中的关节数，T为一个序列中的帧数，输入数据x的大小为Cin×T×V，其中Cin = 3表示每个关节的3D坐标。需要注意的是，在多智能体动作中，不同骨架被视为不同的样本。</p><p>我们提出的方法包括三个主要步骤。首先，在预处理模块中，为了提取更多信息，输入数据x被转换为x，然后传递到所有GCN流程。其次，对于每个流，x中的骨架关节将通过与一个掩码矩阵的逐元素乘积进行过滤，==该矩阵记录当前未激活的关节==。==这些关节通过累积由先前流程的激活模块计算的激活图进行区分。？？？？？？？？？？==在这里，每个流的掩码矩阵被初始化为与x相同形状的全1矩阵。==在掩码操作之后，每个流的输入数据仅包含前面流程未激活的关节，====并通过基线网络传递，以基于不完整的骨架关节获得特征表示。==</p><p>最后，在输出模块中，所有流的特征被串联，并使用带有Softmax激活函数的全连接层获得输入x的最终类别。这三个步骤将在下一节详细讨论。</p><ul><li><strong>RA-GCN提出的动机与设计：</strong><ol><li><strong>动机：</strong> 为了提高动作识别模型的鲁棒性。</li><li><strong>设计：</strong> RA-GCN是一个多流网络，每个流负责从一组激活的关节中提取特征，确保即使某些关节被遮挡，模型仍能获取判别性信息。</li></ol></li><li><strong>RA-GCN的输入数据处理：</strong><ol><li><strong>数据描述：</strong> 输入数据x的大小为Cin×T×V，其中Cin = 3表示每个关节的3D坐标，在多智能体动作中不同骨架被视为不同样本。</li><li><strong>预处理：</strong> 对输入数据x进行预处理，转换为x，以提取更多信息。</li></ol></li><li><strong>RA-GCN的处理步骤：</strong><ol><li><strong>第一步：</strong> 在预处理模块中，通过将输入数据x转换为x，提取更多信息。</li><li><strong>第二步：</strong> 对于每个流，通过与掩码矩阵的逐元素乘积，过滤x中的骨架关节，记录当前未激活的关节。这确保每个流的输入数据仅包含前面流程未激活的关节。</li><li><strong>第三步：</strong> 在输出模块中，将所有流的特征串联，使用Softmax激活函数的全连接层获得输入x的最终类别。</li></ol></li></ul><p>总体而言，RA-GCN通过多流网络的设计，充分利用所有骨架关节提取判别性特征，从而增强了动作识别模型的鲁棒性。</p><h3 id="A-Data-Preprocessing">A. Data Preprocessing</h3><ul><li><p>A. 数据预处理：</p><ol><li><strong>相似动作问题：</strong> 通常，一些动作，如脱鞋和穿鞋，在空间特征上非常相似。为了解决这个问题，传统的基于RGB的方法引入了一种称为光流（Optical Flow）的复杂技术，用于精确描绘运动特征。</li><li><strong>相对坐标和运动特征：</strong> 文献[28]指出，==关节的相对坐标通常比绝对坐标更具信息量。==受此启发，我们的模型==应用几何特征（如相对坐标）和运动特征（如时间位移）以增加动作识别的判别信息==。因此，在将输入数据分发到所有GCN流之前，需要对输入数据进行预处理。</li><li><strong>相对坐标的计算：</strong> 相对坐标可以看作是每一帧中<strong>所有关节与中心关节（中间脊柱）之间的差异x˙r，如图3(a)所示。通过这种方式，所有关节都被转换为相对坐标，更加鲁棒地适应位置变化。</strong></li><li><strong>运动特征的提取：</strong> 为了提取更多信息，我们通过计算˙xt（xt+1 − xt）得到运动特征，其中xt表示第t帧的特征图，如图3(b)所示。然后，通过串联x、˙xr和˙xt，得到x。</li></ol></li><li><p>A. 数据预处理模块总结：</p><ol><li><strong>相似动作处理：</strong> 针对一些在空间特征上极为相似的动作，采用了光流技术，用于准确描绘运动特征，以解决这一问题。</li><li><strong>相对坐标和运动特征应用：</strong> 受到相对坐标相较于绝对坐标更具信息量的启发，引入了几何特征（相对坐标）和运动特征（时间位移）来增加动作识别的判别性信息。</li><li><strong>相对坐标计算：</strong> 将每个关节与中心关节之间的差异作为相对坐标，提高了模型对位置变化的鲁棒性。</li><li><strong>运动特征提取：</strong> 通过计算相邻帧的特征差异，提取更多的运动特征，从而增强了对动作的判别能力。</li></ol></li></ul><h3 id="B-Richly-Activated-GCN">B. Richly Activated GCN</h3><h4 id="1-Baseline-Model">1) Baseline Model:</h4><ul><li><ol><li>基线模型：</li></ol><ol><li><strong>ST-GCN基线：</strong> 我们方法的基线是ST-GCN [10]，由十个图卷积层组成。Yan等人[10]将空间图卷积操作表示为： fout(<em>v<strong>t</strong>i</em>)=∑<em>v<strong>t</strong>j</em>∈<em>B</em>(<em>v<strong>t</strong>i</em>)<em>Z<strong>t</strong>i</em>(<em>v<strong>t</strong>j</em>)1fin(<em>v<strong>t</strong>j</em>)⋅<em>w</em>(<em>l<strong>t</strong>i</em>(<em>v<strong>t</strong>j</em>)) 其中fin和fout分别是输入和输出特征图，<em>v<strong>t</strong>i</em>表示第t帧中的第i个关节，也可以看作在这个过程中的根关节，)<em>B</em>(<em>v<strong>t</strong>i</em>)是<em>v<strong>t</strong>i</em>的邻居集，标准化项<em>Z<strong>t</strong>i</em>用于平衡不同邻居的贡献，<em>w</em>(⋅)是由若干1×1 Conv层实现的权重函数，<em>l<strong>t</strong>i</em>(⋅)表示标签函数。[10]中有三个标签函数，但我们的方法中只选择了基于距离的标签函数，它定义为<em>l<strong>t</strong>i</em>(<em>v<strong>t</strong>j</em>)=<em>d</em>(<em>v<strong>t</strong>i</em>,<em>v<strong>t</strong>j</em>)。这意味着邻居集<em>B</em>(<em>v<strong>t</strong>i</em>)根据<em>v<strong>t</strong>j</em>与根关节<em>v<strong>t</strong>i</em>之间的图距离被划分为若干子集。例如，如果关节<em>v<strong>t</strong>j</em>直接与根关节<em>v<strong>t</strong>i</em>相连，则<em>d</em>(<em>v<strong>t</strong>i</em>,<em>v<strong>t</strong>j</em>)=1。具有相同距离的关节将形成一个子集，并共享一个可学习的权重函数<em>w</em>(⋅)。</li><li><strong>空间图卷积的实现：</strong> 通过邻接矩阵A实现空间图卷积，将Eq.1转化为： fout=∑<em>d</em>=0<em>D<strong>W</strong>d</em>⋅fin(Λ−1/2<em>d<strong>A</strong>d</em>Λ−1/2<em>d</em>⊗<em>M**d</em>) 其中D是预定义的最大距离，Ad是距离d的邻接矩阵，Λ<em>ii**d</em>=∑<em>k<strong>A</strong>ik**d</em>+<em>α</em>是归一化对角矩阵，<em>A<strong>ik</strong>d</em>表示Ad的第i行第k列的元素，<em>α</em>设置为一个小值，如10−410−4，以避免Λd中的空行。对于每个邻接矩阵，我们将其与一个可学习矩阵Md相伴随，该矩阵表示一个骨架中所有边的重要性。</li><li><strong>时空卷积块：</strong> 空间图卷积块后使用1×L卷积层提取特征图fout的时序信息，其中L是时间窗口大小。空间和时序卷积块之后分别跟有BatchNorm层和ReLU层，并且整个ST-GCN层包含一个残差连接。此外，每个空间卷积块和时序卷积块之间都添加了一个丢弃层，丢弃概率为0.5，以避免过拟合。ST-GCN层的结构如图4所示。</li></ol></li><li><ol><li>基线模型总结：</li></ol><ol><li><strong>ST-GCN基线结构：</strong> 采用ST-GCN [10]作为基线模型，由十个图卷积层组成。空间图卷积操作通过权重函数和标签函数处理每个关节，其中使用基于距离的标签函数以及邻接矩阵来划分邻居关节。</li><li><strong>空间图卷积实现：</strong> 利用邻接矩阵A实现空间图卷积，通过一系列矩阵操作和权重调整，考虑了骨架中不同关节之间的图距离和边的重要性。</li><li><strong>时空卷积块：</strong> 使用1×L卷积层提取时序信息，采用残差连接，BatchNorm层和ReLU层用于增强模型的表达能力，同时在空间和时序卷积块之间引入了丢弃层以防止过拟合。</li></ol></li></ul><h4 id="2-Activation-Module">2) Activation Module:</h4><ul><li><strong>2) 激活模块：</strong><ol><li><strong>RA-GCN中的激活模块：</strong> RA-GCN中的==激活模块用于区分每个流的激活关节==，==然后通过积累前序流的激活图来引导新流的学习过程==。该过程主要通过将CAM技术[20]扩展到GCN领域来实现。CAM技术的原始目的是在CNN中定位特定类别的图像区域，==其中scorec被定义为类别c的所有像素的分数==，每个像素的分数为<em>scorec</em>(<em>x</em>,<em>y</em>)=∑<em>k<strong>w</strong>c</em>,<em>k</em>⋅<em>f**k</em>(<em>x</em>,<em>y</em>)。</li><li><strong>适用于骨架数据的CAM技术：</strong> 在这里，将特征图fk(·, ·)中的坐标(x, y)替换为帧数t和骨架序列中的关节号i，从而能够定位激活的关节。这里，类别c被选择为真实类别。使用scores表示所有关节对于真实类别和第s个流的分数图。为确定由相应流激活的关节，使用预定义的阈值δ，并通过以下计算得到maps_c的激活图：<em>ma<strong>p</strong>s**c</em>=<em>ε</em>(max(<em>score<strong>s</strong>c</em>)<em>score<strong>s</strong>c</em>−<em>δ</em>) 其中ε(·)是==Heaviside阶跃函数==，max(·)表示最大值函数。然后，第s个流的mask矩阵表示为：<em>ma<strong>s</strong>k**s</em>=(⨂<em>i</em>=1<em>s</em>−1<em>ma<strong>s</strong>k**i</em>)⊗(1−<em>ma<strong>p</strong>s**s</em>−1) 这里的⨂⨂表示第s个流之前的所有mask矩阵的逐元素乘积。特别地，第一个流的mask矩阵是全1矩阵。最终，第s个流的输入将由以下方式得到： <em>x**s</em>=<em>x</em>′⊗<em>ma<strong>s</strong>k**s</em> 其中′<em>x</em>′是预处理后的骨架表示。==方程5和方程6说明第s个流的输入仅包含前序流未激活的关节，从而RA-GCN将充分探索所有关节的判别特征。==</li></ol></li><li><strong>激活模块总结：</strong><ol><li><strong>CAM技术的应用：</strong> 将CAM技术应用于GCN，以定位骨架序列中激活的关节，区分不同类别的关键信息。</li><li><strong>阈值与激活图：</strong> 使用预定义阈值δ来确定激活的关节，并生成相应的激活图maps_c。</li><li><strong>Mask矩阵：</strong> 利用前序流的激活图生成当前流的mask矩阵，确保当前流的输入仅包含前序流未激活的关节。</li><li><strong>RA-GCN特征探索：</strong> RA-GCN通过这一流程充分探索所有关节，从而获得判别性特征。</li></ol></li></ul><h4 id="3-Loss-Function">3) Loss Function:</h4><ul><li><strong>3) 损失函数模块：</strong><ol><li><strong>在我们先前的模型[23]中，损失函数仅监督整个网络，而在本文中，为每个个体网络流扩展了许多额外的损失，以便更有效地学习特征。</strong></li><li><strong>设ˆys ∈ RC为第s个流的输出，ˆy ∈ RC为整个模型的输出，y ∈ RC为地面真实值，其中C是类别数。</strong></li><li><strong>提出的RA-GCN的损失函数为*L*=−*y*log*y*^−∑*s*=1*S**y*log*y*^*s*，其中S是流的数量。</strong></li></ol></li><li><strong>损失函数模块总结：</strong><ol><li><strong>模型扩展：</strong> 与先前的模型相比，本文扩展了损失函数，为每个网络流添加了额外的损失，以提高特征学习效果。</li><li><strong>符号说明：</strong> ˆ<em>ys</em>表示第s个流的输出，^<em>y</em>^表示整个模型的输出，y表示地面真实值，C是类别数。</li><li><strong>损失函数定义：</strong> RA-GCN的损失函数包括总网络损失和每个流的个体损失，由<em>L</em>=−<em>y</em>log<em>y</em>^−∑<em>s</em>=1<em>S**y</em>log<em>y</em>^<em>s</em>表示，其中S是流的数量。</li></ol></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解2用于自监督骨架动作识别的时空遮蔽自动编码器</title>
      <link href="/project/posts/a1a97019.html"/>
      <url>/project/posts/a1a97019.html</url>
      
        <content type="html"><![CDATA[<p>**论文题目：**<strong>SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition.</strong></p><p><strong>论文链接：<a href="https://arxiv.org/pdf/2209.02399.pdf">https://arxiv.org/pdf/2209.02399.pdf</a></strong></p><p><strong>github代码：暂未开源</strong></p><hr><h2 id="一、文章总结">一、文章总结</h2><p>这篇文章是2023年IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)上的一篇文章，针对于自监督骨架动作识别，利用未标记的数据，学习更多的可泛化特征来缓解过拟合问题，减少对大量标记训练数据的需求以减少骨架数据标注所造成的的成本浪费。</p><p>文章以[Masked autoencoders are scalable vision learners.2022]文章为出发点提出了一个用于基于骨骼的自监督3D动作识别的时空掩藏自动编码器框架(SkeletonMAE)。在MAE文章提出的掩蔽和重建思想的基础上，提出了一个基于骨架的编码器-解码器transformer结构来重建掩蔽骨架序列。具体来说，针对骨架序列，分别从骨架的关节级和帧级别两方面提出了一种新的掩蔽策略——时空掩蔽策略。这种预训练策略使得编码器（Encoder）输出具有空间和时间依赖性的可泛化骨架特征。对于输入的骨架序列，编码器被微调为动作识别任务。经过大量实验表明，文出的SkeletonMAE方法取得了显著的性能，并且优于目前最先进的方法。</p><h2 id="二、详细介绍">二、详细介绍</h2><h2 id="1-背景">1.背景</h2><p>人体动作识别是计算机视觉领域的一个基本研究课题，旨在理解人类行为并区分不同的动作。随着深度学习和人体姿势估计方法的蓬勃发展，<strong>人类骨骼数据可以被高效提取为一种高级但轻量级的表示，因此引起了对人类行为和动作分析的广泛关注</strong>。因此，基于3D骨骼的动作识别已经成为人体动作识别中的一个重要研究领域。</p><p>大多数最近的方法侧重于使用全监督学习算法来构建它们的框架：基于卷积神经网络（CNN）的方法，基于循环神经网络（RNN）的方法，基于图卷积网络（GCN）的方法以及基于Transformer的方法广泛应用于骨骼动作识别，并取得了非常好的结果。然而，<strong>完全监督的动作识别容易出现过拟合。<strong>此外，它</strong>需要大量的标记训练数据</strong>，这既昂贵又耗时。为了缓解这些问题，在骨骼动作识别中，自监督学习方法越来越盛行，这些方法利用未标记的数据来学习数据表示。<strong>一些自监督方法考虑了骨骼表示学习的预设任务，使用未标记的骨骼数据，例如动作重建和拼图</strong>。然而，这些<strong>基于预设任务的方法侧重于局部特征，如同一帧内的关节相关性和骨骼比例，并未充分探索时间信息</strong>。最近的一些工作<strong>通过构建不同视图下的骨骼序列，通过数据增强和正负对，训练了基于对比学习框架的对比模型</strong>。尽管这些基于对比学习的方法强调了高级上下文信息，但它们严重依赖于用于提取骨骼特征的关节对的数量，并忽略了不同帧之间的关节相关信息。</p><h2 id="2-方案引出">2.方案引出</h2><p>最近，一种名为&quot;Masked Autoencoders (MAE)&quot;的新的自监督学习方法展示出在计算机视觉任务中具有强大的泛化能力，表现出色。<strong>MAE会对输入图像的大部分区域进行遮盖，然后强制模型仅使用未遮盖的部分来重建原始图像。</strong></p><p>然而，由于以下原因，MAE不能直接用于自监督骨骼动作识别：</p><ul><li>MAE使用了视觉变换器（ViT）结构来处理图像输入。<strong>与不包含时间信息的图像不同，人体骨架序列是从包含丰富语义信息的高信息密度视频中提取出来的：在空间层面，关节特征包含了同一帧内不同关节之间的关系；在时间层面，帧特征代表了同一关节在不同帧之间的运动。</strong></li><li>MAE的遮盖策略仅关注空间域。在处理人体骨架序列数据时，需要一种<strong>空间-时间遮盖策略</strong>。</li></ul><p>为了解决这些问题，文章引入了一种名为&quot;SkeletonMAE&quot;的新型骨架自监督学习方法，旨在进行骨架的<strong>空间-时间表示学习</strong>：</p><ol><li><p>遮盖输入序列是从原始骨架序列中生成的，其中包含了关节坐标（空间）信息和帧（时间）信息；</p></li><li><p>借助空间-时间遮盖策略和编码-解码规则，SkeletonMAE通过基于transformer的编码器和解码器处理遮盖序列，从而获得重建序列，其中空间和时间信息得到了很好的处理（文章借助了transformer在长序列数据的空间-时间表示学习中具有巨大潜力）。</p></li></ol><p>SkeletonMAE的框架如图1所示。具体来说，整个SkeletonMAE流程是根据以下原则设计的。在预训练阶段，使用了空间-时间遮盖策略（具有预设的帧遮盖和关节遮盖比率）来遮盖输入骨架序列的部分部分，包括帧级别和关节级别。为了找到最佳的空间-时间表示学习的权衡点，文章讨论了关<strong>节遮盖和帧遮盖比率的作用</strong>，找到了最佳的比率组合。<strong>编码器用于学习可泛化的特征表示</strong>，而<strong>解码器则用于重建丢失的骨架</strong>。由于处理的是骨架序列，文章<strong>使用了专为处理骨架序列而开发的&quot;空间-时间元组变换器（STTFormer）</strong>&quot;作为网络骨干，而不是ViT。在微调阶段，文章仅使用编码器和简单的输出层来预测动作。动作识别结果表明，文章的方法在没有额外数据的情况下胜过了最先进的自监督学习方法。</p><p><img src="../assets/image-20231210211243715.png" alt="image-20231210211243715"></p><p>图1SkeletonMAE的框架图</p><p><strong>总结：</strong> 本段介绍了一种“SkeletonMAE&quot;的新型自监督学习方法，专门用于骨骼动作识别。该方法解决了现有方法的局限性，同时在标准数据集上取得了自监督设置下的最先进性能。</p><h2 id="三、方法详解">三、方法详解</h2><p>这里，首先在第1节<strong>中介绍了骨架mae的初步内容</strong>。然后，在第2节中**，**<strong>设计了一种骨架数据的时空掩蔽策略</strong>。接下来，在第3节中<strong>分析文章的骨骼mae用于动作识别</strong>。最后，在第4节中介绍了<strong>文章的微调过程。</strong></p><h3 id="1-参数">1.参数</h3><p>先介绍下传统的MAE（Masked Autoencoder）模型。MAE采用了一种不对称的编码器和解码器结构。需要注意的是，解码器的结构与编码器不同，<strong>这意味着可以根据需要自定义解码器以构建高效的预训练模型</strong>。具体来说，MAE中的编码器基于ViT（Vision Transformer），但仅处理未掩盖的图像：首先，图像块经线性投影编码，并加上位置嵌入以形成图像令牌，然后这些令牌经过多个Transformer块的处理。仅加载了一小部分未掩盖的令牌（75％的块被掩盖，其余的作为输入）。至于MAE解码器，它根据原始图像块上的位置信息来解码已掩盖的令牌以进行重构。然后，在像素空间中计算了已掩盖和重构令牌之间的均方误差（MSE）。在预训练后，应用具有简单分类头的预训练编码器进行图像分类任务。</p><p>与MAE不同，<strong>MAE在编码器中应用了ViT，而在解码器中应用了Transformer块进行图像重构</strong>，而文章<strong>选择使用STTFormer来构建编码器和解码器</strong>，因为它具有基于骨架的Transformer结构。<strong>与ViT基于没有时间信息的图像块不同，STTFormer是一种基于骨架数据的Transformer，对于处理时空数据具有很大的潜力。<strong>具体而言，STTFormer将骨架数据分成多个元组（不重叠的部分），并提供了名为Spatio-Temporal Tuples Attention（STTA）的自注意模块，用于提取相邻帧之间的多关节表示。然后，在STTA块之后提出了一种名为Inter-Frame Feature Aggregation（IFFA）的特征聚合模块，用于</strong>提高相似动作识别的学习能力</strong>。STTFormer的结构如图1所示。</p><p>**总结：****该部分介绍了STTFormer模型，该模型与MAE不同，采用了骨架数据驱动的Transformer结构，适用于处理骨架数据的时空信息。**STTFormer的结构包括Spatio-Temporal Tuples Attention（STTA）自注意模块和Inter-Frame Feature Aggregation（IFFA）特征聚合模块，有助于提取和集成骨架数据的多关节表示，从而提高相似动作的识别能力。这一部分强调了STTFormer相对于ViT的优势，特别适用于骨架数据的处理。</p><h3 id="2-时空遮蔽策略">2.时空遮蔽策略</h3><p>文章提出了一种时空遮蔽方法，<strong>用于对骨架序列输入的一部分进行遮蔽</strong>，文章的遮蔽策略流程如图2所示。</p><p><img src="../assets/image-20231210211313386.png" alt="image-20231210211313386"></p><p><strong>图2掩码遮蔽策略流程图</strong></p><p>图2显示了文章的帧级遮蔽方法。根据预设的帧遮蔽比率，<strong>随机去除一部分帧并存储它们的索引，然后剩下的帧将通过关节级别的空间遮蔽方法进行处理</strong>。</p><p>如图2所示，在所有输入帧上实施了时空遮蔽方法后，<strong>剩下的帧将通过空间遮蔽策略进行处理</strong>。根据预设的关节遮蔽比率，<strong>会随机地在每个未遮蔽的帧中遮蔽部分关节</strong>。值得注意的是，在这个随机的空间遮蔽方法中，<strong>被遮蔽的关节的索引不是固定的</strong>，这意味着<strong>不同帧中的相同关节可能被遮蔽或不被遮蔽</strong>。这种简单的方法如图3(b)所示。除了这种遮蔽方法，文章<strong>还引入了一个具有固定索引的关节遮蔽策略，如图3©所示</strong>。<strong>不同帧中具有相同索引的关节将根据关节遮蔽比率一起被遮蔽或不被遮蔽</strong>。</p><p>**总结：这一部分描述了作者提出的骨架序列的时空遮蔽方法。**这种方法分为时序遮蔽和空间遮蔽两个阶段。时序遮蔽是在帧级别进行的，随机去除一部分帧。然后，在剩余的帧中，进行空间遮蔽，其中部分关节也会被随机遮蔽。作者还介绍了两种空间遮蔽策略，一种是随机遮蔽关节的策略，另一种是基于固定索引的关节遮蔽策略。这两种策略在后续实验中进行了比较。</p><p><img src="../assets/image-20231210211424249.png" alt="image-20231210211424249"></p><p><strong>图3：两中屏蔽策略的说明。(a)先进行框架掩模，然后在空间层次上对节点进行随机掩模;(b)用随机的指数遮盖关节c*<em>)用固定指数遮盖关节*</em></strong></p><h3 id="3-SkeletonMAE框架结构">3.SkeletonMAE框架结构</h3><p>本节描述了SkeletonMAE的主要组件，例如编码器、解码器、重构序列、损失函数和骨架动作识别的微调流程。流程和SkeletonMAE的结构如图1所示。</p><p>SkeletonMAE编码器：文章的编码器基于STTFormer，<strong>只处理可见的骨架标记</strong>。给定一个骨架序列作为输入，文章分别应用了<strong>帧遮蔽和关节遮蔽方法</strong>。这些在空间和时间上未遮蔽的标记被馈送到<strong>SkeletonMAE编码器，将输入映射到时空嵌入特征</strong>。</p><p>SkeletonMAE解码器：文章的<strong>解码器也采用了STTFormer的结构</strong>。与MAE中的解码器相同，SkeletonMAE解码器中的<strong>时空嵌入特征被处理以重构原始序列</strong>。同时，<strong>为了保留用于重构的位置信息，还引入了位置嵌入。解码器的输出是重构序列，应与未遮蔽的原始序列相同。</strong></p><p>框架重构：文章使用均方误差（MSE）损失来衡量重构的结果。在这种情况下，文章<strong>计算原始骨架序列与重构序列之间的MSE损失</strong>，如下所示：</p><p><img src="../assets/image-20231210211450969.png" alt="image-20231210211450969"></p><p>其中i是帧的索引，N是样本数量，S是输入序列，S∗是重构序列。</p><p>**总结：这一部分描述了SkeletonMAE的架构，包括编码器、解码器和重构方法。**编码器和解码器都基于STTFormer结构，用于处理骨架序列的重构。重构的质量通过均方误差损失来衡量，以衡量原始骨架序列与重构序列之间的差异。</p><h3 id="4-动作识别微调">4.动作识别微调</h3><p>为了评估SkeletonMAE学习骨架表示的能力，文章<strong>加载了从预训练中获得的参数权重</strong>，**用所有的训练数据对模型进行微调，然后使用识别准确度来预测每个动作的标签。**微调的过程如图1(b)所示。与最新的基于对比的自监督骨架动作识别方法不同，<strong>这些方法通过线性评估协议验证模型，文章只关注骨架动作识别任务的端到端微调</strong>。</p><p>总结：这一部分描述了如何评估SkeletonMAE对骨架表示的学习能力。首先，他们使用从预训练中获得的参数权重对模型进行微调，然后使用准确度来评估每个动作的标签预测。与其他方法不同，他们侧重于端到端微调而不是线性评估协议。这是评估SkeletonMAE性能的关键步骤。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文理解1基于骨骼的动态密集图卷积网络人体运动预测</title>
      <link href="/project/posts/1c9fef06.html"/>
      <url>/project/posts/1c9fef06.html</url>
      
        <content type="html"><![CDATA[<h1>Dynamic Dense Graph Convolutional Network for Skeleton-Based Human Motion Prediction基于骨骼的动态密集图卷积网络人体运动预测</h1><h2 id="Abstract">Abstract</h2><p>摘要—图卷积网络（GCN）通常遵循神经消息传递框架，用于建模骨骼关节之间的依赖关系，在基于骨骼的人体动作预测任务中取得了较大成功。然而，如何从骨骼序列构建图以及如何在图上执行消息传递仍然是开放的问题，这严重影响了GCN的性能。为解决这两个问题，本文提出了一种动态稠密图卷积网络（DD-GCN），它构建了一个密集图并实现了集成的动态消息传递。具体而言，我们使用4D邻接建模构建了一个密集图，作为在不同抽象层次上对运动序列进行全面表示。基于密集图，我们提出了一个动态消息传递框架，从数据中动态学习，生成反映图中节点之间样本特定相关性的独特消息。在基准的Human 3.6M和CMU Mocap数据集上进行的广泛实验证实了我们的DD-GCN的有效性，尤其在使用长期和我们提出的极长期协议时，明显优于最先进的基于GCN的方法。</p><ul><li>本文介绍了一种称为Dynamic Dense Graph Convolutional Network（DD-GCN）的方法，用于骨骼基础的人体运动预测任务。</li><li>图卷积网络（GCN）通常用于建模骨骼关节之间的依赖关系，但是构建图和在图上执行消息传递仍然是开放问题，影响了GCN的性能。</li><li>DD-GCN解决了构建图和在图上执行消息传递的问题，它构建了一个密集图，并实现了集成的动态消息传递。</li><li>采用4D邻接建模，构建密集图，全面表示不同层次的运动序列。</li><li>提出了动态消息传递框架，该框架能够从数据中动态学习，生成反映图中节点之间样本特定相关性的独特消息。</li><li>在Human 3.6M和CMU Mocap数据集上的广泛实验证实了DD-GCN的有效性，特别是在采用长期和提出的极长期协议时，其性能明显优于最先进的基于GCN的方法。</li></ul><h2 id="Ⅰ-INTRODUCTION">Ⅰ INTRODUCTION</h2><ul><li><strong>引言：</strong> 人体运动预测引起了广泛关注，因为它与许多实际应用有关，如人机交互、自动驾驶，以及与人类动作识别等研究主题相关。</li><li><strong>骨架基础的人体运动预测：</strong> 骨架基础的人体运动预测专注于使用人体骨架数据进行预测。随着现成的深度传感器的普及，骨架数据变得容易收集，并且相当稳健，对场景照明等环境干扰具有很强的抵抗力。</li><li><strong>问题核心：</strong> 核心问题是更好地推断未来的人体骨架。早期的工作大多使用循环神经网络（RNN）来处理人体运动的时序性，但是它们的训练难度带来了挑战。</li><li><strong>深度神经网络：</strong> 研究人员还在卷积神经网络（CNN）中取得了早期成功，但是它们将骨架数据处理为具有类似网格结构的图像数据，忽视了人体的非欧几里得特性。</li><li><strong>MLP-Mixer和图卷积网络（GCN）：</strong> 有望的人体运动预测方法包括MLP-Mixer和图卷积网络（GCN）。MLP-Mixer最近被引入人体运动预测，取得了卓越的性能，同时非常高效。GCN更全面地将人体骨架视为图，具有建模关节点和边缘的关节骨结构的固有能力。</li><li><strong>现有方法问题：</strong> 目前的先进人体运动预测方法通常应用GCN从骨架关节中提取特征。无论具体实例如何，它们都遵循一般的神经消息传递框架，涉及图中消息的聚合和更新。这些基于GCN的方法的性能严重依赖于两个主要方面，即图的构建和消息传递。</li></ul><h3 id="A-Graph-Construction">A. Graph Construction</h3><ul><li><strong>图构建：</strong> 基于GCN的人体运动预测方法通常构建一个（多尺度）骨架图进行空间建模，或构建一个轨迹图进行时间建模。</li><li><strong>骨架图构建：</strong> 在Learning Trajectory Dependencies (LTD) [24]中，使用隐式骨架图（见图1(a)）捕获每个骨架内关节之间的空间依赖关系。在Multi-Scale Residual (MSR-GCN) [25]中，将骨架图扩展为多尺度版本（见图1(b)），以学习跨多个尺度的空间依赖关系。在Better Initial Guesses (PGBIG) [26]中，使用额外的轨迹图（见图1©）学习不同骨架之间的时间依赖关系。</li><li><strong>时空图的必要性：</strong> 为了编码骨架之间的时空依赖关系，首先出现一个问题：是否在使用空间图或时间图时，使用时空图有助于建模时空依赖关系？</li><li><strong>全局视角的答案：</strong> 我们通过提出图构建的全局视角来回答这个问题。具体地，我们构建了一个带有4D邻接建模的密集图（见图1(d)），其中涉及跨多尺度时空骨架的顶点和边。先前的（多尺度）骨架图或轨迹图可以被视为我们密集图的特例。因此，我们的密集图预计能更灵活地建模多尺度时空依赖关系。</li></ul><p><strong>总结：</strong> 该部分介绍了基于GCN的人体运动预测方法中的图构建方法，说明了先前的方法中使用的骨架图和轨迹图，并提出了一个全局视角的方法，构建了一个更加灵活的密集图，以更好地建模多尺度时空依赖关系。</p><h3 id="B-Message-Passing">B. Message Passing</h3><ul><li><strong>消息传递：</strong> 在图构建之后，基于GCN的人体运动预测方法使用空间消息传递或用于特征传播的分离消息传递。具体而言，LTD [24] 和 MSR-GCN [25] 在（多尺度）骨架图上使用空间消息传递。与此同时，PGBIG [26] 在骨架图和轨迹图上使用独立的消息传递。考虑到这些消息传递方法是为特定图设计的，第二个问题出现了：如何为我们新提出的密集图设计特定的消息传递方法？</li><li><strong>设计动态消息传递方法：</strong> 我们通过设计一种集成的动态消息传递方法来回答这个问题（见图1(d)），它在两个方面与先前的消息传递方法不同。首先，它使得能够直接在多尺度时空骨骼关节之间进行消息传递，而先前的方法忽略了空间和时间骨骼关节之间的消息传递。其次，它为每个骨架序列启用了数据相关的消息传递，从而进一步扩展了我们的方法在时空建模方面的灵活性。相比之下，先前的消息传递方法以静态方式工作，共享参数化的图在所有样本中。</li><li><strong>动态密集图卷积网络（DD-GCN）的开发：</strong> 考虑到图构建和消息传递两方面，我们为基于骨架的人体运动预测开发了一个Dynamic Dense Graph Convolutional Network (DD-GCN)。我们的DD-GCN由多个Dynamic Dense Graph Convolution (DD-GC)块组成，每个块都采用多通道设计，以在我们提出的带有4D邻接建模的密集图上执行Single-Level Message Passing (SLMP)和Cross-Level Message Passing (CLMP)。</li><li><strong>贡献：</strong> 我们的贡献有三个方面：<ol><li>为了促进基于骨架的人体运动预测，我们提出了一个密集图，其中包括4D邻接建模，以从全局角度表示不同抽象层次上的运动序列，从而使消息传递层能够有效捕捉长程时空依赖关系。</li><li>我们提出了一种新颖的动态消息传递框架，其中聚合器动态学习数据，并通过利用图中节点之间的样本特定关联性生成信息性的消息。这样的方式具有更高的可解释性和表示能力，而只需非常少的额外参数。整体的动态密集图卷积网络（DD-GCN）结合了这两个提案，实现了更有效和高效的特征学习。</li><li>在两个具有挑战性的基准数据集上进行的广泛实验，即H3.6M和CMU Mocap，一致显示我们提出的DD-GCN在传统的长期（1000毫秒）预测任务和我们提出的极长期（2000毫秒）预测任务中均取得了最先进的性能。</li></ol></li></ul><p>消息传递<br>在构建图形之后，基于GCN的人体动作预测方法使用空间消息传递或分离的消息传递来进行特征传播。具体而言，LTD [24] 和 MSR-GCN [25] 在（多尺度）骨骼图上使用空间消息传递。同时，PGBIG [26] 在骨骼图和轨迹图上使用分离的消息传递。考虑到这些消息传递方法是为特定图形设计的，第二个问题出现了：如何为我们新提出的密集图设计特定的消息传递方法？</p><p>我们通过设计一种集成的动态消息传递方法（如图1（d）所示）来回答这个问题，与之前的消息传递方法有两个不同之处。首先，它能够实现跨多尺度时空骨骼关节的直接消息传递，而之前的方法忽略了空间和时间骨骼关节之间的消息传递。其次，它能够为每个骨骼序列实现数据相关的消息传递，进一步扩展了我们的方法对空间-时间建模的灵活性。相比之下，之前的消息传递方法以静态方式工作，将参数化的图形共享给所有样本。综合考虑图形构建和消息传递，我们为基于骨骼的人体动作预测开发了一种动态密集图卷积网络（DD-GCN）。我们的DD-GCN由多个动态密集图卷积（DD-GC）块组成，每个块都采用多路径设计，在我们提出的具有4D邻接建模的密集图上执行单级消息传递（SLMP）和交叉级消息传递（CLMP）。</p><p>我们的贡献有三个方面：（1）为了促进基于骨骼的人体动作预测，我们提出了一个具有4D邻接建模的密集图，从全局的角度将运动序列以不同抽象级别作为整体表示，使得消息传递层能够有效捕捉长程时空依赖关系。（2）我们提出了一种新颖的动态消息传递框架，其中聚合器从数据中动态学习，并通过利用图中节点之间的样本特定关联性生成信息丰富的消息。它们以一种经济高效的方式实现，以牺牲极少的额外参数为代价，提供了更高的可解释性和表征能力。（3）在两个具有挑战性的基准数据集（H3.6M和CMU Mocap）上进行的大量实验证明，我们提出的DD-GCN在传统的长期（1000毫秒）预测任务和我们提出的极长期（2000毫秒）预测任务中都取得了最先进的性能。</p><p>要点总结：</p><ul><li>GCN-based方法在构建图形后使用空间消息传递或分离消息传递进行特征传播。</li><li>之前的消息传递方法忽略了空间和时间骨骼关节之间的消息传递。</li><li>为了解决这个问题，提出了一种集成的动态消息传递方法，可以进行跨多尺度时空骨骼关节的直接消息传递，并实现数据相关的消息传递。</li><li>开发了Dynamic Dense Graph Convolutional Network (DD-GCN)，其中包含多个Dynamic Dense Graph Convolution (DD-GC)块，用于在具有4D邻接建模的密集图上执行单级和交叉级消息传递。<br>根据您提供的段落，以下是&quot;Message Passing&quot;章节的要点总结：</li><li>GCN-based方法在构建图形后使用空间消息传递或分离消息传递进行特征传播。</li><li>LTD和MSR-GCN使用空间消息传递，PGBIG使用分离消息传递。</li><li>针对新提出的密集图，需要设计特定的消息传递方法。</li><li>提出了一种集成的动态消息传递方法，与之前的方法有两个不同之处。</li><li>新方法可以直接在多尺度时空骨骼关节之间进行消息传递，而之前的方法忽略了空间和时间关节之间的传递。</li><li>新方法可以为每个骨骼序列实现数据相关的消息传递，扩展了方法在空间-时间建模方面的灵活性。</li><li>开发了Dynamic Dense Graph Convolutional Network (DD-GCN)，包含多个Dynamic Dense Graph Convolution (DD-GC)块。</li><li>DD-GCN在具有4D邻接建模的密集图上执行单级和交叉级消息传递。</li><li>提出的方法有三个贡献：引入4D邻接建模的密集图，提出动态消息传递框架，实现更有效和高效的特征学习。</li><li>在H3.6M和CMU Mocap两个基准数据集上进行的实验证明，提出的DD-GCN在传统和极长期预测任务中取得了最先进的性能。</li></ul><p>这些要点涵盖了&quot;Message Passing&quot;章节的关键信息。</p><h2 id="ⅡRELATED-WORK">ⅡRELATED WORK</h2><h3 id="A-Neural-Message-Passing-on-Graphs">A. Neural Message Passing on Graphs</h3><p>神经图消息传递 消息传递神经网络（MPNN）是图深度学习的通用框架[28]。GCNs [29]和许多其他图网络，如图注意力网络（GAT）[30]，都是它的特例。该框架涉及节点特征的聚合和更新。自问世以来，已经提出了不同类型的聚合器[31]。均值（求和）聚合器简单地取邻域内特征向量的逐元素加权平均值（求和）。平均或求和操作可以替换为另一种缩减函数，例如逐元素最大值或最小值[32]。在另一项研究中[33]，将求和聚合器与MLP结合以增加图网络的理论容量。这些简单的排列不变操作可以参数化。例如，均值聚合器中的权重可以设置为可训练参数，这在GCNs [29]，[34]，[35]，[36]，[37]，[38]中很常见。上述讨论的聚合器以静态方式工作，共享所有输入样本的参数，并不考虑样本特定关系。关于动态聚合，一种常见方法是使用注意力实现聚合[39]，它为每个邻居分配一个重要性得分以衡量其影响。在GAT中，聚合器应用重要性得分作为权重来定义邻居的加权和。在另一种最近的方法中[40]，最初是为了解决人类动作识别问题，聚合以一种精炼的方式进行，其中包括应用于所有通道的共享聚合和对应于不同通道的多个通道聚合。 图结构化时间序列表数据可以被视为在空间和时间域中定义的一般图形，通常被称为“时空图”[41]，[42]，[43]，[44]，[45]，[46]，[47]。为了充分利用它，一些最新方法依赖于完全卷积结构从时空图中提取时空特征[41]，[42]，[44]，[48]，[49]，[50]。这些结构的网络通常被称为“时空图卷积网络”。现有工作中时空图卷积的实现主要分为两种类型。第一种类型主要侧重于空间图卷积进行空间建模，并额外进行时间卷积进行时间建模[41]，[49]，[50]，[51]。第二种类型代表了一种统一的方法，直接使用图卷积捕获时空相关性[48]，[52]。</p><p>根据上述内容，Neural Message Passing on Graphs章节主要包括以下要点：</p><ol><li>消息传递神经网络（MPNN）作为图深度学习的通用框架，涉及节点特征的聚合和更新。</li><li>不同类型的聚合器，如均值聚合器和求和聚合器，以及动态聚合方法，比如使用注意力实现聚合，对图神经网络进行了探讨和比较。</li><li>对图结构化时间序列表数据的处理方法，包括将其视为“时空图”以及使用完全卷积结构从中提取时空特征的方法。</li></ol><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20231210215222315.png" alt="image-20231210215222315"></p><p>图上的神经消息传递</p><p>消息传递神经网络（Message Passing Neural Network，MPNN）是图上深度学习的通用框架[28]。GCN和其他图网络（如图注意力网络，Graph Attention Network，GAT）[30]都是它的特例。该框架涉及节点特征的聚合和更新。自提出以来，已经提出了不同类型的聚合器[31]。均值（求和）聚合器简单地对邻域中的特征向量进行逐元素加权平均（求和）。平均或求和操作可以用替代的规约函数替换，如逐元素最大或最小值[32]。在另一项工作中[33]，将求和聚合器与多层感知机（MLP）结合，以增加图网络的理论容量。这些简单的置换不变操作可以参数化。例如，均值聚合器中的权重可以设置为可训练参数，这在GCN中是常见的做法[29]，[34]，[35]，[36]，[37]，[38]。上面讨论的聚合器以静态方式工作，共享参数在所有输入样本中使用，并且不考虑样本特定的关系。关于动态聚合，一种常见的方法是使用注意力来实现聚合[39]，它为每个邻居分配一个重要性分数以加权其影响力。在GAT中，聚合器将重要性分数作为权重，定义邻居的加权和。在另一种最近的方法中[40]，最初提出用于处理人体动作识别，聚合以一种细化的方式实现，其中包括应用于所有通道的共享聚合和与不同通道对应的多个通道聚合。</p><p>图结构化的时间序列数据可以被视为在空间和时间域中定义的一般图，通常称为“时空图”[41]，[42]，[43]，[44]，[45]，[46]，[47]。为了充分利用它，一些最新的方法依赖于完全卷积结构，从时空图中提取时空特征[41]，[42]，[44]，[48]，[49]，[50]。这些结构的网络通常被称为“时空图卷积网络”。现有工作中的时空图卷积的实现主要分为两种类型。第一种类型主要关注空间图卷积进行空间建模，并额外进行时间卷积进行时间建模[41]，[49]，[50]，[51]。第二种类型采用统一的方法，直接使用图卷积来捕捉时空相关性[48]，[52]。</p><p>根据您提供的段落，以下是&quot;Neural Message Passing on Graphs&quot;章节的要点总结：</p><ul><li>消息传递神经网络（MPNN）是图上深度学习的通用框架。</li><li>GCN和其他图网络如GAT是MPNN的特例。</li><li>MPNN涉及节点特征的聚合和更新。</li><li>聚合器的不同类型和实现方式，如均值聚合器、求和聚合器、使用可训练参数的聚合器等。</li><li>动态聚合的常见方法是使用注意力机制，将重要性分数作为权重来定义邻居的加权和。</li><li>时空图可以被视为在空间和时间域中定义的一般图。</li><li>最新的方法使用完全卷积结构从时空图中提取时空特征。</li><li>时空图卷积网络主要关注空间建模和时间建模，可以分为两种类型的实现方法。</li><li>第一种类型主要侧重于空间图卷积和额外的时间卷积，用于空间和时间的建模。</li><li>第二种类型采用统一的方法，直接使用图卷积来捕捉时空相关性。</li></ul><p>总的来说，&quot;Neural Message Passing on Graphs&quot;章节介绍了消息传递神经网络（MPNN）的通用框架以及其在图上深度学习中的应用。它涵盖了不同类型的聚合器和动态聚合方法，并讨论了时空图在空间和时间建模中的应用。这个章节为理解和应用图上的神经消息传递提供了基础知识。</p><ul><li><strong>神经消息传递在图上：</strong> <strong>消息传递神经网络（MPNN）</strong> 是图深度学习的通用框架 [28]。其中 <strong>GCNs</strong> [29] 和许多其他图网络，如 <strong>Graph Attention Network (GAT)</strong> [30]，都是它的特例。该框架涉及节点特征的聚合和更新。自问世以来，不同风格的聚合器已被提出 [31]。均值（和）聚合器简单地取相邻特征向量的逐元素加权平均值（和）。平均或和操作可以替换为替代的减少函数，如逐元素的最大或最小值 [32]。在另一项工作中 [33]，和聚合器与MLP结合以增加图网络的理论容量。这些简单的排列不变操作可以被参数化。例如，均值聚合器中的权重可以被设置为可训练参数，这在GCNs [29]，[34]，[35]，[36]，[37]，[38]中是典型的。上面讨论的聚合器以静态方式工作，共享参数在所有输入样本中，并且不考虑样本特定的关系。关于动态聚合，一种流行的方法是使用注意力 [39] 来实现聚合，它为每个邻居分配一个重要性分数以衡量其影响。在 <strong>GAT</strong> 中，聚合器将重要性分数作为权重，定义邻居的加权和。在另一种最近的方法中 [40]，最初是为了解决人体动作识别问题提出的，聚合是以一种精炼的方式实现的，其中包括应用于所有通道的共享聚合和与不同通道相对应的多通道聚合。</li><li><strong>总结：</strong><ol><li><strong>MPNN</strong> 是图深度学习的通用框架，包括 <strong>GCNs</strong> 和其他图网络，如 <strong>GAT</strong>。</li><li>不同类型的聚合器，如均值聚合器、和聚合器，以及通过 <strong>MLPs</strong> 结合的和聚合器等，被广泛用于静态方式的消息传递。</li><li>针对动态聚合，使用 <strong>注意力</strong> 分配重要性分数，例如 <strong>GAT</strong> 的做法。</li><li>空时图结构化的时间序列数据可以被视为在空间和时间域中定义的一般图，通常称为“时空图”。</li><li>最近的方法倾向于使用完全卷积结构从时空图中提取特征。</li><li>存在两种主要类型的时空图卷积的实现方式：一种主要关注空间建模，另一种通过图卷积直接捕捉时空相关性。</li></ol></li></ul><h3 id="B-Skeleton-Based-Human-Motion-Prediction">B. Skeleton-Based Human Motion Prediction</h3><p>骨架基础的人体动作预测</p><p>处理基于骨架的人体动作预测任务的方法需要建模过去、未来事件和不完整观测之间的关系。较早的方法是基于表示学习的，受到骨架动作分析[53]，[54]，[55]，[56]，[57]，[58]等一系列工作的启发。图卷积网络（GCN）作为对非欧几里得数据推广卷积的一般化方法[59]，已成为人体动作预测的首选骨干。在本小节中，我们从两个角度介绍了基于GCN的人体动作预测方法，并提供了相关的研究论文作为示例。</p><p>1）图构建：LTD [24]将运动序列表示为轨迹空间中的隐式全连接图，其中每个坐标轨迹对应一个节点，并将编码的时间信息作为节点特征。虽然他们在该领域中首次使用GCN，但他们并未利用人体的运动学属性，如关节连接和身体部分的动力链。LDR [60]引入了一个基于自然邻近性的预定义拓扑结构的姿势图，视作对隐式图的补充。研究人员还寻求利用潜在的解剖关系来支持人体运动。因此，逐渐倾向于使用多尺度图[25]，[61]，[62]。DMGNN [61] 使用动态多尺度图来表示不同尺度的身体部分。分割是通过对预定义的关节组进行平均来手动完成的。MSR-GCN [25]使用多层感知器来减少或增加关节数量以构建多尺度图。前者缺乏灵活性，后者在图的可解释性方面缺乏清晰度。</p><p>2）图卷积：在早期的方法中，根据空间邻近性实现了简单的消息传递，以学习仅依赖空间的依赖关系[63]，并采用额外的时间CNN（例如TCNs）[64]来学习仅依赖时间的依赖关系[60]，[61]，这不能有效地建模时空关系。一些最近的工作[26]，[65]开始同时对空间和时间依赖关系应用图卷积，另外在关节轨迹上构建时间图，并分别在空间和时间图上实现图卷积。这些方法通常以静态方式工作，共享参数化图，不能处理多样化的数据相关关系。不同个体之间存在高度复杂的不同运动模式，突显了动态方法的必要性[40]。与我们相关的另一项工作是动态路由[66]，它根据协议进行动态聚合。</p><p>我们的工作与现有方法的区别在于，我们建立了一个密集图，从整个动作序列的全局角度进行4D邻接建模，而不是基于轨迹或姿势。另一个关键区别是，所提出的图卷积是根据图中节点之间的数据相关关系动态实施的，而现有的方法是根据所有样本共享参数化图静态实施图卷积。</p><p>总结：Skeleton-Based Human Motion Prediction章节主要介绍了基于骨架的人体动作预测方法。针对人体动作预测任务，该章节讨论了建模过去、未来事件和不完整观测之间关系的需求，并详细介绍了基于GCN的方法。该章节重点讨论了图构建和图卷积两个方面的方法和技术，强调了动态方法的必要性以应对不同个体之间高度复杂的不同运动模式，从而为人体动作预测的研究和实践提供了有益的指导和启示。</p><p><strong>骨架驱动的人体运动预测：</strong> 针对基于骨架的人体运动预测任务的方法需要对过去、未来事件和不完整观测之间的关系进行建模。早期的方法基于表示学习，受到骨架动作分析的启发 [53]，[54]，[55]，[56]，[57]，[58]，形成了一系列的研究。作为对非欧几里得数据的卷积的一般化 [59]，图卷积网络已经成为人体运动预测的首选骨干选择。在这一小节中，我们从两个角度介绍基于 <strong>GCN</strong> 的人体运动预测方法，并附上相关的研究论文。</p><ol><li>图构建：<ul><li><strong>LTD</strong> [24] 将运动序列表示为轨迹空间中的隐式全连接图，其中每个坐标的轨迹对应一个节点，具有编码的时间信息作为节点特征。尽管它为在该领域使用 <strong>GCNs</strong> 打开了先河，但它并没有利用人体的运动学属性，如关节的连接性和身体部分的动力学链。<strong>LDR</strong> [60] 引入了一个具有预定义拓扑的姿势图，基于自然邻接性，被视为对隐式图的补充。研究人员还试图利用潜在的解剖关系来构建多层次的图 [25]，[61]，[62]。<strong>DMGNN</strong> [61] 使用动态多尺度图来表示不同尺度下的身体部分。分割是通过对预定义的关节组进行平均而手动执行的。<strong>MSR-GCN</strong> [25] 使用多层感知器来减少或增加关节数量以构建多尺度图。前者缺乏灵活性，而后者在图的可解释性方面缺乏明晰性。</li></ul></li><li>图卷积：<ul><li>在早期的方法中，根据空间邻接性实施简单的消息传递来学习仅有空间依赖性 [63]，并使用附加的时间 <strong>CNNs</strong>（例如 <strong>TCNs</strong>） [64] 来学习仅有时间依赖性 [60]，[61]，这无法有效地建模时空关系。一些最近的工作 [26]，[65] 开始在关节轨迹上构建时间图，并在空间和时间图上分别实施图卷积，从而将图卷积应用于空间和时间依赖性。这些方法主要以静态方式工作，共享参数化图在所有样本中，并且不能处理多样的数据相关关系。不同个体底层的不同运动模式通常非常复杂，突显了对动态方法的需求 [40]。与我们的工作相关的另一个工作是动态路由 [66]，它基于协议路由定义动态聚合。</li></ul></li></ol><p><strong>总结：</strong></p><ol><li>方法需要对过去、未来事件和不完整观测之间的关系进行建模。</li><li>早期方法主要基于表示学习，受到骨架动作分析的启发。</li><li><strong>GCNs</strong> 是人体运动预测的首选骨干，但如何构建图和进行消息传递仍然是开放问题。</li><li>图构建方面的方法包括隐式全连接图、姿势图和多尺度图。</li><li>图卷积方面的方法包括对空间和时间依赖性的建模，但静态图卷积无法处理多样的数据相关关系。</li><li>与现有方法的不同之处在于我们从全局视角构建具有 4D 邻接建模的密集图，而且我们提出的图卷积根据图中节点之间的数据相关关系实现动态实现。</li></ol><h2 id="ⅢPROBLEM-FORMULATION-AND-PRELIMINARIES">ⅢPROBLEM FORMULATION AND PRELIMINARIES</h2><p>问题制定和初步准备</p><h3 id="A-Notations">A. Notations</h3><ul><li><p><strong>符号表示：</strong> 假设一个运动序列包含 T 个连续的帧，在每个姿势中有 M 个关节。那么运动序列为 X1:T = [X1, X2, · · · , XT] ∈ RT×M×D，其中 Xt ∈ RM×D 表示时间 t 的姿势，每个关节有 D 个特征。给定观察到的运动序列 X1:Th（其中 Th &lt; T），人体运动预测的任务是准确生成未来的运动序列 XTh+1:T。</p><p><strong>形式上，图是一个三元组 G = (V, E, W)，其中 V = {1, 2, · · · , N} 是 N 个节点的集合，E ⊆ V×V 是一组边，定义为有序对 (n, m)，而 W: E → R 是从边集到标量 w 的映射，其中 wnm 表示 (n, m) 边的权重。节点 m 的邻域 NG(m) 是由 m 的相邻节点组成的 V 的子集。图可以由其邻接矩阵 A ∈ RN×N 指定，其中 Amn 指定从节点 n 到节点 m 的边。</strong></p></li></ul><p><strong>总结：</strong></p><ol><li>运动序列 X1:T 包含 T 个连续帧，每个帧有 M 个关节。</li><li>任务是基于观察到的运动序列 X1:Th 预测未来的运动序列 XTh+1:T。</li><li>图 G 由节点集 V、边集 E 和权重映射 W 组成。</li><li>邻接矩阵 A 描述了图的连接关系。</li></ol><h3 id="B-Neural-Message-Passing">B. Neural Message Passing</h3><p><strong>神经消息传递：</strong> 在每个消息传递层中，每个节点的隐藏表示根据从其邻域聚合的信息进行更新。设 F ∈ RN×D 为节点特征矩阵，其中 fi ∈ RD 是节点 i 的 D 维特征向量。这个消息传递的更新操作可以表示为以下两个步骤：</p><p><img src="../assets/image-20231208205219996.png" alt="image-20231208205219996"></p><p>其中 h(k)i 是第 k 层时节点 i 的表示，h(0)i = fi。AGGREGATE 是一个可微分函数，将一组向量映射到单个向量，而更新函数将从节点 i 的邻域聚合的消息 m(k)i 与 h(k−1)i 结合以生成更新后的表示。许多消息传递框架依赖于线性操作，然后是逐元素的非线性操作 [67]，可以通过以下矩阵操作有效执行：</p><p><em>H</em>(<em>k</em>)=<em>PROP<strong>A</strong>G<strong>A</strong>TE</em>(<em>H</em>(<em>k</em>−1),<em>A</em>(<em>k</em>);<em>θ</em>(<em>k</em>)),(2)</p><p>其中 H(0) = F，PROPAGATE 是一个依赖于邻接矩阵 A 和参数 θ(k) 的传播函数。</p><h2 id="Ⅳ-DENSE-GRAPH-CONSTRUCTION">Ⅳ. DENSE GRAPH CONSTRUCTION</h2><p>现有的许多方法都采用了一种直观的方法，即根据位姿的空间邻接来构建图，从而使运动序列以一系列沿时间轴不相交的位姿图为特征。通过这种方式，使用GCNs聚合特征，然后使用时态cnn(例如，tcn)学习特征。我们认为，这种独立的特征学习并不足以捕捉复杂的时空动态模式下的运动序列。</p><h3 id="A-Single-Level-Dense-Graph-With-4D-Adjacency">A. Single-Level Dense Graph With 4D Adjacency</h3><p>A. 具有4D邻接的单层密集图： 考虑运动序列由单个密集图 Gst = (Vst, Est, Wst) 建模的一般情况。节点集 Vst 由 T·M 个节点组成，代表运动序列中的所有关节，其中节点 vi j 代表时间 i 的第 j 个关节。如果它们由表征运动的时空关系相关联，边 (vmn, vi j) 将连接 vmn 和 vi j。Wst：Est → R 是从边集到权重值 w 的映射，其中 wmni j 是 (vmn, vi j) 的权重。节点 vi j 的时空邻域定义为 Nst(vi j) = {vmn | (vmn, vi j) ∈ Est}。该图可以由一个四维张量 A ∈ RT×M×T×M 完全指定，其逐元素定义如下：</p><p>A 可以被重新整形为一个方阵 A ∈ RTM×TM，可解释为一个块矩阵：</p><p>其中 Aim = Ai,:,m,: ∈ RM×M 是一个方块，其元素为 (Aim) jn = Ai, j,m,n。主对角块指定不同姿势下的空间邻接，所有块的主对角线指定不同轨迹下的时间邻接。节点特征由三维张量 F ∈ RT×M×D 表示，其中 Fi = Fi,:,: ∈ RM×D 表示时间 i 的姿势特征矩阵，每个节点对应于矩阵中的一行向量，使得 fi j = Fi, j,: ∈ RD 是节点 vi j 的特征向量。</p><p>Single-Level Dense Graph With 4D Adjacency章节总结：</p><ul><li>考虑运动序列的一般情况，通过单个密集图来建模，图中的节点表示所有关节。</li><li>图的结构由四维张量 A 完全指定，其中元素定义了节点之间的空间和时间关系。</li><li>A 被重新整形为方阵，具有块矩阵的结构，主对角块指定了不同姿势下的空间邻接，而所有块的主对角线指定了不同轨迹下的时间邻接。</li><li>节点特征由三维张量 F 表示，其中每个节点的特征向量与姿势特征矩阵相关联。</li></ul><h3 id="B-Adaptive-Dense-Graph-Construction">B. Adaptive Dense Graph Construction</h3><p>自适应密集图构建：</p><p>为了利用人体骨骼的先验知识，我们可以根据自然连接性来指定密集图。也就是说，如果骨架中的相应关节由一根骨骼连接，那么边将连接同一帧上的两个节点或者跨越两个不同帧。然而，这样的图在人体运动方面是低级表示，因为它涉及将人体低级分割为由关节连接的骨骼。然而，人体展示出各种各样的运动类型，以至于仅仅使用密集图是不够的。为了解决这一挑战，我们提出根据人体解剖结构构建密集图，以在高层和低层都对人体运动建模。</p><p>给定描述最精细尺度上的运动模式的密集图 G(0) = (V(0), E(0), W(0)，我们从中派生出一系列 S 个密集图，这些图在不同较粗的尺度上，表示为 G(s) = (V(s), E(s), W(s))，其中 s = 1, 2, · · · , S。请注意，为了简化符号，我们省略了下标“st”。密集图 G(s) 是通过学习骨架变换矩阵 Z(s) ∈ RM×Ms 构建的，该矩阵将 G(0) 中的节点分组为 Ms 个部分（图 3）。</p><p>形式上，设 F(0) i ∈ RM×D 表示在任何给定时间 i 的 G(0) 中的姿势特征矩阵。然后我们按如下方式计算较粗图 G(s) 中的姿势特征：</p><p>我们将 Z(s) 设置为可训练的参数，并基于解剖结构的预定义初始值。为了进行跨层学习，我们构建了一系列双分图的跨层图。整体密集图由所有单层密集图和跨层图组成，描述了不同抽象层次上的多样且复杂的时空运动模式。其独特的表示能力使得特征学习更加精确和有效。</p><p>Adaptive Dense Graph Construction章节总结：</p><ul><li>通过自然连接性，提出了根据人体解剖结构构建密集图的方法。</li><li>在最精细尺度上描述运动模式的密集图 G(0) 被用来派生一系列在不同较粗尺度上的密集图 G(s)。</li><li>密集图 G(s) 是通过学习骨架变换矩阵 Z(s) 构建的，该矩阵将节点分组为不同部分。</li><li>为了进行跨层学习，构建了一系列跨层图，整体密集图由单层密集图和跨层图组成，描述了不同抽象层次上的多样和复杂的时空运动模式。</li></ul><h3 id="C-Induced-Local-Subgraph">C. Induced Local Subgraph</h3><p>诱导局部子图：</p><p>给定一个密集图 Gst = (Vst, Est, Wst) 及其对应的 A，我们将与每个姿势相对应的姿势子图定义为 Gst 的诱导子图，由表示姿势的所有节点和连接它们的所有边组成。姿势子图在空间领域中建模人体骨架的姿势。形式上，时间 i 的姿势子图用 Gˆi = ( ˆVi, Eˆi, Wˆi ) 表示，其中 Vˆi = {vi j | j = 1, 2, · · · , M}。然后密集图 Gst 部分地由一系列 T 个不相交的姿势子图 Gˆ1，Gˆ2，· · ·，GˆT 来表征，它们的节点集和边集满足：Vst = ˆV1 ∪ ˆV2 ∪ · · · ∪ ˆVT; Est ⫌ Eˆ1 ∪ ˆE2 ∪ · · · ∪ ˆET。请注意，我们使用与 Eq. (4) 中对应 Gst 的块矩阵 A。然后，每个姿势子图 Gˆi 都由主对角块 Ai i = Ai,:,i,: 完全指定，其中 i = 1, 2, · · · , T。A 的非对角块指定了两个不同姿势之间的交叉连接。由空间领域中的姿势子图推导而来，轨迹子图是 Gst 的诱导子图，定义为与第 j 个关节对应的 G˜j = ( ˜Vj, E˜j, W˜j )，其中 V˜j = {vi j |i = 1, 2, · · · , T}。总共有 M 个不相交的轨迹子图，用于在时间领域中建模运动序列。类似于姿势子图，轨迹子图由 A 的所有块的主对角线指定，即对于所有 j = 1, 2, · · · , M。这一部分在本节中呈现的初步概念规范旨在促进实施图卷积的下一步。</p><p>Induced Local Subgraph章节总结：</p><ul><li>对于给定的密集图 Gst，定义了与每个姿势对应的姿势子图，该子图由表示姿势的节点和它们之间的边组成。</li><li>通过主对角块和非对角块的组合，密集图 Gst 被分解为一系列姿势子图，这些子图在空间领域中建模人体骨架的姿势。</li><li>类似于姿势子图，轨迹子图是与关节对应的诱导子图，在时间领域中用于建模运动序列。</li><li>这些概念规范旨在为实施图卷积的后续步骤提供便利。</li></ul><h2 id="Ⅴ-DYNAMIC-DENSE-GRAPH-CONVOLUTIONAL-NETWORK">Ⅴ. DYNAMIC DENSE GRAPH CONVOLUTIONAL NETWORK</h2><p>在上面的小节中，我们以一种相对抽象的方式讨论了通用的神经消息传递框架，其中涉及到AGGREGATE和UPDATE函数。在本小节中，我们给出了这些函数的具体实例，以便实现提议的DD-GCN。请注意，为了简化符号，必要时我们将省略下标“st”和上标表示该层。在本工作中，我们提出了一个动态消息传递框架，在该框架中，我们利用聚合器动态学习输入数据，并根据节点间的特定样本相关性生成消息，以反映底层的独特关系。我们对节点级更新应用了联合特定的投影。与此相反，以往的方法只是简单地对相邻向量进行加权平均，将权值预先定义或参数化为所有样本共享的边缘属性，并使用所有节点之间的共享投影矩阵实现更新。</p><h3 id="A-Dynamic-Message-Passing">A. Dynamic Message Passing</h3><p>动态消息传递：</p><p>聚合步骤旨在从邻域生成信息丰富的消息，以便促进后续的更新。聚合本质上是一个集合函数，将邻域中的一组表示向量映射到一个向量。形式上，AGGREGATE 函数定义如下：</p><p>其中 dout 是该层最终输出的维度，αi jmn 是所有样本共享的权重，φ(hmn, hi j ) 是在前向传递中推断的依赖于数据的权重。在第 0 层的初始节点表示是特征向量，即</p><p>h(0)i j = fi j = Fi, j,:。共享项 αi jmn 被设置为可训练参数，以反映通用的共同模式：</p><p>其中 | · | 表示集合的基数。依赖于数据的项 φ(hmn, hi j ) 的设计旨在为每个样本建模节点之间的特征关系，其中 φ : Rdin × Rdin → R 是从两个向量到标量的映射。具体而言，我们设计了两个非线性权重函数 φ1 和 φ2，如下所示：</p><p>我们对依赖于数据的权重进行缩放操作，即将其除以 √ dout。通过这样做，如果其增长过大，它将被缩小到接近 αi jmn 的值，以便共享权重不会在生成消息时无效。减小权重可以缓解在后续更新步骤中梯度消失问题，使其远离可能导致梯度消失的区域。</p><p>简而言之，在聚合步骤中，我们改进了现有方法中通常采用的均值聚合器，使其采用依赖于数据的加权平均来生成用于更新的独特消息。DD-GCN 中的 UPDATE 函数接收来自邻域的消息和节点先前的表示作为输入，并生成新的更新表示，定义如下：</p><p>其中 σ 表示逐元素的非线性（例如，tanh），b ∈ Rdout 是偏置项，而 2j ∈ Rdout×din 是与第 j 个关节对应的可训练的联合特定投影矩阵。</p><p>Dynamic Message Passing章节总结：</p><ul><li>在动态消息传递中，聚合步骤旨在生成信息丰富的消息，以促进后续的更新。</li><li>聚合操作使用一个集合函数，将邻域中的表示向量映射到一个向量。</li><li>聚合中使用了共享的可训练参数和依赖于数据的权重，以建模节点之间的特征关系。</li><li>通过缩放操作，依赖于数据的权重被调整，以避免在后续更新步骤中梯度消失问题。</li><li>更新步骤采用了依赖于数据的加权平均，生成用于节点更新的新表示。</li></ul><h3 id="B-Adaptive-Multi-Level-Message-Passing">B. Adaptive Multi-Level Message Passing</h3><p>自适应多层消息传递：</p><p>在前一段介绍的基本消息传递框架的基础上，我们设计了一个多路径框架，其中包含轻量级的横向连接，以捕捉和融合不同层次上的信息特征。给定 G(0) 和相应的节点 vi j 的特征向量 f(0)i j，我们可以使用骨架变换 Z(s)（如公式（5））在任何 s = 1, 2, · · · , S 中获得 G(s) 中的节点特征 f(s)i j。在每个路径中，我们首先将输入节点特征 f(s)i j 编码成隐藏表示 h(s)i j，然后使用前面讨论的 DD-GC 层进行学习。此外，在每个阶段之后，我们在尺度 0 处交错插入多个轻量级的跨层消息传递模块，以从其他层次获取信息，作为对原始层次上特征学习的指导。给定任意时间 i 的姿势表示 H(s)i ∈ RMs×ds，跨层消息传递模块将其作为输入，并生成跨层消息 H(s,0)i ∈ RM×d，通过依赖于跨层图的图卷积：</p><p>H(s,0)i = ¯Z(s)A(s)H(s)i 2(s)，(10) 其中 2(s) ∈ Rds×d 是可训练的投影矩阵，A(s) ∈ RMs×Ms 是可训练的邻接矩阵，初始化为 A(s) = I，而 Z¯(s) ∈ RM×Ms 是逆骨架变换矩阵，定义如下： Z¯(s) = 1(Z(s) ≠ 0)。(11) 投影矩阵 2(s) 仅在两个层次的特征维度不匹配时使用。跨层消息传递模块通过在不同姿势之间共享投影和邻接矩阵来轻量化。然后，跨层表示与上一阶段的原始层次上的表示融合： h(0)i j = UPDATEcross-level(h(0)i j, Σs=1H(s,0)i j) = h(0)i j + Σs=1H(s,0)i j。(12) 为了简化符号，我们用 h(0,0)i j 表示 h(0)i j，并将上述方程重写为： h(0)i j = Σs=0H(s,0)i j。(13)</p><h3 id="C-Dynamic-Dense-Graph-Convolution">C. Dynamic Dense Graph Convolution</h3><p>动态密集图卷积：</p><p>在前面的段落中，我们深入介绍了不同层次上的节点级别的消息传递框架的定义。我们将它们统一起来，用张量符号表示总体动态密集图卷积（DD-GC）。为了符号清晰，我们将使用上标表示层次，并省略层次指标。提出的 DD-GC 定义如下：</p><p>其中 A¯ 和 8¯ 分别是归一化后的 A 和缩放的 8，其中 A¯ i, j,m,n = 1 |N(vi j )|Ai, j,m,n； 8¯ i, j,m,n = √1 dout φ(H(s) m,n,:, H(s) i, j,:)。在 s = 0 的情况下，我们将 Z¯ (s) 和 A(s) 设为单位矩阵。第 0 层的初始表示是输入节点特征，即 H(0) = F。</p><p>Dynamic Dense Graph Convolution章节总结：</p><ul><li>在不同层次上深入介绍了节点级别的消息传递框架的定义。</li><li>统一了这些定义，提出了整体的动态密集图卷积（DD-GC）。</li><li>DD-GC 使用了归一化的邻接矩阵 A¯ 和缩放的权重矩阵 8¯，并通过张量表示进行定义。</li><li>在 s = 0 的情况下，将 Z¯(s) 和 A(s) 设为单位矩阵。</li><li>初始表示是输入节点特征，即 H(0) = F。</li></ul><h2 id="Ⅵ-NETWORK-ARCHITECTURE">Ⅵ. NETWORK ARCHITECTURE</h2><p>所提出的DD-GCN的总体架构如图2所示。该网络首先以原始骨架序列作为输入，构造稠密图，并在不同的层次上提取相应的特征。然后使用L动态稠密图卷积(DD-GC)块的堆栈进行图卷积。在每个块中，首先使用单层消息传递(SLMP)块聚合和更新特性。然后，跨级别消息传递(CLMP)块在不同的路径上传播特性。每一层的特征都是动态学习的，它依赖于图中节点之间的共享邻接关系以及样本特定的相关性。来自不同层次的特征在具有跨层次邻接的二部图上传播。在实现中，我们总共使用了3个(L = 3) DD-GC块和3个(S = 2)级别的图表示。这两个超参数是模型性能/复杂性权衡的具体情况。通过骨架变换(图3)，将三个层次的关节或身体部位的个数分别设置为22、11、2，这是在网络最开始的图构造步骤中通过学习相应的骨架变换得到的。此外，我们采用了一个全局剩余连接，正如在以前的工作中通常采用的。并且我们在最高级别的路径中应用了一个额外的跳过连接，它试图使用一个SLMP块直接保存来自前几轮消息传递的信息。</p><h3 id="A-Single-Level-Message-Passing">A. Single-Level Message Passing</h3><p>单层消息传递：</p><p>在每个单层消息传递（SLMP）块中，特征以一种动态的方式进行学习，这取决于图中节点之间的共享邻接性以及样本特定的相关性。一个 SLMP 块包括图卷积、批量归一化、随机失活和激活。表 I 展示了 DD-GCN 中前三个 SLMP 块的结构。</p><p>Single-Level Message Passing章节总结：</p><ul><li>单层消息传递块（SLMP）通过共享邻接性和节点之间的样本特定相关性以动态方式学习特征。</li><li>SLMP 块包括图卷积、批量归一化、随机失活和激活操作。</li><li>表 I 展示了 DD-GCN 中前三个 SLMP 块的结构。</li></ul><h3 id="B-Cross-Level-Message-Passing">B. Cross-Level Message Passing</h3><p>跨层消息传递：</p><p>在每个跨层消息传递（CLMP）块中，不同层次的特征通过具有跨层邻接性的二分图进行传播。我们在表 II 中展示了从级别 1 到级别 0 的 CLMP 的示例情况。</p><p>Cross-Level Message Passing章节总结：</p><ul><li>跨层消息传递块（CLMP）用于在具有跨层邻接性的二分图上传播不同层次的特征。</li><li>在表 II 中提供了从级别 1 到级别 0 的 CLMP 的示例情况。</li></ul><h3 id="C-An-Alternative-for-Message-Passing">C. An Alternative for Message Passing</h3><p>消息传递的另一种方式：</p><p>我们对动态消息传递框架的构想是通用的，可以使用不同的实现规范来实例化。考虑到准确性和计算成本，我们提供了单层消息传递块的另一种设计，其中共享的 4D 邻接性被替换为两个 3D 邻接张量，分别包含姿势子图和轨迹子图中对应边的 4D 邻接张量元素的一部分。这可以看作是 DD-GCN 中原始 SLMP 块的一种特殊情况，其中我们屏蔽了 4D 邻接张量中的某些元素。表 III 显示了这种 SLMP 块的结构。</p><p>An Alternative for Message Passing章节总结：</p><ul><li>动态消息传递框架的构想是通用的，可以通过不同的实现规范进行实例化。</li><li>为了平衡准确性和计算成本，提供了单层消息传递块的另一种设计，其中共享的 4D 邻接性被替换为两个 3D 邻接张量。</li><li>这种设计可视为 DD-GCN 中原始 SLMP 块的特殊情况，通过屏蔽 4D 邻接张量中的某些元素。</li></ul><h3 id="D-Training">D. Training</h3><p>训练：</p><p>为了训练我们的模型，我们利用关节位置均方误差（MPJPE）[70]。在给定历史动作序列的情况下，模型应该生成尽可能接近相应真实值的未来动作序列。设 Xn 是模型生成的第 n 个输出样本，Xˆn 是相应的真实值。然后，对于总共 N 个训练样本，损失函数定义如下：</p><p><img src="../assets/image-20231210213937540.png" alt="image-20231210213937540"></p><h2 id="Ⅶ-ANALYSIS-OF-GRAPH-CONVOLUTION">Ⅶ. ANALYSIS OF GRAPH CONVOLUTION</h2><p>自从LTD[24]的开创性工作以来，人们在人体运动预测中提出了各种各样的图形卷积。我们通过将它们重新表述成一个统一的形式来分析它们，并在特征聚合和更新方面与我们的进行比较。我们进一步证明了所提出的DD-GC是它们的一般化。</p><h3 id="A-Graph-Convolution-on-Subgraph">A. Graph Convolution on Subgraph</h3><p>回想一下，我们使用稠密图来表示整个骨架序列，4D张量a∈RT×M×T×M表示它的邻接性，4D张量可以被重塑为块矩阵a∈RTM×TM(式(4))。</p><h4 id="1-Spatial-Graph-Convolution-on-Pose-Subgraph">1.Spatial Graph Convolution on Pose Subgraph:</h4><p>姿势子图上的空间图卷积：</p><p>如第四节所讨论的，与第 i 帧对应的姿势子图 Gˆ i 完全由主对角块 Ai i（其中 i = 1, 2, · · · , T）指定。</p><p>在姿势子图上的空间图卷积被定义为节点表示在每个姿势内的传播，可以通过矩阵运算进行，如下所示：</p><p>其中 H(k) ∈ RTM×d(k) 是第 k 层的隐藏表示，2(k) ∈ Rd(k−1)×d(k) 是可训练的投影矩阵，Ms 是一个固定的二进制掩码， ⊙ 表示逐元素乘积。可以很容易地看出，如果我们从公式（6）中去除 φ(hmn, hi j ) 项并对 A 应用掩码，使得 A 中的所有非对角块都是零矩阵，那么这就是我们 DD-GC 的一个特殊情况。在这种情况下，DD-GC 变成了静态空间图卷积，而 4D 邻接变为了大小为 T × M × M 的 3D 邻接。在更特殊的情况下，当主对角块相等时，即 A11 = A22 = · · · = ATT，或者等价地，当所有姿势共享相同的空间邻接性时，3D 邻接缩减为大小为 M × M 的 2D 邻接，这在大多数先前的工作中被采用[24]，[25]，[60]，[61]。</p><p>Spatial Graph Convolution on Pose Subgraph章节总结：</p><ul><li>空间图卷积在姿势子图上进行，该子图由主对角块指定。</li><li>空间图卷积定义为在每个姿势内传播节点表示。</li><li>它可以通过矩阵运算实现，其中使用了隐藏表示、可训练的投影矩阵和固定二进制掩码。</li><li>这是动态空间图卷积（DD-GC）的一个特殊情况，当去除了 DD-GC 中的某些项并应用掩码时，可以得到空间图卷积。</li><li>在更特殊的情况下，当主对角块相等时，3D 邻接缩减为大小为 M × M 的 2D 邻接，这在先前的工作中很常见。</li></ul><h4 id="2-Temporal-Graph-Convolution-on-Trajectory-Subgraph">2.Temporal Graph Convolution on Trajectory Subgraph:</h4><p>轨迹子图上的时空图卷积：</p><p>类似于姿势子图，与第 j 个关节点对应的轨迹子图 G˜ j 被 A 的子矩阵指定，该子矩阵是通过从 A 中的所有块的每个主对角线中取第 j 个元素形成的，相当于取 A:, j,:, j 并将其重塑为矩阵。轨迹子图上的时空图卷积可以表述为：</p><p>这也是 DD-GC 的一个特殊情况，当我们从公式（6）中去除动态项并适当地对 4D 邻接应用掩码，使得 A 中的每个块都是一个对角矩阵 3im = diag(Ai,1,m,1, Ai,2,m,2, · · · , Ai,M,m,M)。在这种情况下，DD-GC 变成了时空图卷积，而 4D 邻接缩减为大小为 M× T × T 的 3D 邻接。在更特殊的情况下，当每个块中对角线的元素相等时，即 3im = aimI，其中 aim 是一个标量，或等价地，当所有轨迹共享相同的时空邻接性时，3D 邻接缩减为大小为 T × T 的 2D 邻接，其元素为 i, m = 1, 2, · · · , T 的 aim。 先前的工作通常使用时间卷积（时间CNN）进行时间建模[24]，[60]，[61]。在最近的工作中，它们被时空图卷积所取代，通常与空间图卷积一起使用，在这种情况下，图卷积分别在空间图和时间图上执行[26]，[65]。然而，这些图卷积都可以视为 DD-GC 的特殊情况。</p><p>Temporal Graph Convolution on Trajectory Subgraph章节总结：</p><ul><li>与姿势子图类似，轨迹子图由邻接矩阵 A 的子矩阵指定。</li><li>轨迹子图上的时空图卷积定义为在每个关节点的轨迹内传播节点表示。</li><li>这是 DD-GC 的一个特殊情况，当去除公式（6）中的动态项并适用适当的掩码时，可以得到轨迹子图上的时空图卷积。</li><li>在更特殊的情况下，当每个块中对角线的元素相等时，即 3im = aimI，或者当所有轨迹共享相同的时空邻接性时，3D 邻接缩减为大小为 T × T 的 2D 邻接。</li><li>先前的工作通常使用时间卷积（时间CNN）进行时间建模，而最近的工作中则采用了时空图卷积，通常与空间图卷积一起使用。</li></ul><h3 id="B-Generalized-Aggregation-and-Update">B. Generalized Aggregation and Update</h3><p>我们已经在式(1)中给出了消息传递的一般形式，其中包括一个聚合和一个更新步骤。</p><h4 id="1-Neighborhood-Aggregation">1.Neighborhood Aggregation:</h4><p>邻域聚合：</p><p>邻域聚合：在大多数基于GCN的人体动作预测方法中，聚合操作通常采用邻居表示的加权和或平均值生成消息，其中权重设置为可训练参数。其表述为： ,<em>m**j</em>=AGGREGATE({<em>h**n</em>∣<em>v**n</em>∈<em>N</em>(<em>v**j</em>)})=∑<em>v**n</em>∈<em>N</em>(<em>v**j</em>)<em>α<strong>jn</strong>h**n</em>, 其中 <em>α**jn</em> 是边 (n, j) 的可训练权重，a<em>j</em>=1,2,…,<em>M</em> 表示聚合是在当前姿势内的关节点之间进行的，而不是整个骨架序列，这形成了如前所述的空间图卷积。由于可训练权重被所有样本共享，这是一种静态聚合方法，无法学习生成表征每个样本的特定消息。公式（18）可以泛化为DD-GC中的公式（6），具有更强的表示能力。</p><p>Neighborhood Aggregation章节总结：</p><ul><li>大多数基于GCN的人体动作预测方法中采用邻域聚合操作。</li><li>邻域聚合通常使用加权和或平均值生成邻居节点的消息，权重为可训练参数。</li><li>公式（18）表示静态聚合方法，权重被所有样本共享，无法学习生成特定于样本的消息。</li><li>这种静态聚合方法可以通过DD-GC的公式（6）进行泛化，具有更强的表示能力。</li></ul><h4 id="2-Update-Methods">2.Update Methods:</h4><p>更新方法：</p><p>更新方法：在基于GCN的方法中，聚合步骤通常受到研究者最多的关注，涉及提出新颖的体系结构和变体[67]。接下来，我们将关注更新步骤。在大多数现有方法中，更新操作涉及使用投影矩阵将低维表示投影到潜在特征空间[71]，并且该投影矩阵由图中的所有节点共享。其表述为：<em>h**j</em>=UPDATE(<em>h**j</em>,<em>m**j</em>)=<em>σ</em>(2(<em>h**j</em>+<em>m**j</em>)+<em>b</em>), 其中 2∈R<em>d</em>in×<em>d</em>out 是由所有节点共享的可训练投影矩阵。公式（19）可以泛化为DD-GC中的公式（9），具有关节特定的投影。</p><p>Update Methods章节总结：</p><ul><li>基于GCN的方法中，聚合步骤通常受到研究者的关注，涉及提出新颖的体系结构和变体。</li><li>更新步骤在现有方法中涉及使用投影矩阵将低维表示投影到潜在特征空间，该投影矩阵由图中的所有节点共享。</li><li>公式（19）表示更新操作，其中  2∈R<em>d</em>in×<em>d</em>out 是由DD-GC中的公式（9），具有关节特定的投影。</li><li>公式（19）可以泛化为DD-GC中的公式（9），具有关节特定的投影。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>骨架-微信公众号1</title>
      <link href="/project/posts/a205de1f.html"/>
      <url>/project/posts/a205de1f.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-基于骨架数据的人体行为识别的简介">1.基于骨架数据的人体行为识别的简介</h2><h3 id="1-人体行为识别">1.人体行为识别****</h3><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_gif/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSic9OPuwwMwwIY7weX91zicJur78zhSMySuPzzxg2aiaorxgIgvlaNz9SpA/640?wx_fmt=gif&amp;from=appmsg" alt="img"></p><p>​<strong>动作识别样例</strong></p><p>人体行为识别是指通过特定的算法，从人体本身的特征和所完成动作的特征出发，识别出人物具体动作的分类任务。它在智能监控、视频检索等方面有着广泛的应用。准确地提取到每个行为的语义信息，来描述其空间及时间上的动态变化，是这个方向的主要挑战。</p><h3 id="2-基于骨架数据的人体行为识别">2.基于骨架数据的人体行为识别</h3><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSiciaXbEicKSibR7dm6pTbJIfSQfdKNGJ1aj78YrzCq0BtbF1CTIBYaAroKQ/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>骨架图示例（左）和骨架提取示例（右）</strong></p><p>​    <img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicUXnMaAPJuygx3pwylmKtyXGsicgnKtRqyqHM1kW8kSnTkVdciam55yYA/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>多人行为识别（握手示例）</strong></p><p>随着 Kinect 深度相机的出现和人体姿态估计算法的发展，研究人员可以轻松地获取人体的3D骨架关键点信息，骨架序列的采集变得更加便捷、成本更低，使骨架数据具有计算量小的优点。与传统的RGB视频数据相比，人体骨骼有着更完整的结构信息，可以避免背景、光照以及视角变化所产生的噪声影响，对复杂的场景有着更强的鲁棒性，可以大大地提高识别效率。凭借这些优势，基于骨架数据的人体行为识别具有深远的研究意义。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_gif/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicz9M0AAUVF4iczVmXCeoYepn1Mk9YBvxLxfUZ6yicuqRRnsDIkibRLyKJA/640?wx_fmt=gif&amp;from=appmsg" alt="img"></p><p>​<strong>视频图片处理实现3D姿势提取实现图像网格恢复实现准确分类</strong></p><p><strong>3.骨架动作行为识别发展历程</strong></p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicPqUbfCXkcgIMO3hmLqGLnbMJaVJo5OMtxZRBUXkDHE7gwSsApKK6iaQ/640?wx_fmt=png&amp;from=appmsg" alt="img"><strong>发展历程图</strong></p><p>基于传统的骨架行为识别方法利用手工提取特征方法，通过一系列三维操作（如关节旋转和平移）手动提取特征，使用人类工程特征检测器和描述符来构建特征向量。然后通过训练通用分类器来执行分类。</p><h4 id="3-0传统方法">3.0传统方法</h4><p>基于传统的骨架行为识别方法利用手工提取特征方法，通过一系列三维操作（如关节旋转和平移）手动提取特征，使用人类工程特征检测器和描述符来构建特征向量，然后训练通用分类器来执行分类过程。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicYOX6Thtux1ZsOclyqpGkwA8QpV0pHZy1oJJuE1ZkzicP3jmhF5OaqicA/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p><strong>基于深度学习方法主要分为基于卷积神经网络的行为识别方法、基于循环神经网络的行为识别方法和基于图卷积网络的行为识别方法；</strong></p><h4 id="3-1基于卷积神经网络（CNN）的行为识别方法">3.1基于卷积神经网络（CNN）的行为识别方法</h4><p>基于CNN的方法具备学习高级语义线索的能力，广泛应用于处理2D图像。基于卷积神经网络的行为识别方法一般将人体骨架序列从矢量序列转换为伪图像，伪图像的宽度、高度和通道分别代表骨架序列的关节、帧和坐标，形成伪图像，其中行代表不同的关节，列代表不同的帧，随后通过卷积神经网络进行动作识别。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSic3ic5DVib4wFB5HdghyxmeQRcRU0dnTzjpUAv7QqLGY91PFY05n2gsuXQ/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>卷积神经网络模型图</strong></p><h4 id="3-2基于循环神经网络（RNN）的行为识别方法">3.2基于循环神经网络（RNN）的行为识别方法</h4><p>循环神经网络在处理可变长度的序列数据方面具备更强大的能力，更适合模拟动作的运动和时间依赖性。基于循环神经网络RNNs的方法通常将关节数据表示为若干向量序列，其中每个向量序列包括了每帧中各关节的位置数据，随后这些数据被馈送到循环神经网络中以进行动作识别。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicormD7yKu8NoqRbkp9qIELr26tucqZX3GLKGndnicdhh8Y6Xt9iaxrG8A/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>循环神经网络模型</strong></p><blockquote><p><strong>总结:基于CNN和RNN方法都不能很好的处理非欧式空间中的图数据，因为将图结构数据转换为伪图像或坐标向量再进行卷积会损失原始的信息。</strong></p><p>CNN更加适合处理规则的网格数据，比如图像（由像素点构成），但是它很难对图结构数据进行有效的卷积。</p><p>RNN适合处理时序性的问题，但是很难感知骨架数据的空间构型，也很难有效的建模长时间序列上关节点之间的依赖关系。</p></blockquote><h4 id="3-3基于图卷积网络（GCN）的行为识别方法">3.3基于图卷积网络（GCN）的行为识别方法</h4><p>人体骨骼是非欧几里德的图数据，将关节根据骨骼联接构成图，每一帧的骨架是一个独立的图，作为空间信息。沿着时间维度连接骨架关节来扩展空间图，以构建时空图。经过多层卷积提取空间和时间特征，最后再分类。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicG2Rpv26xvhnYJX3rX6RbcqtBFW3iazOrPGxUU9GPwlRFjoDhCia7FPHw/640?wx_fmt=png&amp;from=appmsg" alt="img"><strong>图卷积网络模型</strong></p><h2 id="2-基于骨架数据的人体行为识别的数据集">2.基于骨架数据的人体行为识别的数据集</h2><h3 id="2-1数据集介绍">2.1数据集介绍</h3><p>本方向的常用数据集主要包括：NTU-RGB+D60，NTU-RGB+D120，Kinetics400，以及NW-UCLA等</p><p>以最为常见的NTU-RGB+D60 数据集为例：NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicw42PakbuRH7pMPIOicnfleH66It5qeeg2D0cpn5fEAqgwkNrnmtvIbg/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>NTU-RGB+D60文件样例</strong></p><p><strong>文件命名方式如下：</strong></p><p>•<strong>S</strong>后面跟的是<strong>设置号</strong>（1-17）</p><p>•<strong>C</strong>后面跟的是**相机**<strong>ID</strong>（1-3）</p><p>•<strong>P</strong>后面跟的是**人物**<strong>ID</strong>（1-40）</p><p>•<strong>R</strong>后面跟的是<strong>动作执行的遍数</strong>（1-2）</p><p>•<strong>A</strong>后面跟的是<strong>动作的分类</strong>（1-60）</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicwibL2aBMIYRWU02tLAjOmHpVn8OmbtkqG9Kd8NtxQXbic240J4h58q1Q/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>骨架轮廓图【每个骨架图有25个骨骼点】</strong></p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicKCXnPNkXr8HweRxH9HM933dyrM3rpWicndtzGib24Tibv1PAh9Wib8msJw/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>NTU-RGB+D 60个动作类别图</strong></p><ul><li><p>102表示共有102帧3d骨架图；</p></li><li><p>1是指单张骨架图中有多少骨架</p></li><li><p>72057594037931691是指追踪的骨架编号；</p></li><li><p>接下来是6个整型数，分别是clipedEdges，handLeftConfidence，handLeftState，handRightConfidence，handRightState，isResticted，该行接下来是骨架的x，y偏置（骨架位置？），该行最后一个数字是该骨架的trackingState（？）；25表示有25个关键点（关节）信息，接下来有25行；</p></li><li><p>表示每帧每个关节点有12个数据，分别代表——<strong>‘x’, ‘y’, ‘z’,</strong> ‘depthX’, ‘depthY’, ‘colorX’, ‘colorY’,‘orientationW’, ‘orientationX’, ‘orientationY’,‘orientationZ’, ‘trackingState’。</p></li><li><p>每个关键点信息行上有11个数据，前三个是关键点的3D位置（x，y，z），第四五个是在2D帧中的位置（用于匹配IR帧），第六第七个是在2D帧中的位置（用于匹配RGB帧），第八第九第十第十一是该关键点的方向（WXYZ）第十二个是该关键点的trackingState是keleton的60个动作类别</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSic1e64ClOib0wsWH5K9atiaXvDa6PkkSE1vqDOBiccEHc2tAD0gBfiaLmL3g/640?wx_fmt=png&amp;from=appmsg" alt="img"><strong>数据文件详细图</strong></p></li></ul><h3 id="2-2数据集可视化">2.2数据集可视化</h3><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicze7y3fyxVHuaBHAiag1joCrptLBfEl2dkto2kPeWx5ulVLEoG6Pm1NA/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>数据集可视化示例</strong></p><p>在这里将.<em>skeleton</em>文件利用python可视化程序将数据读取出来之后，利用可视化方法将所有的动作按照时间顺序进行展示如上</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_gif/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSic9ur0NtyqicichiatbTtadTPzibvmMopNmDicPBWRwOJUlgsftYJjndG1Svg/640?wx_fmt=gif&amp;from=appmsg" alt="img"></p><p>​<strong>Action : hugging</strong></p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_gif/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSictdJQAyPIp4AHBOR4qPPNzHibpxY4DjIicYdib3FcVy6NZJEC7vjLibDwDQ/640?wx_fmt=gif&amp;from=appmsg" alt="img"></p><p>​<strong>Action : cheer up</strong></p><h2 id="3-基于骨架数据的人体行为识别的经典算法">3.基于骨架数据的人体行为识别的经典算法</h2><p>类比图片的分类问题，我们将图片的数据集（常见的有Cifar10，COCO，Imagenet）封装成 (N,C,H,W)，分别代表batchsize，通道数（RGB的数值），图片的宽和高。</p><p>骨架数据是不规则的图结构。我们通过循环遍历.skeleton文件，读取其中的 (x,y,z) 的坐标信息，并将其封装成可以输入进网络的格式 (N,C,T,V,M)，其中：</p><p>N 代表batchsize;</p><p>C 代表通道数，通常为关节点的坐标（x,y,z）;</p><p>T 代表帧数;</p><p>V 代表关节点数;</p><p>M 代表执行本动作的人数（单人动作或双人动作）；</p><p><strong>因为动作是在时间上是持续的，所以骨骼数据多出了一个时间维度T。</strong> <strong>那么如何处理关节点在空间维度上以及时间维度上的动作变化呢？</strong></p><h3 id="3-1STGCN">3.1STGCN</h3><p><strong>Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</strong></p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicrGEiaUxrBYKBv6Z0ibrdIy9zgBXQZYDniaFhicbW97d2jKBCByibhuGcYQA/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>该篇论文于2018年发表在AAAI上。简称ST-GCN。</p><p>论文的主要思想：在骨架序列上构造时空图，应用多层时空图卷积(spatial-temporal graph convolution)，逐步在图上生成更高层次的特征图 （feature map）。最后使用Softmax分类器分类到相应的动作类别。</p><p>这是GCN首次应用于基于骨架数据的人体行为识别，后续工作中端到端的模型大多都建立在它的基础上。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicphNiaI080ia3ruLFgVEFc83tiahFwVbNfyLXNHVHuXFHJ7wWRm0axVupg/640?wx_fmt=png&amp;from=appmsg" alt="img"><strong>整体框架略缩图</strong></p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicDMtSFUbcH5ULKfiaPicyxeFxeMjBotr7HpVOKGCCqJsXcBuyho5gdibjQ/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>这篇文章引用量已经达到3450次，足以可见其影响力。</p><h3 id="3-2具体方法">3.2具体方法</h3><p>作者开创性地设计了一种骨架序列通用表示，即同一帧之内不同节点的连接和不同帧之间同一节点的连接，命名为骨架时空图，在骨架时空图的基础上将图卷积操作融入行为识别，构建出了时空图卷积网络模型，允许特征沿着空间和时间维度进行整合。</p><p>蓝色点表示关节点；各关节点之间的边（即图中颜色为蓝色的边）是由人体骨架的自然连接定义的，即<strong>空间边</strong>。</p><p>帧之间的边（即图中颜色为绿色的边）连接前后时间帧之间的相同关节，即<strong>时间边</strong>。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicdkSlBZXXk3sssKGR41jMAmRLHPw5z3batBVS25ibXLiaNcmKaNKqd7Ww/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><p>​<strong>时空骨架构图</strong></p><p>引入一个可学习的权重矩阵（与邻接矩阵等大小）与邻接矩阵按位相乘。用来赋予邻接矩阵中重要边（节点）较大的权重且抑制非重要边（节点）的权重。</p><p>将加权后的邻接矩阵与骨架序列送至GCN中进行运算。利用TCN网络，实现时间维度信息的聚合。</p><p>引入了残差结构（一个CNN+BN）计算获得Res，与GCN的输出按位相加，实现空间维度信息的聚合。</p><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicoVUKGJkBFI3b2TPnD3nouzGEl77aLWUDj5Oxs4Vys6IoHZiaONa3UPg/640?wx_fmt=png&amp;from=appmsg" alt="img"><strong>模型详细解析</strong></p><h2 id="4基于骨架数据的人体行为识别的学习路线">4基于骨架数据的人体行为识别的学习路线</h2><h3 id="4-1基于骨架的行为识别的发展方向">4.1基于骨架的行为识别的发展方向</h3><p><img src="https://mmbiz.qpic.cn/sz_mmbiz_png/QM9JnEQN3JYQj2xaxXAbn9gfzWq88OSicWRVicuibK1xGY0PB3BuHfamza2HM26ib0jC2I5FXibAk69ydibSuNK0T94w/640?wx_fmt=png&amp;from=appmsg" alt="img"></p><h3 id="4-2学习路线">4.2学习路线</h3><p>从基本的ST-GCN的代码入手（<a href="https://github.com/yysijie/st-gcn%EF%BC%89">https://github.com/yysijie/st-gcn）</a></p><ol><li>深入理解<strong>数据集</strong>（从.skeleton文件中提取需要的关节点信息）</li></ol><p>（<a href="https://blog.csdn.net/weixin_51450749/article/details/111768242%EF%BC%89">https://blog.csdn.net/weixin_51450749/article/details/111768242）</a></p><ol><li>学会<strong>可视化</strong>数据集</li><li>利用<strong>Dataset</strong>和<strong>Dataloader</strong>模块进行封装 ——&gt;**（**<strong>N,C,T,V,W）</strong></li><li>分析<strong>网络结构</strong>（给定网络一个输入，得到输出，利用一些可视化的第三方库，帮助理解）</li><li>学习<strong>模型训练方式</strong>（定义模型，损失，优化器等，保存模型训练过程中的信息，比如准确率，权重等）</li><li>为了能在服务器上训练模型，要学会简单的部署和运维，熟悉一些基础的<strong>Linux命令****行</strong>。</li></ol><h2 id="5小论文学习与准备">5小论文学习与准备</h2><h3 id="论文阅读">论文阅读</h3><h4 id="自己整理过的论文记录-引用的没有详细整理">自己整理过的论文记录 引用的没有详细整理</h4><h5 id="1-文献阅读1">1.文献阅读1</h5><p>Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition</p><h5 id="2-文献阅读2">2.文献阅读2</h5><p>STAR: Sparse Transformer-based Action Recognition</p><h5 id="3-文献阅读">3.文献阅读</h5><p>Language Supervised Training for Skeleton-based Action Recognition</p><h5 id="4文献阅读4">4文献阅读4</h5><blockquote><p>TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential</p></blockquote><p>Learning Heterogeneous Spatial–Temporal Context for Skeleton-Based Action Recognition学习异构时空上下文的骨架动作识别</p><h5 id="5文献阅读7">5文献阅读7</h5><p>Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比</p><p>Multilevel Spatial–Temporal Excited Graph Network for Skeleton-Based Action Recognition</p><blockquote><p>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</p></blockquote><blockquote><p>LAC - Latent Action Composition for Skeleton-based Action Segmentation</p></blockquote><blockquote><p>Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</p></blockquote><h5 id="6文献阅读5">6文献阅读5</h5><p>Self-Adaptive Graph With Nonlocal Attention Network for Skeleton-Based Action Recognition基于非局部注意力网络的自适应图骨架动作识别</p><p>Hypergraph Transformer for Skeleton-based Action Recognition基于骨架的动作识别超图Transformer</p><p>GRAPH CONTRASTIVE LEARNING FOR SKELETON BASED ACTION RECOGNITION</p><p>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations用于基于骨架的动作识别和不断增强的分层一致对比学习</p><p><strong>InfoGCN: Representation Learning for Human Skeleton-based Action Recognition—InfoGCN:基于骨骼的动作识别的表征学习</strong></p><blockquote><p>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</p></blockquote><blockquote><p>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</p></blockquote><blockquote><p>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</p></blockquote><blockquote></blockquote><h5 id="7文献阅读3（transformer1）">7文献阅读3（transformer1）</h5><blockquote><p>1 IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action RecognitionIIP变压器：基于骨架的动作识别中的内部-部件间Transformer</p></blockquote><blockquote><p>2 Spatial Temporal Transformer Network for Skeleton-based Action Recognition基于时空Transformer网络的骨架动作识别</p></blockquote><blockquote><p>3 Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization基于互信息估计和最大化的零镜头骨架动作识别</p></blockquote><blockquote><p>4 Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient基于骨骼运动梯度的骨骼人体动作识别的硬无盒对抗攻击</p></blockquote><blockquote><p>5 STST: Spatial-Temporal Specialized Transformer for Skeleton-based Action Recognition 用于基于骨架的动作识别的时空专用Transformer</p></blockquote><h5 id="8文献略读6（transformer2）">8文献略读6（transformer2）</h5><blockquote><p>6 STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action RecognitionSTMT：一种用于MoCap动作识别的时空网格Transformer</p></blockquote><blockquote><p>7 Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition基于骨架的分层分解图卷积网络动作识别</p></blockquote><blockquote><p>8 Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition跨领域三维动作识别的领域共享和目标特定特征聚类协同算法</p></blockquote><blockquote><p>9 Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning 基于无监督骨架的动作学习全局局部运动Transformer</p></blockquote><blockquote><p>10 Zoom Transformer for Skeleton-Based Group Activity Recognition基于骨架的群体活动识别的缩放Transformer</p></blockquote><blockquote><p>11 Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition基于骨架的动作识别时空元组Transformer</p></blockquote><blockquote><p>12Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition基于骨骼的动作识别的局部和全局时空Transformer</p></blockquote><h5 id="9文献阅读8">9文献阅读8</h5><h5 id="10对比学习">10对比学习</h5><h3 id="实验模型">实验模型</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spring源码编译记录</title>
      <link href="/project/posts/f19dd8d4.html"/>
      <url>/project/posts/f19dd8d4.html</url>
      
        <content type="html"><![CDATA[<h1>学习教程</h1><p>【周瑜大都督在线笔记】<a href="https://www.yuque.com/renyong-jmovm/spring/bs9d13">https://www.yuque.com/renyong-jmovm/spring/bs9d13</a></p><p>【mksz420翔仔-420-剑指Java自研框架，决胜Spring源码-399元-完结】本地视频第二节 【从官网下载后 在本地使用命令进行编译运行】</p><p><img src="../assets/image-20231206084311771.png" alt="image-20231206084311771"></p><h1>官网编译记录【失败告终，gradle 下载后编译失败】</h1><p>spring源码官网<a href="https://github.com/spring-projects/spring-framework/tags?after=v5.0.16.RELEASE">Tags · spring-projects/spring-framework · GitHub</a>  【<a href="https://github.com/spring-projects/spring-framework/tree/v5.2.0.RELEASE">GitHub - spring-projects/spring-framework at v5.2.0.RELEASE</a>】</p><h2 id="对应视频教程">对应视频教程</h2><p><img src="../assets/image-20231206084710612.png" alt="image-20231206084710612"></p><p><img src="../assets/image-20231206084741344.png" alt="image-20231206084741344"></p><p>可以根据框中的 tags 来进行版本的选择</p><blockquote><p>在视频中选择的是 5.2 系列 【2019年视频】</p></blockquote><p><img src="../assets/image-20231206084822038.png" alt="image-20231206084822038"></p><blockquote><p>下载到本地之后，向下转，找到下边的官网提供的编译教程</p></blockquote><p><img src="../assets/image-20231206085016645.png" alt="image-20231206085016645"></p><blockquote><p><a href="https://github.com/spring-projects/spring-framework/wiki/Build-from-Source">Build from Source · spring-projects/spring-framework Wiki · GitHub</a>  进入教程如下界面</p></blockquote><p><img src="../assets/image-20231206085127634.png" alt="image-20231206085127634"></p><blockquote><p>before you start  会告诉你所需要的编译环境</p></blockquote><blockquote><p>这里因为都需要jdk17 或者20  还自己弄了两个jdk的新环境教程如下</p><p><a href="https://www.cnblogs.com/zhanglongxin233/p/13170094.html">java在一台电脑上装两个或多个jdk如何配置环境变量，并实现jdk切换 - 我不吃鸡儿 - 博客园 (cnblogs.com)</a></p><p>现在java环境因为编译失败 仍然改成了jdk8</p></blockquote><p>配置好环境后向下翻滚找到对应的导入idea的教程</p><p><img src="../assets/image-20231206085655407.png" alt="image-20231206085655407"></p><blockquote><p><a href="https://github.com/spring-projects/spring-framework/blob/main/import-into-idea.md">spring-framework/import-into-idea.md at main · spring-projects/spring-framework · GitHub</a></p></blockquote><p><img src="../assets/image-20231206085738056.png" alt="image-20231206085738056"></p><p>执行完成这四步之后就可以在idea中完成导入并进行编译</p><blockquote><p>因为视频中的教程执行是在苹果系统上进行的，所以这里找了个新的教程去执行</p></blockquote><h2 id="windows教程">windows教程</h2><p><a href="https://blog.csdn.net/u010936936/article/details/103404842">Spring5.x源码编译及导入IDEA全过程及踩坑记录_spring5.x本地编译-CSDN博客</a></p><p>按照教程 这里失败在这里，一直展示对应的gradel 下载完成之后，.java文件报错，自己不知道为什么，也懒得去弄了</p><p><img src="../assets/image-20231206090230678.png" alt="image-20231206090230678"></p><p>错误结果</p><p><img src="../assets/image-20231206090958375.png" alt="image-20231206090958375"></p><blockquote><p>总结下来就是坑太多，自己从头编译 确实会碰到很多问题</p></blockquote><h2 id="其他资源">其他资源</h2><p>gradel 配置阿里云环境教程</p><blockquote><p><a href="https://juejin.cn/s/gradle%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E9%95%9C%E5%83%8F">gradle配置阿里云镜像-掘金 (juejin.cn)</a></p><p><a href="https://developer.aliyun.com/article/1128320">Gradle、Maven使用阿里镜像源-阿里云开发者社区 (aliyun.com)</a></p><p><a href="https://www.jianshu.com/p/a0baf8368d63">gradle阿里云镜像配置 - 简书 (jianshu.com)</a></p><p><a href="https://developer.aliyun.com/article/1128320">Gradle、Maven使用阿里镜像源-阿里云开发者社区 (aliyun.com)</a></p></blockquote><p>本地配置下载gradel</p><p><a href="https://blog.csdn.net/zhaokai0130/article/details/81008719">Windows下gradle的安装与配置_windows配置gradle-CSDN博客</a></p><h1>周瑜编译教程</h1><p>【周瑜大都督在线笔记】<a href="https://www.yuque.com/renyong-jmovm/spring/bs9d13">https://www.yuque.com/renyong-jmovm/spring/bs9d13</a></p><p>idea2019升级到2021</p><h2 id="idea升级教程">idea升级教程</h2><p><a href="https://blog.51cto.com/ruanjianlaowang/5501792">升级最新IDEA版本（2022.1.1）_51CTO博客_IDEA最新版本</a></p><blockquote><p>教程中先将之前的配置导出，然后在下载新的之后，在进行导入，结果idea下载之后直接给导入了进来，自定义配置好了</p><p>同时插件教程中说的是需要自己下载，结果新的idea也自己自动下载过了</p></blockquote><p>卸载教程</p><p><a href="https://www.exception.site/essay/how-to-free-use-idea-20201">IntelliJ IDEA 2021.3.2最新激活注册码，破解版下载（永久激活哟） - 异常教程 (exception.site)</a></p><blockquote><p>按照上边的教程中先卸载后，在下载新的，现在还没有破解成功后边再看</p></blockquote><p><a href="https://www.exception.site/essay/idea-reset-eval">IntelliJ IDEA 2023.2.5激活破解图文教程（亲测有用，永久激活） - 异常教程 (exception.site)</a></p><p><a href="https://www.exception.site/article/26">https://www.exception.site/article/26</a>)</p><h2 id="破解成功">破解成功</h2><p>D:\2020.12.29idea\JetBrains  2023 最新全家桶激活（10.30更新）\JetBrains  2023 最新全家桶激活\方式1：无限重置30天试用期补丁（适合老版本）</p><p>2023最新的idea破解资源</p><p><a href="https://pan.baidu.com/s/1wIHDnxbNBruUCwjqezV3hg?pwd=yt91">JetBrains 2023 最新全家桶激活（10.30更新）.zip_免费高速下载|百度网盘-分享无限制 (baidu.com)</a></p><p>21.2</p><p><a href="https://www.exception.site/article/27">IDEA 2021.2最新永久激活码(永不失效，亲测成功) - 异常教程 (exception.site)</a></p><p>20.2</p><p>[IDEA 破解教程 2020.2.2（免费激活至 2089 年，亲测有效） - 异常教程 (exception.site)](</p><h2 id="碰到的问题">碰到的问题</h2><p>新的idea安装之后，会打不开</p><p><a href="https://blog.csdn.net/weixin_55118477/article/details/124700713">解决 IDEA 安装完成后打不开的问题_idea安装后打不开_#避风港#的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/danchaohui/article/details/104395990">IntelliJ IDEA 安装好打不开解决方法_idea打不开-CSDN博客</a></p><p>删除替换一下一个文件就可以了</p><p><img src="../assets/image-20231206092300593.png" alt="image-20231206092300593"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>spring2</title>
      <link href="/project/posts/2ed059e5.html"/>
      <url>/project/posts/2ed059e5.html</url>
      
        <content type="html"><![CDATA[<h1>3.spring组件学习</h1><h2 id="1-bean生成方式">1.bean生成方式</h2><blockquote><p>所有的bean对象生成过程中 并不是直接生成的，而是先经过生成对应的beanDefinition对象，然后生成对应的bean对象</p></blockquote><blockquote><p>使用@Bean 或者 @Component 注解、使用xml设置等这种方法 生成bean对象 都是声明式的方法生成bean对象</p></blockquote><blockquote><p>还有一种方法是使用编程式的方法进行bean对象的生成，下边图像是一个示例</p></blockquote><p><img src="../assets/image-20231128214819896.png" alt="image-20231128214819896"></p><h2 id="2-spring的bean解析方式">2.spring的bean解析方式</h2><h3 id="1-beanDefinitionReader">1.beanDefinitionReader</h3><p><img src="../assets/image-20231128215447380.png" alt="image-20231128215447380"></p><blockquote><p>使用beanDefitionReader 直接传入类的class 【运行时类】 可以直接生成对应的bean对象并放入到spring容器中</p></blockquote><blockquote><p>使用beanDefinition对象 读取一个class对象 然后生成对应的bean对象</p></blockquote><p>而这个方法 就是从AnnotationConfigApplicationContext类中实现，底层是使用上边提的两个方法实现的</p><blockquote><p>AnnotationConfigApplicationContext详解</p></blockquote><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">这句代码是使用Spring框架中的AnnotationConfigApplicationContext类创建一个应用程序上下文（ApplicationContext）。它指示Spring容器使用AppConfig类作为配置类来初始化和配置应用程序的组件。</span><br><span class="line"></span><br><span class="line">具体来说，AnnotationConfigApplicationContext是Spring提供的一个实现了ApplicationContext接口的类，它可以通过注解配置来初始化和管理Spring容器中的Bean。</span><br><span class="line"></span><br><span class="line">在这个例子中，AppConfig是一个Java类，通常用于定义和配置Spring应用程序的组件，如Bean、依赖关系、AOP等。通过传递AppConfig.class给AnnotationConfigApplicationContext的构造函数，告诉Spring容器使用AppConfig类作为配置类进行加载和初始化。</span><br><span class="line"></span><br><span class="line">在运行这行代码后，Spring容器将会根据AppConfig类中的配置信息，扫描和注册相应的Bean定义，创建Bean实例，并将其纳入Spring容器的管理范围内。之后，你可以通过Spring容器来获取和使用这些被管理的Bean。</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line"></span><br><span class="line">java</span><br><span class="line">AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);</span><br><span class="line">MyBean myBean = context.getBean(MyBean.class);</span><br><span class="line">myBean.doSomething();</span><br><span class="line">这里通过context.getBean()方法获取了MyBean类型的Bean实例，并调用了它的doSomething()方法。</span><br><span class="line"></span><br><span class="line">总结起来，这句代码的作用是创建一个基于注解配置的Spring应用程序上下文，并根据AppConfig类中的配置信息初始化和管理Spring容器中的Bean。</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="../assets/image-20231128215800977.png" alt="image-20231128215800977"></p><blockquote><p>传个我一个类，然后我就能读取对应的beanDefinition对象，生成一个bean对象</p></blockquote><h3 id="2-XmlBeanDefinitionReader">2.XmlBeanDefinitionReader</h3><p><img src="../assets/image-20231128220840412.png" alt="image-20231128220840412"></p><blockquote><p>类似于上边的beanDefinitionReader 从.xml文件中读取类定义</p></blockquote><blockquote><p>beanDefition 对象扫描器，将路径下边的所有beanDefinition扫描出来放放到spring容器中</p></blockquote><p><img src="../assets/image-20231128220640107.png" alt="image-20231128220640107"></p><h3 id="3-bean对象的解析方式">3.bean对象的解析方式</h3><p><img src="../assets/image-20231128221521511.png" alt="image-20231128221521511"></p><blockquote><p>既然有两种bean的定义方式，spring容器就一定有两种解析bean定义的方式，一种是注册解析，另一种就是扫描解析</p></blockquote><blockquote><p>扫描的过程也是跟spring生成bean周期一样，先去找所有的类，然后找到对应的注解，有注解的类才生成bean对象</p></blockquote><h2 id="3-beanFactory-VS-ApplicationContext">3.beanFactory VS ApplicationContext</h2><h3 id="1-ApplicationContext">1.ApplicationContext</h3><p><img src="../assets/image-20231129091055589.png" alt="image-20231129091055589"></p><p>ApplicationContext 继承 BeanFactory  既是BeanFactory  也拥有一些BeanFactory 没有的功能</p><p>例如 EnvironmentCapable  获取环境变量功能  ==MessageSource  实现国际化== ApplicationEventPublisher 事件发布功能    ResourcePatternResolver  拥有并去解析某些资源的功能</p><p>在applicationContext中 也是使用的下边的DefaltListableBeanFactory  生成一个BeanFactory 工厂，然后来生成对应的bean对象</p><blockquote><p>applicationContext 的父类中继承了GenEricApplicationContext【如下图】</p></blockquote><p><img src="../assets/image-20231129093006336.png" alt="image-20231129093006336"></p><blockquote><p>父类继承的GenEricApplicationContext 中实现了DefaltListableBeanFactory 属性赋值，初始化BeanFactory属性 来new一个DefaltListableBeanFactory</p></blockquote><p><img src="../assets/image-20231129092837944.png" alt="image-20231129092837944"></p><h3 id="2-beanFactory">2.beanFactory</h3><blockquote><p>beanFactory 可以直接利用DefaltListableBeanFactory 这个类 来生成一个beanFactory 并直接用工厂来生成一个bean对象</p></blockquote><p><img src="../assets/image-20231129091758456.png" alt="image-20231129091758456"></p><p>可以把spring容器理解成 一个BeanFactory 也可以理解成一个 ApplicationContext  或者是一个BeandefinitionMap 用来装载bean对象</p><blockquote><p>下边使用DefaultListableBeanFactory 可以直接生成一个BeanFactory，直接用来生成bean</p></blockquote><p><img src="../assets/image-20231129093537653.png" alt="image-20231129093537653"></p><blockquote><p>DefaultListableBeanFactory 继承并实现了下图中很多的接口，所以能够有很多的功能，如果自己手写spring容器的时候，可以自己实现相应的接口，不需要这个功能的话就不需要实现接口</p></blockquote><p><img src="../assets/image-20231129095244637.png" alt="image-20231129095244637"></p><h3 id="3-ListableBeanFactory">3.ListableBeanFactory</h3><p>ListableBeanFactory有很多的功能</p><p><img src="../assets/image-20231129095721633.png" alt="image-20231129095721633"></p><blockquote><p>统计beandefinition  看是否包含某个beanDefinition  获得所有的beanDefition的名字 等函数</p></blockquote><p><img src="../assets/image-20231129095819122.png" alt="image-20231129095819122"></p><h3 id="4-ConfigurableBeanFactory">4.ConfigurableBeanFactory</h3><p>配置生成BeanFactory 例如classloader 等配置生成beanFactory</p><h3 id="5-AutowireCapableBeanfactory">5.AutowireCapableBeanfactory</h3><p>自动注入BeanFactory 功能</p><h2 id="4-国际化">4.国际化</h2><blockquote><p>利用applicationContext 实现国际化</p></blockquote><blockquote><p>国际化就是你在配置文件中 配置一个语言  然后后边可以在业务方法中利用某个方法去获取对应的语言的别名 知道用的是什么语言</p></blockquote><h3 id="使用实例">使用实例</h3><blockquote><p>设定语言</p></blockquote><p><img src="../assets/image-20231129100806911.png" alt="image-20231129100806911"></p><blockquote><p>添加英语语言</p></blockquote><p><img src="../assets/image-20231129100827417.png" alt="image-20231129100827417"></p><blockquote><p>根据语言设定内容</p></blockquote><p><img src="../assets/image-20231129100921040.png" alt="image-20231129100921040"></p><p><img src="../assets/image-20231129100949526.png" alt="image-20231129100949526"></p><p>默认语言 test 设置为a 英语设置为b</p><blockquote><p>定义bean对象</p></blockquote><p><img src="../assets/image-20231129101022098.png" alt="image-20231129101022098"></p><p>这里的message 名字对应的是 两个语言的文件名字 message</p><blockquote><p>获取语言key  对应的value 值</p><p>因为在配置中 设定的值是b 所以展示出来b</p></blockquote><p><img src="../assets/image-20231129100711161.png" alt="image-20231129100711161"></p><h4 id="业务中的具体用法">业务中的具体用法</h4><p>先实现接口 然后设定applicationContext属性值，后边直接给application 赋值后 直接在test() 函数中应用获取massage即可</p><p><img src="../assets/image-20231129100501280.png" alt="image-20231129100501280"></p><h2 id="5-资源加载">5.资源加载</h2><blockquote><p>可以直接在资源加载时候 利用已经写好的类 或者方法进行资源加载</p><p>从而减少从头开始自己实现的资源消耗</p></blockquote><h3 id="获取运行时环境">获取运行时环境</h3><blockquote><p>上边函数 获取的是操作系统层面的环境变量</p><p>而下边的函数 获取的是java运行时候的环境变量 -D指定的操作系统</p></blockquote><p><img src="../assets/image-20231129201716997.png" alt="image-20231129201716997"></p><p>@PropertySource(“classpath:spring.properties”)</p><blockquote><p>PropertySource 方法也可以直接获取对应环境变量值</p></blockquote><p><img src="../assets/image-20231129202327807.png" alt="image-20231129202327807"></p><h4 id="元数据">元数据</h4><blockquote><p>第三行数据的hasMetaAnnotation 判定的是 该类是否拥有Component注解 但是如果该类使用了@Service注解修饰  因为@Service注解底层包含@Component 注解 所以返回的仍为true 包含Componnent</p><p>但是第四行 只会判断该类是否直接有@Component注解修饰 不会去判断注解底层是否包含</p></blockquote><p><img src="../assets/image-20231130093222605.png" alt="image-20231130093222605"></p><h4 id="FactoryBean与-bean区别">FactoryBean与@bean区别</h4><p><img src="../assets/image-20231130095413216.png" alt="image-20231130095413216"></p><p>FactoryBean 虽然重命名一个ZhouyuFactoryBean，但是在最后获取到bean的类型的时候 是一个UserService 类型</p><p><img src="../assets/image-20231130095721836.png" alt="image-20231130095721836"></p><p>使用@Bean 注解也能直接实现一个bean对象的创建</p><p>但是@Bean只会生成一个bean对象 就是UserService对象，并且都会走bean声明周期这一套流程</p><p>而使用FactoryBean  接口 实现一个UserService对象 的过程会创建两个bean，并且第一个ZhouyuFactoryBean会走一套全流程的bean对象生命周期，而创建出来的UserService对象只会走一下初始化后，因为要满足一下aop功能，而不会走全套的bean生命周期</p><h1>5.spring声明周期下</h1><blockquote><p>本节课讲spring内部怎样根据一个beanDefinition去创建出一个bean对象的</p></blockquote><blockquote><p>getBean()的三种用法</p></blockquote><blockquote><p>根据 现有的beanName找beanDefinition  ，如果在当前的spring容器中找不到， 则根据传入的类型去父类中找beanDefinition</p></blockquote><blockquote><p>循环依赖 的错误情况 dependOn的排除</p><p>如果有@dependOn注解修饰 则先创建出被依赖的bean对象【实现过程中 设定两个map 存储两个相反的依赖关系对象】</p></blockquote><h4 id="scope-值为其他【配合springmvc的session-和-request】">scope 值为其他【配合springmvc的session 和 request】</h4><blockquote><p>==lambad表达式 在java中应用==</p><p><img src="../assets/image-20231204214716581.png" alt="image-20231204214716581"></p><blockquote><p>springmvc中 attribute.set  attribute.get 回调</p><p>session  request</p><p>springMvc 怎样拿到的bean</p></blockquote></blockquote><blockquote><p>学习到1h06 需要学习bean的创建过程中 初始化前 那一套过程</p></blockquote><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20231211110536802.png" alt="image-20231211110536802"></p><p>D:\JDK\install\jre\bin</p><p>D:\JDK\install\bin</p><p><img src="../assets/image-20231130214912949.png" alt="image-20231130214912949"></p><p>D:\JDK\install</p><p><img src="../assets/image-20231130221152868.png" alt="image-20231130221152868"></p><p><img src="../assets/image-20231130221237165.png" alt="image-20231130221237165"></p><p><img src="../assets/image-20231202095608876.png" alt="image-20231202095608876"></p><p><img src="../assets/image-20231202095617694.png" alt="image-20231202095617694"></p><p><a href="https://www.exception.site/essay/idea-reset-eval">IntelliJ IDEA 2023.2.5激活破解图文教程（亲测有用，永久激活） - 异常教程 (exception.site)</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>行为识别之骨架动作行为识别</title>
      <link href="/project/posts/e38c8f30.html"/>
      <url>/project/posts/e38c8f30.html</url>
      
        <content type="html"><![CDATA[<h1>行为识别之骨架动作行为识别</h1><h2 id="1-基于骨架数据的人体行为识别的简介">1.<strong>基于骨架数据的人体行为识别的</strong>简介</h2><h3 id="1-人体行为识别">1.人体行为识别</h3><blockquote><p>动图</p></blockquote><p>​         人体行为识别是指通过特定的算法，从人体本身的特征和所完成动作的特征出发，识别出人物具体动作的分类任务。它在智能监控、视频检索等方面有着广泛的应用。准确地提取到每个行为的语义信息，来描述其空间及时间上的动态变化，是这个方向的主要挑战。</p><h3 id="2-基于骨架数据的人体行为识别">2.基于骨架数据的人体行为识别</h3><p><img src="../assets/image-20231124160552639.png" alt="image-20231124160552639"></p><p>​         随着 Kinect 深度相机的出现和人体姿态估计算法的发展，研究人员可以轻松地获取人体的3D骨架关键点信息，骨架序列的采集变得更加便捷、成本更低，使骨架数据具有计算量小的优点。与传统的RGB视频数据相比，人体骨骼有着更完整的结构信息，可以避免背景、光照以及视角变化所产生的噪声影响，对复杂的场景有着更强的鲁棒性，可以大大地提高识别效率。凭借这些优势，基于骨架数据的人体行为识别具有深远的研究意义。</p><p><img src="../assets/image-20231124160747597.png" alt="image-20231124160747597"></p><blockquote><p>动图</p></blockquote><h3 id="3-骨架动作行为识别发展历程">3.骨架动作行为识别发展历程</h3><p><img src="../assets/image-20231124162939949.png" alt="image-20231124162939949"></p><p>​发展历程图</p><p>​         基于传统的骨架行为识别方法利用手工提取特征方法，通过一系列三维操作（如关节旋转和平移）手动提取特征，使用人类工程特征检测器和描述符来构建特征向量。然后通过训练通用分类器来执行分类。</p><p><img src="../assets/image-20231124163843373.png" alt="image-20231124163843373"></p><p>​         <strong>基于深度学习方法主要分为基于卷积神经网络的行为识别方法、基于循环神经网络的行为识别方法和基于图卷积网络的行为识别方法；</strong></p><h4 id="3-1基于卷积神经网络（CNN）的行为识别方法">3.1基于卷积神经网络（CNN）的行为识别方法</h4><p>​        基于CNN的方法具备学习高级语义线索的能力，广泛应用于处理2D图像。基于卷积神经网络的行为识别方法一般将人体骨架序列从矢量序列转换为伪图像，伪图像的宽度、高度和通道分别代表骨架序列的关节、帧和坐标，形成伪图像，其中行代表不同的关节，列代表不同的帧，随后通过卷积神经网络进行动作识别。</p><p><img src="../assets/image-20231124164101048.png" alt="image-20231124164101048"></p><h4 id="3-2基于循环神经网络（RNN）的行为识别方法">3.2基于循环神经网络（RNN）的行为识别方法</h4><p>​         循环神经网络在处理可变长度的序列数据方面具备更强大的能力，更适合模拟动作的运动和时间依赖性。基于循环神经网络RNNs的方法通常将关节数据表示为若干向量序列，其中每个向量序列包括了每帧中各关节的位置数据，随后这些数据被馈送到循环神经网络中以进行动作识别。</p><p><img src="../assets/image-20231124164148386.png" alt="image-20231124164148386"></p><blockquote><p>总结:基于CNN和RNN方法都不能很好的处理非欧式空间中的图数据，因为将图结构数据转换为伪图像或坐标向量再进行卷积会损失原始的信息。</p><p>​CNN更加适合处理规则的网格数据，比如图像（由像素点构成），但是它很难对图结构数据进行有效的卷积。</p><p>​RNN适合处理时序性的问题，但是很难感知骨架数据的空间构型，也很难有效的建模长时间序列上关节点之间的依赖关系。</p></blockquote><h4 id="3-3基于图卷积网络（GCN）的行为识别方法">3.3基于图卷积网络（GCN）的行为识别方法</h4><p>人体骨骼是非欧几里德的图数据，将关节根据骨骼联接构成图，每一帧的骨架是一个独立的图，作为空间信息。沿着时间维度连接骨架关节来扩展空间图，以构建时空图。经过多层卷积提取空间和时间特征，最后再分类。</p><p><img src="../assets/image-20231124164529713.png" alt="image-20231124164529713"></p><h2 id="2-基于骨架数据的人体行为识别的数据集">2.基于骨架数据的人体行为识别的数据集</h2><h3 id="2-1数据集介绍">2.1数据集介绍</h3><p>本方向的常用数据集主要包括：NTU-RGB+D60，NTU-RGB+D120，Kinetics400，以及NW-UCLA等</p><p>以最为常见的NTU-RGB+D60 数据集为例：</p><p>NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p><img src="../assets/image-20231124164617216.png" alt="image-20231124164617216"></p><p><strong>文件命名方式如下：</strong></p><p>•S后面跟的是<strong>设置号</strong>（1-17）</p><p>•C后面跟的是<strong>相机****ID</strong>（1-3）</p><p>•P后面跟的是<strong>人物****ID</strong>（1-40）</p><p>•R后面跟的是<strong>动作执行的遍数</strong>（1-2）</p><p>•A后面跟的是<strong>动作的分类</strong>（1-60）</p><p><img src="../assets/image-20231124164628923.png" alt="image-20231124164628923"></p><p><strong>骨架轮廓图【每个都有25个骨骼点】</strong></p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p><img src="../assets/image-20231124164744736.png" alt="image-20231124164744736"></p><p>​<strong>NTU-RGB+D的60个动作类别图</strong></p><ul><li>102表示共有102帧3d骨架图；</li><li>1是指单张骨架图中有多少骨架</li><li>72057594037931691是指追踪的骨架编号；</li><li>接下来是6个整型数，分别是clipedEdges，handLeftConfidence，handLeftState，handRightConfidence，handRightState，isResticted，该行接下来是骨架的x，y偏置（骨架位置？），该行最后一个数字是该骨架的trackingState（？）；25表示有25个关键点（关节）信息，接下来有25行；</li><li>表示每帧每个关节点有12个数据，分别代表——<strong>‘x’, ‘y’, ‘z’,</strong> ‘depthX’, ‘depthY’, ‘colorX’, ‘colorY’,‘orientationW’, ‘orientationX’, ‘orientationY’,‘orientationZ’, ‘trackingState’。</li><li>每个关键点信息行上有11个数据，前三个是关键点的3D位置（x，y，z），第四五个是在2D帧中的位置（用于匹配IR帧），第六第七个是在2D帧中的位置（用于匹配RGB帧），第八第九第十第十一是该关键点的方向（WXYZ）第十二个是该关键点的trackingState是keleton的60个动作类别</li></ul><p><img src="../assets/image-20231124164854085.png" alt="image-20231124164854085"></p><p>​       骨骼数据文件详细图</p><h3 id="2-2数据集可视化">2.2数据集可视化</h3><p><img src="../assets/image-20231124215750042.png" alt="image-20231124215750042"></p><p>Action : <strong>hugging</strong></p><blockquote><p>动图</p></blockquote><p>Action : <strong>cheer up</strong></p><blockquote><p>动图</p></blockquote><h2 id="3-基于骨架数据的人体行为识别的经典算法">3.基于骨架数据的人体行为识别的经典算法</h2><p>类比图片的分类问题，我们将图片的数据集（常见的有Cifar10，COCO，Imagenet）封装成 (N,C,H,W)，分别代表batchsize，通道数（RGB的数值），图片的宽和高。</p><p>骨架数据是不规则的图结构。我们通过循环遍历.skeleton文件，读取其中的 (x,y,z) 的坐标信息，并将其封装成可以输入进网络的格式 (N,C,T,V,M)，其中：</p><p>N 代表batchsize，</p><p>C 代表通道数，通常为关节点的坐标（x,y,z）</p><p>T 代表帧数，</p><p>V 代表关节点数，</p><p>M 代表执行本动作的人数（单人动作或双人动作）。</p><p>因为动作是在时间上是持续的，所以骨骼数据多出了一个时间维度T。</p><blockquote><p>那么如何处理关节点在空间维度上以及时间维度上的动作变化呢？</p></blockquote><h3 id="3-1STGCN">3.1STGCN</h3><p><img src="../assets/image-20231124215935538.png" alt="image-20231124215935538"></p><p><img src="../assets/image-20231124215940166.png" alt="image-20231124215940166"></p><p>该篇论文于2018年发表在AAAI上。简称ST-GCN。</p><p>论文的主要思想：在骨架序列上构造时空图，应用多层时空图卷积(spatial-temporal graph convolution)，逐步在图上生成更高层次的特征图 （feature map）。最后使用Softmax分类器分类到相应的动作类别。</p><p>这是GCN首次应用于基于骨架数据的人体行为识别，后续工作中端到端的模型大多都建立在它的基础上。</p><blockquote></blockquote><p><img src="../assets/image-20231124220004203.png" alt="image-20231124220004203"></p><p>这篇文章引用量已经达到3450次，足以可见其影响力。</p><p><img src="../assets/image-20231124215951844.png" alt="image-20231124215951844"></p><blockquote><p>引用数</p></blockquote><h3 id="3-2具体方法">3.2具体方法</h3><p>作者开创性地设计了一种骨架序列通用表示，即同一帧之内不同节点的连接和不同帧之间同一节点的连接，命名为骨架时空图，在骨架时空图的基础上将图卷积操作融入行为识别，构建出了时空图卷积网络模型，允许特征沿着空间和时间维度进行整合。</p><p>蓝色点表示关节点；</p><p>各关节点之间的边（即图中颜色为蓝色的边）是由人体骨架的自然连接定义的，即<strong>空间边</strong>。</p><p>帧之间的边（即图中颜色为绿色的边）连接前后时间帧之间的相同关节，即<strong>时间边</strong>。</p><p><img src="../assets/image-20231124220206335.png" alt="image-20231124220206335"></p><p>引入一个可学习的权重矩阵（与邻接矩阵等大小）与邻接矩阵按位相乘。用来赋予邻接矩阵中重要边（节点）较大的权重且抑制非重要边（节点）的权重。</p><p>将加权后的邻接矩阵与骨架序列送至GCN中进行运算。</p><p>利用TCN网络，实现时间维度信息的聚合。</p><p>引入了残差结构（一个CNN+BN）计算获得Res，与GCN的输出按位相加，实现空间维度信息的聚合。</p><p><img src="../assets/image-20231124220320922.png" alt="image-20231124220320922"></p><h2 id="4基于骨架数据的人体行为识别的学习路线">4基于骨架数据的人体行为识别的学习路线</h2><h3 id="4-1基于骨架的行为识别的发展方向image-20231124220341280">4.1基于骨架的行为识别的发展方向<img src="../assets/image-20231124220341280.png" alt="image-20231124220341280"></h3><h3 id="4-2学习路线">4.2学习路线</h3><p>从基本的ST-GCN的代码入手（<a href="https://github.com/yysijie/st-gcn%EF%BC%89">https://github.com/yysijie/st-gcn）</a></p><ol><li>深入理解数据集（从.skeleton文件中提取需要的关节点信息）</li></ol><p>（<a href="https://blog.csdn.net/weixin_51450749/article/details/111768242%EF%BC%89">https://blog.csdn.net/weixin_51450749/article/details/111768242）</a></p><ol start="2"><li><p>学会可视化数据集</p></li><li><p>利用<strong>Dataset</strong>和<strong>Dataloader</strong>模块进行封装 ——&gt;<strong>（<strong><strong>N,C,T,V,W</strong></strong>）</strong></p></li><li><p>分析网络结构（给定网络一个输入，得到输出，利用一些可视化的第三方库，帮助理解）</p></li><li><p>学习<strong>模型训练方式</strong>（定义模型，损失，优化器等，保存模型训练过程中的信息，比如准确率，权重等）</p></li><li><p>为了能在服务器上训练模型，要学会简单的部署和运维，熟悉一些基础的Linux命令行。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>行为识别之骨架动作行为识别</title>
      <link href="/project/posts/cea7f2ca.html"/>
      <url>/project/posts/cea7f2ca.html</url>
      
        <content type="html"><![CDATA[<h1>行为识别之骨架动作行为识别</h1><h2 id="1-基于骨架数据的人体行为识别的简介">1.<strong>基于骨架数据的人体行为识别的</strong>简介</h2><h3 id="1-人体行为识别">1.人体行为识别</h3><blockquote><p>动图</p></blockquote><p>​         人体行为识别是指通过特定的算法，从人体本身的特征和所完成动作的特征出发，识别出人物具体动作的分类任务。它在智能监控、视频检索等方面有着广泛的应用。准确地提取到每个行为的语义信息，来描述其空间及时间上的动态变化，是这个方向的主要挑战。</p><h3 id="2-基于骨架数据的人体行为识别">2.基于骨架数据的人体行为识别</h3><p><img src="../assets/image-20231124160552639.png" alt="image-20231124160552639"></p><p><img src="../assets/image-20231124160552639.png" alt="image-20231124160552639"></p><p>​         随着 Kinect 深度相机的出现和人体姿态估计算法的发展，研究人员可以轻松地获取人体的3D骨架关键点信息，骨架序列的采集变得更加便捷、成本更低，使骨架数据具有计算量小的优点。与传统的RGB视频数据相比，人体骨骼有着更完整的结构信息，可以避免背景、光照以及视角变化所产生的噪声影响，对复杂的场景有着更强的鲁棒性，可以大大地提高识别效率。凭借这些优势，基于骨架数据的人体行为识别具有深远的研究意义。</p><p><img src="../assets/image-20231124160747597.png" alt="image-20231124160747597"></p><blockquote><p>动图</p></blockquote><h3 id="3-骨架动作行为识别发展历程">3.骨架动作行为识别发展历程</h3><p><img src="../assets/image-20231124162939949.png" alt="image-20231124162939949"></p><p>​发展历程图</p><p>​         基于传统的骨架行为识别方法利用手工提取特征方法，通过一系列三维操作（如关节旋转和平移）手动提取特征，使用人类工程特征检测器和描述符来构建特征向量。然后通过训练通用分类器来执行分类。</p><p><img src="../assets/image-20231124163843373.png" alt="image-20231124163843373"></p><p>​         <strong>基于深度学习方法主要分为基于卷积神经网络的行为识别方法、基于循环神经网络的行为识别方法和基于图卷积网络的行为识别方法；</strong></p><h4 id="3-1基于卷积神经网络（CNN）的行为识别方法">3.1基于卷积神经网络（CNN）的行为识别方法</h4><p>​        基于CNN的方法具备学习高级语义线索的能力，广泛应用于处理2D图像。基于卷积神经网络的行为识别方法一般将人体骨架序列从矢量序列转换为伪图像，伪图像的宽度、高度和通道分别代表骨架序列的关节、帧和坐标，形成伪图像，其中行代表不同的关节，列代表不同的帧，随后通过卷积神经网络进行动作识别。</p><p><img src="../assets/image-20231124164101048.png" alt="image-20231124164101048"></p><h4 id="3-2基于循环神经网络（RNN）的行为识别方法">3.2基于循环神经网络（RNN）的行为识别方法</h4><p>​         循环神经网络在处理可变长度的序列数据方面具备更强大的能力，更适合模拟动作的运动和时间依赖性。基于循环神经网络RNNs的方法通常将关节数据表示为若干向量序列，其中每个向量序列包括了每帧中各关节的位置数据，随后这些数据被馈送到循环神经网络中以进行动作识别。</p><p><img src="../assets/image-20231124164148386.png" alt="image-20231124164148386"></p><blockquote><p>总结:基于CNN和RNN方法都不能很好的处理非欧式空间中的图数据，因为将图结构数据转换为伪图像或坐标向量再进行卷积会损失原始的信息。</p><p>​CNN更加适合处理规则的网格数据，比如图像（由像素点构成），但是它很难对图结构数据进行有效的卷积。</p><p>​RNN适合处理时序性的问题，但是很难感知骨架数据的空间构型，也很难有效的建模长时间序列上关节点之间的依赖关系。</p></blockquote><h4 id="3-3基于图卷积网络（GCN）的行为识别方法">3.3基于图卷积网络（GCN）的行为识别方法</h4><p>人体骨骼是非欧几里德的图数据，将关节根据骨骼联接构成图，每一帧的骨架是一个独立的图，作为空间信息。沿着时间维度连接骨架关节来扩展空间图，以构建时空图。经过多层卷积提取空间和时间特征，最后再分类。</p><p><img src="../assets/image-20231124164529713.png" alt="image-20231124164529713"></p><h2 id="2-基于骨架数据的人体行为识别的数据集">2.基于骨架数据的人体行为识别的数据集</h2><h3 id="2-1数据集介绍">2.1数据集介绍</h3><p>本方向的常用数据集主要包括：NTU-RGB+D60，NTU-RGB+D120，Kinetics400，以及NW-UCLA等</p><p>以最为常见的NTU-RGB+D60 数据集为例：</p><p>NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p><img src="../assets/image-20231124164617216.png" alt="image-20231124164617216"></p><p><strong>文件命名方式如下：</strong></p><p>•S后面跟的是<strong>设置号</strong>（1-17）</p><p>•C后面跟的是<strong>相机****ID</strong>（1-3）</p><p>•P后面跟的是<strong>人物****ID</strong>（1-40）</p><p>•R后面跟的是<strong>动作执行的遍数</strong>（1-2）</p><p>•A后面跟的是<strong>动作的分类</strong>（1-60）</p><p><img src="../assets/image-20231124164628923.png" alt="image-20231124164628923"></p><p><strong>骨架轮廓图【每个都有25个骨骼点】</strong></p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p>NTU-RGB+D60数据集包含60个种类的动作，共56880个样本，其中有40类为日常行为动作，11类为双人互动的动作。该数据集由微软 Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括深度信息、3D骨架信息、RGB视频以及红外序列。</p><p><img src="../assets/image-20231124164744736.png" alt="image-20231124164744736"></p><p>​<strong>NTU-RGB+D的60个动作类别图</strong></p><ul><li>102表示共有102帧3d骨架图；</li><li>1是指单张骨架图中有多少骨架</li><li>72057594037931691是指追踪的骨架编号；</li><li>接下来是6个整型数，分别是clipedEdges，handLeftConfidence，handLeftState，handRightConfidence，handRightState，isResticted，该行接下来是骨架的x，y偏置（骨架位置？），该行最后一个数字是该骨架的trackingState（？）；25表示有25个关键点（关节）信息，接下来有25行；</li><li>表示每帧每个关节点有12个数据，分别代表——<strong>‘x’, ‘y’, ‘z’,</strong> ‘depthX’, ‘depthY’, ‘colorX’, ‘colorY’,‘orientationW’, ‘orientationX’, ‘orientationY’,‘orientationZ’, ‘trackingState’。</li><li>每个关键点信息行上有11个数据，前三个是关键点的3D位置（x，y，z），第四五个是在2D帧中的位置（用于匹配IR帧），第六第七个是在2D帧中的位置（用于匹配RGB帧），第八第九第十第十一是该关键点的方向（WXYZ）第十二个是该关键点的trackingState是keleton的60个动作类别</li></ul><p><img src="../assets/image-20231124164854085.png" alt="image-20231124164854085"></p><p>​       骨骼数据文件详细图</p><h3 id="2-2数据集可视化">2.2数据集可视化</h3><p><img src="../assets/image-20231124215750042.png" alt="image-20231124215750042"></p><p>Action : <strong>hugging</strong></p><blockquote><p>动图</p></blockquote><p>Action : <strong>cheer up</strong></p><blockquote><p>动图</p></blockquote><h2 id="3-基于骨架数据的人体行为识别的经典算法">3.基于骨架数据的人体行为识别的经典算法</h2><p>类比图片的分类问题，我们将图片的数据集（常见的有Cifar10，COCO，Imagenet）封装成 (N,C,H,W)，分别代表batchsize，通道数（RGB的数值），图片的宽和高。</p><p>骨架数据是不规则的图结构。我们通过循环遍历.skeleton文件，读取其中的 (x,y,z) 的坐标信息，并将其封装成可以输入进网络的格式 (N,C,T,V,M)，其中：</p><p>N 代表batchsize，</p><p>C 代表通道数，通常为关节点的坐标（x,y,z）</p><p>T 代表帧数，</p><p>V 代表关节点数，</p><p>M 代表执行本动作的人数（单人动作或双人动作）。</p><p>因为动作是在时间上是持续的，所以骨骼数据多出了一个时间维度T。</p><blockquote><p>那么如何处理关节点在空间维度上以及时间维度上的动作变化呢？</p></blockquote><h3 id="3-1STGCN">3.1STGCN</h3><p><img src="../assets/image-20231124215935538.png" alt="image-20231124215935538"></p><p><img src="../assets/image-20231124215940166.png" alt="image-20231124215940166"></p><p>该篇论文于2018年发表在AAAI上。简称ST-GCN。</p><p>论文的主要思想：在骨架序列上构造时空图，应用多层时空图卷积(spatial-temporal graph convolution)，逐步在图上生成更高层次的特征图 （feature map）。最后使用Softmax分类器分类到相应的动作类别。</p><p>这是GCN首次应用于基于骨架数据的人体行为识别，后续工作中端到端的模型大多都建立在它的基础上。</p><blockquote></blockquote><p><img src="../assets/image-20231124220004203.png" alt="image-20231124220004203"></p><p>这篇文章引用量已经达到3450次，足以可见其影响力。</p><p><img src="../assets/image-20231124215951844.png" alt="image-20231124215951844"></p><blockquote><p>引用数</p></blockquote><h3 id="3-2具体方法">3.2具体方法</h3><p>作者开创性地设计了一种骨架序列通用表示，即同一帧之内不同节点的连接和不同帧之间同一节点的连接，命名为骨架时空图，在骨架时空图的基础上将图卷积操作融入行为识别，构建出了时空图卷积网络模型，允许特征沿着空间和时间维度进行整合。</p><p>蓝色点表示关节点；</p><p>各关节点之间的边（即图中颜色为蓝色的边）是由人体骨架的自然连接定义的，即<strong>空间边</strong>。</p><p>帧之间的边（即图中颜色为绿色的边）连接前后时间帧之间的相同关节，即<strong>时间边</strong>。</p><p><img src="../assets/image-20231124220206335.png" alt="image-20231124220206335"></p><p>引入一个可学习的权重矩阵（与邻接矩阵等大小）与邻接矩阵按位相乘。用来赋予邻接矩阵中重要边（节点）较大的权重且抑制非重要边（节点）的权重。</p><p>将加权后的邻接矩阵与骨架序列送至GCN中进行运算。</p><p>利用TCN网络，实现时间维度信息的聚合。</p><p>引入了残差结构（一个CNN+BN）计算获得Res，与GCN的输出按位相加，实现空间维度信息的聚合。</p><p><img src="../assets/image-20231124220320922.png" alt="image-20231124220320922"></p><h2 id="4基于骨架数据的人体行为识别的学习路线">4基于骨架数据的人体行为识别的学习路线</h2><h3 id="4-1基于骨架的行为识别的发展方向image-20231124220341280">4.1基于骨架的行为识别的发展方向<img src="../assets/image-20231124220341280.png" alt="image-20231124220341280"></h3><h3 id="4-2学习路线">4.2学习路线</h3><p>从基本的ST-GCN的代码入手（<a href="https://github.com/yysijie/st-gcn%EF%BC%89">https://github.com/yysijie/st-gcn）</a></p><ol><li>深入理解数据集（从.skeleton文件中提取需要的关节点信息）</li></ol><p>（<a href="https://blog.csdn.net/weixin_51450749/article/details/111768242%EF%BC%89">https://blog.csdn.net/weixin_51450749/article/details/111768242）</a></p><ol start="2"><li><p>学会可视化数据集</p></li><li><p>利用<strong>Dataset</strong>和<strong>Dataloader</strong>模块进行封装 ——&gt;<strong>（<strong><strong>N,C,T,V,W</strong></strong>）</strong></p></li><li><p>分析网络结构（给定网络一个输入，得到输出，利用一些可视化的第三方库，帮助理解）</p></li><li><p>学习<strong>模型训练方式</strong>（定义模型，损失，优化器等，保存模型训练过程中的信息，比如准确率，权重等）</p></li><li><p>为了能在服务器上训练模型，要学会简单的部署和运维，熟悉一些基础的Linux命令行。</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/project/posts/b7d9085f.html"/>
      <url>/project/posts/b7d9085f.html</url>
      
        <content type="html"><![CDATA[<h1>Spring学习</h1><h1>1.spring理论学习（第一讲）</h1><blockquote><p>周瑜在线笔记链接<a href="https://www.yuque.com/renyong-jmovm/spring/it8ayo">1、Spring底层核心原理解析 (yuque.com)</a></p><p>学习周瑜讲解spring源码的第一节课笔记总结记录</p></blockquote><blockquote><p>主要内容目录</p><ol><li>Bean的生命周期底层原理</li><li>依赖注入底层原理</li><li>初始化底层原理</li><li>推断构造方法底层原理</li><li>AOP底层原理</li><li>Spring事务底层原理</li></ol></blockquote><h3 id="Bean的创建生命周期VS-Bean对象的销毁生命周期">Bean的创建生命周期VS Bean对象的销毁生命周期</h3><blockquote><p>在java中Application Context与ClassPathXmlApplicationContext之间的关系</p></blockquote><p>在Java中，Application Context是一个接口，它是Spring框架中用于管理Bean的核心接口之一。它定义了一些方法，例如get Bean()和registerShutdownHook()，用于获取Bean并在应用程序关闭时销毁Bean。</p><p>ClassPathXmlApplicationContext是Application Context接口的一个实现类，它从class path中读取XML文件来配置Bean。它会在应用程序启动时自动装载并实例化所有Bean，然后将它们放入一个容器中，以便在应用程序中使用。</p><p>因此，ClassPathXmlApplicationContext是Application Context接口的一种具体实现，它通过读取XML文件来实现Bean的管理和配置。其他实现Application Context接口的类还包括FileSystemXmlApplicationContext和AnnotationConfigApplicationContext等。</p><p>因此，ClassPathXmlApplicationContext是Application Context接口的一种具体实现，它通过读取XML文件来实现Bean的管理和配置。其他实现Application Context接口的类还包括FileSystemXmlApplicationContext和AnnotationConfigApplicationContext等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ApplicationContext</span> <span class="variable">applicationContext</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ClassPathXmlApplicationContext</span>(<span class="string">&quot;spring.xml&quot;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>使用ApplicationContext 接口对实现类的返回结果进行接收，ClassPathXmlApplicationContext是实现类</p><p>上边代码通过解析spring.xml文件 获取对应的bean对象以及包的扫描路径后，</p></blockquote><h2 id="Bean的创建生命周期">Bean的创建生命周期***</h2><p><img src="../assets/image-20231120105410148.png" alt="image-20231120105410148"></p><h3 id="1-获得UserService运行时类">1.获得UserService运行时类</h3><blockquote><p>调用UserService.getClass()方法</p></blockquote><h3 id="2-推断构造方法">2.推断构造方法</h3><p>推断构造方法的过程中，会选择出一个构造方法【有参/无参】【有参的话会推断构造方法的参数值】</p><blockquote><p>下图是一个无参，一个有参构造方法，则会调用默认的无参构造方法</p></blockquote><p><img src="../assets/image-20231120105455948.png" alt="image-20231120105455948"></p><blockquote><p>默认一个构造方法，如果有无参则用无参，否则只能放置唯一一个自定义构造方法，否则报错【一个的话就用那个，多个的话用无参】</p></blockquote><blockquote><p>在确定有参构造的传入参数值时候，会先确定类型，根据传入参数的类型去找spring容器中的对象；如果spring容器中有该类且多个bean对象，则会跟上边的机制一样，多个选传参名称相同的bean对象【在spring容器中有类似于map的表Map&lt; key,value&gt; key是对象的名称，value则是对象值，根据对应的类型先选中对应的map，再根据名称key选中value】，只有一个bean对象的话，则不用管传入参数的名称，直接选那一个就行</p></blockquote><blockquote><p>下边演示如何在项目中定义多个bean对象</p></blockquote><p><code>test.java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*//        测试spring容器中几个orderService对象</span></span><br><span class="line"><span class="comment">//        一共三个 两个是在appconfig中定义的@Bean对象，一个是在orderService中使用@Conpont定义的</span></span><br><span class="line"><span class="comment">        System.out.println(context.getBean(&quot;orderService&quot;));</span></span><br><span class="line"><span class="comment">        System.out.println(context.getBean(&quot;orderService1&quot;));</span></span><br><span class="line"><span class="comment">        System.out.println(context.getBean(&quot;orderService2&quot;));*/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>AppConfig.java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.southwind.springboot001;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.southwind.springboot001.service.OrderService;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.ComponentScan;</span><br><span class="line"><span class="keyword">import</span> org.springframework.core.annotation.Order;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ComponentScan(&quot;com.southwind&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AppConfig</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//    @Bean</span></span><br><span class="line"><span class="comment">//    public OrderService orderService1()&#123;</span></span><br><span class="line"><span class="comment">//        return new OrderService();</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"><span class="comment">//    @Bean</span></span><br><span class="line"><span class="comment">//    public OrderService orderService2()&#123;</span></span><br><span class="line"><span class="comment">//        return new OrderService();</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>orderService.java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.southwind.springboot001.service;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OrderService</span> &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>在java项目中，使用@Bean定义的bean对象 会覆盖 @component 定义的对象【这个是可以在项目中配置的，配置允许覆盖的情况下，是可以直接进行覆盖的】</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.southwind.springboot001;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.southwind.springboot001.service.OrderService;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.ComponentScan;</span><br><span class="line"><span class="meta">@ComponentScan(&quot;com.southwind&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AppConfig</span> &#123;</span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> OrderService <span class="title function_">orderService</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">OrderService</span>();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.southwind.springboot001.service;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">OrderService</span> &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>上边AppConfig.java代码中的orderService 对象覆盖了spring容器中使用@Component配置的orderService 对象</p></blockquote><blockquote><p>当在spring容器系统中有多个bean对象的时候，有参的构造方法的参数赋值可能会有多个选择，但是当参数的名称与map中所含有的所有bean对象名称不同的时候，就会报错，找不到bean对象</p></blockquote><p><img src="../assets/image-20231120230423458.png" alt="image-20231120230423458"></p><blockquote><p>当在spring容器系统中含有多个有参构造方法，但是没有无参构造方法的时候，系统就会报错，因为找不到默认的构造方法报错 这个时候，可以使用@Autoried注解 指定某一个方法，将方法进行制定后，在构造对象的时候，会使用该方法进行构造对象</p></blockquote><h3 id="3-依赖注入">3.依赖注入</h3><h4 id="spring容器怎样为UserService类中修饰了某个注解的属性赋值">spring容器怎样为UserService类中修饰了某个注解的属性赋值</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//        spring如何确定某个类的对象中的有几个属性被某个注解修饰【以UserService类 @Autowired注解为例】</span></span><br><span class="line"><span class="comment">//        for (Field field : userService.getClass().getFields()) &#123;//获得该类中所有对象属性并遍历</span></span><br><span class="line"><span class="comment">//            if (field.isAnnotationPresent(Autowired.class))&#123;// 如果该属性有某个注解修饰</span></span><br><span class="line"><span class="comment">//                field.set(userService,***);//对该对象的这个注解赋值为 ***</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br></pre></td></tr></table></figure><blockquote><p>DI依赖注入 也会先根据对应的类型 然后再根据对应的名称进行赋值，在spring容器中找出对应的对象给属性赋值</p></blockquote><h3 id="4-初始化前（调用方法）">4.初始化前（调用方法）</h3><blockquote><p>初始化前，为某个类中方法的属性执行特殊函数，使用@PostContruct注解修饰</p></blockquote><p><img src="../assets/image-20231120104235910.png" alt="image-20231120104235910"></p><h4 id="spring如何直接调用UserService类的对象中修饰了某个注解的几个方法">spring如何直接调用UserService类的对象中修饰了某个注解的几个方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">////        spring如何直接调用UserService类的对象中修饰了某个注解的几个方法【以UserService类 @PostConstruct注解为例】</span></span><br><span class="line"><span class="comment">//        for (Method method : userService.getClass().getMethods()) &#123;</span></span><br><span class="line"><span class="comment">//            if (method.isAnnotationPresent(PostConstruct.class))&#123;</span></span><br><span class="line"><span class="comment">//                method.invoke(userService,null);</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br></pre></td></tr></table></figure><h3 id="5-初始化（implement-接口）">5.初始化（implement 接口）</h3><p>初始化时设定InitializingBean接口，并重写必须要的afterPropertiesSet方法</p><p><img src="../assets/image-20231120103328903.png" alt="image-20231120103328903"></p><h4 id="如何判断一个对象是否实现InitializingBean接口">如何判断一个对象是否实现InitializingBean接口</h4><blockquote><p>先判断是是否实现接口，然后将对象强转成某个接口来执行接口的方法</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(（UserService1）.<span class="keyword">instanceof</span>(InitializingBean))&#123;</span><br><span class="line"></span><br><span class="line">((InitializingBean)对象).afterPropertiesSet()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Spring源码</p></blockquote><p><img src="../assets/image-20231120104904164.png" alt="image-20231120104904164"></p><h3 id="6-初始化后-AOP">6.初始化后(AOP)</h3><blockquote><p>代理对象切面编程，最后是代理对象成为我们使用的bean对象</p></blockquote><blockquote><p>切面相关的注解 @EnableAspectJAutoProxy</p></blockquote><p>会生成 target  对象 使用target 对象 进行代理对象实现</p><p>利用target 执行代理对象的业务，完成数据的赋值</p><p>为什么不给代理对象的orderService 数据赋值呢 因为赋值之后 没什么用，所以就不去赋值了</p><p>好像是代理对象只是收到了接口，就是知道有哪些方法，但是具体的实现还是只有委托对象里才有实现方法，不能说直接把实现方法又拷一份在代理对象里吧</p><p>结合楠哥的视频 深入理解spring的AOP【面向切面变成】与【动态代理和反射机制实现】</p><p><img src="../assets/image-20231121160736449.png" alt="image-20231121160736449"></p><blockquote><p>怎样确定一个对象有没有被切</p></blockquote><p><img src="../assets/image-20231121222002196.png" alt="image-20231121222002196"></p><ol><li>先找出spring容器中所有的切面对象</li><li>遍历所有的切面对象</li><li>遍历所有的切面对象的方法</li><li>找出与当前的UserService对象相关的切面方法<ol><li>如果有与当前UserService对象相关的方法的话 则说明当前对象被切了</li><li>这时候将所有的与UserService对象的方法存到一个缓存中 map&lt; UserService.class,<a href="http://method.name">method.name</a> &gt;,方便在后边的执行切面的时候进行直接访问【也就是在子类调用父类的方法的时候直接从map中进行取值】</li></ol></li></ol><h3 id="Spring事务">Spring事务</h3><blockquote><p>打开事务相关的注解</p><p>@EnanleTransactionManagement</p></blockquote><p><img src="../assets/image-20231122084449911.png" alt="image-20231122084449911"></p><p>Spring事务的代理对象执行某个方法时的步骤：</p><ol><li>判断当前执行的方法是否存在@Transactional注解</li><li>如果存在，则利用事务管理器（TransactionMananger）新建一个数据库连接</li><li>修改数据库连接的autocommit为false</li><li>执行target.test()，执行程序员所写的业务逻辑代码，也就是执行sql</li><li>执行完了之后如果没有出现异常，则提交，否则回滚</li></ol><blockquote><p>事务开启失效的现象和原因</p></blockquote><blockquote><p>调用代理对象才能够产生报错，不然的话 会调用的是当前执行方法的普通对象，普通对象中不存在这代理冲突，所以不会抛出异常</p></blockquote><h4 id="传播-的propagation-never">传播 的propagation = never</h4><p>为什么 在加了@configuration 注解后 事务才会生效 否则就跟没有事务一样，抛出异常仍然可以插入数据呢？</p><p><img src="../assets/image-20231122093422081.png" alt="image-20231122093422081"></p><blockquote><p>jdbcTemplate 与 transationManager 拿到的DataSource不是一个对象 导致执行的时候不能利用事务管理器 管理的commit的事务代理模式进行事务管理，所以不是事务操作</p></blockquote><h3 id="实验">实验</h3><ol><li>测试事务在抛出异常的时候 能否插入到数据库中【能够插入】——虽然抛出异常 但是却能插入到<ol><li>appConfig 中少写了 一个@Configuration注解 ==待解决==</li><li>查看事务执行的 代理对象 与target 哪个在执行test方法 【理解代理对象执行事务的过程】<img src="../assets/image-20231122090918364.png" alt="image-20231122090918364"><img src="../assets/image-20231122090932469.png" alt="image-20231122090932469"><img src="../assets/image-20231122090951331.png" alt="image-20231122090951331"><ol><li>事务代理对象【使用事务管理器】 VS 正常的代理对象<img src="../assets/image-20231122090909080.png" alt="image-20231122090909080"></li></ol></li></ol></li><li>传播 实现事务失效【本该抛出异常 但没有】<ol><li>没抛异常 是因为使用的普通对象执行的当前方法 普通对象没有事务关系，所以 对于事务注解 是没有用的<img src="../assets/image-20231122090232329.png" alt="image-20231122090232329"></li><li>事务失效纠正【新建一个UserServiceBean 并设置为代理对象执行a.()方法】<img src="../assets/image-20231122090644822.png" alt="image-20231122090644822"><img src="../assets/image-20231122090530789.png" alt="image-20231122090530789"><ol><li>UserServiceBean对象会在spring容器中存在一个代理对象，在执行依赖注入后，调用的是代理对象，所以UserServiceBean.a()方法 会导致调用的是事务对象，所以报错</li></ol></li><li>直接自己依赖注入UserService代理对象执行即可 VS  this.a()<ol><li>自己注入自己  <img src="../assets/image-20231122090838974.png" alt="image-20231122090838974"></li></ol></li></ol></li></ol><p>@Transactional 注解 是不是自己类的代理对象在执行这个方法 如果是本类的代理对象执行的话则会never报错，不然的话会正常执行</p><p>同一个DataSource 事务才会有用 不然的话 会执行失败</p><h1>2.手写spring源码</h1><h3 id="2-1懒加载与单例对象">2.1懒加载与单例对象</h3><blockquote><p>懒加载（@lazy）与非懒加载</p><p>​懒加载对象是在spring对象生成的时候不生成对象，只有在getBean()函数执行的时候，才会创建bean对象</p><p>​非懒加载对象是在spring对象生成的时候就直接生成对象，到时候在getBean()的时候直接拿就可以</p><p>懒加载和非懒加载对象 在自己试验运行的时候 能够看出区别: 懒加载对象 的构造方法会和test()函数的执行位置靠近，但是非懒加载的对象的构造函数 执行 是在spring对象创建的时候就执行了，test方法在后边执行</p></blockquote><blockquote><p>单例对象与原型对象</p><p>​单例对象 在多次getBean() 对象执行的时候，拿到的是同一个对象，获取的是相同对象</p><p>​原型对象（也就是多例对象） 在执行多次getBean() 方法的时候，每次都会新创建一个对象，获得的是不同的对象</p></blockquote><blockquote><p>懒加载的单例方式</p></blockquote><p><img src="../assets/image-20231126100325796.png" alt="image-20231126100325796"></p><blockquote><p>非懒加载的单例方式</p><p>在spring启动的时候就创建对象，</p></blockquote><p><img src="../assets/image-20231126100752995.png" alt="image-20231126100752995"></p><blockquote><p>非懒加载方式 在创建完成对象之后 在进行类加载等操作</p></blockquote><p><img src="../assets/image-20231126100820877.png" alt="image-20231126100820877"></p><blockquote><p>先进行构造方法执行，在实现方法调用</p></blockquote><p><img src="../assets/image-20231126100906138.png" alt="image-20231126100906138"></p><h3 id="2-2手写spring过程记录">2.2手写spring过程记录</h3><p><img src="../assets/image-20231126101058083.png" alt="image-20231126101058083"></p><blockquote><p>BeanDefinition BeanPostProcessor 概念以及意义</p></blockquote><h4 id="1-创建工程">1.创建工程</h4><p><img src="../assets/image-20231126101241790.png" alt="image-20231126101241790"></p><blockquote><p>UserService 进行bean对象测试使用，用于创建对象类</p><p>Test进行测试我们手写的spring</p><p>Spring是自己手写的spring源码</p></blockquote><h4 id="2-创建spring类——MzlApplicationContext">2.创建spring类——MzlApplicationContext</h4><blockquote><p>因为正常使用的时候会new ApplicationContext 对象 ，所以模拟的话 也要先写一个这种类</p></blockquote><blockquote><p>类进行传参   所以需要写一个属性，接收传入的参数 以及对应属性的构造方法</p></blockquote><blockquote><p>提供一个getBean（）方法  根据传入的beanName获取bean对象</p></blockquote><p><img src="../assets/image-20231126101712705.png" alt="image-20231126101712705"></p><h3 id="3-配置扫包-——AppConfig">3.配置扫包 ——AppConfig</h3><p><img src="../assets/image-20231126101859985.png" alt="image-20231126101859985"></p><p>在spring中创建ComponentScan 注解，用于在AppConfig 中进行包扫描</p><h3 id="4-Component注解取名字">4.Component注解取名字</h3><p><img src="../assets/image-20231126102046169.png" alt="image-20231126102046169"></p><blockquote><p>写一个Component 注解 用于UserService 类上 进行重命名</p></blockquote><h3 id="5-扫描bean对象-分析底层">5.扫描bean对象 分析底层***</h3><blockquote><p>自己写的spring容器的话，需要先创建出一个单例bean，默认是非懒加载的</p><p>但是在创建之前，spring容器要先确定有多少bean对象要创建，也就是对应着使用多少个类以及哪几个类需要进行创建对象放到spring容器中，所以需要先进行<strong>扫描</strong>，根据传入的appConfig路径进行类扫描</p></blockquote><blockquote><p>先模拟扫描，在进行类创建</p></blockquote><h4 id="1-获取对应的扫描路径">1.获取对应的扫描路径</h4><p><img src="../assets/image-20231126102542798.png" alt="image-20231126102542798"></p><h4 id="2-根据扫描路径-获取路径下边的类并进行解析注解">2.根据扫描路径 获取路径下边的类并进行解析注解</h4><blockquote><p>target 是程序运行时需要扫描的路径，而并非我们写好的源代码，所以要现获取target 路径</p></blockquote><p><img src="../assets/image-20231126102659515.png" alt="image-20231126102659515"></p><p>==ZhouyuApplicationContext.java  是由Application Class Loader进行加载的==</p><p><img src="../assets/image-20231126103606408.png" alt="image-20231126103606408"></p><blockquote><p>复制过来结果就是下边的</p><p>Application Class Loader使用java加载的路径会根据class Path 去找，同时最后一行就是我们要找的classes路径</p></blockquote><p><img src="../assets/image-20231126103620189.png" alt="image-20231126103620189"></p><blockquote><p>类加载器有固定的加载路径，其他两个如上边的路径</p><p>Application Class Loader 根据class path 去加载类</p><p>我们所有写的源码在经过编译后 会放入到classes 中也就是Application Class Loader 根据class path下边Application Class Loader 就是管理的classes路径，所以利用Application Class Loader 就可以获得对应的classes路径</p></blockquote><h4 id="3-根据注解路径获取绝对路径-Service下的类文件">3.根据注解路径获取绝对路径 Service下的类文件</h4><blockquote><p>这三行代码的作用是获取指定包下的资源路径。让我来解释一下：</p><ol><li><code>ClassLoader classLoader = MzlApplicationContext.class.getClassLoader();</code> 这一行代码通过<code>MzlApplicationContext</code>类的类加载器来获取一个<code>ClassLoader</code>对象。类加载器用于加载类和其他资源。</li><li><code>String path= &quot;com/zhouyu/service&quot;;</code> 这一行代码定义了一个字符串变量<code>path</code>，内容为<code>&quot;com/zhouyu/service&quot;</code>，表示需要获取的资源路径。</li><li><code>URL resource = classLoader.getResource(path);</code> 这一行代码使用前面获取的类加载器来获取指定路径的资源，返回一个<code>URL</code>对象，该对象指向指定资源的位置。</li></ol><p>综合这三行代码的作用就是：通过类加载器获取指定包下的资源路径，并将路径存储在<code>URL</code>对象<code>resource</code>中，以便后续使用该资源。</p></blockquote><p><img src="../assets/image-20231126105840975.png" alt="image-20231126105840975"></p><blockquote><p>根据当前的相对路径 获取对对于相对路径而言的classes中的上一层路径 也就是service路径</p><p>上图第26行代码是根据path 查找classLoader中含有path的路径，改路径指向资源的存储位置，将改路径返回封装为url</p></blockquote><p><img src="../assets/image-20231126143543649.png" alt="image-20231126143543649"></p><blockquote><p>获取到对应的service下的文件路径</p><p><img src="../assets/image-20231126144236387.png" alt="image-20231126144236387"></p></blockquote><h4 id="4-判断路径下边的文件是否有component注解【是否是bean对象】">4.判断路径下边的文件是否有component注解【是否是bean对象】</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">            <span class="comment">/****</span></span><br><span class="line"><span class="comment">            * 解析一个文件是否有某个注解的时候，需要先获取到对应的类来进行解析</span></span><br><span class="line"><span class="comment">            * 现在获取到的是.class文件路径 D:\2023.1.29java\IDEA_project\Spring_demo\target\classes\com\zhouyu\service\UserService.class</span></span><br><span class="line"><span class="comment">            * 需要更改成&quot;com.zhouyu.service.UserService&quot;路径后 在利用路径和classload加载器获得对应的类</span></span><br><span class="line"><span class="comment">            ****/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//先截取路径从 D:\2023.1.29java\IDEA_project\Spring_demo\target\classes\com\zhouyu\service\UserService.class</span></span><br><span class="line"><span class="comment">//                变成 com\zhouyu\service\UserService</span></span><br><span class="line"><span class="comment">//                在使用替换\ 成.</span></span><br></pre></td></tr></table></figure><blockquote><p>判断注解的案例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">absolutePath = absolutePath.substring(absolutePath.indexOf(<span class="string">&quot;com&quot;</span>),absolutePath.indexOf(<span class="string">&quot;.class&quot;</span>));</span><br><span class="line">absolutePath = absolutePath.replace(<span class="string">&quot;\\&quot;</span>,<span class="string">&quot;.&quot;</span>);</span><br><span class="line">System.out.println(absolutePath);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    Class&lt;?&gt; clazz = classLoader.loadClass(absolutePath);</span><br><span class="line">    <span class="keyword">if</span> (clazz.isAnnotationPresent(Component.class)) &#123;<span class="comment">// 判断是否有Component注解</span></span><br><span class="line">        <span class="comment">// 有注解则应该设定为bean对象加载到spring容器中</span></span><br><span class="line">        System.out.println(<span class="string">&quot;2&quot;</span>);</span><br><span class="line">        System.out.println(clazz+);</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>clazz.isAnnotationPresent(Component.class)  判断是否有Component注解该类</p></blockquote><h4 id="5-判断是单例还是原型bean对象">5.判断是单例还是原型bean对象</h4><h4 id="6-设定beanDefinition对象存储bean特性">6.设定beanDefinition对象存储bean特性***</h4><p><img src="../assets/image-20231127113752127.png" alt="image-20231127113752127"></p><p>beanName取值</p><ol><li>直接用注解上的配置值</li><li>利用类名小写首字母配置beanName【Spring源码写法】</li></ol><h4 id="7-存储所有bean对象">7.存储所有bean对象</h4><h3 id="6-getBean-解析">6.getBean() 解析</h3><h4 id="1-解析beanDefinitionMap">1.解析beanDefinitionMap</h4><p>根据传入的beanName获取Bean对象</p><blockquote><p>没有对应的beanName对象 则抛出异常，否则判断是单例还是原型</p></blockquote><p><img src="../assets/image-20231126152332664.png" alt="image-20231126152332664"></p><h4 id="2-单例处理">2.单例处理</h4><blockquote><p>根据对应的beanDefinition 对象，获取scope值，如果scope值等于singleton 或者没有scope 则说明是单例bean对象，需要进行下边处理</p></blockquote><blockquote><p>单例bean对象 在整个spring使用过程中，一个单例类只存在一个对应的bean对象，所以需要将所有的单例类存储在一个map中==【单例池】==，以供所有的程序使用该对象</p></blockquote><blockquote><p>所以在getBean(beanName)函数中，首先根据传入的beanName去map中找，如果没有标明之前从没有创建过对应beanName的bean对象，则需要调用创建函数去创建一个bean对象并存入到map中，map的key是beanName，value是该对象；<br>如果在map中存在对应beanName的bean对象，则直接获取并返回就可以</p></blockquote><h4 id="3-原型处理">3.原型处理</h4><blockquote><p>根据对应的beanDefinition 对象，获取scope值，如果scope值等于prototype则说明是原型bean对象，需要进行下边处理</p></blockquote><blockquote><p>原型bean 需要在每次调用getBean()函数时候，创建一次对应的类对象，所以直接调用createBean（）函数 直接返回即可</p></blockquote><p><img src="../assets/image-20231127095619315.png" alt="image-20231127095619315"></p><h3 id="7-创建bean对象">7.创建bean对象</h3><blockquote><p>经过扫包之后，将所有的类扫描并获取到对应的beanDefinitionMap后，可以根据map内容进行创建</p></blockquote><h4 id="1-单例对象">1.单例对象</h4><p>遍历beanDefinitionMap，获取对应的beanName 和 beanDefinition对象，如果该类的scope==是单例的话，表明可以先创建出来存入到单例池中==，等在调用该bean对象的时候可以直接在单例池中获取即可</p><h4 id="2-原型对象">2.原型对象</h4><p>如果是原型对象的话，可以在每次调用getBean() 的时候，直接调用createBean方法，根据传入的参数 beanName和 beanDefinition对象进行创建并返回，调用一次创建并返回一次</p><h4 id="3-创建实现">3.创建实现</h4><blockquote><p>在创建bean的时候， 可以根据beanDefinition对象，先获取到该类的getType()方法 获取对应的class，然后直接利用反射机制去调用对应的构造方法，==这里使用默认的无参构造方法==， 直接将该对象返回即可</p></blockquote><p><img src="../assets/image-20231127103050038.png" alt="image-20231127103050038"></p><h3 id="8-依赖注入">8.依赖注入</h3><blockquote><p>依赖注入是在创建出 普通的对象之后【调用对应的构造方法之后】进行依赖注入</p></blockquote><blockquote><p>依赖注入 设定值的时候，需要先根据对应的属性类型 去找【单例池 或者 beanDefinitionMap中  找到之后再去根据对应的属性值进行判断，如果找到多个对于该属性的值，那么就设定该属性名进行去找对象，并进行赋值】</p></blockquote><h5 id="依赖注入实现">依赖注入实现</h5><p><img src="../assets/image-20231127204510803.png" alt="image-20231127204510803"></p><blockquote><p>代码实现上 直接根据beanName进行的赋值，而没有先去找对应的类，然后在去找对应该类中所有对象中名字为beanName的对象赋值</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">                instance = clazz.getConstructor().newInstance();<span class="comment">// 利用反射 直接生成对应的实例对象【这里直接调用了无参构造，没有进行</span></span><br><span class="line"><span class="comment">//                推断构造方法的过程// 】</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**依赖注入的实现 需要在调用完构造方法之后，依次判定所有的属性，判断如果属性中有Autowired注解的话，则对该属性进行赋值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//                先用反射获得所有的属性，然后判断是否有注解修饰</span></span><br><span class="line">                <span class="keyword">for</span> (Field field : clazz.getDeclaredFields()) &#123;<span class="comment">//这个方法是获取的对应的私有属性操作</span></span><br><span class="line"><span class="comment">//                for (Field field : clazz.getFields()) &#123;// 这个方法是获取的public的属性</span></span><br><span class="line">                    <span class="keyword">if</span> (field.isAnnotationPresent(Autowired.class))&#123;</span><br><span class="line">                        field.setAccessible(<span class="literal">true</span>);<span class="comment">// 这里所有用反射赋值私有属性的时候 必须先进行设定 允许权限</span></span><br><span class="line">                        field.set(instance,getBean(field.getName()));<span class="comment">//给instance这个对象的filed属性赋值 ,这里直接根据所对应的属性的名字【也就是依赖注入的beanName】进行赋值了</span></span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20231127204315152.png" alt="image-20231127204315152"></p><h3 id="9-初始化">9.初始化</h3><h4 id="1-重写InitializingBean接口">1. 重写InitializingBean接口</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.spring;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">InitializingBean</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">afterPropertiesSet</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-UserService测试用例">2.UserService测试用例</h4><blockquote><p>测试用例 必须实现一个InitializingBean接口</p><p>如果要执行初始化的话，需要先实现一个InitializingBean接口，并重写对应的afterPropertiesSet()方法</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhouyu.service;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.spring.Autowired;</span><br><span class="line"><span class="keyword">import</span> com.spring.Component;</span><br><span class="line"><span class="keyword">import</span> com.spring.InitializingBean;</span><br><span class="line"><span class="keyword">import</span> com.spring.Scope;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component(&quot;userService&quot;)</span></span><br><span class="line"><span class="meta">@Scope(&quot;prototype&quot;)</span></span><br><span class="line"><span class="comment">//如果要执行初始化的话，需要先实现一个InitializingBean接口，并重写对应的afterPropertiesSet()方法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UserService</span> <span class="keyword">implements</span> <span class="title class_">InitializingBean</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String beanName=<span class="string">&quot;moa&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> OrderService orderService;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">test</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(orderService);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">afterPropertiesSet</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;这里进行初始化&quot;</span>);</span><br><span class="line">        <span class="comment">// 初始化的时候是为了执行特定功能，例如admin 从数据库中查出数据并赋值</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-spring判定">3.spring判定</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//                执行完成依赖注入后，再进行初始化，判断是否实现了InitializingBean接口，并利用反射调用执行初始化的afterPropertiesSet()方法</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (instance <span class="keyword">instanceof</span> InitializingBean) &#123;</span><br><span class="line">                    ((InitializingBean)instance).afterPropertiesSet();<span class="comment">//如果</span></span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure><h3 id="10-BeanPostProcessor">10 BeanPostProcessor ***</h3><blockquote><p>实现初始化前和初始化后，都需要一个BeanPostProcessor接口 ，在接口中重写两个方法完成初始化前和初始化后</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.spring;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BeanPostProcessor</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">default</span> Object <span class="title function_">postProcessBeforeInitialization</span><span class="params">(Object bean, String beanName)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">default</span> Object <span class="title function_">postProcessAfterInitialization</span><span class="params">(Object bean, String beanName)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> bean;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="初始化前">初始化前</h4><h4 id="初始化后">初始化后</h4><h3 id="BeanPostProcessor扩展">BeanPostProcessor扩展</h3><h4 id="1-Aop底层逻辑-使用BeanPostProcessor">1.Aop底层逻辑 使用BeanPostProcessor</h4><blockquote><p>如果定义了一个ZhouyuBeanProcessor的时候 【实现了BeanPostProcess接口】，就需要重写PostProcessAfterInitialization方法，那么每一个spring的bean对象都会调用这个方法，然后将该方法进行执行，所以如果想要只是针对某一个bean对象的话，就需要进行判断，判断执行该方法的beanName 在实现相应的业务</p></blockquote><p><img src="../assets/image-20231128204200587.png" alt="image-20231128204200587"></p><blockquote><p>图片中所有的bean对象都会执行相应的方法</p></blockquote><p><img src="../assets/image-20231128204526390.png" alt="image-20231128204526390"></p><p>如果只针对UserService 则按照上边的if判断</p><h5 id="AOP-动态代理实现">AOP 动态代理实现</h5><blockquote><p>下边的PostProcessAfterInitialization 方法，根据传入的bean对象以及对应的beanName值 进行代理对象创建【使用java的动态代理机制，实现了传入bean的所有的接口 并返回一个代理对象 proxy对象】</p></blockquote><p><img src="../assets/image-20231128205427799.png" alt="image-20231128205427799"></p><blockquote><p>上边的图片代码 就是模拟的aop实现，先在PostProcessAfterInitialization方法中进行动态代理，然后这个方法在哪里调用呢？</p></blockquote><blockquote><p>这个方法 在createBean() 方法里边进行调用</p></blockquote><blockquote><p>下边三张图片是createBean的方法，在创建bean对象的时候，一开始传入的是普通对象，普通对象进行依赖注入给属性赋值，然后进行初始化，在之后调用PostProcessAfterInitialization 方法【这时候该方法生成一个代理对象并放入到单例池中保存为普通bean对象的代理对象】getBean()的时候 就是获得的普通对象的代理对象</p></blockquote><p><img src="../assets/image-20231128205940195.png" alt="image-20231128205940195"></p><p><img src="../assets/image-20231128210133831.png" alt="image-20231128210133831"></p><p><img src="../assets/image-20231128210225696.png" alt="image-20231128210225696"></p><blockquote><p>上边在调用PostProcessAfterInitialization 方法的时候，传入的是普通的对象，经过这个方法之后，返回的是该对象的代理对象，并在代理对象中调用普通对象的方法实现切面逻辑以及业务方法的调用</p></blockquote><p><img src="../assets/image-20231128205826329.png" alt="image-20231128205826329"></p><blockquote><p>注意在调用业务逻辑的时候，是使用的普通对象进行的业务逻辑方法的执行，而不是代理对象执行的该方法</p></blockquote><h4 id="2-利用spring的BeanPostProcessor机制进行扩展">2.利用spring的BeanPostProcessor机制进行扩展</h4><blockquote><p>利用spring的PostProcessAfterInitialization  和 PostProcessBeforeInitialization 方法实现其他功能</p></blockquote><blockquote><p>例如自己定义一个自己的注解 【修饰字段值】，该注解实现的功能 是调用自己注解的时候，能够给调用该注解的时候，将注解内的值传给该属性值,在获得该属性值的时候 属性值就自动被赋值</p></blockquote><blockquote></blockquote><p><img src="../assets/image-20231128211136714.png" alt="image-20231128211136714"></p><p>在bean的初始化前进行调用方法，【在方法中实现刚刚的注解功能】</p><blockquote><p>这里 使用自己定义的方法，将所有的属性进行判断，如果有属性，则获得自己写的注解的内容并把内容赋值给对应的属性</p></blockquote><p><img src="../assets/image-20231128211304976.png" alt="image-20231128211304976"></p><h4 id="3-spring的aware回调机制实现">3. spring的aware回调机制实现</h4><blockquote><p>在每一个spring的bean对象中都有一个BeanName属性，如果该对象想给BeanName 值赋值的话，就需要利用下边的BeanNameAware 接口</p><p><img src="../assets/image-20231128211845437.png" alt="image-20231128211845437"></p></blockquote><h5 id="模拟BeanNameAware接口实现">模拟BeanNameAware接口实现</h5><blockquote><p>这个aware的回调 是在spring的依赖注入之后，初始化之前进行调用</p></blockquote><p><img src="../assets/image-20231128212507749.png" alt="image-20231128212507749"></p><p>在实现的时候 就需要进行判断当前的bean对象 是否实现了回调接口，如果实现的话就将该实例强制转换成该接口并调用该方法即可</p><h3 id="11-总结">11.总结</h3><blockquote><p>先手写了spring</p><p>扫描，创建，依赖注入，beanDefinition  beanPostProcessor</p></blockquote><h4 id="1-扫描">1.扫描</h4><p>先扫描路径，根据路径去找有component注解的所有类，将所有类进行扫描反射创建beanDefinition对象放入到BeanDefinitionMap中</p><p>同时额外的判断这个类是不是一个BeanPostProcessor，是的话就会进行实例化，存储到BeanPostProcessorList中，在初始化前和初始化后进行调用 完成相应的功能</p><p>然后在扫描所有的类，如果有单例对象，则直接创建并存入到单例池中，方便后便直接进行调用</p><p><img src="../assets/image-20231128213008780.png" alt="image-20231128213008780"></p><h4 id="2-创建">2.创建</h4><p>创建一个bean的时候，先进行实例化，然后完成依赖注入属性，然后完成接口检查，【完成各种回调】</p><p>，并执行所有的BeanPostProcessor 方法==【在扫描的时候创建的，在这里执行】==然后判断是否需要进行InitializingBean 进行初始化</p><h4 id="3-理解spring容器">3.理解spring容器</h4><p>可以把spring容器理解成 一个BeanFactory 也可以理解成一个 ApplicationContext  或者是一个BeandefinitionMap 用来装载bean对象</p><h1>4.bean生命周期上</h1><blockquote><p>这节课主要讲的内容</p><blockquote><p>spring的扫描源码</p><p>合并beanDefinition</p><p>创建非懒加载的单例bean</p><p>扫描类的过程【涉及到的beanFactory  工厂】</p></blockquote></blockquote><p><img src="../assets/image-20231203220827557.png" alt="image-20231203220827557"></p><h3 id="合并的beanDefinition">合并的beanDefinition</h3><blockquote><p>获取BeanDefinition  就是从RootBeanDefinition 中取出合并后的beanDefinition</p></blockquote><p><img src="../assets/image-20231203221029560.png" alt="image-20231203221029560"></p><blockquote><p>RootBeanDefinition 定义</p></blockquote><p><img src="../assets/image-20231203220942272.png" alt="image-20231203220942272"></p><blockquote><p>mergeBeanDefinition 是将子类与父类 进行合并之后的bean对象，而不是直接使用的预定义好的BeanDefinition对象</p><p>【合并的beanDefinition 对象 内部的属性值 完成了子beanDefinition 与父beanDefinition 的属性赋值】</p></blockquote><p><img src="../assets/image-20231203221157693.png" alt="image-20231203221157693"></p><blockquote><p>合并过程的话 如果有一个父bean 一个字bean  那么合并的话 并不会直接覆盖两个bean中的任何一个，而是重新生成一个bean对象 进行复制后并完成相应逻辑的赋值【因为两个中有可能任意一个可能是其他的bean的父亲，如果被修改了 则其他的子bean会被修改了】</p></blockquote><blockquote><p>在合并过程中 涉及到递归 可能你的父亲也有父亲需要先进行赋值，所以先完成父亲的属性赋值，然后在完成自己根据父亲的值，如果自己自定义，则不从父亲那里获取值，如果自己传入的值没有定义，则从父亲那里取值后设置到当前bean对象中并存入到MergeBeanDefinition 中</p></blockquote><p><img src="../assets/image-20231204093105587.png" alt="image-20231204093105587"></p><blockquote><p>对应于第三节课的beanFactory  会生成两种对象【一种是执行了全部的bean生命周期，而另一种只是执行了对应的初始化后生命周期】，在获取了对应的bean对象之后，判断是否继承了FactoryBean接口；</p><p>如果没有获取到对name的bean单例对象，那么就需要判断容器中有没有对应的beanDefinition对象并判断该beanDefinition对象是否实现了对应的接口如果没有的话，需要判断该beanDefinition对象的父类有没有实现对应接口</p></blockquote><p><img src="../assets/image-20231204094819085.png" alt="image-20231204094819085"></p><blockquote><p>子BeanFactory 和 父BeanFactory</p></blockquote><p><img src="../assets/image-20231204103720149.png" alt="image-20231204103720149"></p><blockquote><p>这里做一些测试</p><p><img src="../assets/image-20231204103849362.png" alt="image-20231204103849362"></p><blockquote><p>如果使用了SmartFactoryBean() 接口 可以实现在调用getBean() 方法之前，就可以在单例池中获得对应的ZhouyuFactoryBean，</p><p>在上边的缓存池中 能够找到利用ZhouyuFactoryBean创建的User对象 这些对象在启动spring的时候就已经被创建</p></blockquote></blockquote><h1>第二阶段自研spring框架</h1><blockquote><p>除了看课程之外，spring源码的学习方式</p></blockquote><p><img src="../assets/image-20231203213923685.png" alt="image-20231203213923685"></p><blockquote><p>spring官方文档 <a href="https://docs.spring.io/spring-framework/reference/index.html">Spring Framework Documentation :: Spring Framework</a></p><p>学习设计模式+ 常用的注解和标签的作用并记录</p></blockquote><p><img src="../assets/image-20231203214044667.png" alt="image-20231203214044667"></p><p>RC 发版前的确定版本</p><p><img src="../assets/image-20231203214130250.png" alt="image-20231203214130250"></p><blockquote><p>实例</p></blockquote><p><img src="../assets/image-20231203214207272.png" alt="image-20231203214207272"></p><p><img src="../assets/image-20231203214243827.png" alt="image-20231203214243827"></p><blockquote><p>课程自研实现一个spring框架</p></blockquote><p><img src="../assets/image-20231203214347197.png" alt="image-20231203214347197"></p><blockquote><p>自研框架模型</p></blockquote><p><img src="../assets/image-20231203214629754.png" alt="image-20231203214629754"></p><blockquote><p>来自Masked Motion Predictors are Strong 3D Action Representation Learners**蒙面动作预测器是强大的3D动作表示学习者*</p></blockquote><p>对比学习方法</p><p>[23]Linguo Li, Minsi Wang, Bingbing Ni, Hang Wang,<br>Jiancheng Yang, andWenjun Zhang. <strong>3D human action repre-</strong><br><strong>sentation learning via cross-view consistency pursuit.</strong> In Pro-<br>ceedings of the IEEE/CVF Conference on Computer Vision<br>and Pattern Recognition (CVPR), pages 4741–4750, 2021.<br>2, 3, 6, 7</p><p>[48]Fida Mohammad Thoker, Hazel Doughty, and Cees GM<br>Snoek. <strong>Skeleton-contrastive 3D action representation learn-</strong><br><strong>ing.</strong> In Proceedings ofthe ACM International Conference on<br>Multimedia (ACM MM), pages 1655–1663, 2021. 2, 3, 5, 6,<br>7<br>[49] Guo Tianyu, Liu Hong, Chen Zhan, Liu Mengyuan, Wang<br>Tao, and Ding Runwei. <strong>Contrastive learning from ex-</strong><br><strong>tremely augmented skeleton sequences for self-supervised</strong><br><strong>action recognition.</strong> In Proceedings of the AAAI Conference<br>on Artificial Intelligence (AAAI), 2022. 2, 3, 6, 7</p><p>重建任务</p><p>[27] Lilang Lin, Sijie Song, Wenhan Yang, and Jiaying Liu.<br>MS2L: <strong>Multi-task self-supervised learning for skeleton</strong><br><strong>based action recognition</strong>. In Proceedings of the 28th ACM<br>International Conference on Multimedia (ACM MM), pages<br>2490–2498, 2020. 2, 3, 6, 7</p><p>[46] Kun Su, Xiulong Liu, and Eli Shlizerman. PREDICT &amp;<br>CLUSTER: <strong>Unsupervised skeleton based action recognition.</strong><br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition (CVPR), pages 9631–9640,</p><p>[62] Nenggan Zheng, JunWen, Risheng Liu, Liangqu Long, Jian-<br>hua Dai, and Zhefeng Gong. <strong>Unsupervised representation</strong><br><strong>learning with long-term dynamics for skeleton based action</strong><br><strong>recognition.</strong> In Proceedings of the AAAI Conference on Ar-<br>tificial Intelligence (AAAI), pages 2644–2651, 2018. 2, 3, 4,<br>6, 7</p><p>带有思路的去看文章，慢慢加深对骨架动作识别方向的了解</p><p>以目的为导向，区分清楚自己要干的事情【必须要做和 自主去做的事情】</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>个人杂谈</title>
      <link href="/project/posts/c111b07c.html"/>
      <url>/project/posts/c111b07c.html</url>
      
        <content type="html"><![CDATA[<h1>2023.12.25-12.31 周模板</h1><p>目标:</p><blockquote><p>准备29汇报的同时 在29日完成周报，为30 号开会做准备+ 31号回家</p></blockquote><h2 id="每周要干的事情">每周要干的事情</h2><h3 id="1-写微信公众号">1.写微信公众号</h3><p>InfoGCN可以发一篇公众号文章</p><p>open pose 是对视频处理获得骨架数据的</p><p>slow fast 是用在视频处理的</p><p>【肯定可以做论文，需要做的很细。但是工程不同于研究，有很多脏活，由不得不干】【做好方案后，多找一点人来实现，得拆好，才好分出去，都耦合着，就不好搞了】【可能会有些做数据处理的工作】可以写到公众号里头，吸引更多的交流与合作</p><p>多人的文章不太多，我看到过2篇，一篇用扩散模型做多人动作识别的，【多人的比较难 <a href="https://huggingface.co/lllyasviel/sd-controlnet-openpose">https://huggingface.co/lllyasviel/sd-controlnet-openpose</a> 动作的语言大模型  这个东西开放，机会会比较多   咱们可以多宣传  应该会认识一些产业界的人】</p><p>【<a href="https://huggingface.co/lllyasviel/sd-controlnet-openpose">https://huggingface.co/lllyasviel/sd-controlnet-openpose</a>   这个上班时候，部署一下  】</p><p>【这个咱们周一晚上讨论下，你的论文的事情。  打算做图算法来做创新  】 老哥  看看我下周吧   这周我再找找有没有相关的实现啥的【先写一下。然后看哪里不行，再补一补。  先整一版本     估计1-2周应该可以搞定。做实验的时候，也知道去优化什么指标。缺的就先空着    先把问答题，变成填空题。 后面有目的去填就好了。 我这有A100】</p><p>【nvidia 动作识别   <a href="https://arxiv.org/abs/2211.15603">https://arxiv.org/abs/2211.15603</a>      这一篇也是动作有关的大模型    <a href="https://arxiv.org/abs/2201.05739">https://arxiv.org/abs/2201.05739</a>    GNN 做行为识别的 】</p><p>【这个挺火的 gnn 或者gcn  在骨架方向  嗯嗯，可以写一篇。 写完，我再美化一下】</p><p>【教育大模型的那一片，阅读数量上9千多了   咱们把动作识别的，写文的时候，往应用上靠，应该会吸引很多。      】</p><blockquote><p>汇总链接 <a href="https://doc.weixin.qq.com/doc/w3_AdgAaQawAMcKhygtVzOSpaDb1WcMA?scode=AFgAiwfiAGIirBQIxTAdgAaQawAMc">https://doc.weixin.qq.com/doc/w3_AdgAaQawAMcKhygtVzOSpaDb1WcMA?scode=AFgAiwfiAGIirBQIxTAdgAaQawAMc</a></p></blockquote><blockquote><p>这三篇论文，你看看能不能找到对应的代码</p></blockquote><h3 id="2-写周报">2.写周报</h3><p>1.读论文</p><p>2.准备小论文 跑实验</p><p>3.电子文档 研一交流学习</p><h3 id="3-读书">3.读书</h3><p>古典老师</p><p>樊登老师听书</p><p>新闻周刊</p><p>世界周刊</p><p>开讲了</p><h3 id="4-写博客">4.写博客</h3><p>1.学习go 开源</p><p>2.java面试准备</p><p>3.做项目+刷算法题</p><p>4.蓝桥杯准备——学算法</p><h1>2023.12.18-12.24 周模板</h1><p>24日  安排略满 ：  弄博客 + 开会 + 弄龟 ==&gt; 没写周报</p><h1>2023.12.19日</h1><h3 id="脑中思绪放空整理">脑中思绪放空整理</h3><blockquote><p>现在是晚上21.10分，刚刚健身完之后，看了一下古典老师的个人发展共读会第一天体验课晚上的课程，又跟姚普哥哥聊了聊文章和项目，自己整理下脑中要做的事情，感觉一团乱遭</p></blockquote><p>古典老师的  <strong>个人发展共读会</strong></p><h1>2023.12.15日</h1><p>又快到了写年终总结的时候了</p><blockquote><p>12.14日看了封神 和 奇异博士2  剧情感觉真的太拉了</p><blockquote><p>奇异博士感觉有种看恐怖片的感觉，越看越无感</p><p>封神看剧情的话，刚开始还挺不错，但是越往后看越感觉是对于君臣和父子之间的一种污蔑，完全不符合人之常理</p></blockquote><p>基于以上两个电影都是因为需要看剧情和特效，判断都是基于剧情进行做出的，这种特效大片还是要去电影院去看</p></blockquote><h2 id="想法-》行动-》成长成果">想法== 》行动== 》成长成果</h2><ol><li>做一些熵减的事情，世界在逐步的变得混沌，我们需要自己做一些熵减的事情，来保持自己的平衡<ol><li>比如早上每天自己做一些运动，只要我自己做了，对于自己最难的事情，我都做好了，那没有什么能打败我自己</li><li>从易到难，比如摆一些完美角度等</li></ol></li><li>会控制好自己的心力，不浪费过多的经历在一些无关紧要的事情上</li><li>尝试学会去列清单</li><li>多去体验和尝试新东西，多去做事情</li><li>允许自己去放松，看剧，我就看剧去体验</li></ol><blockquote><p>把自己的现有的东西，重写一下去搬到自己的博客上去</p><p>重新捡起来自己要用的东西</p></blockquote><blockquote><p>聚焦不要放到一个点上，放到整个3维框架中</p><ol><li>每天有没有新东西出现<ol><li>新功能？新设计?新产品</li><li>新电影？新的文章？</li><li>新书？ 新思路？新思想</li></ol></li></ol></blockquote><h1>2023.12.7日</h1><blockquote><p>马增龙出品，尽力是精品</p><p>大家都是很善良的，都是香的与自己和解，但更高一层的人香的是怎样去掠夺资产，尽量去丰盈自己</p></blockquote><p>想了很久的东西，先把脑子空出来 装其他的东西</p><blockquote><ol><li>如何快速适应环境<ol><li>周边人【快速建立圈子】</li><li>摆脱圈子的束缚</li></ol></li></ol></blockquote><blockquote><p>所有的万物 都是视频  应该是动态变化的</p></blockquote><blockquote><p>时势 VS 英雄</p><p>大部分的东西 应该是在了解之后 在成功</p><p>也有运气成分</p></blockquote><h1>2023.11.14</h1><p><img src="../assets/@%5BD3K%60UFFCEEUA%7DXPP18WFC.png" alt="img"></p><h1>23.11.8</h1><p>很多人会把目标和结果弄混了，就像考研，考研的话 他是你自己的目标吗？ 好像从一开始的时候 可能认知里就是找个事情干，然后你就全心全意的去干了，但是现在认知貌似提高了，他不应该也不是你的结果了，你是上岸了，但这不是你想要的目标，他应该是你的结果，你的目标是你要慢慢在适当的年龄变成一个你要成为的人，一个全新的人；</p><blockquote><p>自己的本性就是一个不愿意停下来，愿意去折腾的一个人，这是自己对自己的一个认识</p></blockquote><p>自己想过，什么东西是自己最没有力气去抵抗的？</p><blockquote><p>一是年龄，也就是时间，这个你自己永远无法抵抗，时间一定会慢慢走过</p><p>二是出身，也就是出生的时候就定好了，你自己这一盘棋就这样下</p><p>第三个应该是成长，成长是你自己可以选择，也可以自己不用选择，他一定会发生，只是不确定是主动还是被动</p></blockquote><blockquote><p>技术和道义 应该分开</p><p>小技巧归小技巧，小聪明归小聪明，但智慧 才是你真正向往和追求的，也就是正义和真理</p></blockquote><p>清空下脑子里边的东西</p><p><img src="../assets/image-20231106211114742.png" alt="image-20231106211114742"></p><p><img src="../assets/image-20231030203709559.png" alt="image-20231030203709559"></p><p>一般 要用管理员身份做<br>【所有和计算机有关的软件 包括你的ps pr ai 还有画图软件啥的】<br>安装不要选中文路径</p><p>安全下边的 路径很准<br><img src="../assets/87%7B35%25YCBWJYOWUNUWXQ5ZI.png" alt="img"></p><p>亲爱的美女 你一会可以试试 win+ e</p><p><img src="../assets/DJ%7DE@R_VY%3C!--swig%EF%BF%BC0--%3EYXBT.png" alt="img"><img src="../assets/RC@UZROJ4A7N1SE1%7D8E4%7DMU.png" alt="img"><img src="../assets/%7DX52YEPEFPDCT7YJ0DEY$GC.png" alt="img">![img](file:///C:\Users\Lenovo\AppData\Roaming\Tencent\Users\1494360481\QQ\WinTemp\RichOle$4PT~R480$%C6TU9K4)T(XB.png)</p><p><a href="https://www.bilibili.com/video/BV1514y1i7BF/">https://www.bilibili.com/video/BV1514y1i7BF/</a>   【许光汉  焦迈奇 很帅，很阳光，我想成为这样的一个男生，让人感觉很有魅力，这应该是对于大部分女生都渴望为之倾倒的样子吧~】</p><p>【人活着所必需求的乐趣  &amp;&amp;  个人更高层的乐趣】</p><p>吃喝拉撒睡</p><blockquote><p>懂得世界上很多的东西，</p></blockquote><p>以中有足乐者，不知口体之奉不若人也<br>尽吾志也虽不能至者，可以无悔矣</p><blockquote><p>2023.7 30</p><p>发现自己在1Hour内 做了很多事情，自己静不下心来，专注不了一件事情去做了</p></blockquote><blockquote><p>每天开启新的尝试，把昨天的文件都关上，这样开启的时候，才会有成就感，而且也会有新鲜感</p></blockquote><p>==试试每日任务每日结束==</p><blockquote><p>先从刷题开始  慢慢积累</p></blockquote><p><strong>2023.8.24</strong></p><p>跟妈妈聊天，走路走了好久，释怀了好多【找不到好的咱找不到坏的嘛，你自己选的路，自己走，爸妈老了不用你管！！】</p><blockquote><p>想想你之前考研那段路，不比现在难多了嘛，干嘛不把工夫用在平时呢？</p></blockquote><blockquote><p>温水煮青蛙这种痛苦 才会消磨人的意志</p></blockquote><p>十年北伐，六出祁山  诸葛亮最后好不容易火烧司马懿，结果碰上大雨，<strong>知天易，逆天难</strong>，谋事在人，成事在天</p><p>老李 与段鹏 和 和尚 情分比当官更重要</p><blockquote><p>老娘 虽然没有文化，但是明事理</p></blockquote><p><strong>2023年8.26</strong></p><blockquote><p>楚云飞看到自己的部将被老李打的节节败退，大吼道</p></blockquote><p>当军人要有骨气，自己要有胆气，有志气</p><blockquote><p>你是一个大男子汉，不应该为了小事情或者小的不如意喝闷酒，应该有气魄！！</p><p>张白鹿真的对于李云龙很包容，而且也非常非常的适合李云龙，感觉要比田雨在生活上更适合的</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读8</title>
      <link href="/project/posts/c67f8574.html"/>
      <url>/project/posts/c67f8574.html</url>
      
        <content type="html"><![CDATA[<h1>Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</h1><h2 id="Skeleton-AdaIN">Skeleton AdaIN</h2><p><img src="../assets/image-20231023151836519.png" alt="image-20231023151836519"></p><p>骨架数据交换，进行均值和方差交换</p><p>这个方法 在其他文章中也提到过</p><p><img src="../assets/image-20231023151942561.png" alt="image-20231023151942561"></p><h1>Adaptive Local-Component-aware Graph Convolutional Network for One-shot Skeleton-based Action Recognition</h1><p>这一段主要介绍了一种用于骨骼动作识别的方法，特别是在一次性学习（OSL）的背景下。让我详细解释一下：</p><ol><li><strong>之前的方法：</strong> 早期的骨骼动作识别方法将骨骼序列转换为信号图像，然后将其作为图像进行分类。然而，这种转换会改变原始骨骼结构，导致邻近身体关节之间的空间关联信息丢失。因此，有了更理想的方法，可以通过使用骨骼的原生特征来进行度量学习的一次性学习。</li><li><strong>过去的解决方案：</strong> 之前的方法尝试了度量学习的一次性学习，它们分别采用不同的方法来实现这一目标，但它们的结果仍然不如基于信号图像的方法。它们的分类过程中，将编码特征进行平均池化以生成单一的全局嵌入，用于相似性比较。对于经过充分监督训练的情况，这种表示可以自动积累足够的区分性学习，以识别潜在的动作不变特征。</li><li><strong>问题：</strong> 但是，对于来自少数新类别的稀疏示例，它们的全局嵌入的普遍性可能会受到局部特征的影响，并无法有效地专注于需要的不变特征以进行精确的识别。作者认为，通过基于身体部位划分或连续时间段对局部特征进行明确区分，可以直观地区分关键/无关的动作模式，用于有效/无效的识别线索。</li><li><strong>解决方案：</strong> 为了解决这些问题，作者提出了自适应局部组件感知图卷积网络（ALCA-GCN）方法，这是第一个度量学习方法，它以局部嵌入的距离作为一次性骨骼动作识别的主要决定因素。该方法将实例级别的相似性比较分解为每个时间段下每个身体部位的本地可比单元的选择性总和。</li><li><strong>如何工作：</strong> 方法首先使用一个编码骨骼特征的骨干网络，提取了层次化的空间-时间特征，用于身体部位级别的模式和骨骼级别的上下文。然后，嵌入函数对编码特征执行平均池化，为每个分割的本地组件生成独立的比较单元表示。在计算总相似性时，模型按顺序汇总了给定支持和查询输入之间的每个对齐单元的嵌入距离，并针对来自动作关键/无关单元的决策影响应用自适应强调/抑制。</li><li><strong>贡献：</strong> 这一方法的贡献包括：<ul><li>提出了ALCA-GCN，这是一种新颖的度量学习方法，用于骨骼动作识别。</li><li>ALCA-GCN通过对齐的比较单元之间的嵌入距离的选择性总和来确定两个骨骼序列之间的总相似性。</li><li>ALCA-GCN在广泛的一次性学习实验设置下使用NTU-RGB+D 120数据集取得了优于先前所有相关论文的结果，表明该模型实现了最先进的性能。</li></ul></li></ol><p>这一段描述了ALCA-GCN的体系结构，该方法用于骨骼动作识别。下面是详细解释：</p><ol><li><strong>基本架构：</strong> ALCA-GCN方法遵循基于度量的解决方案的基本框架，包括一个编码骨干网络，一个用于建模表示矩阵的嵌入器以及一个基于线性度量的分类器。</li><li><strong>移除视点差异：</strong> 为了消除视角差异，首先将所有输入数据统一到一个正面视角，这是通过某种方式进行的。</li><li><strong>骨干网络：</strong> 作者采用了ST-GCN网络F作为原型的编码骨干。ST-GCN网络允许对每个身体关节的多个邻居进行空间特征卷积，其遵循预定义的（子）结构。通过维护原始的骨骼级别卷积，作者还设计了独立的内核矩阵，用于身体部位级别的卷积。修改后的F现在捕获了每个关节所属身体部位下的周围特征以及骨架范围内的全局关系。</li><li><strong>特征提取：</strong> 使用F从经过预处理的输入x中获取总特征f，将其池化为基于身体部位的全局表示Gf。这包含了4个身体部位（头部，手，躯干和腿）在3个时间段下的一组本地嵌入gf，它们被视为用于本地相似性匹配的基本比较单元。</li><li><strong>自适应依赖学习：</strong> 为了增强/抑制动作关键/无关组件的差异学习，作者在Gf之后附加了一个自适应依赖学习（ADL）模块，该模块可以自适应地调整每个单元的内容影响以获取全局匹配结果。</li><li><strong>聚合和训练：</strong> 每个单元还聚合了一个平均全局嵌入，作为实例级别的约束来施加类内聚类一致性。最后，通过训练，使用给定查询和其所属类别支持示例的所有对齐单元之间的欧氏距离总和，模型学习依赖于动作关键本地模式相似性的强烈分类。</li><li><strong>差异处理：</strong> 与噪声单元的不同之处在于，通过使用其他高关注上下文来修复其原始嵌入，以更正它们。</li><li><strong>后续部分：</strong> 本节的其余部分将进一步描述每个模型组件的细节。</li></ol><p>总的来说，这一部分描述了ALCA-GCN方法的架构，包括它如何处理骨骼数据以进行度量学习，如何捕获局部模式以进行骨骼动作识别，并如何进行模型训练以强调动作关键的局部模式的相似性。这是一种先进的方法，用于提高骨骼动作识别的性能。</p><p>图2给出了ALCA-GCN的体系结构。我们的方法遵循基于度量的解决方案的基本框架，包括编码骨干、建模表示矩阵的嵌入器和基于线性度量的分类器。为了消除视点方差，我们首先将所有输入统一为一个正面视角(见第4.2节)。我们采用ST-GCN[38]网络F作为原型编码骨干，因为它允许按照预定义的(子)结构对每个身体关节的多个邻居进行空间特征卷积。在保持原有骨架级卷积的基础上，我们设计了独立的核矩阵用于体-部分级卷积。改进后的F现在捕获了每个关节在其所属身体部位下的周围特征以及在骨骼范围内的相对全局关系。我们使用F从一个预处理过的输入x中获得总的特征F，并将其作为一个基于部分的全局表示Gf，其中包含了一组局部嵌入的Gf，用于3个颞骨切片下的4个身体部位(头、手、躯干和腿)。它们被视为局部相似度匹配的基本比较单元。为了增强/抑制动作关键/无关成分的区分学习，Gf后面附加了自适应依赖学习(ADL)模块，自适应调整每个单元的内容影响对全局匹配结果的影响。平均的全局嵌入也被聚合到每个单元中，作为一个实例级的约束，以施加类内集群一致性。最后，使用给定查询的所有对齐单元与所属类支持示例之间的欧几里德距离和进行训练，模型学习以强烈依赖动作关键局部模式的相似性进行分类。与噪声单元的差异被用其他高度注意的环境来补充它们的原始嵌入来修正。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>technology-π1</title>
      <link href="/project/posts/71f495ac.html"/>
      <url>/project/posts/71f495ac.html</url>
      
        <content type="html"><![CDATA[<h1>技术派项目实战</h1><blockquote><p>学习一个新的技术派付费项目，将所有的技术学会应用</p></blockquote><p><img src="../assets/image-20231017112333121.png" alt="image-20231017112333121"></p><p><img src="../assets/image-20231017112354788.png" alt="image-20231017112354788"></p><h2 id="大厂篇">大厂篇</h2><blockquote><p>学习大厂中怎样</p></blockquote><h2 id="技术派的架构方案设计【概要设计】">技术派的架构方案设计【概要设计】</h2><blockquote><p>概要设计 主要介绍整体的框架结构，包括但不限于 系统包括什么样的业务，主要的功能，角色 以及 设计方案和业务架构</p></blockquote><h2 id="技术派的技术方案设计【详细设计】">技术派的技术方案设计【详细设计】</h2><blockquote><p>详细设计包括 系统模块分类，各个模块的主要功能 各个模块的表结构 以及对应业务功能的实现方式</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>masked</title>
      <link href="/project/posts/cea7f2ca.html"/>
      <url>/project/posts/cea7f2ca.html</url>
      
        <content type="html"><![CDATA[<h1>【代码待更新】Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence</h1><h2 id="abstract">abstract</h2><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240303111656152.png" alt="image-20240303111656152"></p><p>自我监督的预训练范式在基于机器人的动作识别领域得到了广泛的研究。特别是基于掩码预测的方法将预训练的性能推向了一个新的高度。然而，这些方法采取低级别的功能，如原始的关节坐标或时间运动，作为预测目标的掩蔽区域，这是次优的。在本文中，我们表明，使用高层次的上下文特征作为预测目标，可以实现上级性能。具体来说，我们提出了一个简单而有效的自监督3D动作表示学习框架，它利用基于transformerbased的教师编码器，以未掩蔽的训练样本作为输入，以创建潜在的上下文表示作为预测目标。受益于自注意机制，由教师编码器生成的潜在表示可以包含整个训练样本的全局上下文，从而导致更丰富的训练任务。此外，考虑到骨架序列中的高时间相关性，我们提出了一种运动感知的管掩蔽策略，该策略将骨架序列划分为几个管，并根据运动先验在每个管内执行持久掩蔽，从而迫使模型建立长距离时空连接并专注于动作语义丰富的区域。在NTU-60、NTU-120和PKU-MMD数据集上的大量实验表明，我们提出的方法优于以前的方法，并取得了最先进的结果。可以在https://github.com/ Ruizhuo-Xu/Xueton 2 vec上获得Xueton 2 vec的源代码。</p><h2 id="introduction">introduction</h2><p>引言</p><p>人类动作识别在现实世界中有着重要的应用，例如安全、人机交互和虚拟现实。深度传感器的发展以及姿态估计算法的进步[4, 12, 41]已将基于骨架的动作识别推向热门研究主题，这归功于它的计算效率、背景鲁棒性和隐私保护。一系列完全监督的基于骨架的人类动作识别方法已经被开发，这些方法使用了CNNs [10, 19]、RNNs [24, 46]和GCNs [5, 43]。尽管它们展示了有希望的性能，但这些方法依赖于大量手动标注的数据，这些数据昂贵、耗时且劳动强度大。这种情况激励我们去探索3D动作的自监督表示学习。</p><p>早期工作[21, 29, 33, 47]已经使用各种前置任务，如运动预测、拼图识别和掩码重建，来学习3D动作表示。最近，对比学习方法[15, 22, 28, 30]变得突出。然而，这些方法通常需要精心设计的数据增强，并倾向于鼓励编码器学习更全局的表示，从而忽视了局部时空信息。随着变压器模型[37]的兴起，基于掩码预测任务的自监督预训练方法在视觉表示学习[15, 22, 28, 30]中已成为主流。如SkeletonMAE [39, 42]和MAMP [27]等作品已尝试将MAE [17]方法转移到3D动作表示学习领域，取得了有希望的结果。然而，这些类MAE方法通过关注原始关节坐标或时间运动等低级高频细节作为学习目标，效率低下地利用了模型容量，这对于建模高级时空结构是次优的。我们相信，使用更高级别的预测目标将指导模型学习更好的表示，并提高预训练性能。</p><p>受此思想的启发，我们提出了Skeleton2vec，一个简单而高效的3D动作表示学习的自监督框架。Skeleton2vec解决了现有类MAE方法的局限性，如图1所示，它利用了上下文化的预测目标。我们遵循data2vec[1, 2]的工作，使用一个教师编码器来处理未掩码的训练样本，以生成作为目标的潜在上下文化表示。然后我们使用一个学生编码器，以掩码版本的样本为输入，并结合非对称解码器来预测掩码位置的数据表示。整个模型基于标准的变压器架构。自注意机制确保构建的目标是上下文化的，包含来自整个样本的信息，使它们比孤立目标（例如原始关节坐标）或基于局部上下文的目标（例如时间运动）更丰富。</p><p>此外，考虑到3D骨架序列中强大的时空相关性，我们提出了一个感知运动的管状掩码策略。最初，我们将输入的骨架序列沿时间轴分成多个管状部分，在每个管状部分内的帧共享一个掩码图，以避免相邻帧之间的信息泄露。这迫使模型从遥远的时间步骤中提取信息以进行更好的预测。然后我们根据身体关节在每个管状部分内的空间运动强度来指导掩码关节的采样。运动强度较高的关节将以更高的概率被掩码，允许模型更多地关注具有丰富动作语义的时空区域。与随机掩码相比，我们的方法更好地利用了3D骨架序列的时空特性和运动先验，有效地提高了预训练性能。</p><p>总结一下，本工作的主要贡献有三个方面： • 我们提出了Skeleton2vec框架，它使用来自教师编码器的上下文化表示作为预测目标，使学习到的表示具有更强的语义关联。 • 我们引入了一个感知运动的管状掩码策略，该策略根据空间运动强度对管状部分内的关节进行持久掩码，迫使模型构建更好的长距离时空连接并关注更富有语义的区域。 • 我们在三个大规模的基于3D骨架的动作识别数据集上验证了我们方法的有效性，并取得了最先进的结果。</p><h3 id="总结">总结</h3><ol><li>人类动作识别具有安全、人机交互和虚拟现实等多种实际应用价值。深度传感器和姿态估计算法的进步使基于骨架的动作识别成为一个热门研究方向，因为它高效、鲁棒且能保护隐私。</li><li>尽管现有的完全监督的基于骨架的动作识别方法取得了一定的成果，但这些方法依赖大量手动标注数据，这一过程成本高且耗时。</li><li>因此，研究者开始探索自监督学习方法来学习3D动作的表示，早期工作尝试了多种前置任务，而最近的研究开始利用对比学习。</li><li>然而，对比学习方法常常需要复杂的数据增强，并且可能忽视局部时空信息。受到变压器模型和自监督预训练方法的影响，研究者开始采用掩码预测任务。</li><li>Skeleton2vec是一个自监督框架，用于学习3D动作表示，它克服了现有方法的局限并使用上下文化的预测目标，通过教师编码器生成丰富的表示作为预测目标。</li><li>为了更好地利用骨架序列的时空特性，提出了一种新的运动感知管状掩码策略，通过掩码强度较高的关节来引导模型关注动作信息丰富的区域。</li><li>这项工作的主要贡献包括提出新的框架、引入新的掩码策略，并且在三个大规模3D骨架动作识别数据集上验证了方法的有效性，取得了领先的结果。</li></ol><h1>【未开源】SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition用于自监督骨架动作识别的时空掩蔽自动编码器</h1><blockquote><p>[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr<br>Doll´ar, and Ross Girshick. Masked autoencoders are scalable<br>vision learners. In Proceedings ofthe IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pages 16000–<br>16009, 2022.</p></blockquote><p>随着深度学习技术的蓬勃发展，基于<strong>骨骼的全监督动作识别技术</strong>取得了长足的进步。然而，这些方法需要足够的标注数据，而这些数据不易获得。相比之下，基于<strong>骨骼的自我监督动作识别则吸引了更多的关注</strong>。利用未标记的数据，可以学习更多的可泛化特征来缓解过拟合问题，减少对大量标记训练数据的需求。在MAE[15]的启发下，我们提出了一个用于<strong>基于骨骼的自监督3D动作识别的时空掩藏自动编码器框架(骨架MAE)</strong>。在==<strong>MAE的掩蔽和重建管道的基础上，我们利用一个基于骨架的编码器-解码器transformer结构来重建掩蔽骨架序列</strong>==。针对骨架序列，分别从<strong>关节级和帧级两方面提出了一种新的掩蔽策略——时空掩蔽策略</strong>。这种预训练策略使得<strong>编码器输出具有空间和时间依赖性的可泛化骨架特征</strong>。<strong>给定未隐藏的骨架序列，编码器被微调为动作识别任务</strong>。大量实验表明，在NTU RGB+D和NTU RGB+D 120数据集上，我们的骨架mae都取得了显著的性能，并且优于目前最先进的方法。</p><h2 id="introduction-2">introduction</h2><p>人体动作识别是计算机视觉领域的一个基本研究课题，旨在理解人类行为并区分不同的动作[38]。随着深度学习和人体姿势估计方法的蓬勃发展[2, 39, 37]，<strong>人类骨骼数据可以被高效提取为一种高级但轻量级的表示</strong>，因此引起了对人类行为和动作分析的广泛关注。因此，基于3D骨骼的动作识别已经成为人体动作识别中的一个重要研究领域。</p><p>大多数最近的方法侧重于使用全监督学习算法来构建它们的框架：基于卷积神经网络（CNN）的方法[11, 48]，基于循环神经网络（RNN）的方法[43, 34]，基于图卷积网络（GCN）的方法[46, 33, 49, 6]以及基于Transformer的方法[29, 30]广泛应用于骨骼动作识别，并取得了非常好的结果。然而，<strong>完全监督的动作识别容易出现过拟合。<strong>此外，它</strong>需要大量的标记训练数据</strong>，这既昂贵又耗时。为了缓解这些问题，在骨骼动作识别中，自监督学习方法越来越盛行，这些方法利用未标记的数据来学习数据表示。<strong>一些自监督方法考虑了骨骼表示学习的预设任务，使用未标记的骨骼数据，例如动作重建[7]和拼图[22]</strong>。然而，这些<strong>基于预设任务的方法侧重于局部特征，如同一帧内的关节相关性和骨骼比例，并未充分探索时间信息</strong>。最近的一些工作[20, 14]<strong>通过构建不同视图下的骨骼序列，通过数据增强和正负对，训练了基于对比学习框架的对比模型</strong>。尽管这些基于对比学习的方法==强调了高级上下文信息，但它们严重依赖于用于提取骨骼特征的关节对的数量，并忽略了不同帧之间的关节相关信息==。</p><p>自监督任务的发展路径</p><blockquote><p>基于重建不同视图下的骨架数据，使用数据增强和正负对</p><p>Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition.</p><p>3d human action representation learning via cross-view consistency pursuit</p><p>基于未标记数据使用拼图和动作重建</p></blockquote><p>最近，一种名为&quot;Masked Autoencoders (MAE)&quot;的新的自监督学习方法[15]展示出在计算机视觉任务中具有强大的泛化能力，表现出色。<strong>MAE会对输入图像的大部分区域进行遮盖，然后强制模型仅使用未遮盖的部分来重建原始图像。</strong></p><p>然而，由于以下原因，MAE[15]不能直接用于自监督骨骼动作识别：</p><ul><li>MAE[15]使用了视觉变换器（ViT）[10]结构来处理图像输入。<strong>与不包含时间信息的图像不同，人体骨架序列是从包含丰富语义信息的高信息密度视频中提取出来的：在空间层面，关节特征包含了同一帧内不同关节之间的关系；在时间层面，帧特征代表了同一关节在不同帧之间的运动。</strong></li><li>MAE的遮盖策略仅关注空间域。在处理人体骨架序列数据时，需要一种<strong>空间-时间遮盖策略</strong>。</li></ul><p>为了解决这些问题，我们引入了一种名为&quot;SkeletonMAE&quot;的新型骨架自监督学习方法，旨在进行骨架的<strong>空间-时间表示学习</strong>：1) 遮盖输入序列是从原始骨架序列中生成的，其中包含了关节坐标（空间）信息和帧（时间）信息；2) 借助空间-时间遮盖策略和编码-解码规则，SkeletonMAE通过基于transformer的编码器和解码器处理遮盖序列，从而获得重建序列，其中空间和时间信息得到了很好的处理（transformer在长序列数据的空间-时间表示学习中具有巨大潜力）。</p><p>SkeletonMAE的框架如图1所示。具体来说，整个SkeletonMAE流程是根据以下原则设计的。在预训练阶段，使用了空间-时间遮盖策略（具有预设的帧遮盖和关节遮盖比率）来遮盖输入骨架序列的部分部分，包括帧级别和关节级别（Sec. 3.2）。为了找到最佳的空间-时间表示学习的权衡点，我们讨论了关<strong>节遮盖和帧遮盖比率的作用</strong>，找到了最佳的比率组合。<strong>编码器用于学习可泛化的特征表示</strong>，而<strong>解码器则用于重建丢失的骨架</strong>。由于我们处理的是骨架序列，我们<strong>使用了专为处理骨架序列而开发的&quot;空间-时间元组变换器（STTFormer）</strong>&quot;[30]作为我们的网络骨干，而不是ViT [10]。在微调阶段，我们仅使用编码器和简单的输出层来预测动作。动作识别结果表明，我们的方法在没有额外数据的情况下胜过了最先进的自监督学习方法。</p><p><strong>总结：</strong> 本段介绍了一种名为&quot;SkeletonMAE&quot;的新型自监督学习方法，专门用于骨骼动作识别。该方法解决了现有方法的局限性，同时在标准数据集上取得了自监督设置下的最先进性能。</p><p><strong>Summary</strong></p><p>这个章节介绍了人体动作识别的背景和问题。随着深度学习和人体姿势估计技术的发展，骨骼数据成为了一种重要的高级表示方式，用于动作识别。目前的方法主要集中在全监督学习算法上，但这些方法容易过拟合，而且需要大量标记数据。为了克服这些问题，自监督学习方法变得越来越受欢迎，但现有方法主要关注局部特征，而<strong>忽略了时间信息和不同帧之间的关节相关性</strong>。为了解决这些问题，引入了一种新的方法，<strong>该方法使用自监督学习和对比学习框架来改进骨骼动作识别</strong>，特别是在处理类似的空间外观和不一致的时间依赖性时，具有更好的性能。该方法引入了拓扑编码和方向平均对称表面度量等新颖概念，以提高模型的性能。实验结果表明，这一方法在现有方法的基础上取得了显著的改进。</p><h2 id="Related-work">Related work</h2><h3 id="2-1-Supervised-skeleton-based-action-recognition">2.1. Supervised skeleton-based action recognition</h3><p>以下是对输入段落的逐句翻译：</p><p>在深度学习出现之前，骨骼动作识别工作中使用手工制作的技术来提取空间-时间特征[44, 40, 41]。近年来，由于其强大的特征提取和表示学习能力，深度学习已广泛用于骨骼动作识别领域。而其中大多数是完全监督的。基于循环神经网络（如LSTMs）[12, 50, 24]的方法被广泛用于处理骨架数据。与此同时，基于卷积神经网络（CNNs）的方法[35, 18, 45]也被引入到骨骼动作识别中。然而，由RNN或CNN提取的数据表示过于简单，无法全面呈现骨骼数据的空间-时间特征。因此，自然而然地引入了基于图卷积网络（GCN）的方法[46, 33, 49, 6]，以模拟来自骨架数据的拓扑图特征。最近，随着视觉变换器（ViT）的成功[10]，基于变换器的模型由于其学习全局表示的能力而成为顺序骨架数据分析的强大架构[29, 30, 52, 47, 5, 51, 27]。因此，我们在研究中采用了基于骨骼的变换器（STTFormer [30]）作为骨架序列处理的骨干网络。</p><p><strong>总结：</strong> 这一段描述了在深度学习之前，骨骼动作识别领域使用手工制作的技术来提取空间-时间特征。近年来，深度学习在这个领域得到广泛应用，但大多数方法都是完全监督的。然后介绍了基于循环神经网络（RNN）和卷积神经网络（CNN）的方法，但它们提取的数据表示太简单，不能充分表现骨骼数据的复杂空间-时间特征。因此，引入了基于图卷积网络（GCN）的方法，以更好地建模骨架数据中的拓扑图特征。最近，基于变换器的模型，如视觉变换器（ViT），由于其全局表示学习的能力，成为顺序骨架数据分析的强大架构，因此在研究中采用了基于骨骼的变换器（STTFormer）作为骨干网络。</p><h3 id="2-2-Self-supervised-skeleton-based-action-recognition">2.2. Self-supervised skeleton-based action recognition</h3><p>自监督学习旨在在不使用标记数据的情况下提取特征表示，并在基于图像和视频的表示学习中取得了令人满意的性能[36, 26, 9, 13]。更多自监督表示学习方法采用所谓的对比学习方式[16, 4, 21, 3]来提高性能。受到对比学习结构的启发，最近的骨骼表示学习工作在自监督骨骼动作识别中取得了一些令人鼓舞的进展。MS2L [22]引入了一种多任务自监督学习框架，通过运动预测和拼图识别来提取关节表示。CrosSCLR [20]开发了一种基于对比学习的框架，用于从骨架数据中学习单视图和跨视图表示。在CrosSCLR之后，AimCLR [14]采用了极端的数据增强策略，增加了额外的硬对比对，旨在从骨架数据中学习更通用的表示。</p><p><strong>总结：</strong> 这一段介绍了自监督学习的概念，即在没有标记数据的情况下提取特征表示，并指出它在图像和视频表示学习中取得了良好的性能。此外，提到了采用对比学习方式的自监督表示学习方法，这种方法在骨骼表示学习领域也取得了一些进展。例如，MS2L引入了一个多任务的自监督学习框架，用于从骨架数据中提取关节表示。CrosSCLR开发了一个基于对比学习的框架，用于学习单视图和跨视图的骨架表示。AimCLR则采用了极端的数据增强策略，以学习更通用的骨架数据表示。</p><p>==Lilang Lin, Sijie Song,Wenhan Yang, and Jiaying Liu. Ms2l:<br>Multi-task self-supervised learning for skeleton based action<br>recognition. In Proceedings of the 28th ACM International<br>Conference on Multimedia, pages 2490–2498, 2020.==</p><h3 id="2-3-Masked-Autoencoder">2.3&quot;Masked Autoencoder&quot;</h3><p>“Masked Autoencoder” 是一种自编码器（Autoencoder）的变种，用于学习数据的压缩表示，特别是在自然语言处理领域中广泛使用。这种自编码器之所以称为 “masked”，<strong>是因为它的训练过程通常涉及到将输入数据中的某些部分（通常是随机选择的一些元素）掩盖或遮挡，然后尝试通过模型来预测这些被掩盖的部分</strong>。</p><p>下面是一些关于 Masked Autoencoder 的关键概念和工作原理：</p><ol><li><strong>自编码器（Autoencoder）</strong>：自编码器是一种神经网络结构，它有一个编码器和一个解码器。编码器将输入数据映射到低维表示（潜在空间），然后解码器将潜在表示映射回原始数据的尺寸。自编码器的目标是学习一种紧凑的数据表示，以最小化输入和重构之间的差异。</li><li><strong>Masked Autoencoder的特点</strong>：在 Masked Autoencoder 中，输入数据的一部分通常会被随机掩盖或遮挡，这部分被称为 “mask”。模型的目标是从被掩盖的数据中预测出原始数据，同时仍要学习到有意义的表示。这使得模型更有能力学习到数据的局部特征，而不仅仅是整体表示。</li><li><strong>应用领域</strong>：Masked Autoencoder 在自然语言处理中非常有用。例如，对于语言建模任务，可以使用 Masked Autoencoder 预测一句话中的某些单词，而其他单词被掩盖。这有助于模型学习词语之间的关系和句子的结构。</li><li><strong>变种</strong>：基于 Masked Autoencoder 的模型有多种变种，其中最著名的是 BERT（Bidirectional Encoder Representations from Transformers）。BERT 使用双向掩码语言建模作为预训练任务，它的预训练表示在多种自然语言处理任务中都表现出色。</li></ol><p>总之，Masked Autoencoder 是一种用于学习数据表示的神经网络模型，通过随机掩盖部分输入数据并预测它们，模型能够学习到有意义的特征表示。它在自然语言处理中有广泛应用，但也可用于其他领域的序列数据建模和特征学习。</p><h2 id="3-Methodology">3. Methodology</h2><p>在这一节中，我们首先在<strong>3.1节中介绍了骨架mae的初步内容</strong>。然后，在第<strong>3.2节中，我们设计了一种骨架数据的时空掩蔽策略</strong>。接下来，我们在第3.3节中分析我们的骨骼mae用于动作识别。最后，我们在第3.4节中介绍了我们的微调过程。</p><h3 id="3-1-Preliminaries">3.1. Preliminaries</h3><p>这里介绍了MAE（Masked Autoencoder）模型。MAE采用了一种不对称的编码器和解码器结构。需要注意的是，解码器的结构与编码器不同，<strong>这意味着可以根据需要自定义解码器以构建高效的预训练模型</strong>。具体来说，MAE中的编码器基于ViT（Vision Transformer），但仅处理未掩盖的图像：首先，==图像块经线性投影编码，并加上位置嵌入以形成图像令牌，然后这些令牌经过多个Transformer块的处理。仅加载了一小部分未掩盖的令牌（75％的块被掩盖，其余的作为输入）。至于MAE解码器，它根据原始图像块上的位置信息来解码已掩盖的令牌以进行重构。然后，在像素空间中计算了已掩盖和重构令牌之间的均方误差（MSE==。在预训练后，应用具有简单分类头的预训练编码器进行图像分类任务。</p><p>总结：该部分介绍了MAE模型的结构和工作方式。MAE包括一个编码器和一个解码器，它们以不对称的方式构建。编码器基于ViT进行图像编码，但仅处理未掩盖的图像部分，而解码器负责重构已掩盖的图像部分。在预训练后，预训练编码器可用于图像分类任务。这种结构的不对称性为构建高效的预训练模型提供了灵活性。</p><p>STTFormer [30]。与MAE不同，<strong>MAE在编码器中应用了ViT，而在解码器中应用了Transformer块进行图像重构</strong>，而我们<strong>选择使用STTFormer来构建编码器和解码器</strong>，因为它具有基于骨架的Transformer结构。<strong>与ViT基于没有时间信息的图像块不同，STTFormer是一种基于骨架数据的Transformer，对于处理时空数据具有很大的潜力。<strong>具体而言，</strong>== STTFormer将骨架数据分成多个元组（不重叠的部分），并提供了名为Spatio-Temporal Tuples Attention（STTA）的自注意模块，用于提取相邻帧之间的多关节表示==</strong>。然后，在STTA块之后提出了一种名为Inter-Frame Feature Aggregation（IFFA）的特征聚合模块，用于<strong>提高相似动作识别的学习能力</strong>。STTFormer的结构如图1所示。</p><p>总结：该部分介绍了STTFormer模型，该模型与MAE不同，采用了骨架数据驱动的Transformer结构，适用于处理骨架数据的时空信息。STTFormer的结构包括Spatio-Temporal Tuples Attention（STTA）自注意模块和Inter-Frame Feature Aggregation（IFFA）特征聚合模块，有助于提取和集成骨架数据的多关节表示，从而提高相似动作的识别能力。这一部分强调了STTFormer相对于ViT的优势，特别适用于骨架数据的处理。</p><h3 id="3-2-Spatial-temporal-masking-strategy">3.2. Spatial-temporal masking strategy</h3><p>我们提出了一种时空遮蔽方法，<strong>用于对骨架序列输入的一部分进行遮蔽</strong>，我们的遮蔽策略流程如图2所示。</p><p>时空遮蔽方法。图2显示了我们的帧级遮蔽方法。根据预设的帧遮蔽比率，<strong>随机去除一部分帧并存储它们的索引，然后剩下的帧将通过关节级别的空间遮蔽方法进行处理</strong>。</p><p>空间遮蔽方法。如图2所示，在所有输入帧上实施了时空遮蔽方法后，<strong>剩下的帧将通过空间遮蔽策略进行处理</strong>。根据预设的关节遮蔽比率，我们<strong>会随机地在每个未遮蔽的帧中遮蔽部分关节</strong>。值得注意的是，在这个随机的空间遮蔽方法中，<strong>被遮蔽的关节的索引不是固定的</strong>，这意味着<strong>不同帧中的相同关节可能被遮蔽或不被遮蔽</strong>。这种简单的方法如图3(b)所示。除了这种遮蔽方法，我们<strong>还引入了一个具有固定索引的关节遮蔽策略，如图3©所示</strong>。<strong>不同帧中具有相同索引的关节将根据关节遮蔽比率一起被遮蔽或不被遮蔽</strong>。我们在第4.3节中进行了实验，比较了这两种遮蔽策略。</p><p>总结：这一部分描述了作者提出的骨架序列的时空遮蔽方法。这种方法分为时序遮蔽和空间遮蔽两个阶段。时序遮蔽是在帧级别进行的，随机去除一部分帧。然后，在剩余的帧中，进行空间遮蔽，其中部分关节也会被随机遮蔽。作者还介绍了两种空间遮蔽策略，一种是随机遮蔽关节的策略，另一种是基于固定索引的关节遮蔽策略。这两种策略在后续实验中进行了比较。</p><h3 id="3-3-SkeletonMAE-architecture">3.3. SkeletonMAE architecture</h3><p>我们描述了SkeletonMAE的主要组件，例如编码器、解码器、重构序列、损失函数和骨架动作识别的微调流程。流程和SkeletonMAE的结构如图1所示。</p><p>SkeletonMAE编码器。我们的编码器基于STTFormer，<strong>只处理可见的骨架标记</strong>。给定一个骨架序列作为输入，我们分别应用了<strong>帧遮蔽和关节遮蔽方法</strong>。这些在空间和时间上未遮蔽的标记被馈送到<strong>SkeletonMAE编码器，将输入映射到时空嵌入特征</strong>。</p><p>SkeletonMAE解码器。我们的<strong>解码器也采用了STTFormer的结构</strong>。与MAE中的解码器相同，SkeletonMAE解码器中的<strong>时空嵌入特征被处理以重构原始序列</strong>。同时，<strong>为了保留用于重构的位置信息，还引入了位置嵌入。解码器的输出是重构序列，应与未遮蔽的原始序列相同。</strong></p><p>重构。我们使用均方误差（MSE）损失来衡量重构的结果。在这种情况下，我们<strong>计算原始骨架序列与重构序列之间的MSE损失</strong>，如下所示：</p><p>MSE = 1 N ΣN i=1 |Si − S∗ i |2， 其中i是帧的索引，N是样本数量，S是输入序列，S∗是重构序列。</p><p>总结：这一部分描述了SkeletonMAE的架构，包括编码器、解码器和重构方法。编码器和解码器都基于STTFormer结构，用于处理骨架序列的重构。重构的质量通过均方误差损失来衡量，以衡量原始骨架序列与重构序列之间的差异。</p><h3 id="3-4-Fine-tuning-for-skeleton-action-recognition">3.4. Fine-tuning for skeleton action recognition</h3><p>为了评估SkeletonMAE学习骨架表示的能力，我们<strong>加载了从预训练中获得的参数权重</strong>，**用所有的训练数据对模型进行微调，然后使用识别准确度来预测每个动作的标签。**微调的过程如图1(b)所示。与最新的基于对比的自监督骨架动作识别方法[20, 14]不同，<strong>这些方法通过线性评估协议验证模型，我们只关注骨架动作识别任务的端到端微调</strong>。</p><p>总结：这一部分描述了如何评估SkeletonMAE对骨架表示的学习能力。首先，他们使用从预训练中获得的参数权重对模型进行微调，然后使用准确度来评估每个动作的标签预测。与其他方法不同，他们侧重于端到端微调而不是线性评估协议。这是评估SkeletonMAE性能的关键步骤。</p><h2 id="详细介绍">详细介绍</h2><p>SkeletonMAE的框架如图1所示。具体来说，整个SkeletonMAE流程是根据以下原则设计的。在预训练阶段，使用了空间-时间遮盖策略（具有预设的帧遮盖和关节遮盖比率）来遮盖输入骨架序列的部分部分，包括帧级别和关节级别（Sec. 3.2）。为了找到最佳的空间-时间表示学习的权衡点，我们讨论了关<strong>节遮盖和帧遮盖比率的作用</strong>，找到了最佳的比率组合。<strong>编码器用于学习可泛化的特征表示</strong>，而<strong>解码器则用于重建丢失的骨架</strong>。由于我们处理的是骨架序列，我们<strong>使用了专为处理骨架序列而开发的&quot;空间-时间元组变换器（STTFormer）</strong>&quot;[30]作为我们的网络骨干，而不是ViT [10]。在微调阶段，我们仅使用编码器和简单的输出层来预测动作。动作识别结果表明，我们的方法在没有额外数据的情况下胜过了最先进的自监督学习方法。</p><p><strong>总结：</strong> 本段介绍了一种名为&quot;SkeletonMAE&quot;的新型自监督学习方法，专门用于骨骼动作识别。该方法解决了现有方法的局限性，同时在标准数据集上取得了自监督设置下的最先进性能。</p><p><strong>Summary</strong></p><p>这个章节介绍了人体动作识别的背景和问题。随着深度学习和人体姿势估计技术的发展，骨骼数据成为了一种重要的高级表示方式，用于动作识别。目前的方法主要集中在全监督学习算法上，但这些方法容易过拟合，而且需要大量标记数据。为了克服这些问题，自监督学习方法变得越来越受欢迎，但现有方法主要关注局部特征，而<strong>忽略了时间信息和不同帧之间的关节相关性</strong>。为了解决这些问题，引入了一种新的方法，<strong>该方法使用自监督学习和对比学习框架来改进骨骼动作识别</strong>，特别是在处理类似的空间外观和不一致的时间依赖性时，具有更好的性能。该方法引入了拓扑编码和方向平均对称表面度量等新颖概念，以提高模型的性能。实验结果表明，这一方法在现有方法的基础上取得了显著的改进。</p><h2 id="方法详解">方法详解</h2><p>在这一节中，我们首先在<strong>第1节中介绍了骨架mae的初步内容</strong>。然后，在第****第2*<em>节中，我们设计了一种骨架数据的时空掩蔽策略</em>*。接下来，我们在第****第3*<em>节<strong>中分析我们的骨骼mae用于动作识别。最后，我们在第</strong></em>*第4**节**节中介绍了我们的微调过程。</p><p><img src="../assets/640" alt="Image"></p><h3 id="参数">参数</h3><p>这里介绍了MAE（Masked Autoencoder）模型。MAE采用了一种不对称的编码器和解码器结构。需要注意的是，解码器的结构与编码器不同，<strong>这意味着可以根据需要自定义解码器以构建高效的预训练模型</strong>。具体来说，MAE中的编码器基于ViT（Vision Transformer），但仅处理未掩盖的图像：首先，图像块经线性投影编码，并加上位置嵌入以形成图像令牌，然后这些令牌经过多个Transformer块的处理。仅加载了一小部分未掩盖的令牌（75％的块被掩盖，其余的作为输入）。至于MAE解码器，它根据原始图像块上的位置信息来解码已掩盖的令牌以进行重构。然后，在像素空间中计算了已掩盖和重构令牌之间的均方误差（MSE。在预训练后，应用具有简单分类头的预训练编码器进行图像分类任务。</p><p>总结：该部分介绍了MAE模型的结构和工作方式。MAE包括一个编码器和一个解码器，它们以不对称的方式构建。编码器基于ViT进行图像编码，但仅处理未掩盖的图像部分，而解码器负责重构已掩盖的图像部分。在预训练后，预训练编码器可用于图像分类任务。这种结构的不对称性为构建高效的预训练模型提供了灵活性。</p><p>STTFormer [30]。与MAE不同，<strong>MAE在编码器中应用了ViT，而在解码器中应用了Transformer块进行图像重构</strong>，而我们<strong>选择使用STTFormer来构建编码器和解码器</strong>，因为它具有基于骨架的Transformer结构。<strong>与ViT基于没有时间信息的图像块不同，STTFormer是一种基于骨架数据的Transformer，对于处理时空数据具有很大的潜力。<strong>具体而言，<strong>STTFormer将骨架数据分成多个元组（不重叠的部分），并提供了名为Spatio-Temporal Tuples Attention（STTA）的自注意模块，用于提取相邻帧之间的多关节表示</strong>。然后，在STTA块之后提出了一种名为Inter-Frame Feature Aggregation（IFFA）的特征聚合模块，用于</strong>提高相似动作识别的学习能力</strong>。STTFormer的结构如图1所示。</p><p>总结：该部分介绍了STTFormer模型，该模型与MAE不同，采用了骨架数据驱动的Transformer结构，适用于处理骨架数据的时空信息。STTFormer的结构包括Spatio-Temporal Tuples Attention（STTA）自注意模块和Inter-Frame Feature Aggregation（IFFA）特征聚合模块，有助于提取和集成骨架数据的多关节表示，从而提高相似动作的识别能力。这一部分强调了STTFormer相对于ViT的优势，特别适用于骨架数据的处理。</p><h3 id="时空遮蔽策略">时空遮蔽策略</h3><p>我们提出了一种时空遮蔽方法，<strong>用于对骨架序列输入的一部分进行遮蔽</strong>，我们的遮蔽策略流程如图2所示。</p><p>时空遮蔽方法。图2显示了我们的帧级遮蔽方法。根据预设的帧遮蔽比率，<strong>随机去除一部分帧并存储它们的索引，然后剩下的帧将通过关节级别的空间遮蔽方法进行处理</strong>。</p><p>空间遮蔽方法。如图2所示，在所有输入帧上实施了时空遮蔽方法后，<strong>剩下的帧将通过空间遮蔽策略进行处理</strong>。根据预设的关节遮蔽比率，我们<strong>会随机地在每个未遮蔽的帧中遮蔽部分关节</strong>。值得注意的是，在这个随机的空间遮蔽方法中，<strong>被遮蔽的关节的索引不是固定的</strong>，这意味着<strong>不同帧中的相同关节可能被遮蔽或不被遮蔽</strong>。这种简单的方法如图3(b)所示。除了这种遮蔽方法，我们<strong>还引入了一个具有固定索引的关节遮蔽策略，如图3©所示</strong>。<strong>不同帧中具有相同索引的关节将根据关节遮蔽比率一起被遮蔽或不被遮蔽</strong>。我们在第4.3节中进行了实验，比较了这两种遮蔽策略。</p><p>总结：这一部分描述了作者提出的骨架序列的时空遮蔽方法。这种方法分为时序遮蔽和空间遮蔽两个阶段。时序遮蔽是在帧级别进行的，随机去除一部分帧。然后，在剩余的帧中，进行空间遮蔽，其中部分关节也会被随机遮蔽。作者还介绍了两种空间遮蔽策略，一种是随机遮蔽关节的策略，另一种是基于固定索引的关节遮蔽策略。这两种策略在后续实验中进行了比较。</p><h3 id="SkeletonMAE框架结构">SkeletonMAE框架结构</h3><p>我们描述了SkeletonMAE的主要组件，例如编码器、解码器、重构序列、损失函数和骨架动作识别的微调流程。流程和SkeletonMAE的结构如图1所示。</p><p>SkeletonMAE编码器。我们的编码器基于STTFormer，<strong>只处理可见的骨架标记</strong>。给定一个骨架序列作为输入，我们分别应用了<strong>帧遮蔽和关节遮蔽方法</strong>。这些在空间和时间上未遮蔽的标记被馈送到<strong>SkeletonMAE编码器，将输入映射到时空嵌入特征</strong>。</p><p>SkeletonMAE解码器。我们的<strong>解码器也采用了STTFormer的结构</strong>。与MAE中的解码器相同，SkeletonMAE解码器中的<strong>时空嵌入特征被处理以重构原始序列</strong>。同时，<strong>为了保留用于重构的位置信息，还引入了位置嵌入。解码器的输出是重构序列，应与未遮蔽的原始序列相同。</strong></p><p>重构。我们使用均方误差（MSE）损失来衡量重构的结果。在这种情况下，我们<strong>计算原始骨架序列与重构序列之间的MSE损失</strong>，如下所示：</p><p>MSE = 1 N ΣN i=1 |Si − S∗ i |2， 其中i是帧的索引，N是样本数量，S是输入序列，S∗是重构序列。</p><p>总结：这一部分描述了SkeletonMAE的架构，包括编码器、解码器和重构方法。编码器和解码器都基于STTFormer结构，用于处理骨架序列的重构。重构的质量通过均方误差损失来衡量，以衡量原始骨架序列与重构序列之间的差异。</p><h3 id="动作识别微调">动作识别微调</h3><p>为了评估SkeletonMAE学习骨架表示的能力，我们<strong>加载了从预训练中获得的参数权重</strong>，**用所有的训练数据对模型进行微调，然后使用识别准确度来预测每个动作的标签。**微调的过程如图1(b)所示。与最新的基于对比的自监督骨架动作识别方法[20, 14]不同，<strong>这些方法通过线性评估协议验证模型，我们只关注骨架动作识别任务的端到端微调</strong>。</p><p>总结：这一部分描述了如何评估SkeletonMAE对骨架表示的学习能力。首先，他们使用从预训练中获得的参数权重对模型进行微调，然后使用准确度来评估每个动作的标签预测。与其他方法不同，他们侧重于端到端微调而不是线性评估协议。这是评估SkeletonMAE性能的关键步骤。</p><h1>Masked Motion Predictors are Strong 3D Action Representation Learners<strong>蒙面动作预测器是强大的3D动作表示学习者</strong></h1><h2 id="Abstract">Abstract</h2><p>在3D人体动作识别中，有限的监督数据使充分发挥强大网络（如变换器）的建模潜力变得具有挑战性。因此，研究人员一直积极探索<strong>有效的自监督预训练策略</strong>。在这项工作中，我们展示了<strong>与按照常见的假设任务对人体关节进行遮挡自部件重建不同</strong>，==明确的上下文运动建模对于学习3D动作识别的有效特征表示成功至关重要==。形式上，我们提出了Masked Motion Prediction (MAMP) 框架。具体来说，提出的MAMP以经过遮挡的时空骨骼序列作为输入，并<strong>预测了被遮挡的人体关节的对应时间运动</strong>。考虑到骨骼序列的高时间冗余性，我们的MAMP中的<strong>运动信息还充当经验语义丰富性先验，指导了遮挡过程</strong>，促进更好地关注语义丰富的时间区域。在NTU-60、NTU-120和PKU-MMD数据集上进行的大量实验表明，提出的MAMP预训练大大提高了采用的普通变换器的性能，实现了无花哨的最先进结果。我们的MAMP源代码可在https://github.com/maoyunyao/MAMP上找到。</p><p>要点总结：</p><ul><li>3D人体动作识别中受限的监督数据使建模成为挑战。</li><li>研究人员研究了自监督预训练策略。</li><li>本工作提出了Masked Motion Prediction (MAMP) 框架，与先前的自监督任务不同，它<strong>专注于明确的上下文运动建模</strong>。</li><li>MAMP使用<strong>遮挡的骨骼序列作为输入，预测遮挡的关节的时间运动</strong>。</li><li>MAMP利用骨骼序列的时间冗余性，并将运动信息作为先验，以更好地关注语义丰富的时间区域。</li><li>实验证明，MAMP预训练大大提高了3D动作识别的性能，达到最先进的结果。源代码可在https://github.com/maoyunyao/MAMP上找到。</li></ul><p>// 添加阿里云 maven 地址<br>maven { url’<a href="http://maven.aliyun.com/nexus/content/groups/public/">http://maven.aliyun.com/nexus/content/groups/public/</a>’ }<br>maven { url’<a href="http://maven.aliyun.com/nexus/content/repositories/jcenter">http://maven.aliyun.com/nexus/content/repositories/jcenter</a>’ }<br>maven { url’<a href="https://maven.aliyun.com/repository/public/">https://maven.aliyun.com/repository/public/</a>’ }<br>maven { url’<a href="https://maven.aliyun.com/repository/google/">https://maven.aliyun.com/repository/google/</a>’ }<br>maven { url’<a href="https://maven.aliyun.com/repository/jcenter/">https://maven.aliyun.com/repository/jcenter/</a>’ }<br>maven { url’<a href="https://maven.aliyun.com/repository/central/">https://maven.aliyun.com/repository/central/</a>’ }<br>maven { url’htttps://repo.spring.io/plugins-release’}<br>// ————————————————<br>// 版权声明：本文为CSDN博主「TMS320VC5257H」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>// 原文链接：<a href="https://blog.csdn.net/youngwah292/article/details/110743407">https://blog.csdn.net/youngwah292/article/details/110743407</a></p><h2 id="1-Introduction">1. Introduction</h2><ol><li>如何准确识别人类动作一直是计算机视觉领域的长期挑战。最近，随着深度感知和姿态估计技术的进步，基于<strong>骨架的3D人体动作识别</strong>已经成为一个新兴问题，对于人机互动、视频监控、虚拟现实等一系列应用具有重要意义。尽管骨架在计算效率和背景稳健性方面具有优势，<strong>但现有的监督式3D动作识别方法严重依赖于良好注释的训练序列，而获取这些注释需要耗费大量时间和精力。</strong></li></ol><p>要点总结：</p><ul><li><p>准确识别人类动作一直是计算机视觉领域的挑战。</p></li><li><p>基于骨架的3D人体动作识别因深度感知和姿态估计技术的进步而成为新兴问题，适用于多种应用。</p></li><li><p>==现有的监督式方法需要大量的已注释训练数据，这些数据获取耗时且需要大量工作==</p></li><li><p>Furthermore, limited supervision also leads to the overfitting issue in general models, especially for transformers that are with weak inductive bias and high model capacity. 还有，有限的监督还会导致通用模型，特别是对于**==感应偏置较弱、模型容量较大的transformers，出现过拟合问题==**</p></li><li><p>These facts motivate the exploration of self-supervised 3D action representation learning. 这些事实激发了对自监督3D动作表示学习的探索。</p></li><li><p>在文献中，最初为图像开发的流行托词任务已被用于3D动作表示学习，如着色[58]、重建[62,46,27]、对比学习[23,48,49]等</p></li><li><p>Among them, contrastive learning once dominated 3D action representation learning with its concise framework and promising performance. 在其中，<strong>==对比学习==曾经以其简洁的框架和有希望的性能主导了3D动作表示学习。</strong></p></li><li><p>Nevertheless, as a global representation learner, it still suffers from certain limitations, such as the lack of explicit constraints for temporal context modeling and the over-reliance on heuristic action data augmentations, impeding its further exploration of 3D actions. 然而，作为全局表示学习器，它仍然存在一些限制，比如**==缺乏明确的时间上下文建模约束以及对启发式动作数据增强的过分依赖==，这阻碍了其对3D动作的进一步探索。**</p></li><li><p>Recently, as transformers flourish in computer vision, masked autoencoder (MAE) has attracted a surge of research interest for its exceptional performance. 最近，随着变换器在计算机视觉领域的繁荣，**遮蔽自动编码器（MAE）**因其卓越的性能引起了大量研究兴趣。</p></li><li><p>Given that a 3D skeleton serves as an abstract representation of human behaviors, there has been growing interest in applying the MAE concept to 3D action representation learning, to capture the underlying spatio-temporal dynamics of skeleton sequences. 鉴于3D骨架充当了人类行为的抽象表示，人们越来越有兴趣<strong>将MAE概念应用于3D动作表示学习，以捕捉骨架序列的潜在时空动态。</strong></p></li><li><p>Early attempts generally followed the practice of images, employing masked self-reconstruction of human joints as the pre-training pretext. 早期的尝试通常<strong>遵循图像的惯例</strong>，采用人体关节的遮蔽自重建作为预训练方法。</p></li><li><p>Despite considerable effort, we argue that the network is not effectively directed to prioritize contextual motion modeling in such a self-reconstruction objective, which is, however, crucial for comprehending 3D actions as the appearance information is greatly erased in human skeletons. 尽管付出了相当大的努力，我们认为网络在这种==<strong>自重建目标</strong>中<strong>没有有效地优先考虑上下文动作建模</strong>==，然而，这对于理解3D动作至关重要，因为<strong>人类骨架中的外观信息被大大抹去</strong>。==如何更好地在自监督3D动作表示学习中探索上下文运动线索是一个有效的问题==</p></li><li><p>由于有限的监督数据，强大网络（如变换器）在3D人体动作识别中很难发挥潜力。</p></li><li><p>自监督预训练策略成为解决这个问题的方法，特别是对于3D动作识别。</p></li><li><p>以前为图像开发的预文本任务已经用于3D动作识别，如上色、重建、对比学习等。</p></li><li><p><strong>对比学习曾经主导3D动作识别</strong>，但存在一些限制，如<strong>时间上下文建模不足以及对启发式动作数据增强的过度依赖</strong>。</p></li><li><p><strong>遮蔽自动编码器</strong>（MAE）引起了人们的关注，因为它在计算机视觉中表现出色。</p></li><li><p>3D骨架作为人类行为的抽象表示，吸引了<strong>将MAE概念应用于3D动作识别学习</strong>的兴趣。</p></li><li><p>尽管付出了努力，<strong>自重建的网络没有有效地优先考虑上下文运动建模</strong>，这对于理解3D动作至关重要，因为人类骨架中的外观信息被大大抹去。</p></li><li><p>By consolidating this idea, we introduce Masked Motion Prediction (MAMP), a simple yet effective framework to address the problem of self-supervised 3D action representation learning. 通过巩固这一想法，我们引入了<strong>Masked Motion Prediction（MAMP）</strong>，这是一个简单但有效的框架，用于解决自监督的3D动作表示学习问题。</p></li><li><p>Specifically, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and turns to predict the corresponding temporal motion of the masked</p></li><li><p><strong>mmap以掩蔽的时空骨骼序列为输入，转向预测掩蔽的人体关节相应的时间运动。</strong></p></li><li><p>In this way, the network is directly encouraged for contextual motion modeling. 这样，<strong>网络被直接鼓励进行上下文运动建模。</strong></p></li><li><p>Moreover, given the observation that moments with significant motion are often critical for human action understanding, in our MAMP, the temporal motion is used not only as the pre-training objective but also as an empirical semantic richness prior that effectively guiding the skeleton masking process. 此外，<strong>鉴于观察到==重要的动作时刻通常对于理解人体动作至关重要==，在我们的MAMP中，时间运动不仅用作预训练目标，还用作经验性的语义丰富性先验，有效地引导骨架遮蔽过程。</strong></p></li><li><p>Compared to the random version, the proposed motion-aware masking strategy takes additional temporal motion intensity as input. It first converts the input intensity into a probability distribution and then utilizes the re-parameterization technique for efficient probability-guided masked token sampling. 与随机版本相比，提出的<strong>具有运动感知的遮蔽策略以额外的时间运动强度作为输入</strong>。<strong>它首先将输入强度转换为概率分布，然后利用重新参数化技术进行高效的概率引导的遮蔽标记抽样。</strong></p></li><li><p>As a result, joints with significant motion are masked with a higher probability, facilitating better attention to semantically rich temporal regions. 结果，<strong>具有显著运动的关节以更高的概率被遮蔽，有助于更好地关注语义丰富的时间区域。</strong></p></li><li><p>As illustrated in Figure 1, compared to masked self-reconstruction of human joints, masked motion prediction acts as a more effective pretext task for 3D action representation learning. 如图1所示，<strong>与对人体关节进行遮蔽的自重建相比，遮蔽的运动预测作为3D动作表示学习的更有效的 pretext task</strong>。</p></li><li><p>It substantially alleviates the problem that the transformers cannot fully unleash their modeling potential for human actions due to the scarcity of annotated 3D skeletons. 这在很大程度上<strong>缓解了变换器不能充分发挥其对人体动作建模潜力的问题，因为缺乏已注释的3D骨架。</strong></p></li><li><p>The adopted vanilla transformer sets a series of state-of-the-art records in 3D action recognition after MAMP pre-training, without the need for bells and whistles such as multi-stream ensembling. 采用的普通变换器在MAMP预训练后在3D动作识别中创造了一系列最新记录，而无需多通道集成等繁琐操作。</p></li><li><p>Specifically, compared to training from scratch, our MAMP demonstrates significant absolute performance improvements of 10.0% and 13.2% on the challenging cross-subject protocol of NTU RGB+D 60 and NTU RGB+D 120 datasets, resulting in top-1 accuracy of 93.1% and 90.0%, respectively. 具体来说，与从零开始训练相比，我们的MAMP在NTU RGB+D 60和NTU RGB+D 120数据集的<strong>具有挑战性的跨主题协议上</strong>表现出显著的绝对性能改进，分别达到了93.1%和90.0%的top-1准确度。</p></li><li><p>We hope this simple yet effective framework will serve as a strong baseline that facilitates</p></li></ul><p>要点总结：</p><ul><li>介绍了<strong>Masked Motion Prediction</strong>（MAMP），这是一个简单而有效的框架，用于自监督3D动作表示学习。</li><li>MAMP通过<strong>遮蔽的骨架序列预测遮蔽的人体关节的时间运动，以鼓励上下文运动建模</strong>。</li><li>引入了<strong>具有运动感知的遮蔽策略，这策略以额外的时间运动强度为输入，用于提高关注语义丰富的时间区域</strong>。</li><li>与随机版本相比，此策略以更高的概率遮蔽具有显著运动的关节。</li><li>相对于自重建，遮蔽的运动预测对于3D动作表示学习是更有效的借口任务。</li><li>MAMP极大地改进了3D动作的<strong>建模能力</strong>，使得普通变换器在MAMP预训练后创造了最新的记录。</li><li>MAMP在具有挑战性的数据集上表现出显著的绝对性能改进，为未来3D动作预训练研究提供了强大的基准。</li><li>我们提出了<strong>掩蔽运动预测来学习3D动作表示</strong>，这大大<strong>缓解了传统掩蔽自我重建范式中上下文运动建模不足</strong>的问题。</li><li>我们设计了运动感知掩蔽策略，该策略**==将运动强度作为自适应关节掩蔽的经验语义丰富度==**。</li><li>我们在三个普遍的基准上进行了广泛的实验，以验证我们方法的有效性。值得一提的是，通过我们提出的MAMP, vanilla transformer首次实现了3D动作识别的最佳记录。</li></ul><blockquote><p>缺乏上下文的运动建模  就直接鼓励进行上下文运动建模</p><p>时间运动建模还用作经验性的语义丰富性先验，有效地引导骨架遮蔽过程</p><p>具有运动感知的遮蔽策略以额外的时间运动强度作为输入？？</p></blockquote><h2 id="2-Related-work">2.Related work</h2><h3 id="Supervised-3D-Action-Recognition">Supervised 3D Action Recognition</h3><ul><li>监督式3D动作识别</li><li>How to better model the dynamic skeletons for supervised action recognition is an extensively studied problem.</li><li><strong>如何更好地对监督式动作识别建模动态骨骼</strong>是一个广泛研究的问题。</li><li>In many early works, RNNs are favored for their excellent sequential modeling capability, such as the hierarchical RNN model proposed in [14] and the 2D Spatio-Temporal LSTM in [30, 29].</li><li>在许多早期研究中，由于其出色的时序建模能力，例如[14]中提出的分层RNN模型和[30, 29]中的2D时空LSTM，RNNs备受青睐。</li><li>In view of the great success of CNNs [21, 18] in image understanding, some methods also try to apply it to 3D action recognition.</li><li>鉴于CNNs在图像理解中取得的巨大成功，一些方法也尝试将其应用于3D动作识别。</li><li>To cater for the input format, [13] and [22] treat the skeleton sequence as a three-channel (x, y, and z coordinates) pseudo-image, with the number of frames and joints as height and width, respectively.</li><li>为适应输入格式，[13]和[22]将骨骼序列视为三通道（x、y和z坐标）的伪图像，其中帧数和关节数分别作为高度和宽度。</li><li>Considering the natural connections between joints, ST-GCN [55] introduces the Graph Neural Networks (GCNs) for skeleton modeling, where the convolution kernels are elaborately designed according to the skeleton topology.</li><li>考虑到关节之间的自然联系，ST-GCN [55]引入了图神经网络（GCNs）用于骨骼建模，其中卷积核根据骨骼拓扑结构精心设计。</li><li>The astonishing performance of ST-GCN has led the trend of GCN-based 3D action recognition, with numerous subsequent improvements emerging in input streams [51, 26, 40], kernel design [5, 61, 39, 32], etc.</li><li>ST-GCN的出色性能引领了基于GCN的3D动作识别趋势，出现了众多后续改进，涵盖了输入流[51, 26, 40]、卷积核设计[5, 61, 39, 32]等多个方面。</li><li>Recent approaches [36, 35, 41] try to introduce the popular vision transformer into 3D action recognition. However, under limited training data, vanilla transformers with weak inductive bias cannot be fully trained.</li><li>最近的方法[36, 35, 41]尝试将流行的视觉变换器引入3D动作识别。然而，在有限的训练数据下，具有弱感应偏差的普通变换器无法充分训练。</li><li>Therefore, many customized designs are required in existing supervised attempts, such as temporal convolution [36], graph convolution [35, 36], space-time separation [41], etc.</li><li>因此，<strong>在现有的监督尝试中需要许多定制的设计，例如时间卷积[36]、图卷积[35, 36]、时空分离[41]等。</strong></li><li>In our approach, we demonstrate that pre-training with masked motion prediction is key to the success of transformers in 3D action recognition.</li><li>在我们的方法中，我们证明了<strong>使用遮蔽运动预测进行预训练是Transformer在3D动作识别中取得成功的关键</strong>。</li><li>The proposed MAMP framework endows the vanilla transformer with unrivaled performance.</li><li>提出的MAMP框架使普通变换器具有无与伦比的性能。</li></ul><p>要点总结：</p><ul><li>介绍了监督式3D动作识别的问题和不同方法。</li><li>提到了RNNs和CNNs以及GCNs等不同的方法和技术，这些方法都用于尝试更好地建模3D动作。</li><li>引入了视觉变换器，并指出在有限的训练数据下，标准变换器需要许多定制设计。</li><li>作者提出了一种名为MAMP的方法，它通过预训练和遮蔽运动预测来改进变换器在3D动作识别中的性能。</li></ul><h3 id="Self-supervised-3D-Action-Recognition">Self-supervised 3D Action Recognition</h3><ul><li>自监督3D动作识别</li><li>Self-supervised representation learning aims to capture the domain priors from unlabeled data so as to facilitate the application of the model in downstream tasks.</li><li>自监督表示学习旨在从未标记的数据中捕获领域先验知识，以便在下游任务中应用该模型。</li><li>In 3D human action recognition, many pretext tasks have been utilized to explore the action context that resides in the skeleton sequence.</li><li>在3D人体动作识别中，<strong>使用了许多前提任务来探索骨骼序列中存在的动作上下文</strong>。</li><li>Among them, LongT GAN [62] and P&amp;C [46] try to learn 3D action representation by autoencoder-based sequence reconstruction, where the decoder in P&amp;C is further weakened to promote the learning of the feature encoder.</li><li>在其中，LongT GAN [62]和P&amp;C [46]尝试通过<strong>基于自动编码器的序列重建来学习3D动作表示</strong>，其中P&amp;C中的<strong>解码器被进一步削弱以促进特征编码器的学习</strong>。</li><li>In Colorization [58], the skeleton sequences are treated as point clouds and action representation is learned by colorizing each joint based on its spatial and temporal orders.</li><li>在Colorization [58]中，<strong>骨骼序列被视为点云，并且根据其空间和时间顺序对每个关节进行着色以学习动作表示</strong>。</li><li>Recently, many contrastive learning-based approaches [27, 48, 23, 49, 59, 34] have emerged, showing superior performance compared to earlier works.</li><li>最近，出现了<strong>许多基于对比学习的方法 [27, 48, 23, 49, 59, 34]，表现出优于早期方法的性能</strong>。</li><li>To learn better 3D action representation, they either try to dig helpful supervision across different skeleton modalities [23, 34], or explore better action data augmentation [49] and positive sample mining strategies [59].</li><li>为了学习更好的3D动作表示，它们要么尝试在不同的骨骼模态之间获取有用的监督 [23, 34]，要么探索更好的动作数据增强 [49] 和正样本挖掘策略 [59]。</li><li>Nevertheless, as a global feature learner originally designed for images, contrastive learning lacks explicit constraints on the exploration of temporal motion context, limiting its further development for 3D actions.</li><li><strong>尽管对比学习最初是为图像设计的全局特征学习器，但它缺乏对探索时序动作上下文的明确约束，从而限制了其在3D动作领域的进一步发展。</strong></li><li>SkeletonMAE [53] first introduces the idea of MAE[17] into transformer-based 3D action representation learning, where the original joint coordinates of masked regions are predicted.</li></ul><p>Wenhan Wu, Yilei Hua, Ce zheng, Shiqian Wu, Chen Chen,<br>and Aidong Lu.==SkeletonMAE: Spatial-temporal masked au-<br>toencoders for self-supervised skeleton action recognition.==<br>arXiv preprint arXiv:2209.02399, 2022. 1, 3, 4, 6, 7, 8</p><p>==Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr<br>Doll´ar, and Ross Girshick. Masked autoencoders are scalable<br>vision learners. In Proceedings ofthe IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition (CVPR), pages<br>16000–16009, 2022. 2, 3, 4, 5, 8==</p><ul><li>SkeletonMAE [53]首次将MAE[17]的思想==引入基于变换器的3D动作表示学习中，其中预测遮蔽区域的原始关节坐标==。</li><li>In our approach, we demonstrate that such a self-reconstruction objective is sub-optimal for learning 3D action representation.</li><li>在我们的方法中，我们证明了这种自重建目标不是学习3D动作表示的最佳选择。</li><li>Therefore, we introduce the Masked Motion Prediction (MAMP) framework for explicit contextual motion modeling, resulting in significantly better performance compared to raw skeleton reconstruction.</li><li>因此，我们引入了Masked Motion Prediction（MAMP）框架，==用于明确的上下文动作建模==，与原始骨骼重建相比，性能显著更好。</li></ul><p>总结：</p><p>该段描述了自监督3D动作识别领域的研究，其中各种前提任务和方法被用来探索骨骼序列中的动作上下文。虽然许多方法已经出现，包括对比学习等，但文章指出对比学习对于探</p><h3 id="Masked-Visual-Prediction">Masked Visual Prediction</h3><ul><li>掩盖的视觉预测</li><li>With the development of vision transformers [4, 12, 50], the masked prediction derived from the autoencoder [1] has revived again.</li><li>随着视觉变换器的发展 [4, 12, 50]，从自动编码器 [1] 导出的掩盖预测再次复活。</li><li>Similar to the BERT [11] pre-training in NLP, the input tokens are randomly masked and corresponding objectives are predicted, which can be the raw pixels [17], HOG features [52], or token ids from offline learned dVAEs [2].</li><li>类似于自然语言处理中的BERT [11] 预训练，输入令牌被随机遮蔽，然后预测相应的目标，这些目标可以是原始像素 [17]、HOG特征 [52] 或来自离线学习的dVAEs [2] 的令牌ID。</li><li>Recently, there have also been attempts [45, 57] to use optical flow or temporal difference of images as the auxiliary reconstruction objectives, but inferior performance is observed when they are applied alone.</li><li>最近，也有尝试 [45, 57] 使用光流或图像的时间差作为辅助重建目标，但当它们单独应用时性能较差。</li><li>This is largely attributed to the high redundancy of the raw images, where the key foreground motion is difficult to be pre-extracted accurately.</li><li>这在很大程度上归因于原始图像的高冗余性，其中关键的前景运动难以准确预先提取。</li><li>In our approach, we employ the idea of masked visual prediction for 3D action representation learning, with the temporal skeleton motion adopted as the only reconstruction target.</li><li>在我们的方法中，我们采用了==掩盖的视觉预测的思想==来学习3D动作表示，其中==采用了时间骨架运动作为唯一的重建目标==。</li><li>Different from images, the explicit temporal correspondence of joints in the human skeleton sequence enables the ready extraction of their accurate motion context.</li><li>与图像不同，人体骨架序列中<strong>关节的明确时间对应关系使得能够轻松提取其准确的运动背景</strong>。</li><li>Furthermore, we also incorporate motion intensity as the semantic richness prior to guide the masking process.</li><li>此外，我们还将运动强度作为语义丰富性的先验，以引导遮盖过程。</li></ul><p>总结：</p><p>该段介绍了&quot;Masked Visual Prediction&quot;，一种用于3D动作表示学习的方法。这个方法在视觉变换器的发展中得以复兴，类似于NLP中的BERT预训练，采用了随机遮蔽的输入令牌，以及相应的目标预测。不同于图像，3D骨架序列中的关节具有明确的时间对应关系，使得能够更容易地提取准确的运动背景。此外，==运动强度也被纳入作为语义丰富性的先验来引导遮盖过程==。</p><h2 id="Our-Method">Our Method</h2><h3 id="3-1-Overview">3.1. Overview</h3><p>图2展示了我们提出的遮罩运动预测（MAMP）框架的总体流程。</p><p>它以骨架序列S ∈ RTs×V×Cs作为输入，该序列是从原始数据中随机裁剪并调整为固定的时间长度Ts。其中，V和Cs分别是关节数量和坐标通道数量。</p><p>输入的运动序列M ∈ RTs×V×Cs也被提取出来，其定义为时间维度上的差分（第一帧手动填充）。</p><p>与大多数视觉变换器一样，输入关节被线性映射为关节嵌入E ∈ RTe×V×Ce。</p><p>然后，应用了运动感知遮罩策略，根据时间运动强度的指导，对大多数嵌入特征进行遮罩处理。</p><p>剩下的特征由编码器-解码器架构处理，其中变换器编码器从未被遮罩的关节嵌入中学习表示，变换器解码器基于可学习的遮罩标记和变换器编码器的潜在表示执行上下文建模。</p><p>与MAE [17]不同，后者用于表示学习并重建原始信号，MAMP采用了运动预测头，该头以解码后的特征作为输入并预测输入骨架序列的时间运动。</p><p>在上述的预训练之后，只有关节嵌入层和变换器编码器被保留供下游应用使用。</p><p><strong>总结：</strong> 该段落描述了MAMP框架的总体流程，包括输入骨架序列的处理、关节嵌入、运动感知遮罩策略以及运动预测，以及在预训练后保留的用于下游应用的层。这个框架旨在通过自监督学习提取骨架序列的有用表示。</p><h3 id="3-2-Joint-Embedding">3.2. Joint Embedding</h3><p>在大多数基于Transformer的尝试中[35, 41, 36]，每个时空骨架关节都被单独嵌入，导致了大量的输入标记。然而，在我们的方法中，考虑到时间上的冗余，我们将输入的骨架序列S ∈ RTs×V×Cs分为在时间上不重叠的段S′ ∈ RTe×V×l×Cs，其中l是每个段的长度，Te = Ts/l。在每个段中，具有相同空间位置的关节被一起嵌入：E = \texttt {JointEmbed}(S’) ∈ \mathbb {R}^{T_e \times V \times C_e}，其中Ce是嵌入特征的维度。与原始的骨架序列相比，嵌入E的时间分辨率降低了l倍，从而提高了计算效率。</p><p>总结：这段文字描述了一个在骨架动作识别中的创新方法，采用了分段嵌入策略，将原始的骨架序列分成不重叠的时间段，将具有相同空间位置的关节一起嵌入，以减少输入标记数量和提高计算效率。这一方法有望在处理大规模骨架数据时更加高效。</p><h3 id="3-3-Motion-Extraction">3.3. Motion Extraction</h3><p><img src="../assets/image-20231023200018904.png" alt="image-20231023200018904"></p><h3 id="3-4-Motion-Aware-Masking">3.4. Motion-Aware Masking</h3><p><img src="../assets/image-20231023200157110.png" alt="image-20231023200157110"></p><h3 id="3-5-Masked-Motion-Prediction">3.5. Masked Motion Prediction</h3><p><img src="../assets/image-20231023200246428.png" alt="image-20231023200246428"></p><h1>Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning    提示与masked运动建模的对比:  面向通用的3D动作表示学习</h1><h2 id="1introduction">1introduction</h2><p>我们提出了用于多粒度表示的PCM3，它以一种<strong>互惠的方式集成了掩蔽骨架预测和对比学习范式</strong>。我们使用<strong>掩蔽预测网络来产生更多不同的正向运动视图来进行对比学习</strong>。同时，<strong>生成的梯度利用高级语义信息依次传播和引导掩蔽预测学习</strong>。</p><p>•考虑到<strong>不同的数据视图和预训练任务可能会产生相互干扰</strong>，我们在<strong>多任务预训练中引入了领域特异性提示和任务特异性提示</strong>。这些<strong>可训练的提示</strong>使模型能<strong>够对不同的骨架实现更有区别的表示</strong>。</p><p>•我们执行严格的<strong>定量实验</strong>，<strong>评估最先进的自我监督的3D动作表示学习技术的泛化效果</strong>，在5个下游任务，包括识别，检索，检测和运动预测，在未损坏和损坏的骨骼。我们的研究为研究界提供了一个全面的基准，我们相信它可以提供有价值的见解，并有助于该领域的未来研究。</p><p>INTRODUCTION</p><ul><li>引言</li></ul><p>Human activity understanding is a crucial problem in multi-media processing on account of its significant role in real-life applications, such as human-robotics interaction [18], healthcare [25] and autonomous driving [2]</p><ul><li>人类活动理解是多媒体处理中的一个关键问题，因为它在现实生活中的应用中起着重要作用，如人机交互[18]、医疗[25]和自动驾驶[2]等。</li></ul><p>As a highly efficient representation for human activity understanding, 3D skeletons represent the human form by 3D coordinates of key body joints. In comparison to other representations such as RGB videos and depth data, skeletons are lightweight, compact, and privacy-preserving. Owing to these competitive advantages, skeletons have become widely used in human action analysis.</p><ul><li>作为人类活动理解的高效表示，<strong>3D骨架通过关键身体关节的3D坐标来表示人体形态</strong>。与其他表示方法（如RGB视频和深度数据）相比，骨架轻巧、紧凑且具有隐私保护性。由于这些竞争优势，骨架已广泛用于人体动作分析。</li></ul><p>Many efforts have been made on the supervised skeleton-based action learning [6, 9, 35, 47]. However, the performance of these methods heavily relies on a huge amount of labeled data, which requires time-consuming and expensive data annotation work. This inherent shortcoming of full supervision limits their applications in the real world. Therefore, more and more attention has been paid to self-supervised 3D action representation learning recently to learn meaningful features from unlabeled data.</p><ul><li>在监督的基于骨架的动作学习方面已经做出了很多努力[6, 9, 35, 47]。然而，**这些方法的性能严重依赖于大量的已标记数据，**这需要耗时且昂贵的数据注释工作。**全面监督的固有缺陷限制了它们在现实世界中的应用。**因此，最近越来越多的关注已经转向了自监督的3D动作表示学习，以从未标记的数据中学习有意义的特征。</li></ul><p>Self-supervised 3D action representation learning research has mainly focused on two paradigms: reconstruction-based and contrastive learning-based methods. Reconstruction-based methods leverage an encoder-decoder architecture to learn representations by predicting masked skeletons (i.e., masked modeling) or reconstructing original data. These methods focus on joint-level feature modeling and capture spatial-temporal relationships. In contrast, contrastive learning-based methods use data augmentations to construct positive/negative pairs, and apply an instance discrimination task to learn sequence-level semantic features.</p><ul><li><strong>自监督3D动作表示学习的研究主要集中在两种范式上：<strong>基于重建和基于对比学习的方法。基于重建的方法</strong>利用编码器-解码器架构通过预测掩盖的骨架</strong>（<strong>即掩模建模</strong>）或重建原始数据来学习表示。这些方法<strong>专注于关节级特征建模并捕获时空关系</strong>。相比之下，<strong>基于对比学习的方法使用数据增强来构建正负对，并应用实例判别任务来学习序列级语义特征</strong>。</li></ul><p>However, it is noticed that most recent representation learning methods focus on the single paradigm to model the joint-level (by masked skeleton modeling) [45, 53] or sequence-level (by contrastive learning) [8, 20, 26, 52] features solely. As a result, it is difficult for these methods to generalize well to different downstream tasks, e.g., recognition task and motion prediction, because they cannot learn representations of different granularity simultaneously and effectively. Although some works [21, 40, 43] make valuable efforts to combine the above two approaches to learn richer representations, only mediocre improvement is observed. It is because simply combining them ignores the interference due to the gaps between feature modeling mechanisms of masked prediction and contrastive learning [30], and fails to utilize the potential synergy. These problems limit the generalization power of the model, and versatile 3D action representation learning remains a challenging and under-explored area.</p><ul><li>但需要注意的是，大多数最近的<strong>表示学习方法专注于单一范式，<strong><strong><strong>仅对关节级（通过掩蔽骨架建模）</strong></strong>[45, 53] 或</strong>序列级（通过对比学习）</strong>[8, 20, 26, 52] <strong>特征进行建模</strong>。因此，这些方法难以很好地推广到不同的下游任务，<strong>例如识别任务和动作预测</strong>，因为它们<strong>不能同时有效地学习不同粒度的表示</strong>。尽管一些工作[21, 40, 43]付出了努力，<strong>将上述两种方法结合起来学习更丰富的表示</strong>，但只有一般的改进。这是因为简单地将它们结合起来**忽略了由于掩蔽预测和对比学习的特征建模机制之间差距而产生的干扰，未能充分利用潜在的协同作用。**这些问题限制了模型的泛化能力，<strong>多功能的3D动作表示学习仍然是一个具有挑战性且未被充分探讨的领域。</strong></li></ul><p>To this end, we propose the prompted contrast with masked motion modeling, PCM3, which explores the mutual collaboration between the above two paradigms for versatile 3D action representation learning as shown in Figure 1. Specifically, the well-designed inter-intra-contrastive learning and topology-based masked skeleton prediction are first proposed as the basic pipelines. Furthermore, we connect the two tasks and explore the synergy between them. The views in masked prediction training are utilized as novel positive samples for contrastive learning. In turn, the masked prediction branch is also updated via the gradients from the contrastive learning branch for higher-level semantic guidance. Meanwhile, to reduce the distraction of learning between different pretext tasks and data views, we</p><ul><li>为此，我们提出了PCM3，即<strong>采用具有掩盖运动建模的提示对比方法</strong>，它<strong>探索了上述两种范式之间的相互协作</strong>，用图1所示的方式进行多功能3D动作表示学习。具体而言，我们首次<strong>提出了精心设计的内外对比学习和基于拓扑的掩模骨架预测作为基本流程</strong>。此外，我们将这两个任务连接起来，<strong>探索它们之间的协同作用</strong>。<strong>掩模预测训练中的视图被用作对比学习的新颖正样本</strong>。反过来，<strong>掩模预测分支也通过对比学习分支的梯度进行更新</strong>，<strong>以进行更高级别的语义引导</strong>。同时，为了<strong>减少不同前提任务和数据视图之间的学习干扰</strong>，我们提出了<strong>双提示多任务预训练策略</strong>。应用了两种类型的提示，即<strong>领域特定提示和任务特定提示</strong>，以明确指导模型从不同的数据视图/任务中学习。我们进行了广泛的实验，涵盖了五个下游任务，以提供全面的评估。所提出的方法与最先进的方法相比，展现出有前途的泛化能力。</li></ul><p>Our contributions can be summarized as follows:</p><ul><li>我们的贡献可以总结如下：</li></ul><p>• We propose PCM3 for multi-granularity representations, which integrates masked skeleton prediction and contrastive learning paradigms in a mutually beneficial manner. • 我们提出了PCM3，用于<strong>多粒度表示</strong>，它<strong>以相互有益的方式将掩模骨架预测和对比学习范式相结合</strong>。</p><p>• We employ the masked prediction network to generate more diverse positive motion views for contrastive learning. • 我们<strong>使用掩模预测网络生成更多多样化的正向动作视图，用于对比学习</strong>。</p><p>• Meanwhile, the generated gradients are propagated and guide masked prediction learning in turn with high-level semantic information. • 与此同时，<strong>生成的梯度被传播并通过高级语义信息反过来指导掩模预测学习</strong>。</p><p>• Considering that different data views and pretraining tasks can cause mutual interference, we introduce domain-specific prompts and task-specific prompts for the multi-task pretraining. • 考虑到不同的数据视图和预训练任务可能会相互干扰，我们<strong>引入了领域特定提示和任务特定提示进行多任务预训练</strong>。</p><p>• We perform rigorous quantitative experiments to assess the generalization efficacy of state-of-the-art self-supervised 3D action representation learning techniques across five downstream tasks, including recognition, retrieval, detection, and motion prediction, on both uncorrupted and corrupted skeletons. • 我们进行了严格的定量实验，评估了最先进的自监督3D动作表示学习技术在五个下游任务中的泛化效能，包括识别、检索、检测和动作预测，在未损坏和损坏的骨骼上进行。</p><p>• Our study serves as a comprehensive benchmark for the research community, and we believe it can provide valuable insights and aid future investigation in this field. • 我们的研究为研究社区提供了全面的基准，我们相信它可以提供有价值的见解并帮助未来的研究。</p><h2 id="2-RELATEDWORKS">2 RELATEDWORKS</h2><h3 id="2-1-Skeleton-based-Action-Recognition">2.1 Skeleton-based Action Recognition</h3><p>随着深度学习的显著进步，我们研究了用于基于骨架的动作识别的循环神经网络（RNN）、卷积神经网络（CNN）、图卷积网络（GCN）和变换器（transformer）等方法。</p><p>RNN已广泛用于模拟时间依赖性和捕捉基于骨架的动作识别中的运动特征。</p><p>在[9]中的工作中，使用RNN来处理骨架数据，将其视为序列数据。</p><p>之后，Song等人在[37, 38]中提出了<strong>使用注意机制和多模态信息来增强特征表示。</strong></p><p>其他一些工作[15, 24]将每个<strong>骨架序列转换为类似图像</strong>的表示，并应用CNN模型来提取空间-时间信息。</p><p>最近，由于人体的自然拓扑结构，GCN方法引起了更多关注。</p><p>许多工作[6, 35, 47]<strong>将GCN应用于空间和时间维度</strong>[47]，在监督的基于骨架的动作识别中取得了显著的成果。</p><p>同时，<strong>变换器模型</strong>[29, 36]也显示出有希望的结果，因为它可以<strong>通过注意机制学习长期时间依赖性。</strong></p><p>然而，这些监督性作品依赖于大规模标记数据来训练模型。在本文中，我们研究了自监督的三维动作表示学习。</p><p><strong>总结：</strong> 该段落讨论了用于基于骨架的动作识别的不同方法，包括RNN、CNN、GCN和transformer。它提到了这些方法的应用领域以及它们的一些优势和限制，特别强调了监督方法需要大量标记数据，而本文将探讨自监督学习方法。</p><h3 id="2-2-Contrastive-Learning-for-Skeleton">2.2 Contrastive Learning for Skeleton</h3><p>对比学习已被证明对于骨架表示学习非常有效。</p><p>一种流行的研究观点是<strong>骨架增强</strong>，这对于学到的表示质量至关重要。</p><p>郭等人在当前的<strong>对比学习管道中探索了极端的增强</strong>。</p><p>张等人提出了<strong>分层一致性对比学习，以利用更强大的增强</strong>。</p><p>另一个视角是探索骨架中不<strong>同视图的知识。</strong></p><p>ISC使用图像、图和序列表示进行交叉对比学习。</p><p>李等人挖掘了潜在的正例，利用了不同的骨架模态，即关节、骨骼、运动，并根据相似度重新加权训练样本。</p><p>毛等人在不同视图之间进行了相互蒸馏。</p><p>与上述作品不同，我们提出将基于遮罩的建模前提任务与对比学习相结合，以对关节级别和序列级别的特征进行建模，以获得更通用的表示学习。</p><p><strong>总结：</strong> 该段落讨论了在骨架表示学习中应用对比学习的有效性，并提到了不同方法和观点，包括极端增强、分层一致性对比学习、交叉对比学习等。最后，介绍了该工作的方法，即将基于遮罩的建模与对比学习相结合，以获得更通用的表示学习。</p><h3 id="2-3-Masked-Image-Skeleton-Modeling">2.3 Masked Image/Skeleton Modeling</h3><p><strong>2.3 基于图像/骨架的遮罩建模</strong></p><p><strong>遮罩建模已在堆叠去噪自动编码器中得到探索</strong>[42]，其中<strong>遮罩操作被视为向原始数据添加噪音。</strong></p><p>最近，<strong>遮罩建模</strong>在图像表示学习的自监督学习[13, 46]中取得了显著成功。</p><p>对于骨架数据，LongT GAN [53]直接使用了一个基于自编码器的模型，通过附加的对抗训练策略进行优化。</p><p>一些作品[21, 40]将运动预测前提任务应用于学习骨架序列中的时间依赖性。</p><p>受到遮罩自动编码器[13]的启发，吴等人[45]提出了一种遮罩骨架自动编码器，用于学习空间-时间关系。</p><p>在本文中，我们探索了遮罩建模和对比学习之间的协同作用，并提出了一种基于拓扑结构的遮罩策略，以进一步增强表示学习。</p><p><strong>总结：</strong> 该段落讨论了遮罩建模在图像和骨架领域的应用，以及一些相关的自监督学习方法。它还介绍了本文的方法，即将遮罩建模与对比学习相结合，并提出了一种新的基于拓扑结构的遮罩策略以增强表示学习。</p><h2 id="3-THE-PROPOSED-METHOD-PCM3">3 THE PROPOSED METHOD: PCM3</h2><p>在这一部分中，我们首先描述了我们设计的对比学习管道(章节3.1)以及我们提出的基于拓扑的掩码建模方法(章节3.2)。然后，我们在第3.3节中进一步介绍了这两个任务之间的协同探索。基于提示的前训练策略和整个模型在3.4节给出。</p><p>3.1节 提出的整体基线</p><p>3.2节 提出的基于拓扑的掩码建模方法</p><p>3.3节 提出的两个pretext tasks 之间的协同合作</p><p>3.4节 提出的基于prompted预训练任务 和整个模型</p><h3 id="3-1-Skeleton-Contrastive-Learning">3.1 Skeleton Contrastive Learning</h3><p>出于清晰起见，对于图像/骨架的对比学习的经典设计是根据以前的工作[1, 3, 14]提供的，通常包括以下组件：</p><p>• 数据增强模块包含一系列手动数据转换，用于构建原始数据的不同视图，这些视图被视为共享相同语义的正样本。</p><p>• 编码器 𝑓(·) 用作从输入空间到潜在特征空间的映射函数。</p><p>• 嵌入投影 ℎ(·) 在编码器 𝑓(·) 之后连续应用，将编码特征映射到一个嵌入空间，其中应用自监督损失。</p><p>• 自监督损失旨在<strong>最大化正样本之间的相似性</strong>，<strong>执行特征聚类操作以获得可区分的表示空间。</strong></p><p>我们的==对比学习设计基于MoCo v2== [4]。具体来说，我们<strong>引入了骨架内部和骨架间的变换以及关系知识蒸馏，以帮助模型捕获不同的运动模式并提高表示学习</strong>。</p><p><strong>总结：</strong> 这一段描述了对比学习的经典设计，包括数据增强模块、编码器、嵌入投影和自监督损失。对比学习的设计基于MoCo v2，并引入了骨架内部和骨架间的变换以及关系知识蒸馏，以增强模型对多样化运动模式的捕获和表示学习。这些组件和方法用于构建骨架的自监督学习框架，以提高骨架表示的质量。</p><h4 id="1-Intra-skeleton-transformation-learning">1) Intra-skeleton transformation learning.</h4><ol><li>**骨架内部变换学习。**我们利用以下变换：时间裁剪-调整大小，剪切，关节抖动，遵循以前的研究[26, 41]。具体来说，给定骨架序列 𝑥，通过上述变换构建正样本对 (𝑠𝑖𝑛𝑡𝑟𝑎, 𝑠′)。然后，通过查询/关键编码器 𝑓𝑞 (·)/𝑓𝑘 (·) 和嵌入投影 ℎ𝑞 (·)/ℎ𝑘 (·)，获得相应的特征表示 (𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′)。同时，维护一个存储大量负样本的内存队列 M 用于对比学习。我们通过 InfoNCE 目标 [28] 优化整个网络： L𝐼𝑛𝑡𝑟𝑎 𝐼𝑛𝑓𝑜 = − log exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑧′/𝜏) exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑧′/𝜏) + Í 𝑖=1 exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑚𝑖/𝜏) , (1) 其中 𝑚𝑖 是 M 中与第 𝑖 个负样本对应的特征，𝜏 是温度超参数。在训练步骤后，    根据先进先出的策略，批次中的样本将更新到 M 作为负样本。关键编码器是查询编码器的动量更新版本，即 𝜃𝑘 ← 𝛼𝜃𝑘 + (1 − 𝛼)𝜃𝑞，其中 𝜃𝑞 和 𝜃𝑘 是查询编码器和关键编码器的参数，𝛼 ∈ [0, 1) 是动量系数。</li></ol><p><strong>==总结： 这一段描述了骨架内部变换学习的过程。首先，通过一系列变换构建正样本对，然后使用查询/关键编码器和嵌入投影获得特征表示。同时，维护一个存储负样本的内存队列，并通过 InfoNCE 目标来优化网络。关键编码器是查询编码器的动量更新版本，用于提高训练的稳定性==</strong></p><h4 id="2-Inter-skeleton-transformation-learning">2) Inter-skeleton transformation learning.</h4><ol><li>**骨架间转换学习。**受到==混合增强在自监督学习中的成功应用的启发==[17, 19, 34, 49]，我们将 CutMix [48]、ResizeMix [31] 和 Mixup [50] 引入到我们的骨架对比学习中。这些骨架间的变换利用两个不同的样本来生成混合增强的视图。具体来说，给定两个骨架序列 𝑠1, 𝑠2，我们随机选择上述的混合方法，并按以下方式获得混合的骨架数据 𝑠𝑖𝑛𝑡𝑒𝑟：<ol><li>• Mixup [50]：我们根据采样的混合比例 𝜆 对两个骨架序列进行插值，即，𝑠𝑖𝑛𝑡𝑒𝑟 = (1 − 𝜆)𝑠1 + 𝜆𝑠2。</li><li>• CutMix [48]：随机选择的两个骨架序列的区域在时空维度上被剪切和粘贴。𝜆 被定义为替换的关节数与总关节数的比例。</li><li>• ResizeMix [31]：这与 CutMix 类似，但在混合之前先在时序维度上对 𝑠2 进行降采样。 随后，我们可以通过 𝑧𝑖𝑛𝑡𝑒𝑟 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑖𝑛𝑡𝑒𝑟) 获得与混合数据对应的嵌入，然后优化以下损失：</li></ol></li></ol><p><strong>总结：</strong> 这一段描述了骨架间转换学习的过程，其中引入了不同的混合增强方法（Mixup、CutMix和ResizeMix）来利用两个不同样本生成混合骨架数据。通过这些混合数据，可以获得相应的嵌入表示，并进行损失优化。==这种方法有助于模型捕获多样的运动模式并提高表示学习的效果==。</p><h4 id="3-Relational-Knowledge-Distillation">3) Relational Knowledge Distillation.</h4><blockquote><p>关系知识蒸馏项 被引入 使得增强对比学习的细粒度表示</p><p>蒸馏项引入了更多的锚点来提高表示质量，从而提高了模型的性能</p></blockquote><ol><li>关系知识蒸馏。为了进一步为对比学习提供精细的语义一致性监督，我们引入了<strong>一种关系知识自蒸馏损失到正样本中</strong>。受到 [26, 44, 52] 的工作的启发，关系知识被建模为𝑧′/𝑧′ 𝑖𝑛𝑡𝑒𝑟与<strong>存储在内存队列 M 中的特征锚点之间的余弦相似性</strong>。关系分布，即与负锚点样本的相似性，被强制保持在每个正样本对之间一致。以与前述的内部变换对应的嵌入对 (𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′) 为例，该损失可以表示为： L𝐼𝑛𝑡𝑟𝑎 𝐾𝐿 = −𝑝 � 𝑧′, 𝜏𝑘</li></ol><p>log 𝑝 � 𝑧𝑖𝑛𝑡𝑟𝑎, 𝜏𝑞</p><p>， 𝑝 𝑗 (𝑧, 𝜏) = exp(𝑧 · 𝑚 𝑗/𝜏) Í 𝑖=1 exp(𝑧 · 𝑚𝑖/𝜏) ， (3) 其中 𝑚𝑖 是 M 中存储的第 𝑖 个特征锚点。 𝜏𝑘 和 𝜏𝑞 是温度超参数，分别设置为 0.05 和 0.1。 这个蒸馏项引入了更多的锚点来挖掘细粒度和语义感知的相似关系[44]，提高了表示质量。</p><p><strong>总结：</strong> 这一段介绍了一种关系知识蒸馏方法，它为对比学习提供了更精细的语义一致性监督。这种方法使用余弦相似度来度量正样本对之间的关系知识，并使其与负锚点样本的相似性保持一致。这个蒸馏项引入了更多的锚点来提高表示质量，从而提高了模型的性能。</p><h3 id="3-2-Masked-Skeleton-Prediction">3.2 Masked Skeleton Prediction</h3><p>以下是对输入段落的逐句翻译：</p><p><strong>3.2 Masked Skeleton Prediction</strong> <strong>3.2 遮蔽骨骼预测</strong></p><p>To further enrich the learned representations by the model, we integrate masked skeleton modeling, and the joint-level feature learning is baked into the training process. This further improves generalization ability, especially for dense prediction downstream tasks, as compared with using only instance-wise discrimination tasks, i.e., contrastive learning. 为了通过模型进一步丰富学到的表示，我们将遮蔽骨骼建模和关节级特征学习融入训练过程中。这进一步提高了泛化能力，特别是对于<strong>密集预测下游任务</strong>，与仅使用逐例判别任务，即对比学习相比。</p><p>First, in terms of the masking strategy, previous works utilize Random Mask to randomly select the masked joints in the spatial-temporal dimension. However, given the redundancy of skeleton sequences, the masked joints can be easily inferred by copying adjacent joints in the spatial or temporal dimension, which is not conducive to modeling meaningful relationships in skeletons. To this end, we propose a Topology-based masking strategy, which masks the skeleton at the body-part level instead of the joint level, i.e., trunk, right-hand, left-hand, right-leg, and left-leg. Meanwhile, we divide the sequences into different clips in the temporal dimension, and the same parts are all masked in a clip, as shown in Figure 3. 首先，==在遮蔽策略方面，先前的工作使用随机遮罩在时空维度中随机选择被遮蔽的关节。然而，鉴于骨骼序列的冗余性，可以通过复制时空维度中相邻的关节轻松地推断遮蔽的关节，这对于建模骨骼中有意义的关系是不利的。因此，我们提出了一种基于拓扑的遮蔽策略，它在身体部位级别而不是关节级别上对骨骼进行遮蔽，即躯干、右手、左手、右腿和左腿。同时，我们将序列分成不同的时间片段，在一个片段中相同的部位都被遮蔽，如图3所示==</p><p>Based on the above masking strategy, we mask the original skeleton 𝑥 and then feed the masked skeleton 𝑠𝑚𝑎𝑠𝑘 into the encoder 𝑓𝑞 (·) to obtain the corresponding features. To predict the masked skeleton region, we employ a decoder 𝑑𝑒𝑐(·), which takes the encoded features as input and outputs the reconstructed skeleton. The MSE loss between the original data 𝑥 and the predicted data 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 is optimized in the masked region: 基于上述遮蔽策略，我们对原始骨骼𝑥进行遮蔽，然后将遮蔽的骨骼𝑠𝑚𝑎𝑠𝑘输入编码器𝑓𝑞(·)以获取相应的特征。为了预测被遮蔽的骨骼区域，我们使用解码器𝑑𝑒𝑐(·)，它以编码特征为输入并输出重建的骨骼。在遮蔽区域中，原始数据𝑥和预测数据𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡之间的均方误差损失被优化：</p><p>L𝑀𝑎𝑠𝑘 = 1 𝑁 𝑁 ∑︁ ||(𝑥 - 𝑑𝑒𝑐 ◦ 𝑓𝑞(𝑠𝑚𝑎𝑠𝑘)) ⊙ (1 - 𝑀)||2, (4) 其中𝑁是所有被遮蔽关节的数量。𝑀是二值遮蔽掩码，其中1和0分别对应于可见关节和被遮蔽关节。1是与𝑀相同形状的全1矩阵。</p><p><strong>总结：</strong> 本节讨论了遮蔽骨骼预测方法，用于丰富模型学到的表示。作者提出了一种基于拓扑的遮蔽策略，该策略在身体部位级别而非关节级别上进行遮蔽。遮蔽的骨骼数据由编码器处理并由解码器进行重建。在被遮蔽的区域内，通过均方误差损失来优化原始数据和预测数据之间的一致性。这种方法有助于提高骨骼数据的泛化能力，特别适用于密集预测任务。</p><h3 id="3-3-Collaboration-between-Contrastive-Learning-and-Masked-Modeling">3.3 Collaboration between Contrastive Learning and Masked Modeling</h3><p>尽管我们提出了用于对比学习和掩蔽骨架预测的新管道，但我们发现，简单地整合这两个范例只会产生平庸的性能收益，如表7所示。这是由于两个任务[30]的特性建模范式之间存在固有的差距，模型不能直接利用它们之间潜在的协同作用。因此，在本部分中，我们将探索合作，并以互利的方式将两个任务联系起来。</p><h4 id="1-Novel-Positive-Pairs-as-Connection">1) Novel Positive Pairs as Connection.</h4><p><strong>First, we utilize special data views in masked prediction training to provide more diverse positive samples for contrastive learning.</strong> 首先，在遮蔽预测训练中，我们利用特殊的数据视图，为对比学习提供更多不同的正样本。↳</p><p><strong>Considering that the masked skeleton naturally simulates occlusion for skeletons, we take masked skeleton 𝑠𝑚𝑎𝑠𝑘 views as challenging positives, to learn the underlying semantic consistency and enhance robustness to occlusion.</strong> 考虑到遮蔽的骨骼自然地模拟了骨骼的遮挡，==我们将遮蔽的骨骼视图视为具有挑战性的正样本==   以学习潜在的语义一致性并增强对遮挡的鲁棒性。</p><p><strong>Meanwhile, we also boost contrastive learning by taking the predicted skeletons output by decoder 𝑑𝑒𝑐(·) as positive samples.</strong> 同时，我们还=通过将解码器 𝑑𝑒𝑐(·) 输出的预测骨骼作为正样本来增强对比学习=。</p><p><strong>Compared with the masked views, the predicted views contain the inherent noise, uncertainty, and diversity brought by continuous training of the model, which contributes to encoding more diverse movement patterns and thus improves generalization capacity.</strong> 与遮蔽视图相比，预测视图包含了由模型的连续训练引入的固有噪声、不确定性和多样性，这有助于编码更多不同的运动模式，从而提高泛化能力。</p><p><strong>In a nutshell, we utilize the masked view 𝑠𝑚𝑎𝑠𝑘 and predicted view 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 as positive samples to connect masked modeling with contrastive learning.</strong> 简而言之，==我们利用遮蔽视图𝑠𝑚𝑎𝑠𝑘和预测视图𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡作为正样本，将遮蔽建模与对比学习连接起来==</p><p><strong>We present all positive (embedding) pairs as follows: {(𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′), (𝑧𝑖𝑛𝑡𝑒𝑟, 𝑧′ 𝑖𝑛𝑡𝑒𝑟), (𝑧𝑚𝑎𝑠𝑘, 𝑧′), (𝑧𝑝𝑟𝑒𝑑𝑖𝑐𝑡, 𝑧′)}.</strong> 我们将所有正（嵌入）对表示为{(𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′), (𝑧𝑖𝑛𝑡𝑒𝑟, 𝑧′ 𝑖𝑛𝑡𝑒𝑟), (𝑧𝑚𝑎𝑠𝑘, 𝑧′), (𝑧𝑝𝑟𝑒𝑑𝑖𝑐𝑡, 𝑧′)}。</p><p><strong>They are obtained by the query/key encoder and projector, respectively. Each positive pair is applied to calculate Eq. 1 for contrastive loss and Eq. 3 for distillation loss. Taking (𝑠𝑚𝑎𝑠𝑘, 𝑠′) as an example, it replaces the 𝑧𝑖𝑛𝑡𝑟𝑎 with 𝑧𝑚𝑎𝑠𝑘 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑚𝑎𝑠𝑘 ) for optimization.</strong> 它们分别由查询/键编码器和投影器获得。每个正对都用于计算对比损失的公式1和蒸馏损失的公式3。以(𝑠𝑚𝑎𝑠𝑘, 𝑠′)为例，它用𝑧𝑚𝑎𝑠𝑘 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑚𝑎𝑠𝑘 )替换𝑧𝑖𝑛𝑡𝑟𝑎以进行优化。</p><p><strong>Note that we use L𝐶𝑜𝑛 and L𝐾𝐿 to represent the total contrastive loss and distillation loss, respectively, which comprise component losses in the form of Eq. 1 and Eq. 3 calculated for all four positive pairs defined in Eq. 5.</strong> 请注意，我们使用L𝐶𝑜𝑛和L𝐾𝐿分别表示总对比损失和蒸馏损失，它们包括公式1和公式3的组件损失，这些损失是为公式5中定义的所有四个正对计算的。</p><p><strong>总结：</strong> 本段讨论了如何利用遮蔽骨骼数据和预测骨骼数据作为正样本来增强对比学习。遮蔽的骨骼模拟了骨骼的遮挡，这有助于学习潜在的语义一致性并提高鲁棒性。与遮蔽视图相比，预测视图包含了模型连续训练带来的固有噪声、不确定性和多样性，这有助于编码更多不同的运动模式，从而提高泛化能力。这些正样本用于对比损失和蒸馏损失的计算，从而提高了训练的效果。</p><h4 id="2-High-Level-Semantic-Guidance-高级语义指导">2) High-Level Semantic Guidance.高级语义指导</h4><p><strong>On the other hand, the gradients of 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 from the contrastive learning branch are propagated to update the reconstructed decoder 𝑑𝑒𝑐(·) as shown in Figure 2.</strong> 另一方面，来自对比学习分支的𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡的梯度被传播以更新重建解码器 𝑑𝑒𝑐(·)，如图2所示。</p><p><strong>It provides the high-level semantic guidance for the skeleton prediction together with the MSE loss in Eq. 4 which serves as joint-level supervision, leading to better masked prediction learning and higher quality of 𝑠𝑝𝑟𝑑𝑖𝑐𝑡 as positive samples.</strong>==这为骨骼预测提供了高级语义指导，与公式4中的均方误差损失一起，作为关节级别的监督，从而带来更好的遮蔽预测学习和更高质量的𝑠𝑝𝑟𝑑𝑖𝑐𝑡作为正样本==</p><p><strong>With the above synergetic designs, the masked prediction task provides novel positive samples as meaningful supplements to the contrastive learning. Meanwhile, with the gradients of contrastive learning propagating to the masked modeling branch, the masked prediction task can be conversely assisted via the high-level semantic guidance provided by contrastive learning task. These designs connect the two tasks and yield better representation quality.</strong> 通过上述协同设计，遮蔽预测任务为对比学习提供了新颖的正样本，作为对比学习的有意义补充。同时，通过对比学习的梯度传播到遮蔽建模分支，遮蔽预测任务可以通过对比学习任务提供的高级语义指导进行相反的协助。这些设计连接了这两个任务并产生更好的表示质量。</p><p><strong>总结：</strong> 本段讨论了如何利用对比学习分支的梯度来更新重建解码器，提供高级语义指导以及均方误差损失，以改进遮蔽预测学习，提高正样本质量。这些设计连接了两个任务，产生更好的表示质量。</p><h3 id="3-4-Dual-Prompted-Multi-Task-Pretraining-双提示多任务提示的预训练过程">3.4 Dual-Prompted Multi-Task Pretraining 双提示多任务提示的预训练过程</h3><blockquote><p>对于两种视图 一个是骨架视图 一个是增强的视图</p></blockquote><p>对于自监督的预训练，<strong>整个模型以多任务的方式对对比学习和掩蔽预测任务进行优化</strong>  然而，对于不同的pretext tasks，如对比学习和掩蔽预测，输入数据来自不同的分布(域)，如增强视图和掩蔽视图。之前的作品直接将它们输入编码器以学习各自的表示。这可能会导致歧义，并干扰从不同数据/任务中学习的特征建模。为此，我们提出了一种新的双提示多任务预训练策略来明确地指导模型从不同的领域/任务中学习。具体而言，设计了领域提示和任务提示两类提示，并将它们作为可培训向量实现，提供培训指导。</p><h4 id="1-Domain-Specific-Prompt">1) Domain-Specific Prompt.</h4><p><strong>1) Domain-Specific Prompt. To deal with different domains of input, we maintain domain-specific prompts for each input view, i.e., 𝑝𝑖𝑛𝑡𝑒𝑟, 𝑝𝑖𝑛𝑡𝑟𝑎, 𝑝𝑚𝑎𝑠𝑘 , and 𝑝𝑝𝑟𝑟𝑒𝑑𝑖𝑐𝑡 , of which the dimension equals to the skeleton spatial size. Then, these domain-specific prompts are added to the corresponding input data (𝑠∗ means any view): 𝑠∗ = 𝑠∗ + 𝑝∗. (6) These decorated skeletons are fed into the encoder for self-supervised pretraining. The trainable prompts enable the model to learn domain-specific knowledge and achieve better representations.</strong></p><ol><li>针对不同领域的输入，我们为每个输入视图维护特定于领域的提示，即 𝑝𝑖𝑛𝑡𝑒𝑟，𝑝𝑖𝑛𝑡𝑟𝑎，𝑝𝑚𝑎𝑠𝑘 和𝑝𝑝𝑟𝑟𝑒𝑑𝑖𝑐𝑡，其维度等于骨架空间大小。然后，将这些特定于领域的提示添加到相应的输入数据（𝑠∗表示任何视图）：𝑠∗ = 𝑠∗ + 𝑝∗。这些装饰后的骨架被送入编码器进行自监督预训练。可训练的提示使模型能够学习领域特定的知识并实现更好的表示。</li></ol><h4 id="2-Task-Specific-Prompt">2) Task-Specific Prompt.</h4><p><strong>For task-specific prompts, we apply deep-feature prompt after encoder instead of input-wise prompt to encourage the encoder to extract more general features for various tasks. After obtaining the representations 𝑓𝑒𝑎𝑡∗ = 𝑓𝑞/𝑓𝑘 (𝑠∗) ∈ Rd, we add the task-specific prompts 𝑝𝑐𝑙 or 𝑝𝑚𝑝 to the 𝑓𝑒𝑎𝑡∗. Specifically, 𝑝𝑐𝑙, 𝑝𝑚𝑝 ∈ Rr where the dimension r &lt; d for efficiency [11], are added to the randomly selected r-dimensional channels from the original feature 𝑓𝑒𝑎𝑡∗. If the feature is to feed into the projector ℎ𝑞/ℎ𝑘 (·) for contrastive learning, the 𝑝𝑐𝑙 is added, otherwise, the 𝑝𝑚𝑝 is added for masked prediction. These prompts can effectively learn task-specific knowledge and reduce interference between different pretext tasks.</strong> 2) 针对任务特定提示，我们在编码器之后应用<strong>深度特征提示</strong>，而不是按输入方式的提示，以鼓励编码器为各种任务提取更通用的特征。在获得表示形式 𝑓𝑒𝑎𝑡∗ = 𝑓𝑞/𝑓𝑘 (𝑠∗) ∈ Rd 后，我们将任务特定的提示 𝑝𝑐𝑙 或 𝑝𝑚𝑝 添加到 𝑓𝑒𝑎𝑡∗。具体来说，𝑝𝑐𝑙，𝑝𝑚𝑝 ∈ Rr，其中 r &lt; d 以提高效率，被添加到原始特征 𝑓𝑒𝑎𝑡∗ 中随机选择的 r 维通道。如果特征将被馈送到用于对比学习的投影仪 ℎ𝑞/ℎ𝑘 (·)，则添加 𝑝𝑐𝑙，否则，添加 𝑝𝑚𝑝 用于遮蔽预测。这些提示可以有效地学习任务特定的知识并减少不同预设任务之间的干扰。</p><p><strong>Overall, the following objective is applied to the whole model as shown in Figure 2: L = L𝐶𝑜𝑛 + 𝜆𝑚L𝑀𝑎𝑠𝑘 + 𝜆𝑘𝑙L𝐾𝐿, (7) where the loss weight 𝜆𝑚 and 𝜆𝑘𝑙 are set to 40.0, 1.0 in implementation. Note that the prompts are tuned only in the pretraining stage since they are targeted for self-supervised pretext tasks rather than downstream tasks. Therefore, we simply drop all prompts after the pretraining stage.</strong> 总的来说，如图2所示，上述目标适用于整个模型：L = L𝐶𝑜𝑛 + 𝜆𝑚L𝑀𝑎𝑠𝑘 + 𝜆𝑘𝑙L𝐾𝐿，其中损失权重 𝜆𝑚 和 𝜆𝑘𝑙 在实施中设置为40.0和1.0。请注意，提示仅在预训练阶段进行调整，因为它们针对的是自监督预设任务，而不是下游任务。因此，我们在预训练阶段后简单地删除所有提示。</p><p><strong>总结：</strong> 这一段讨论了两种类型的提示：特定于领域和特定于任务的提示。特定于领域的提示用于处理不同输入领域，而特定于任务的提示用于鼓励编码器提取更通用的特征以适应不同任务。整个模型的目标函数是这两种提示的组合。这些提示在预训练阶段进行调整，因为它们主要用于自监督预设任务。</p><h1>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training  基于图的骨架序列预训练的掩蔽自动编码器</h1><p>Abstract 2D poses Skeleton sequence representation learning has shown great advantages for action recognition due to its promis- ing ability to model human joints and topology. How- ever, the current methods usually require sufficient labeled data for training computationally expensive models, which is labor-intensive and time-consuming. Moreover, these methods ignore how to utilize the fine-grained dependen- cies among different skeleton joints to pre-train an effi- cient skeleton sequence learning model that can general- ize well across different datasets. In this paper, we pro- pose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehen- sively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmet- ric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into Graph Convolutional Network (GCN) and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained Skele- tonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and out- performs the state-of-the-art self-supervised skeleton-based action recognition methods on FineGym, Diving48, NTU60 and NTU 120 datasets. Additionally, we obtain comparable performance to some fully supervised methods. The code is avaliable at <a href="https://github.com/HongYan1123/">https://github.com/HongYan1123/</a> SkeletonMAE</p><p><strong>二维骨骼序列</strong>表示学习由于具有<strong>良好的关节建模和拓扑结构建模能力</strong>，在动作识别方面显示出了极大的优势。然而，目前的方法通常需要足够的标记数据来训练计算昂贵的模型，这是劳动密集型和耗时的。此外，这些方法<strong>忽略了如何利用不同骨骼关节之间的细粒度依赖关系来预训练一个高效的骨骼序列学习模型</strong>，该模型能够<strong>很好地跨不同数据集泛化</strong>。在本文中，我们提出了一个有效的骨架序列学习框架，称为<strong>骨架序列学习(SSL)**<strong>Skeleton Sequence Learning</strong>。为了全面捕捉人体姿态并获得具有判别性的骨骼序列表示，我们构建了一个非对称的基于图的编码器-解码器预训练架构，名为SkeletonMAE，该算法</strong>将骨架关节序列嵌入到图卷积网络**(Graph Convolutional Network, GCN)中，并<strong>根据人类先验拓扑知识对隐藏的骨架关节和边缘进行重构</strong>。然后，将预训练的SkeletonMAE编码器与****时空表示学习(STRL)模块<strong>集成【<strong>Spatial-Temporal Representation Learning</strong>】</strong>，以构建SSL框架。大量的实验结果表明，我们的SSL在不同的数据集上具有很好的泛化性，并且在FineGym、Diving48、NTU60和NTU 120数据集上的性能优于目前最先进的基于骨骼的自监督动作识别方法。此外，我们获得了与一些完全监督方法相当的性能。代码可以在https://github.com/HongYan1123/ SkeletonMAE上找到</p><h2 id="introduction-3">introduction</h2><ol><li><p>人体动作识别在视频理解领域引起了越来越多的关注[88, 7, 48, 63, 43, 45, 46]，由于其在人机交互[41, 40, 84]、智能监控安全[42, 99]、虚拟现实等方面的广泛应用[2, 14, 64, 54, 55, 89, 30, 80]。从视觉感知的角度来看[24, 47]，即使没有外观信息，人们也可以通过观察关节的运动来识别动作类别。<strong>与 RGB 视频不同[3, 14, 13, 44]，骨架序列只包含人体关键关节的坐标信息，这是高级别的、轻量级的，并且对复杂的背景和各种条件（包括视角、尺度和运动速度）具有鲁棒性[11, 72]</strong>。此外，随着人体姿态估计算法的发展[8, 1]，人体关节（即关键点）的定位方法取得了很大的进展，可以获取准确的骨架序列。目前，现有的 2D 姿态估计方法比 3D 姿态估计方法更准确、更稳健[11]。在图1(a)中，我们使用 HRNet [70] 对 FineGym 数据集[62] 上的两个动作类别进行可视化呈现 2D 姿态。可以看出，2D 姿态能够准确捕捉人体骨架和运动细节。</p><p>由于骨架序列具有建模<strong>人体运动中的多粒度和大变化的</strong>有希望的能力，相较于 RGB 数据，它<strong>更适合区分具有细微差异的相似动作</strong>。为了捕捉具有区分性的空时运动模式，大多数现有的基于骨架的动作识别方法[11, 88, 5]是完全监督的，通常需要大量带标签的数据来训练精心设计的模型，这是耗时且劳动密集的。为了缓解有限标签训练数据的问题，最近<strong>自监督骨架动作识别方法</strong>[32, 18, 68]受到了越来越多的关注。一些对比学习方法[32, 18]<strong>采用数据增强生成正负样本对</strong>，但它们在很大程度上依赖于对比对的数量。随着编码器-解码器[67, 51]的流行，一些方法[94, 68]通过<strong>链路重建重构了掩蔽的骨架序列</strong>，以<strong>鼓励按照图形编码器-解码器的范例保持拓扑接近性</strong>。然而，这些方法<strong>通常擅长链路预测和节点聚类，但在节点和图分类方面表现不佳</strong>。对于==准确的动作识别，不同骨架关节之间的细粒度依赖关系（即图分类）是必不可少的==。因此，==先前<strong>基于自监督学习的方法倾向于忽略不同骨架关节之间的细粒度依赖关系</strong>，这限制了自监督骨架表示的泛化性。如图1(b)-©所示，<strong>手臂关节和边缘对于区分这两个相似的动作是至关重要的</strong>。与 MAE [20] 的随机遮罩策略不同，我们的遮罩策略是与动作相关的，并重构给定动作类别中占主导地位的特定肢体或身体部分。我们的 SkeletonMAE 利用先验的人体拓扑知识来引导遮罩骨架关节和边缘的重构，以实现对关节、拓扑和动作的全面感知==</p><p>为了解决上述挑战，我们提出了一种高效的骨架序列表示学习框架，称为Skeleton Sequence Learning (SSL)。为了<strong>充分发现不同骨架关节之间的细粒度依赖关系，我们构建了一种新颖的不对称图编码器-解码器预训练架构，称为SkeletonMAE，<strong>它</strong>将骨架关节序列嵌入到图卷积网络（GCN）中</strong>。SkeletonMAE旨在<strong>基于先验的人体拓扑知识重构被遮罩的人体骨架关节和边缘</strong>，<strong>以便推断骨架的底层拓扑结构</strong>，<strong>并获得对人体动作的全面认知</strong>。为了<strong>学习区分性的空时骨架表示</strong>，<strong>预训练的SkeletonMAE编码器与空时表示学习（STRL）模块相结合，以学习空时依赖关系</strong>。最后，SSL在动作识别数据集上进行微调。FineGym、Diving48、NTU 60和NTU 120等广泛的实验结果表明，我们的SSL在不同数据集上具有良好的泛化性能，并在很大程度上优于现有方法。我们的贡献总结如下：</p></li></ol><ul><li>为了全面捕捉人体姿态并获得有区别的骨架序列表示，我们提出了一种名为SkeletonMAE的基于图的编码器-解码器预训练架构，将骨架关节序列嵌入GCN，并<strong>利用先验的人体拓扑知识</strong> <strong>指导底层遮罩关节和拓扑的重构</strong>。</li><li>为了<strong>学习骨架序列的全面空时依赖关系</strong>，我们提出了一种<strong>高效的骨架序列学习框架</strong>，称为Skeleton Sequence Learning（SSL），它将预训练的SkeletonMAE编码器与空时表示学习（STRL）模块相结合。</li><li>在FineGym、Diving48、NTU 60和NTU 120等数据集上的广泛实验结果表明，我们的SSL在不同数据集上具有良好的泛化性能，并在很大程度上优于现有的自监督骨架动作识别方法，与现有的完全监督方法相比，性能相当。</li></ul><h2 id="related-work">related work</h2><h3 id="Self-supervised-Learning-for-Skeleton-Sequence">Self-supervised Learning for Skeleton Sequence.</h3><p><strong>Self-supervised Learning for Skeleton Sequence.</strong></p><p>针对未标记的骨架数据，最近自监督学习取得了令人鼓舞的进展，以学到更有效的表示。对于对比学习方法，AS-CAL [58] 和 SkeletonCLR [32] 使用动量编码器进行对比学习，采用了单流骨架序列。AimCLR [18] 采用极端的数据增强策略，添加了额外的困难对比对。<strong>大多数对比学习方法采用数据增强生成正负样本对，但它们在很大程度上依赖于对比对的数量</strong>。对于<strong>生成学习方法</strong>，LongT GAN [94] 提出了编码器-解码器来重构掩蔽的输入序列骨架。基于 LongT GAN，P&amp;C [68] 强化了编码器并削弱了解码器以获取特征表示。Wu 等人 [85] 提出了一种用于自监督3D骨架动作识别的空间-时间掩蔽自编码器框架。Colorization [90] 使用了三对编码器-解码器框架，从骨架点云中学习空间-时间特征。由于<strong>重构标准的限制</strong>，先前的生成方法通常<strong>未能充分发现不同骨架关节之间的细粒度空时依赖关系</strong>。在我们的SkeletonMAE中，我们利用先验的人体拓扑知识推断骨架序列，并获得对动作的全面认知。</p><p>根据输入内容总结 “Self-supervised Learning for Skeleton Sequence.” 章节：</p><p>在这一章节中，作者讨论了最近在未标记骨架数据上取得的自监督学习的进展。他们提到了对比学习方法，如AS-CAL和SkeletonCLR，以及生成学习方法，如LongT GAN和P&amp;C。对比学习方法采用了数据增强生成正负样本对，而生成学习方法则使用编码器-解码器框架进行骨架序列的重构。然而，先前的生成方法由于重构标准的限制，<strong>未能完全发现不同骨架关节之间的细粒度空时依赖关系</strong>。为了解决这个问题，作者介绍了他们的方法SkeletonMAE，该方法利用先验的人体拓扑知识来推断骨架序列，以获得对动作的全面认知。这一方法旨在克服以往方法的局限性，提供更全面的空时表示。</p><h2 id="3-Methodology-2">3. Methodology</h2><p>在本节中，我们介绍了骨架序列学习(SSL)的细节，它包含两部分:1)预训练的骨架序列学习和2)基于预训练的骨架序列学习对下游数据集进行微调。</p><h3 id="3-1Pre-training-SkeletonMAE">3.1Pre-training SkeletonMAE</h3><p>在本节中，我们将介绍<strong>基于图的非对称编码-解码器预训练架构</strong>skeleton mae，在没有监督的情况下学习人体骨骼序列表示。由于图同构网络(GIN)[87]【How powerful are graph neural networks?】提供了更好的归纳偏差，因此它更适合于学习更广义的自监督表示[22]。因此，我们采用GIN作为骨架mae的骨干。此外，我们在表4中评估了骨架mae的不同backbones，包括GIN[87]、GCN[28]和GAT[75]。</p><h4 id="3-1-1SkeletonMAE-Structure">3.1.1SkeletonMAE Structure</h4><ol><li></li></ol><p>受到在自然语言处理 [9]、图像识别 [20] 和视频识别 [73] 中掩蔽自编码器（MAE）有效表示学习的启发，我们专注于人体骨架序列，并构建了一种名为SkeletonMAE的<strong>不对称图形编码器-解码器预训练架构，将骨架序列及其先验拓扑知识嵌入到GIN中</strong>。SkeletonMAE的实现遵循图生成自监督学习的范例。</p><ol start="2"><li><hr><p>我们遵循Kinetics Skeleton数据集的关节标签 [63]。具体而言，如图2(d)所示，我们根据身体的自然部位将所有的N = 17个关节分为R = 6个本地区域：V0, …, V5。值得注意的是，与MAE [20] 的随机遮罩策略选择骨架关节相比，我们的<strong>遮罩策略是动作敏感的</strong>，并且<strong>重构特定支配给定动作类的肢体或身体部位</strong>。然后，我们<strong>遮罩这些骨架区域，并让SkeletonMAE根据相邻关节重构遮罩关节特征及其边缘。通过重构遮罩的骨架关节和边缘，SkeletonMAE可以推断关节的==底层拓扑结构==，获得对动作的全面认知</strong>。</p></li><li></li></ol><p>如图2所示，SkeletonMAE是一种不对称的编码器-解码器架构，包括一个编码器和一个解码器。<strong>编码器由LD GIN层组成</strong>，将输入的2D骨架数据映射到隐藏特征。解码器仅包含一个GIN层，<strong>根据重构标准重新构建隐藏特征</strong>。</p><ol start="4"><li></li></ol><p>根据先前的人体骨架知识，人体骨架可以表示为一个<strong>以关节为顶点、以肢体为边的图形，我们将人体骨架形式化为以下图形结构</strong>。</p><ol start="5"><li></li></ol><p>**N个人体骨架关节和T帧的二维坐标的骨架序列经过以下方式预处理。**具体而言，我们将所有骨架关节（节点特征）及其拓扑(<strong>邻接矩阵</strong>)嵌入到一个结构G中，骨架结构和关节特征融合以获得关节序列矩阵S ∈ RN×T×2。然后，<strong>使用可学习的参数将S线性变换为S ∈ RN×T×D</strong>。我们经验性地将T和D设置为64。</p><p>对于来自S的每个骨架帧X ∈ RN×D，让G = (V,A,X)表示一个骨架，其中V = {v1, v2, …, vN}是包含所有骨架关节的节点集合，N = |V|是关节的数量。关节数量为N = 17。A ∈ {0, 1}N×N是一个邻接矩阵，其中Ai,j = 1表示关节i和j物理上连接，否则为0。vi的特征表示为xi ∈ R1×D。GE、GD分别表示GIN编码器和GIN解码器。</p><p>根据输入内容总结 “SkeletonMAE Structure” 章节：</p><p>在这一章节中，作者介绍了SkeletonMAE结构，这是一种不对称图形编码器-解码器预训练架构，专注于学习人体骨架序列的有效表示。该架构包括一个编码器和一个解码器，其中编码器由LD GIN层组成，用于将输入的2D骨架数据映射到隐藏特征，而解码器仅包含一个GIN层，用于在重构标准的监督下重新构建隐藏特征。<strong>骨架序列被处理为图形结构，其中骨架关节被嵌入到结构G中，形成一个以关节为顶点、以肢体为边的图形。作者还详细描述了骨架序列的预处理步骤和图形结构的表示，以及相应的GIN编码器和解码器。</strong></p><h4 id="3-1-2-Skeleton-Joints-Masking-and-Reconstruction">3.1.2 Skeleton Joints Masking and Reconstruction</h4><p>鉴于先前的人体骨架拓扑A已嵌入（图2），并且我们在3.1.1节中指定了关节的聚合方式。受到GraphMAE [22] 随机重构遮罩图节点的启发，我们的SkeletonMAE基于先前的骨架拓扑<strong>重构了遮罩骨架特征X</strong>，<strong>而不是重构图结构A</strong> [71, 17] 或<strong>同时重构结构A和特征X</strong> [60, 56]。</p><p>**为了遮罩骨架关节特征，我们从V = {V0, …, V5}中随机选择一个或多个关节集，其中包括一个子集V ⊆ V用于遮罩。**对于人体骨架序列，每个关节与其相邻的一些关节进行通信，以表示特定的动作类。因此，<strong>不可行的是对所有动作类的所有关节集进行遮罩。然后，它们的每个特征都用可学习的掩码令牌向量[MASK] = x[M] ∈ RD 进行遮罩</strong>。因此，对于遮罩特征矩阵X中的Vi ∈ V中的关节特征xi，可以定义为xi = x[M]如果vi ∈ V，否则xi = xi。<strong>我们将X ∈ RN×D 设置为SkeletonMAE的输入关节特征矩阵</strong>，其中X中的每个关节特征可以定义为xi = x[M]，xi，i = 1, 2, · · · ,N。因此，遮罩骨架序列可以被形式化为G = (V,A,X)，**SkeletonMAE的目标是在给定局部观察到的关节特征X和输入邻接矩阵A的情况下重构遮罩骨架特征。**SkeletonMAE的重构过程被形式化为：</p><p>�=��(�,�),�∈��×�ℎ<em>H</em>=<em>GE</em>(<em>A</em>,<em>X</em>),<em>H</em>∈<em>RN</em>×<em>D**h</em></p><p>�=��(�,�),�∈��×�<em>Y</em>=<em>G**D</em>(<em>A</em>,<em>H</em>),<em>Y</em>∈<em>RN</em>×<em>D</em></p><p>其中H和Y分别表示编码器输出和解码器输出。SkeletonMAE的目标可以被形式化为最小化X和Y之间的差异。</p><h4 id="3-1-3-Reconstruction-Criterion">3.1.3 Reconstruction Criterion</h4><p><strong>重构准则章节</strong></p><p>对于遮罩自编码器的常见重构<strong>准则在图像和视频任务中是均方误差</strong>（MSE）。然而，对于骨架序列，关节特征的多维连续性使得MSE很难实现有希望的特征重构，因为<strong>MSE对于特征的维度和向量范数很敏感</strong>。受到观察到的l2-归一化在余弦误差中将向量映射到单位超球面并且显著提高训练稳定性的启发，我们使用余弦误差作为重构准则。</p><p>为了<strong>使重构准则集中在</strong>  <strong>不平衡的易和难样本中更难的样本上</strong>，我们引入了SkeletonMAE的Re-weighted Cosine Error（RCE）。**RCE的基本思想是通过将余弦误差与β ≥ 1的幂进行缩放，****可以在训练中降低易样本的贡献。对于高置信度的预测，它们对应的余弦误差通常小于1，并且当缩放因子β &gt; 1时迅速衰减为零。**形式上，给定原始特征X ∈ RN×D和重构输出Y ∈ RN×D，RCE定义为：</p><p>其中表示所<strong>有遮罩关节上重构特征和输入特征之间相似性差距的平均值</strong>。在我们的工作中，β设置为2。</p><p>通过训练SkeletonMAE来重构骨架序列，预训练的SkeletonMAE可以全面感知人体骨架结构并获取具有区分性的动作表示。在预训练之后，SkeletonMAE可以优雅地嵌入到Skeleton Sequence Learning（SSL）框架中进行微调。</p><h3 id="3-2-Fine-tuning-for-Skeleton-Action-Recognition">3.2. Fine-tuning for Skeleton Action Recognition</h3><p>为了评估骨架动作识别的泛化能力，我们在预训练的骨架动作识别模型的基础上，构建了一个完整的骨架动作识别模型——骨架序列学习(skeleton Sequence Learning, SSL)。为了捕捉多人交互，我们整合了两个预训练的骨骼mae编码器来构建时空表征学习(STRL)模块，如图3(b)-©所示。整个SSL由一个Mlayer STRL模型和一个分类器组成。SSL模型最终在具有交叉熵损失的骨架动作识别数据集上进行了微调，以识别动作。</p><h4 id="3-2-1-Spatial-Temporal-Representation-Learning">3.2.1 Spatial-Temporal Representation Learning</h4><p><strong>Spatial-Temporal Representation Learning章节</strong></p><p>STRL包含两个预训练的SkeletonMAE编码器，用于空间建模（SM）。SM的输入是骨架序列S。SM的输出通过1×1卷积与输入连接，以实现残差连接（图3（b））。</p><p>如图3（c）所示，骨架序列S ∈ RN×T×D首先与可学习的时间位置嵌入PE相加，得到骨架序列特征H(l)t ∈ RP×N×D(l)。为了建模多个人类骨架的相互作用，我们从H(l)t获取两个个体特征（P = 2），即两个人的H(l)t,0 ∈ RN×D(l)和H(l)t,1 ∈ RN×D(l)。在这里，我们以第0个人的关节特征为例，第1个人的操作类似。我们将关节表示H(l)t,0和关节的先验知识eA发送到SM模块：</p><p>SM(�(�)�,0)=Repeat(SP(GE � �,�(�)�,0);�)⊕�(�)�,0SM(<em>H</em>(<em>l</em>)<em>t</em>,0)=Repeat(SP(GE<em>A**e</em>,<em>H</em>(<em>l</em>)<em>t</em>,0);<em>N</em>)⊕<em>H</em>(<em>l</em>)<em>t</em>,0</p><p>其中GE是SkeletonMAE编码器，SP(·)表示汇总，Repeat(·;N)表示在求和池化之后将单个关节重复成N个关节表示，然后与H(l)t,0残差相连接以获取全局关节表示SM(H(l)t,0)。通过这种方式，SM模块可以通过单个关节表示获取全局信息，并通过所有关节表示对一些关节特征进行约束。类似地，以相同的方式获得SM(H(l)t,1)。如图3©所示，我们得到包含第0个人和第1个人之间的动作交互的关节特征SM(H(l)t)。根据图卷积的更新规则，可以从H(l)t中获得H(l+1)t在多层GCN中。有关详细信息，请参阅第D节的附录。最终的骨架序列表示定义如下：</p><p>�(�+1)�=�(SM(�(�)�)�(�))<em>H</em>(<em>l</em>+1)<em>t</em>=<em>σ</em>(SM(<em>H</em>(<em>l</em>)<em>t</em>)<em>W</em>(<em>l</em>))</p><p>其中W(l)表示第l层的可训练权重矩阵，σ(·)表示ReLU激活函数。然后，我们采用广泛使用的多尺度时间池化[5, 39]来获取最终输出。最后，由MLP和softmax组成的分类器预测动作类别。</p><p><strong>总结：</strong> Spatial-Temporal Representation Learning（STRL）通过两个预训练的SkeletonMAE编码器进行空间建模，通过图卷积网络（GCN）学习多层表示，最终采用多尺度时间池化获取最终输出，用于预测动作类别。该过程中利用了骨架序列的时空信息，以获得更丰富的表示。</p><h1>Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition基于<strong>小动作集</strong>的对比学习在无监督骨架动作识别中的应用</h1><h2 id="abstract-2">abstract</h2><p>自监督的预训练范式在基于骨架的动作识别中取得了巨大成功。然而，这些方法<strong>平等地对待动态部分和静态部分，并缺乏针对不同部分的自适应设计，这对动作识别的准确性产生了负面影响</strong>。为了实现对两个部分的自适应动作建模，我们提出了一种称为Actionlet-Dependent Contrastive Learning (ActCLR)的方法。动作元（actionlet）被定义为<strong>人体骨架的区分性子集</strong>，有效地<strong>将动作区域分解为更好的动作建模</strong>。具体来说，通过与没有动作的静态锚点进行对比，我们以无监督的方式提取了骨架数据的动作区域，这充当了<strong>动作元</strong>。然后，以动作元为中心，构建了一种动作自适应的数据转换方法。 ==不同的数据转换==<strong>被应用于动作元和非动作元区域，引入更多的多样性</strong>，同时保持它们各自的特征。同时，我们提出了一种<strong>语义感知的特征汇聚方法</strong>，以<strong>区分的方式</strong>构建了<strong>动态和静态区域之间的特征表示</strong>。在NTU RGB+D和PKUMMD上进行的广泛实验表明，所提出的方法实现了显著的动作识别性能。更多的可视化和定量实验证明了我们方法的有效性。</p><p>现在让我为您解释摘要的要点</p><ol><li>自监督的预训练方法在基于骨架的动作识别中取得了巨大成功。</li><li>现有方法在处理骨架数据时<strong>平等地对待动态和静态部分，而缺乏自适应设计，这降低了动作识别的准确性。</strong></li><li>为了实现自适应的动作建模，作者提出了Actionlet-Dependent Contrastive Learning（ActCLR）方法。</li><li><strong>动作元（actionlet）是人体骨架的区分性子集，用于更好地捕捉动作区域。</strong></li><li>通过对比动态和没有动态的静态部分，作者以无监督的方式提取了动作区域。</li><li>作者构建了一种动作自适应的数据转换方法，以区分地应用于动作元和非动作元区域，引入更多的多样性，同时保持它们各自的特征。</li><li>作者还提出了一种语义感知的特征汇聚方法，用于以区分的方式构建动态和静态区域之间的特征表示。</li><li>在NTU RGB+D和PKUMMD数据集上进行的实验表明，所提出的方法在动作识别方面表现出色。</li><li>进一步的可视化和定量实验进一步证明了该方法的有效性。</li></ol><p>总的来说，这项研究旨在提高基于骨架的动作识别的准确性，通过引入动作元和自适应设计，以更好地捕捉不同部分的动作特征。该方法在多个数据集上进行了验证，取得了显著的性能提升。</p><blockquote><p>提出了动作元子集的概念，设定动态域和静态域，【将数据转换成动态数据和静态数据】</p><p>提出了一种语义感知的特征汇聚方式，区分的构建动态和静态数据的特征表示</p></blockquote><h2 id="introduction-4">introduction</h2><ol><li>引言 骨架使用3D坐标位置表示人体关节。与RGB视频和深度数据相比，骨架更轻便、有隐私保护并紧凑，用于表示人体运动。由于骨架在分析中更容易且更具区分度，因此已被广泛用于动作识别任务[19,23,31,32,46,48]。监督式的基于骨架的动作识别方法[3,27,28]已经取得了令人印象深刻的性能。然而，它们的成功<strong>高度依赖于大量的标记训练数据</strong>，这在获取时代价昂贵。为了摆脱对完全监督的依赖，引入了<strong>自监督学习[16, 32, 34, 49]到基于骨架的动作识别中</strong>。它采用了一个两阶段范式，即首先应用预文本任务进行无监督预训练，然后进行微调以用于下游任务。</li></ol><p>根据学习范式，所有方法可以分为两类：基于重建的[14, 32, 41]和基于对比学习的方法。基于<strong>重建的方法通过预测遮蔽的骨架数据来捕获时空相关性</strong>。Zheng等人[49]<strong>首次提出了为基于重建的方法通过预测遮蔽的骨架数据来捕获时空相关性</strong>。此外，基于<strong>对比学习的方法近来显示出了显著的潜力</strong>。这些方法利用<strong>骨架转换生成正样本/负样本</strong>。Rao等人[24]应用了Shear和Crop作为数据增强。<strong>Guo等人[8]进一步提出使用更多的增强，例如旋转、遮罩和翻转，以提高对比学习的一致性</strong>。这些对比学习方法在前一方法中<strong>统一对待骨架序列的不同区域</strong>。然而，运动区域包含更丰富的动作信息，并对动作建模起更大的贡献。因此，在以前的工作中，直接对所有区域应用数据转换是次优的，这可能会对运动相关信息造成过多的干扰。<strong>例如，在举手动作中，如果对手部关节应用遮罩转换，则会完全破坏手部举高的运动信息。这将导致误报问题，即由于正样本之间的信息丢失而导致的语义不一致。因此，有必要为数据序列中的运动和静态区域采用可区分的设计。</strong></p><p>为了解决这些问题，我们提出了一种新的基于动作单元的对比学习方法（ActCLR），**通过不同对待运动和静态区域，<strong>如图1所示。动作单元[38]被定义为==骨架关节的连接结构==。它期望==对一个动作具有很高的代表性，并具有很高的区分度==，以区分该动作与其他动作</strong>。以前的工作中，动作单元是以监督方式定义的，这依赖于动作标签，并与自监督的预文本任务之间存在差距。<strong>为此，在无监督学习环境下，我们建议通过</strong>将动作序列与所有训练数据的平均运动进行比较来获取动作单元，以引导对比学习。**详细来说，<strong>平均运动被定义为数据集中所有系列的平均值。因此，平均运动被用作没有运动的静态锚点。我们将动作序列与平均运动进行对比，以获取差异最大的区域。认为该区域是发生运动的区域，即动作单元。</strong></p><p>基于这个动作单元，我们设计了一种<strong>运动自适应转换策略</strong>。==动作单元区域通过执行提出的语义保持数据转换而进行转换==。具体而言，我们**==仅对非动作单元区域应用更强的数据变换==在运动区域的干扰较小的情况下，这种运动自适应转换策略使模型学到更好的语义一致性并获得更强的泛化性能**。同样，我们利用了==**语义感知的特征汇聚方法==。通过提取动作单元区域的特征，这些特征可以更好地代表运动，而不受静态区域中语义的干扰。</p><p>我们在NTU RGB+D[17,26]和PKUMMD[18]数据集上进行了彻底的实验和详细的分析，以证明我们方法的优越性。与最先进的方法相比，我们的模型在自监督学习中取得了显著的结果。</p><p>总之，我们的贡献总结如下： • 我们提出了一种新颖的无监督动作单元对比学习方法。无监督动作单元被挖掘为与静态锚点，即所有训练数据的平均运动相比，最具区分性的骨架区域。 • 我们设计了一种<strong>运动自适应转换策略用于对比学习</strong>。在动作单元区域，我们采用了语义保持的数据转换。</p><p>以下是文章介绍部分的要点总结：</p><ul><li>骨架数据是一种轻量级、隐私保护且紧凑的人体运动表示方法，已广泛应用于动作识别任务。</li><li>以一种互利监督的方式</li><li>传统的监督式骨架动作识别方法需要大量标记训练数据，而获取这些数据既昂贵又耗时。</li><li>自监督学习已经引入到骨架动作识别中，以减少对完全监督的依赖。</li><li>==方法可以分为基于重建和基于对比学习两类，<strong>对比学习方法</strong>近来表现出了潜力==</li><li>以前的对比学习方法通常对待骨架序列的不同区域，但<strong>运动区域包含更多的动作信息，因此需要区分对待。</strong></li><li>以前的对比学习方法</li><li>作者提出了一种新的<strong>基于动作单元的对比学习方法</strong>（ActCLR），该方法通过<strong>区分对待运动和静态区域来改善自监督学习的性能</strong>。</li><li>动作单元是高度代表性且有区分度的骨架关节连接结构，可以用于区分不同的动作。</li><li>作者提出了一种<strong>运动自适应转换策略，该策略仅对非动作单元区域应用更强的数据转换，以提高模型的泛化性能</strong>。</li><li>==语义感知的特征汇聚方法有助于更好地代表运动单元数据==</li><li>在NTU RGB+D和PKUMMD数据集上的实验表明，该方法在自监督学习中取得了显著的结果。</li></ul><h2 id="related-work-2">related work</h2><h3 id="Skeleton-Based-Action-Recognition">Skeleton-Based Action Recognition</h3><p>Skeleton-Based Action Recognition（基于骨骼的动作识别）是计算机视觉研究中的一个基础且具有挑战性的领域。先前的基于骨骼的动作识别方法通常利用骨骼关节的几何关系来实现。最新的方法更加关注深度神经网络。Du等人应用分层RNN来处理身体关键点。还提出了基于注意力的方法，以自动选择重要的骨骼关节和视频帧，以更自适应地学习有关骨骼关节的同时出现情况。然而，循环神经网络经常受到梯度消失问题的困扰，这可能会导致优化问题。最近，图卷积网络引起了更多关于基于骨骼的动作识别的关注。为了从骨骼数据中提取空间和时间结构特征，Yan等人提出了时空图卷积网络。为了使图形表示更加灵活，注意机制被应用于[3, 27, 28]以自适应地捕获基于空间组成和时间动态的判别特征。</p><p><strong>Skeleton-Based Action Recognition章节解释</strong>：</p><p>Skeleton-Based Action Recognition指的是使用人体骨骼关节的空间坐标信息来识别人类动作的一种计算机视觉任务。在这个领域，研究人员试图通过分析骨骼关节之间的几何关系以及时间序列数据来实现对人类动作的自动识别。早期的方法主要依赖于几何关系和手工设计的特征提取器，但这些方法通常需要大量的人工标记数据。</p><p>随着深度学习方法的发展，基于骨骼的动作识别取得了显著的进展。现代方法采用深度神经网络来学习从骨骼数据中提取有关动作的特征。这些方法可以自动学习骨骼关节之间的关系以及动作的时空特征，从而在动作识别任务中取得更好的性能。</p><p>最近，图卷积网络成为基于骨骼的动作识别中的一个研究热点。这些网络可以更好地捕获骨骼关节之间的关系，从而提高了动作识别的准确性。此外，注意力机制也被引入以自适应地关注对动作识别最重要的信息。因此，Skeleton-Based Action Recognition是一个多领域交叉的研究领域，涉及计算机视觉、深度学习和模式识别等多个方面。</p><h3 id="Contrastive-Learning">Contrastive Learning</h3><p>对比表示学习可以追溯到[9]。以下方法[1, 13, 35, 39, 42]通过将正对比对与负对比对进行对比学习，使正对比对之间的表示比负对比对之间的表示更相似来学习表示。研究人员主要关注如何构建对以学习稳健的表示。由Chen等人提出的SimCLR [2] 使用一系列数据增强方法，如随机裁剪、高斯模糊和颜色扭曲来生成正样本。He等人 [10] 应用了一个采用队列存储负样本的内存模块，队列会随着训练不断更新。在自监督骨骼动作识别中，对比学习也吸引了众多研究人员的关注。Rao等人 [24] 应用了MoCo进行单流的对比学习。为了利用跨流知识，Li等人 [15] 提出了一种多视图对比学习方法，而Thoker等人 [34] 则采用多个模型来学习不同的骨骼表示。Guo等人 [8] 提出使用更极端的数据增强，大大提高了对比学习的效果。Su等人 [33] 通过感知运动一致性和连续性提出了新颖的表示学习方法。在MoCo v2 [10] 后继续，他们利用InfoNCE损失来优化对比学习：</p><p>其中，zi q = gq(fq(Xi q)) 和 zi k = gk(fk(Xi k))。K = PM j=1 exp(sim(zi q,mj)/τ) 和 τ 是温度超参数。fq(·) 是在线编码器，fk(·) 是离线编码器。gq(·) 是在线投影器，gk(·) 是离线投影器。离线编码器fk(·)通过在线编码器fq(·)的动量更新为fk ← αfk + (1 − α)fq，其中α是动量系数。mj是存储在内存库M中的负样本。sim(·, ·) 是余弦相似度。</p><p><strong>Contrastive Learning章节解释</strong>：</p><p>Contrastive Learning（对比学习）是一种自监督学习方法，其主要目标是学习数据表示，使来自相同类别的样本在表示空间中更加接近，而来自不同类别的样本更加分散。在骨骼动作识别中，对比学习被用来学习如何更好地表示骨骼数据以提高动作识别性能。</p><p>该章节提到了对比学习的发展历史和不同的方法，包括SimCLR和MoCo等。这些方法通常通过构建正对比对（相似样本）和负对比对（不相似样本）来进行训练，从而学习数据的有用表示。<strong>对比学习的一个关键优势是它可以在无监督的情况下进行训练，因此不需要大量的标记数据</strong>。</p><p>该章节还介绍了对比学习中的一些关键概念，如<strong>温度超参数、动量系数和余弦相似度</strong>。这些概念在对比学习中起着重要的作用，帮助优化学习过程。</p><p>总之，对比学习是一种有效的自监督学习方法，可用于提高骨骼动作识别等任务的性能。它通过将相似性信息嵌入数据表示中来帮助模型更好地理解和区分不同的数据样本。</p><h2 id="method">method</h2><h3 id="3-1Unsupervised-Actionlet-Selection">3.1Unsupervised Actionlet Selection</h3><blockquote><p>这一段 设定了怎样选择动作元区域</p><p>利用平均数据 X^和 X原数据进行相似度计算</p><p>对于动作相似性 有消极影响 表示 使得动作与平均数据 不相似 则就是运动单元，否则就是静止单元</p></blockquote><ol><li>Traditional actionlet mining methods rely on the action label to identify the motion region, which cannot be employed in the unsupervised learning context.<ul><li>传统的动作部分挖掘方法依赖于动作标签来确定动作区域，这在无监督学习情境下无法应用。</li><li>解释：传统方法通常需要动作标签来确定哪些部分是与动作相关的，但这种方法在无监督学习中不适用，因为无监督学习不依赖于动作标签。</li></ul></li><li>Inspired by contrastive learning, we propose an unsupervised spatio-temporal actionlet selection method to mine the motion region as shown in Fig. 2.<ul><li>受到对比学习的启发，我们提出了一种无监督的时空动作部分选择方法，用于挖掘动作部分，如图2所示。</li><li>解释：受到对比学习的启发，作者提出了一种无监督的方法来选择包含动作信息的部分，以便更好地进行动作识别。</li></ul></li><li>The actionlet is obtained by comparing the differences between an action sequence and the static sequence where we assume no motion takes place.<ul><li>==动作部分是通过比较动作序列与静态序列之间的差异来获得的==我们假设在静态序列中没有动作发生。</li><li>解释：动作部分是通过将动作序列与没有动作发生的静态序列进行比较来确定的，这有助于找到包含动作的区域。</li></ul></li></ol><blockquote><p>使用静态数据作为数据锚点【静态数据视为没有发生动作的序列 也就是所有的数据的平均运动数据】</p></blockquote><h4 id="Average-Motion-as-Static-Anchor">Average Motion as Static Anchor</h4><ol><li>In the process of obtaining the sequence without action occurrence, we observe that most of the action sequences have no action in most of the regions.<ul><li>在获取没有动作发生的序列的过程中，我们观察到大多数动作序列在大多数区域都没有动作。</li><li>解释：在寻找没有动作发生的序列时，作者观察到大多数动作序列的大部分区域都没有动作。</li></ul></li><li>The motion usually occurs in a small localized area, such as the hand or head.<ul><li>运动通常发生在一个小的局部区域，例如手部或头部。</li><li>解释：动作通常发生在身体的特定区域，如手部或头部，而 不是整个身体都在运动。</li></ul></li><li>Therefore, as shown in Fig. 4, we can easily obtain the static anchor via averaging all the actions in the dataset, since most of the sequence has no motion in most of the regions and this average is a relatively static sequence.<ul><li>因此，如图4所示，我们可以通过对数据集中的所有动作进行平均来轻松获得静态锚点，因为大多数序列在大多数区域都没有运动，而这个平均值是一个相对静态的序列。</li><li>解释：作者通过对数据集中的所有动作序列进行平均来获得静态锚点。由于大多数区域没有运动，所以这个平均值可以看作是相对静态的序列。</li></ul></li><li>It is formalized as:<ul><li>公式化表示如下：</li><li>解释：这是对上述概念的正式数学表示，用于表示如何计算静态锚点的平均值。</li></ul></li></ol><p>在这段文字中，作者提到了如何获取静态锚点，即用于对比动作部分的相对静态的序列。作者观察到大多数动作序列的大部分区域都没有动作，因此可以通过对数据集中的所有动作进行平均来获得这个静态锚点。这个静态锚点在后续的动作部分挖掘中起到关键作用。</p><h4 id="Difference-Activation-Mapping-for-Actionlet-Localization">Difference Activation Mapping for Actionlet Localization</h4><ol><li></li></ol><p>这段文字描述了如何在骨骼序列中找到动作发生的区域。首先，他们将骨骼序列输入到离线编码器中，以获取密集特征。然后，他们计算这些特征之间的余弦相似度。接着，他们反向传播并反转相似度的梯度到密集特征，以获得神经元的重要性权重。这些权重通过全局平均池化计算。</p><p>总结：该段落介绍了一个方法，用于确定骨骼序列中动作发生的位置。该方法通过计算特征之间的余弦相似度来识别动作发生的位置，并使用反向传播和梯度反转来计算神经元的重要性权重。这有助于理解动作在骨骼序列中的位置,并减少相似的区域</p><ol><li>These importance weights capture the magnitude of the effect of each channel dimension on the final difference. 这些重要性权重捕捉了每个通道维度对最终差异的影响幅度。</li><li>Therefore, these weights αi 因此，这些权重 αi</li><li>c are considered difference activation mapping. c 被视为差异激活映射。</li><li>We perform a weighted combination of the difference activation mapping and dense features as follows: 我们执行以下方式的差异激活映射和密集特征的加权组合：</li><li>where σ(·) is the activation function and Gvv is the adjacency matrix of skeleton data for importance smoothing</li><li>The linear combination of maps selects features that have a negative influence on the similarity. 地图的线性组合选择对相似度产生负面影响的特征。</li><li><strong>The actionlet region is the area where the value of the generated actionlet Ai 动作区域是生成的动作区域 Ai 的值超过一定阈值的区域，</strong></li><li><strong>tv exceeds a certain threshold, while the non-actionlet region is the remaining part. 超过一定阈值，而非动作区域则是其余部分。</strong></li></ol><p>这段文字描述了如何使用重要性权重来捕捉每个通道维度对最终差异的影响幅度，并将这些权重视为<strong>差异激活映射</strong>。然后，他们执行了一种加权组合，将差异激活映射与密集特征相结合，**以选择对相似度产生负面影响的特征。**最终，==他们定义了动作区域和非动作区域，以区分特征值是否超过了一定阈值==</p><p>总结：这段文字描述了如何使用重要性权重来选择对相似度产生负面影响的特征，以定义动作区域和非动作区域。这有助于识别动作发生的位置。</p><h2 id="3-2-Actionlet-Guided-Contrastive-Learning">3.2. Actionlet-Guided Contrastive Learning</h2><h3 id="Actionlet-Transformation-Tact">Actionlet Transformation Tact:</h3><p><img src="../assets/image-20231023111545179.png" alt="image-20231023111545179"></p><blockquote><p>这段文字描述了在“Actionlet Transformation” 过程中应用的多种数据转换方法，包括空间、时间和时空变换，以及Skeleton AdaIn 方法，用于传输与动作无关的噪声信息，这些转换方法有主语丰富数据并保持动作信息</p></blockquote><h3 id="Non-Actionlet-Transformation-Tnon">Non-Actionlet Transformation Tnon:</h3><p>这段文字涉及了有关 “Non-Actionlet Transformation Tnon” 的数据变换方法。下面是详细解释：</p><ol><li>“Non-Actionlet Transformation Tnon” 意味着对非 “Actionlet” 区域应用额外的数据转换，以获得更强的泛化性能。</li><li>对于非 “Actionlet” 区域，作者应用了两种数据变换方法：<ul><li>“Random Noise”：这是一种在单个数据实例内部应用的数据变换。这里的 “Noise” 指的是噪声。它有较大的方差，这意味着在非 “Actionlet” 区域引入了更多的随机性。</li><li>“Skeleton Mix”：这是一种跨数据实例之间的数据变换。它包括了多种数据混合方法，如 Mixup、CutMix 和 ResizeMix。这些方法是在元素级别混合数据。因为这些变换是在非 “Actionlet” 区域执行的，它们不会改变动作的语义。因此，变换后的数据与原始数据一起用作正样本。</li></ul></li></ol><p>总结：这段文字描述了在非 “Actionlet” 区域中应用的额外数据转换方法，包括在单个数据实例内部引入噪声以及跨不同数据实例的数据混合方法。这些方法旨在提高模型的泛化性能，而不改变动作的语义。</p><h3 id="Actionlet-Dependent-Combination">Actionlet-Dependent Combination:</h3><p>这段文字描述了两个关键的概念：Actionlet-Dependent Combination（基于动作子集的组合）和Semantic-Aware Feature Pooling（语义感知特征池化）。以下是详细解释：</p><ol><li><p><strong>Actionlet-Dependent Combination（基于动作子集的组合）</strong>：这是一种数据合并方法，用于合并两个区域（“Actionlet” 区域和 “Non-Actionlet” 区域）的数据变换结果。合并的方法涉及使用 “Actionlets” 来组合这些数据。公式中的描述表明，最终的合并数据 Xi_trans 是通过以下方式计算的：</p><p>Xi_trans = (Ai_tv * Xi_act) + ((1 - Ai_tv) * Xi_non)</p><p>其中，Xi_act 和 Xi_non 分别是使用 “Actionlet Transformations”（Tact）和 “Non-Actionlet Transformations”（Tnon）变换的结果，Ai_tv 表示 “Actionlet”。</p></li><li><p><strong>Semantic-Aware Feature Pooling（语义感知特征池化）</strong>：这是一种用于从数据中提取动作信息的方法。这个方法在空间和时间维度上聚焦于 “Actionlet” 区域内的特征表示，从而减少了来自其他静态区域对动作特征提取的干扰。这个方法可以用以下方式表达：</p><p>SAFP(h^i_ctv) = Σ (h^i_ctv * (Ai_tv / Σ Ai_tv))</p><p>这个方法旨在有效提取动作信息，使特征更具区分性。作者在线下流程中使用这种语义感知特征池化操作，以提供准确的基准特征。</p></li></ol><p>总结：这段文字描述了如何将两个区域的数据变换结果合并，以及如何使用语义感知特征池化方法从数据中提取动作信息，从而减少其他静态区域对动作信息提取的干扰。这些方法有助于提高模型的性能。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>对比学习</title>
      <link href="/project/posts/71475a2e.html"/>
      <url>/project/posts/71475a2e.html</url>
      
        <content type="html"><![CDATA[<p>非对称损失（Asymmetric Loss）是一种损失函数，通常用于机器学习和统计建模中，用于解决不同类型错误的损失不平衡问题。在某些问题中，不同类型的错误可能具有不同的后果或成本，因此需要一种考虑这种不对称性的损失函数。</p><p>非对称损失通常用于以下情况：</p><ol><li><strong>类别不平衡问题</strong>：在分类问题中，类别分布可能不平衡，即某些类别的样本数量远远少于其他类别。在这种情况下，非对称损失可以用于平衡不同类别的重要性，使模型更加关注少数类别的正确分类。</li><li><strong>错误成本不同</strong>：有些问题中，不同类型的错误可能带来不同的代价。例如，医学诊断中，将疾病误判为健康可能会导致更大的损失，而将健康误判为疾病的代价较小。非对称损失可以根据错误类型的成本差异来调整模型的权重。</li><li><strong>欺诈检测</strong>：在金融欺诈检测中，大多数交易是正常的，但欺诈交易的损失可能会很高。为了提高欺诈检测的性能，可以使用非对称损失来放大欺诈交易分类错误的影响。</li><li><strong>稀有事件检测</strong>：在某些问题中，需要检测罕见事件或异常情况。这些罕见事件的样本数量较少，但其重要性较高。非对称损失可用于更好地捕捉罕见事件。</li></ol><p>具体来说，非对称损失函数会根据问题的特定需求对不同类型的错误进行加权，以确保模型更加关注那些可能导致更大影响的错误。这可以通过调整损失函数的权重或形式来实现。</p><p>常见的非对称损失函数包括Focal Loss和类别加权交叉熵损失（Class-Weighted Cross-Entropy Loss）。这些损失函数在不同的应用中有不同的变体和定义，以满足特定问题的要求。</p><h1>对比学习</h1><h2 id="simCLRV2算法——对比学习的另一个算法">simCLRV2算法——对比学习的另一个算法</h2><p><a href="https://zhuanlan.zhihu.com/p/369579204">理解对比学习 - 知乎 (zhihu.com)</a></p><blockquote><p>上边的博客 是对对比学习的另一种算法的讲解</p><p>在本文中，我将重点介绍<strong>SimCLRv2</strong>，这是Google Brain团队提出的最新的最新对比学习方法之一。 对于其他对比学习方法，例如Facebook的MoCo，我建议您阅读以下文章。</p><blockquote><p>对MoCo的英文解读</p></blockquote><p><a href="https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619">对比自监督学习框架和设计新方法 |作者：威廉·法尔肯 |迈向数据科学 (towardsdatascience.com)</a></p></blockquote><h2 id="InfoNCE-Loss-损失函数">InfoNCE Loss 损失函数</h2><h3 id="NCE损失">NCE损失</h3><p>InfoNce Loss损失 是从Nce Loss 损失演变而来</p><p>NCE的损失函数通用表示:</p><p><img src="../assets/image-20231016164156353.png" alt="image-20231016164156353"></p><p><img src="../assets/50b1f4c418b849c2b9c6e990a2d75160.png" alt="img"></p><p><code>动量对比（MoCo）通过使用对比损失将编码的查询 </code>q<code>与编码的键的字典key匹配来训练视觉表示编码器。字典的键</code>{k0, k1, k2, …}<code> 是通过一组数据样本动态定义的。字典被构建为一个队列，当前的小批量被入队，而最老的小批量被出队，从而与小批量大小分离。这些键由一个缓慢前进的编码器进行编码，该编码器由查询编码器进行动量更新驱动。这种方法为学习视觉表示提供了一个大型且一致的字典。</code></p><blockquote><p>这段代码定义了一个自定义的损失函数 <code>loss_function</code>，用于计算一个批次（batch）的损失。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">q, k, queue, t=<span class="number">0.07</span></span>):</span><br><span class="line">    N = q.shape[<span class="number">0</span>]  <span class="comment"># bacth size</span></span><br><span class="line">    C = q.shape[<span class="number">1</span>]  <span class="comment"># output dim</span></span><br><span class="line">    K = queue.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># import ipdb;ipdb.set_trace()</span></span><br><span class="line">    <span class="comment"># positive pairs</span></span><br><span class="line">    pos = torch.bmm(q.view(N, <span class="number">1</span>, C), k.view(N, C, <span class="number">1</span>)).view(N, <span class="number">1</span>)</span><br><span class="line">    neg = torch.mm(q.view(N, C), queue.view(C, K)).view(N, K)</span><br><span class="line">    <span class="comment"># import ipdb;ipdb.set_trace()</span></span><br><span class="line">    <span class="comment"># calculate the softmax value</span></span><br><span class="line">    pos = torch.exp(torch.div(pos, t))</span><br><span class="line">    neg = torch.<span class="built_in">sum</span>(torch.exp(torch.div(neg, t)),</span><br><span class="line">                    dim=<span class="number">1</span>).view(N, <span class="number">1</span>)</span><br><span class="line">    denominator = pos + neg</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss of a batch</span></span><br><span class="line">    <span class="keyword">return</span> torch.mean(-torch.log(torch.div(pos, denominator)))</span><br><span class="line">   </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">q, k, queue, t=<span class="number">0.07</span></span>):</span><br></pre></td></tr></table></figure><p><code>loss_function</code> 是一个自定义的损失函数，它接受四个参数：<code>q</code>、<code>k</code>、<code>queue</code>、和 <code>t</code>（默认值为0.07）。这些参数用于计算损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">N = q.shape[<span class="number">0</span>]  <span class="comment"># bacth size</span></span><br><span class="line">C = q.shape[<span class="number">1</span>]  <span class="comment"># output dim</span></span><br><span class="line">K = queue.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>这里计算了输入张量 <code>q</code> 的 batch size（<code>N</code>）、输出维度（<code>C</code>），以及 <code>queue</code> 的长度（<code>K</code>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pos = torch.bmm(q.view(N, <span class="number">1</span>, C), k.view(N, C, <span class="number">1</span>)).view(N, <span class="number">1</span>)</span><br><span class="line">neg = torch.mm(q.view(N, C), queue.view(C, K)).view(N, K)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>这两行代码计算了损失中的正样本（<code>pos</code>）和负样本（<code>neg</code>）的分数。<code>q</code> 和 <code>k</code> 是正样本对，<code>queue</code> 是负样本集合。</li><li><code>torch.bmm</code> 执行批次矩阵乘法，计算了正样本对的内积，然后使用 <code>.view</code> 来重新整形张量。</li><li><code>torch.mm</code> 计算了 <code>q</code> 和 <code>queue</code> 之间的矩阵乘法，以计算负样本对的分数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pos = torch.exp(torch.div(pos, t))</span><br><span class="line">neg = torch.<span class="built_in">sum</span>(torch.exp(torch.div(neg, t)),</span><br><span class="line">                 dim=<span class="number">1</span>).view(N, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>这部分代码对正样本和负样本的分数进行了指数运算，并除以温度参数 <code>t</code>。这是 softmax 操作的一部分，它将原始分数转化为概率分布。</li><li>对 <code>pos</code> 使用 <code>torch.div</code> 和 <code>torch.exp</code> 进行指数化。</li><li>对 <code>neg</code> 使用 <code>torch.div</code> 和 <code>torch.exp</code> 进行指数化，然后使用 <code>torch.sum</code> 沿着第一个维度（dim=1）进行求和，并使用 <code>.view</code> 将形状调整为 (N, 1)。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">denominator = pos + neg</span><br></pre></td></tr></table></figure><p>这一行计算了 softmax 操作的分母，即正样本和负样本的指数分数之和。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return torch.mean(-torch.log(torch.div(pos, denominator)))</span><br></pre></td></tr></table></figure><p>最后，这行代码计算了整个批次的损失。它执行以下操作：</p><ul><li>计算了负对数似然损失，即 <code>-torch.log()</code>。</li><li>使用 <code>torch.div</code> 计算了正样本分数和分母的比值。</li><li>使用 <code>torch.mean</code> 计算了批次内所有样本的平均损失。</li></ul><p>总之，这段代码实现了一种损失函数，通常用于度量正样本和负样本之间的相似性，并且在深度学习中的对比学习任务中经常会用到。这个损失函数将输入的 <code>q</code> 和 <code>k</code> 表示的正样本对与 <code>queue</code> 表示的负样本集合进行比较，并计算损失值，用于网络的反向传播和参数优化。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1>现在的研究现状 或者流行的方法</h1><p>对比学习曾经以其简洁的框架和有希望的性能主导了3D动作表示学习。</p><h1>方法存在的问题或者论文要解决的现状缺点</h1><p>作为全局表示学习器，它仍然存在一些限制，比如<strong>缺乏明确的时间上下文建模约束以及对启发式动作数据增强的过分依赖，这阻碍了其对3D动作的进一步探索。</strong></p><h1>学到的新的概念</h1><blockquote><p>一些在分类级别上考虑骨架数据处理情况</p></blockquote><ol><li>语义级别内存库 【在语义级别上 按照每个动作类别进行分类 一个语义级别的内存库包含了来自某个类的全部图的汇总】</li><li>实例级别内存库 【实例级内存库存储了按序列划分的图，其中包含了每个序列的个体属性】</li><li>部位级别 节点级别 身体级别</li><li>序列级别  【按照序列进行划分 】</li></ol><p>Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</p><p>鉴于3D骨架充当了人类行为的抽象表示，人们越来越有兴趣<strong>将MAE概念应用于3D动作表示学习，以捕捉骨架序列的潜在时空动态。</strong></p><p>MAE 概念【masked autoencoder (MAE)】</p><h3 id="“Pre-training-pretext”">“Pre-training pretext”</h3><p>“Pre-training pretext” 是指在自监督学习中的预训练步骤，其中模型被训练以学习有意义的表示（即预训练）而不需要明确的标签信息。在这种方法中，模型接受一组数据，并根据数据的内部结构、关系或其他自动生成的目标来学习表示。==这些自动生成的目标通常是模型的&quot;假设任务&quot;或&quot;预文本任务&quot;，因此它们被称为&quot;pretext&quot;==</p><p>预训练的目的是使模型能够学习到数据中的高级特征，这些特征可以在后续任务中进行微调或迁移，从而提高模型的性能。通常，预训练阶段在大型未标记数据集上进行，因此它充分利用了大量的数据来学习通用的特征表示。</p><p>一旦模型经过预训练，它的权重可以被微调以适应特定的任务，如分类、目标检测、语义分割等。在这种情况下，模型经过预训练的表示通常会提供更好的初始化点，从而加速任务特定模型的训练和提高性能。</p><p>预训练 pretext 任务可以是多样的，取决于具体的应用。例如，在计算机视觉中，预训练任务可以包括图像补全、图像旋转预测、颜色化任务等。在自然语言处理中，预训练任务可以包括语言建模、掩码语言建模（如BERT）等。</p><p>总之，pre-training pretext 是一种无监督或自监督学习的方法，它允许模型在没有明确标签信息的情况下学习到有用的特征表示，以提高后续任务的性能。这一方法已经在各种领域中取得了显著的成功。</p><h3 id="“Masked-Autoencoder”">“Masked Autoencoder”</h3><p>“Masked Autoencoder” 是一种自编码器（Autoencoder）的变种，用于学习数据的压缩表示，特别是在自然语言处理领域中广泛使用。这种自编码器之所以称为 “masked”，是因为它的训练过程通常涉及到将输入数据中的某些部分（通常是随机选择的一些元素）掩盖或遮挡，然后尝试通过模型来预测这些被掩盖的部分。</p><p>下面是一些关于 Masked Autoencoder 的关键概念和工作原理：</p><ol><li><strong>自编码器（Autoencoder）</strong>：自编码器是一种神经网络结构，它有一个编码器和一个解码器。编码器将输入数据映射到低维表示（潜在空间），然后解码器将潜在表示映射回原始数据的尺寸。自编码器的目标是学习一种紧凑的数据表示，以最小化输入和重构之间的差异。</li><li><strong>Masked Autoencoder的特点</strong>：在 Masked Autoencoder 中，输入数据的一部分通常会被随机掩盖或遮挡，这部分被称为 “mask”。<strong>模型的目标是从被掩盖的数据中预测出原始数据，同时仍要学习到有意义的表示。这使得模型更有能力学习到数据的局部特征，而不仅仅是整体表示。</strong></li><li><strong>应用领域</strong>：Masked Autoencoder 在自然语言处理中非常有用。例如，对于语言建模任务，可以使用 Masked Autoencoder 预测一句话中的某些单词，而其他单词被掩盖。这有助于模型学习词语之间的关系和句子的结构。</li><li><strong>变种</strong>：基于 Masked Autoencoder 的模型有多种变种，其中最著名的是 BERT（Bidirectional Encoder Representations from Transformers）。BERT 使用双向掩码语言建模作为预训练任务，它的预训练表示在多种自然语言处理任务中都表现出色。</li></ol><p>总之，Masked Autoencoder 是一种用于学习数据表示的神经网络模型，通过随机掩盖部分输入数据并预测它们，模型能够学习到有意义的特征表示。它在自然语言处理中有广泛应用，但也可用于其他领域的序列数据建模和特征学习。</p><h3 id="2-3-Masked-Image-Skeleton-Modeling">2.3 Masked Image/Skeleton Modeling</h3><p>Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning</p><p>在这个段落中，介绍了一种名为&quot;Masked Image/Skeleton Modeling&quot;的技术。这种技术源自堆叠降噪自编码器（stacked denoising autoencoders），在这种技术中，掩码操作被视为向原始数据添加噪声的方式。最近，这种掩码建模技术在自监督学习中取得了显著的成功，特别是用于图像表示学习。对于骨骼数据，有一种名为LongT GAN的技术，它直接利用基于自动编码器的模型，通过额外的对抗训练策略进行优化。还有一些研究 [21, 40] 将运动预测作为假设任务，以学习骨骼序列中的时间依赖关系。受到掩码自动编码器 [13] 的启发，Wu等人 [45] 提出了一种掩码骨骼自动编码器，用于学习空间 - 时间关系。</p><p>在这篇论文中，作者探讨了掩码建模和对比学习之间的协同作用，并提出了一种基于拓扑的掩码策略，以进一步提升表示学习的效果。这种方法似乎是要通过将掩码应用于骨骼数据，来改进对骨骼数据的建模和学习，以更好地理解骨骼数据中的空间和时间关系。</p><blockquote><p>Prompted Contrast with Masked Motion Modeling Towards Versatile 3D Action Representation Learning</p><p>该文章的对比学习基于 Yalong Bai, Yifan Yang,Wei Zhang, and Tao Mei. 2022. <strong>Directional Self-Supervised</strong><br><strong>Learning for Heavy Image Augmentations.</strong> In IEEE CVPR</p><p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. <strong>A simple framework for contrastive learning of visual representations.</strong> In ICML</p><p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. <strong>Momentum contrast for unsupervised visual representation learning.</strong> In IEEE CVPR.</p></blockquote><p>第二篇介绍上的</p><p><strong>Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning</strong>   <strong>提示与masked 运动建模的对比表示学习:面向通用的3D动作表示</strong></p><blockquote><p>基于对比学习 和 基于重建学习的任务</p><p>使用实例级别和 序列级别</p><p>但需要注意的是，大多数最近的<strong>表示学习方法专注于单一范式，<strong><strong><strong>仅对关节级（通过掩蔽骨架建模）</strong></strong>[45, 53] 或</strong>序列级（通过对比学习）</strong>[8, 20, 26, 52] <strong>特征进行建模</strong>。</p><p>尽管一些工作[21, 40, 43]付出了努力，<strong>将上述两种方法结合起来学习更丰富的表示</strong>，但只有一般的改进。</p></blockquote><blockquote><p><strong>提出了精心设计的内外对比学习和基于拓扑的掩模骨架预测作为基本流程</strong>。此外，我们将这两个任务连接起来，<strong>探索它们之间的协同作用</strong>。<strong>掩模预测训练中的视图被用作对比学习的新颖正样本</strong>。反过来，<strong>掩模预测分支也通过对比学习分支的梯度进行更新</strong>，<strong>以进行更高级别的语义引导</strong>。</p></blockquote><blockquote><p>减少不同前提任务和数据视图之间的学习干扰，我们提出了双提示多任务预训练策略。应用了两种类型的提示，即领域特定提示和任务特定提示，以明确指导模型从不同的数据视图/任务中学习。</p></blockquote><h3 id="集中式学习范式">集中式学习范式</h3><p><strong>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</strong></p><p>Centralized learning paradigm（集中式学习范式）是一种机器学习或深度学习的方法和框架，其中数据和计算资源在一个中央集中的地方进行管理和处理。这与分散式学习或分布式学习相对立，分布式学习通常涉及多个计算节点或设备之间的协作和数据共享。</p><p>在集中式学习范式中，通常有一个中央服务器或计算节点，它集中了所有的数据和计算资源。数据可以从多个来源收集到中央服务器，并且所有的模型训练和推断都在中央服务器上执行。这种范式通常用于以下情况：</p><ol><li><strong>数据集中管理：</strong> 所有数据都被集中管理，以确保数据的安全性和一致性。这对于需要对敏感数据进行管理和保护的应用非常重要。</li><li><strong>模型集中更新：</strong> 在集中式学习中，模型的更新和维护都在中央服务器上进行，而不需要在多个设备之间进行模型同步。</li><li><strong>较小的通信开销：</strong> 由于数据和计算都在一个地方，通信开销通常较低，不需要在不同设备之间传输大量数据。</li><li><strong>适用于某些任务：</strong> 对于一些任务，集中式学习可能是更简单、更有效的方法，尤其是当数据容易获得并且数据规模较小时。</li></ol><p>然而，集中式学习也有一些局限性，特别是对于大规模数据和分布式环境。在这些情况下，分布式学习可能更为适用，因为它允许数据和计算资源分布在多个地方，更好地适应大规模和高度分散的数据集。集中式学习和分布式学习都有各自的应用领域和优势，选择哪种范式通常取决于具体的任务和资源情况。</p><h3 id="现有的时间空间数据transformer-方式">现有的时间空间数据transformer 方式</h3><p><img src="../assets/image-20231023111729738.png" alt="image-20231023111729738"></p><p>Actionlet Transformation Tact:</p><p>四个空间数据转换{剪切，空间翻转，旋转，轴掩模};两个时间数据转换{作物，时间翻转};以及两个时空数据转换{高斯噪声，高斯模糊}。</p><p>skeleton—Adain</p><p>==对比学习早期文章==</p><p>Rao等人应用了[24] MoCo用于使用单一流进行对比学习。到uti lize等【15】人提出了跨流知识，Li等人【34】提出了 多视图对比学习法和Thoker等 采用多种模型，从不同的骨架中学习 陈述。Guo et al.[8]提出使用更多的极端增强，大大提高了效果 对比学习。Su et al.[33]提出了新的表征- 通过感知运动的一致性和连续性来学习 。在MoCo v2[10]之后，他们利用了InfoNCE的损失 为了优化对比学习:</p><h2 id="【24-MoCo-v1】Augmented-skeleton-based-contrastive-action-learning-with-momentum-LSTM-for-unsupervised-action-recognition-Information-Sciences-2021">【24] MoCo v1】Augmented skeleton based contrastive action learning with momentum LSTM for unsupervised action recognition. Information Sciences, 2021</h2><h2 id="【15】3D-human-action-representation-learning-via-cross-view-consistency-pursuit-In-IEEE-CVPR-2021">【15】3D human action representation learning via cross-view consistency pursuit. In IEEE CVPR, 2021</h2><h2 id="34-Skeleton-contrastive-3D-action-representation-learning-In-ACMMM-2021">[34 ] Skeleton-contrastive 3D action representation learning. In ACMMM, 2021.</h2><h2 id="【8】Contrastive-learning-from-extremely-augmented-skeleton-sequences-for-self-supervised-action-recognition-AAAI-2022">【8】Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. AAAI, 2022.</h2><h2 id="33-Self-supervised-3D-skeleton-action-representation-learning-with-motion-consistency-and-continuity-In-IEEE-ICCV-2021">[33]Self-supervised 3D skeleton action representation learning with motion consistency and continuity. In IEEE ICCV, 2021</h2><h2 id="【10-MoCo-v2】Momentum-contrast-for-unsupervised-visual-representation-learning-In-IEEE-CVPR-2020-记忆模块，采用队列存储负样本，队列随着训练不断更新。">【10  MoCo v2】Momentum contrast for unsupervised visual representation learning. In IEEE CVPR, 2020.记忆模块，采用队列存储负样本，队列随着训练不断更新。</h2><h2 id=""></h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文献略读2</title>
      <link href="/project/posts/bca2bed.html"/>
      <url>/project/posts/bca2bed.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>沉迷于环境部署的三天</title>
      <link href="/project/posts/ea46729a.html"/>
      <url>/project/posts/ea46729a.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>2023.10.9号晚上开始弄阿里云服务器   10号 当天 弄数据库+navicate  11号摆烂 也开始琢磨咋部署 12 号痛苦尝试+找视频教程</p></blockquote><p>还是要按照视频教程来，多找资料 多看才能进行</p><p>删除东西之前先保存</p><p>更改重要东西 像密码 数据库等先弄清楚命令 再继续</p><blockquote><p>数据库 安装的bug</p><p>密码设定错误，导致必须重新装一遍 然后导致不能下载sql 版本 卸载的不干净，最后用的windows微软的卸载软件</p><p>全部卸载重装 才能进行的安装</p></blockquote><p>jdk 安装的还是比较顺利</p><p>Navicat 只是个工具，不会改变本地的sql数据库中的东西，当时自己还傻傻的 不敢卸载Navicat</p><p>tomcat 自己在新的服务器上部署 了两个版本，一个是8.5  一个是9.0</p><blockquote><p>在新的服务器上配置的时候，使用了很多的方法，有的是不能够访问后台数据【在输入login的账号密码后一直转圈】，有的是不能访问前端</p><ol><li>把旧的服务器上的tomcat 文件夹直接复制到新服务器上 :x:</li><li>在服务器上重装tomcat 后 把本地的数据进行打包成war 进行上传然后重新tomcat 运行  :x:</li><li>找了哔哩哔哩上的视频教程 把本地的上传上去 还是失败</li><li>把旧服务器桌面上的jndev.zip 解压后上传 重新运行 :x:</li><li>把旧的服务器上的tomcat里边的jndev文件上传后 可以访问了。但是没有数据，显示null，想到是服务器的密码不对，重新把服务器的密码从123456变成了root 成功运行，但是有乱码问题</li></ol></blockquote><blockquote><ol><li>为了省那点服务器钱</li><li>把这个长痛变成短痛</li><li>激励自己必须赶紧去学点新东西了</li><li>中间的一天也是摆烂的一天 看大树君的视频看了很多</li></ol></blockquote><h3 id="服务器javaweb项目部署总结">服务器javaweb项目部署总结</h3><p>新的服务器的访问链接</p><p><a href="http://8.130.139.55/jndev/admin/views/sys_login.html">绠＄悊鍚庡彴</a></p><blockquote><p>现存的bug： 编码问题，一直显示有乱码，需要去改进一下这个bug就可以了 主要需要源码在重新部署</p><p>ssm项目在windows系统上war包放到服务器上但是不能访问后台数据 【已解决】</p><p><a href="https://blog.csdn.net/h_xiao_x/article/details/107012738">springboot打war包部署到tomcat后无法访问的解决方法_war部署到tomcat上无法访问-CSDN博客</a></p><p><a href="https://blog.csdn.net/IT_TIfarmer/article/details/84678188">war包部署到服务器上时，项目无法访问的问题_war包部署后根据路径访问不了文件-CSDN博客</a></p><p><a href="https://bbs.csdn.net/topics/392512266">idea中部署可以运行，war部署到tomcat就不能正常运行了-CSDN社区</a></p></blockquote><p>2023.10.13  资源总结：</p><p>视频教程：这个系列正好是从windows系统部署自己的javaweb项目  全部跟下来教会你怎样安装jdk mysql</p><p><a href="https://www.bilibili.com/video/BV1J8411f7TS?p=11&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">11修改Tomcat默认端口为80_哔哩哔哩_bilibili</a></p><blockquote><p>看过的blog：</p><p><a href="https://developer.aliyun.com/article/766735#:~:text=%E6%8A%8A%E6%9C%AC%E5%9C%B0%E7%9A%84%E9%A1%B9%E7%9B%AE%E7%9A%84war%E5%8C%85%E5%A4%8D%E5%88%B6%E5%88%B0%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8B%E7%9A%84Tomcat%20%E7%9B%AE%E5%BD%95%E7%9A%84webapps%E4%B8%8A%E3%80%82%20%E4%B9%8B%E5%90%8E%E5%90%AF%E5%8A%A8Tomcat%E3%80%82,%E5%A6%82%E5%9B%BE%E6%89%80%E7%A4%BA%E3%80%82%20%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%A1%B9%E7%9B%AE%E6%97%B6%EF%BC%8C%20%E6%8A%8A%E6%9C%AC%E5%9C%B0%E5%9C%B0%E5%9D%80%20localhost%20%E6%94%B9%E6%88%90%E5%85%AC%E7%BD%91IP%E5%9C%B0%E5%9D%80%E5%8D%B3%E5%8F%AF%E3%80%258">关于小白将Web项目部署到阿里云服务器-5个步骤搞定【新手手册】-阿里云开发者社区 (aliyun.com)</a></p><p><a href="https://blog.csdn.net/weixin_42858422/article/details/122762068">java——SSM项目部署上线——（阿里云服务器ECS + 云数据库RDS MySQL）（宝塔）_javaee系统部署到ecs云服务器上,数据库使用rds云数据库_五月CG的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/doublepg13/article/details/120708731">IDEA web项目导出 war 包_idea导出web项目_86上山了、的博客-CSDN博客</a></p><p><a href="https://www.cnblogs.com/dongao/p/13409116.html">JavaWeb项目的云服务器部署 - 大雪初晴丶 - 博客园 (cnblogs.com)</a></p><p><a href="https://blog.csdn.net/qq_44778023/article/details/110118656">在Windows服务器上部署Tomcat并运行SSM项目_windows启动perfect-ssm-CSDN博客</a></p></blockquote><h4 id="服务器部署时开放端口与防火墙">服务器部署时开放端口与防火墙</h4><p><a href="https://help.aliyun.com/zh/eci/user-guide/assign-a-security-group?spm=5176.ecscore_sg.0.0.6d9f4df5vV9gB0&amp;scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%40451784.S_RQW%40ag0%2BBB2%40ag0%2BBB1%40ag0%2Bhot%2Bos0.ID_451784-RL_%E5%AE%89%E5%85%A8%E7%BB%84%E9%85%8D%E7%BD%AE%E8%A7%84%E5%88%99-LOC_console~UND~more~UND~help-OR_ser-V_3-P0_0">如何配置安全组_弹性容器实例-阿里云帮助中心 (aliyun.com)</a></p><p><a href="https://help.aliyun.com/zh/ecs/best-practices-for-windows-firewall-with-advanced-security">高级安全Windows防火墙实践-阿里云帮助中心 (aliyun.com)</a></p><p><a href="https://blog.csdn.net/crazestone0614/article/details/126923555">阿里云服务器开放端口的正确方式（超详细图文教程）_阿里云开放端口_思涛的博客的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/weixin_43770545/article/details/120141060">解决：阿里云服务器添加安全组端口映射后仍无法访问_阿里云安全组配置了端口还是没法访问-CSDN博客</a></p><p>[阿里云服务器 安全组 防火墙 开放端口流程_阿里云服务器防火墙开放端口_謝逅相遇的博客-CSDN博客](<a href="https://blog.csdn.net/xiehou_xie/article/details/113998552#:~:text=%E9%98%BF%E9%87%8C%E4%BA%91%E5%AE%89%E5%85%A8%E7%BB%84">https://blog.csdn.net/xiehou_xie/article/details/113998552#:~:text=阿里云安全组</a> 1 1. 找到对应的系统实例并选中 博主的实例就是在华北2中%2C选中后就可以获得实例的具体信息 2 2. 通过,2 1 2 在防火墙关闭 dead 之后%2C外部就可以通过ip和接口调用到程序了 2. )</p><p><a href="https://developer.aliyun.com/article/616794">windows防火墙设置端口开放技巧-阿里云开发者社区 (aliyun.com)</a></p><h3 id="mysql安装总结">mysql安装总结</h3><p>Mysql的密码 重置 导致的mysql版本重装 结果之前安装的8.2 卸载不干净，再次安装不断的报错</p><p><img src="../assets/image-20231014083031619.png" alt="image-20231014083031619"></p><blockquote><p>安装的版本 是 8.0  但是报错是一样的，一直展示install failed</p></blockquote><p>下载everything 后搜索全部删除之后也是报错</p><p>最后弄了两个小时后 尝试换了一个版本 也是会出错误，但是报错信息变了，所以尝试搜索下新版本的报错信息</p><blockquote><p>1: The older version of MySQL Server 8.0 cannot be removed. Contact your technical support group. 1: 1: MySQL Server 8.0 2: {7C422189-D743-40C0-A721-BF86AA6385CF} 3: 3 1: The action ‘Install’ for product ‘MySQL Server 8.0.33’ failed.</p><p>新安装的8.0.33 版本报错 之前安装的8.0 版本不能卸载</p></blockquote><p>查找原因 搜到了下边的回答: <a href="https://blog.csdn.net/yh1771975467/article/details/78791284">mysql5.7.20安装报不能移除旧版本问题解决-CSDN博客</a></p><p><img src="../assets/image-20231014083339214.png" alt="image-20231014083339214"></p><p>于是一气呵成 下载 windows install cleanup utility  然后选择删除全部 在全部删除 （包括昨晚上安装的jdk）</p><p>==资源总结==</p><p><a href="https://www.lanzouj.com/i65zcfi">Windows_Installer_CleanUp_Utility4.71.1015.exe - 蓝奏云 (lanzouj.com)</a></p><p><img src="../assets/image-20231014083450115.png" alt="image-20231014083450115"></p><blockquote><p>趟过得水 ，踩过的坑，不能解决的方案</p><p><a href="https://blog.csdn.net/qq_44076667/article/details/123571763">MySQL重安装失败_1: download of package ‘mysql server 5.7.30’ faile-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_59252007/article/details/124637069">mysql-server8.0的安装过程日志_1: download of product ‘connector-net’ started fro_道阻且长-行则将至-行而不辍-未来可期的博客-CSDN博客</a></p><p><a href="https://www.cnblogs.com/mojing-milkyway/p/16875983.html">Windows下MySQL安装主要步骤和过程中出现的一些问题 - 龙卷风摧毁停车场a - 博客园 (cnblogs.com)</a></p><p><a href="https://www.cnblogs.com/mojing-milkyway/p/16875983.html">Windows下MySQL安装主要步骤和过程中出现的一些问题 - 龙卷风摧毁停车场a - 博客园 (cnblogs.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/68190605">MySQL完美卸载及安装 - 知乎 (zhihu.com)</a></p></blockquote><h3 id="mysql密码更改">mysql密码更改</h3><p>使用的是mysql8.0 版本 在更新部署到服务器上 javaweb项目的时候，项目部署好之后登陆，不能够进行后台页面访问，因为部署的java项目的sql密码是root，而在服务器上的密码是123456</p><p>所以需要将密码进行修改成root 来符合javaweb项目</p><blockquote><p>问题: 更改mysql的root密码报语法错误   1064 错误代码</p></blockquote><blockquote><p>错误方法 不知道为什么，自己记得下边的方法是可行的，可能是版本问题吧</p></blockquote><p>[MYSQL 修改ROOT密码-阿里云开发者社区 (<a href="http://aliyun.com">aliyun.com</a>)](<a href="https://developer.aliyun.com/article/656213#:~:text=%E6%94%B9%E5%AF%86%E7%A0%81%EF%BC%9Aupdate">https://developer.aliyun.com/article/656213#:~:text=改密码：update</a> user set password%3Dpassword (“123”) where user%3D&quot;root&quot;%3B （别忘了最后加分号）,者 update mysql.user set password%3DPASSWORD (‘123456’) where User%3D’root’%3B)</p><p><a href="https://blog.csdn.net/qq_40757240/article/details/118068317">MySQL修改root用户密码_mysql改密码_折竹丶的博客-CSDN博客</a></p><p>正确的方法</p><p><a href="https://blog.csdn.net/weixin_43645330/article/details/83869490">mysql 修改root密码提示1064语法错误问题解决-CSDN博客</a></p><p><img src="../assets/image-20231014084926203.png" alt="image-20231014084926203"></p><h3 id="tomcat-部署好之后-出现乱码【暂未解决】">tomcat 部署好之后 出现乱码【暂未解决】</h3><p>tomcat乱码  绠$悊錫庡彴</p><p>页面部署之后，一直是有一部分是乱码。应该是服务器编码的问题，在传给访问浏览响应的时候，会一直展示部分乱码</p><p>==暂未解决，仍需继续==</p><p><img src="../assets/image-20231014085935986.png" alt="image-20231014085935986"></p><blockquote><p>做过的尝试</p><p>[idea项目使用tomcat运行乱码问题(全部解决,亲测有效)_idea中tomcat乱码-CSDN博客](<a href="https://blog.csdn.net/Bernie_7/article/details/121742274#:~:text=%E6%89%93%E5%BC%80%E4%BD%A0%E7%9A%84">https://blog.csdn.net/Bernie_7/article/details/121742274#:~:text=打开你的</a> tomcat安装目录 ，进入 conf文件夹 添加修改,修改javac 编译 加上 -encoding%3DUTF-8 这样一来， (重新)启动运行，发现浏览器再无乱码。)</p><p><a href="https://blog.csdn.net/weixin_46248051/article/details/108701411">解决tomcat输出中文到浏览器出现乱码问题_浏览器无法识别tomcat传过来的中文-CSDN博客</a></p><p>[tomcat title乱码_51CTO博客](<a href="https://blog.51cto.com/topic/tomcat-titleluanma.html#:~:text=Tomcat%E4%B9%B1%E7%A0%81">https://blog.51cto.com/topic/tomcat-titleluanma.html#:~:text=Tomcat乱码</a> 解决 一、修改 Tomcat 的conf的server.xml文件加上 URIEncoding%3D&quot;UTF-8&quot; 二、在,tomcat 的bin 目录下的catalina.bat 配置文件中，添加以下设置 set &quot;JAVA_OPTS%3D%JAVA_OPTS% %JSSE_OPTS% -Dfil)</p><p><a href="https://blog.51cto.com/u_6159597/1048498">tomcat编码乱码问题_坚持的技术博客_51CTO博客</a></p><p><a href="https://www.jianshu.com/p/7236d45cd1eb">解决各种tomcat中文乱码问题 - 简书 (jianshu.com)</a></p><p><a href="https://blog.csdn.net/lengjinghk/article/details/51938644">Tomcat解决中文乱码的几种方式_tomcat中文-CSDN博客</a></p><p><a href="https://blog.csdn.net/herojuice/article/details/96350128">访问网页中文出现乱码（Tomcat）-CSDN博客</a></p></blockquote><h3 id="jdk-tomcat-sql-serve-安装过程">jdk tomcat  sql-serve 安装过程</h3><p>tomcat9 <a href="https://blog.csdn.net/qq_41570658/article/details/108357231">Tomcat 9.0.x 的下载、安装、配置、检测（详细讲解）_tomcat9.x-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_43511299/article/details/119784472">最简单的Tomcat9下载安装教程_apache tomcat 9解压中文版下载-CSDN博客</a></p><blockquote><p>jdk sql 安装过程 直接使用本地的路径下 的安装包 和教程即可，一定要先看好教程，再去安装</p><p>H:\43.138.33.151【资源汇总】</p></blockquote><h3 id="IDEA导出-war包">IDEA导出 war包</h3><p><a href="https://blog.csdn.net/doublepg13/article/details/120708731">IDEA web项目导出 war 包_idea导出web项目_86上山了、的博客-CSDN博客</a></p><h3 id="Navicat链接服务器数据库">Navicat链接服务器数据库</h3><blockquote><p>按照视频教程 使用本地的Navicat 也可以直接链接 到服务器上的mysql数据库</p></blockquote><p>在连接的时候 使用服务器的ip 然后配上服务器的 mysql的账号 和密码 可以直接连接上</p><p>并进行数据操作</p>]]></content>
      
      
      <categories>
          
          <category> 日拱一卒 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot项目知识点记录</title>
      <link href="/project/posts/2308c93a.html"/>
      <url>/project/posts/2308c93a.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-熟能生巧">1.熟能生巧</h2><ol><li>开发流程 从前端到后端 开始写一个vue文件 就要先从index.js中进行导入路由设置，然后将vue文件进行配置实现路由跳转</li></ol><h2 id="2-mapper-xml配置错误">2.mapper.xml配置错误</h2><ol><li>自定义的mapper函数，在.mapper 中实现后，【因为是存储在了java文件夹下的mapper文件夹中，所以需要在项目的pom.xml文件中导入扫包路径】不然的话会导致  <strong>org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.southwind.mapper.SystemAdminMapper.findByUsername 报错</strong> 需要进行排查</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line"> <span class="comment">&lt;!--    允许在java目录下读取xml文件 不然会读取不到statement.xml文件 不是xml扫包问题--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--    org.apache.ibatis.binding.BindingException: Invalid bound statement (not found): com.southwind.mapper.SystemAdminMapper.findByUsername 报错--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--     1. Mapper.java 文件的接口名和xml中的namespace名字是否匹配--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--     2. spring.xml中扫包路径是否错误  扫描的是否是mapper包 包含xml文件目录--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--     3. xml文件没有被读取到  可能是因为java目录没有被配置到pom.xml文件中——用下边resources 将java文件加入进去就可以--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resources</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resource</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">directory</span>&gt;</span>src/main/java<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.xml<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resource</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resources</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="3-前端知识点">3.前端知识点</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">axios.get(&#x27;http://localhost:8181/student/findById/&#x27;+_this.$route.query.id).then(function (resp) &#123;</span><br><span class="line">//  调用dormitoryFindById 函数后，返回的是 ResultVo 数据(resp.data 是resultVO) 需要将其中的data拿出来 才是对应的数据值</span><br><span class="line">console.log(_this.ruleForm)</span><br><span class="line"></span><br><span class="line">console.log(resp.data.data)</span><br><span class="line">//  前端调用后台将数据取出之后，只需要将数据回填到表单中就可以，这里对应的表单 和 ruleForm对象已经绑定好了，在填充完ruleForm对象后</span><br><span class="line">  //  会自动填充到绑定的的表单中</span><br><span class="line">_this.ruleForm = resp.data.data</span><br><span class="line">console.log(_this.ruleForm)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>// 上边的代码 调用了后台的findById 方法，返回了一个resp 数据是studentForm数据</p><p>定义如下:</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">package com.southwind.form;</span><br><span class="line"><span class="keyword">import</span> lombok.<span class="built_in">Data</span>;</span><br><span class="line"></span><br><span class="line">@<span class="built_in">Data</span></span><br><span class="line"><span class="keyword">public</span> class StudentForm &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">Integer</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">String</span> number;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">String</span> name;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">String</span> gender;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">Integer</span> dormitoryId;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">Integer</span> oldDormitoryID;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">String</span> state;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="built_in">String</span> createDate;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是在vue使用 的是 _this.reluForm接收的数据</p><p>_this.reluForm的定义如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    ruleForm: &#123;</span><br><span class="line">    number: &#x27;&#x27;,</span><br><span class="line">    name: &#x27;&#x27;,</span><br><span class="line">    gender: &#x27;男&#x27;,</span><br><span class="line">    dormitoryId: &#x27;&#x27;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>上边函数执行完成之后的结果如下：</p><p><img src="../assets/image-20231008213852113.png" alt="image-20231008213852113"></p><blockquote><p>可以看到 一开始没有对_this.reluForm 赋值的时候，数据是只有上边原来定义的四个属性值</p><p>但是在后边赋值之后， 指向的是另一个对象，变成了有6个属性，并且值也变成了对应的值</p><p>==这里的reluForm 说明是一直不变的，跟随着后台的对象 属性和值都相应变化了==</p></blockquote><blockquote><p>后来再执行了新的修改语句，修改了对应的宿舍id的时候，dormitoryId 变成了所选择的dormitoryId 旧的dormitoryId仍然是原来的值，</p><p>再次打印之后，可以看到所展示出来的值是更新之后的</p></blockquote><p>并且reluForm 仍然是6个属性的值，不是原来的4个属性的值了</p><p><img src="../assets/image-20231008214516422.png" alt="image-20231008214516422"></p><h2 id="4-端口被占用问题">4.端口被占用问题</h2><p>Identify and stop the process that’s listening on port 8181 or configure this application to listen on another port. Disconnected from the target VM, address: ‘127.0.0.1:2968’, transport: ‘socket’</p><p><a href="https://blog.csdn.net/qq_26224573/article/details/115873488">Identify and stop the process that’s listening on port 8080 or configure this application to listen_identify and stop the process that’s listening on -CSDN博客</a></p><p><strong>冲春招自律生活day2</strong></p><p><strong>整理：《面渣逆袭—javaSE》</strong></p><p><strong>重温：《面渣逆袭—java集合框架》</strong></p><p><strong>刷：1、《面渣逆袭—java并发编程》        2、《面渣逆袭—jvm》</strong></p><blockquote><p>时间安排不合理，总想做很多，却一个也做不好</p><p>只是在学习，反思和总结的的很少</p><p>只关注时间投入，没有能力的提高，导致自己一直处于初级阶段，没有正向反馈</p></blockquote><p>朝为田舍郎，暮登天子堂”</p><p>将相本无种，男儿当自强</p><p>人不为wei自 成为</p><p>整理的GNN学习资料</p><p><a href="https://www.aliyundrive.com/s/rBtNtLccaM1">https://www.aliyundrive.com/s/rBtNtLccaM1</a><br>Transformer的可解释性：<br><a href="https://arxiv.org/abs/2305.16380">https://arxiv.org/abs/2305.16380</a></p><p>GNN可解释性：<br><a href="https://github.com/flyingdoog/awesome-graph-explainability-papers">https://github.com/flyingdoog/awesome-graph-explainability-papers</a></p><p>1.Dynamic Graph Representation Learning with Neural Networks: A Survey</p><p><a href="https://arxiv.org/pdf/2304.05729.pdf">https://arxiv.org/pdf/2304.05729.pdf</a></p><p>2.Deep learning for dynamic graphs: models and benchmarks</p><p><a href="https://arxiv.org/pdf/2307.06104.pdf">https://arxiv.org/pdf/2307.06104.pdf</a></p><ol start="3"><li>Encoder-Decoder Architecture for Supervised Dynamic Graph Learning: A Survey</li></ol><p><a href="https://arxiv.org/pdf/2203.10480.pdf">https://arxiv.org/pdf/2203.10480.pdf</a></p><p>4.Foundations and modelling of dynamic networks using Dynamic Graph Neural Networks: A survey</p><p><a href="https://arxiv.org/pdf/2005.07496.pdf">https://arxiv.org/pdf/2005.07496.pdf</a></p><p>经典论文</p><p>1.Representation Learning over Dynamic Graphs</p><p><a href="https://arxiv.org/pdf/1803.04051.pdf">https://arxiv.org/pdf/1803.04051.pdf</a></p><p>2.Predicting Dynamic Embedding Trajectory in Temporal Interaction Network</p><p><a href="https://arxiv.org/pdf/1908.01207.pdf">https://arxiv.org/pdf/1908.01207.pdf</a></p><p>3.Temporal Graph Networks for Deep Learning on Dynamic Graphs</p><p><a href="https://arxiv.org/pdf/2006.10637.pdf">https://arxiv.org/pdf/2006.10637.pdf</a></p><p><img src="../assets/image-20231007231122512.png" alt="image-20231007231122512"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>曲水亭街游记</title>
      <link href="/project/posts/709693de.html"/>
      <url>/project/posts/709693de.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>代码详解1</title>
      <link href="/project/posts/9ed60f01.html"/>
      <url>/project/posts/9ed60f01.html</url>
      
        <content type="html"><![CDATA[<h2 id="Python-学习路径选择的有感而发">Python 学习路径选择的有感而发</h2><blockquote><p>作为一个计算机人，应该有些不一样的地方</p><p>而不是直接去bilibili搜索课程，可以使用github的训练课程 或者 一些别的专业接触不到的东西，锻炼自己的专业思维</p></blockquote><ol><li>github的对应开放项目</li></ol><p><a href="https://mp.weixin.qq.com/s/wbA8oK3se4tTj_NlIr6GLw">GitHub 上适合新手的Python项目 (qq.com)</a></p><ol start="2"><li>一些知乎上的指导性意见</li></ol><blockquote><p>作者：高厉害<br>链接：<a href="https://www.zhihu.com/question/375664382/answer/1750261054">https://www.zhihu.com/question/375664382/answer/1750261054</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>想入门 python，b 站或者 <a href="https://www.zhihu.com/search?q=mooc&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">mooc</a> 各种免费资源一大堆。</p><p>想学什么科学<a href="https://www.zhihu.com/search?q=%E8%AE%A1%E7%AE%97%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">计算数据处理</a>，b 站或者 mooc 还是一大堆。</p><p>想换种方式，educative 了解一下，github 申请个学生包，大部分课都是免费的。</p><p>github 上 python 入门的好教程也是有的。</p><p>不要想着什么能在网页上直接运行，很方便，既然下决心要学编程，装环境的各种坑必须自己踩一遍，现在不踩以后也得踩，这东西逃不掉的。</p><p>想学编程，以后遇到的问题只会越来越多，所以学编程最重要的是培养学习能力。</p><p>学习能力是在自己动手实践摸索，不断踩坑试错中锻炼出来的，而不是被人规划着学习路线，手把手拉着你走出来的。</p><p>当然，被人拉着走过的路一定更平坦更舒服，进步一定很快。</p><p>但它可能会成为未来继续学习的瓶颈，这一点我深有体会。</p><hr><p>这问题下边回答怎么多了这么多。。</p><p>确实还没了解过夜曲编程，如果能方便入门也是不错的。</p><p>最近越来越感受到，在计算机领域，想要了解一门技术，一种思想，一个项目，或一个系统的工作原理，即它们是如何运作的，一个非常重要的东西是 <strong>a general <a href="https://www.zhihu.com/search?q=high-level&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">high-level</a> idea。</strong></p><p>这个词来源于一个非常赞的开源仓库 <a href="https://www.zhihu.com/search?q=a-super-tiny-compiler&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">a-super-tiny-compiler</a>。我不知道该如何准确表达它的意思，大概可以理解为对整个思路的一个俯瞰。</p><p>当我们有了这个俯瞰，就可以慢慢去了解它的细节了，下一步的一个良好实践是，把这个思路从底层<a href="https://www.zhihu.com/search?q=%E8%A7%A3%E6%9E%84&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">解构</a>，然后以最简单的方式重新实现它。</p><p>然后我们就几乎彻底搞明白了这个我们想要了解的东西。</p><p>想推一门新课，CS50X，基本上涵盖了当前工业界主要在做的事情。</p><p>这门课是一门<a href="https://www.zhihu.com/search?q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AF%BC%E8%AE%BA&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1750261054%7D">计算机导论</a>，但其内容非常丰富，很有实践意义，希望对计算机技术感兴趣的人，能够在开始的时候对计算机领域有一个大致的了解，然后再慢慢深入，这样学习的话，负担会小一些，路径会更清晰，也会更有趣。</p><p>计算机很有趣。</p><p>CS50是哈佛的，MIT的是6.00.1x</p></blockquote><ol start="3"><li>一些国外的论坛或者国外的社区，哈佛或者其他常青藤的一些高校对外的课程</li><li>知识付费，需要先对要做的事情有些初步的了解之后，【防止被割韭菜】，弄清楚自己最需要的东西——也就是为什么要付费后，可以站在专业的肩膀上进行探索，一定要把自己的目标作为导向，所谓的付费项目也一定是鱼龙混杂，需要自己去判定一下，就算付费以后也要自己慢慢的跟着他的节奏，根据自己的最大的需求为出发点，慢慢排除掉他这个项目中不好或者不足的东西，快速吸取项目中最需要的东西后，抛弃掉项目独立出来坚持下去。</li><li>咸鱼或者二手平台，这其中一定有很多的需要甄别的东西，在过程中慢慢提高自己的甄别能力</li><li>相信专业，不过于相信自己</li></ol><h2 id="相信自己">相信自己</h2><blockquote><p>永远都要相信自己，相信成长，相信自己的价值</p><p>哪怕现在做的事情可能不会成功，但要对事不对人，反思自己在事情中有没有受到磨练，有没有螺旋式成长，努力的吸取营养，向上成长吧！！</p><p>从事情和目标为导向，不断的学习怎样合理的分配自己的时间，努力的向上成长</p></blockquote><blockquote><p>学习新的语言要从自己现在会的上边去找两个语言之间的不同，</p></blockquote><p>把重点放在找到两门语言的区别就好了，不断的去体验新的不同，基础的本质东西是一样的，不断去适应新的东西就好了</p><p>语法上，数据类型上，书写习惯和思路上</p><p>14个小时去体验</p><h3 id="python能干点啥">python能干点啥</h3><blockquote><p>数据分析 做表格 excel</p><p>爬虫 充盈数据</p><p>脚本语言，实现一些繁琐的功能</p><blockquote><p>自动给抢票，自动给回复邮件，自动给实现回复，不想动手的事情用它来干活</p></blockquote><p>表格数据处理</p><p>爬取大数据做分析</p><p>学习网络开发  html js前端+ 后端数据分析  爬虫 +ps 修饰 成为<code>全面型人才</code>—— 接私活</p></blockquote><p>接私活  培训老师 录课 量化投资</p><p>脚本语言 成为自己智能化的开始 + 全面扩宽自己 + ai智能</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读7</title>
      <link href="/project/posts/56c098e5.html"/>
      <url>/project/posts/56c098e5.html</url>
      
        <content type="html"><![CDATA[<h1>Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比</h1><p>这篇论文旨在进行<strong>基于骨骼的无监督的动作表示学习</strong>，并提出了一种新的<strong>分层对比（HiCo）框架</strong>。与现有的基于对比的解决方案不同，通常将输入骨骼序列表示为<strong>实例级特征并在整体上执行对比</strong>，我们提出的<strong>HiCo将输入表示为多个层次的特征</strong>，<strong>并以分层方式进行对比</strong>。具体而言，给定一个人体骨骼序列，我们通过序列到序列（S2S）编码器和统一的下采样模块，<strong>将其表示为来自时间和空间域的不同粒度的多个特征向量</strong>。此外，<strong>分层对比是根据四个级别进行的：实例级、域级、剪辑级和部分级</strong>。此外，HiCo与S2S编码器正交，允许我们灵活地融合最先进的<strong>S2S编码器</strong>。在四个数据集上进行的大量实验，即NTU-60、NTU-120、PKU-MMD I和II，显示HiCo在两个下游任务，包括<strong>动作识别和检索中</strong>，实现了<strong>无监督骨骼动作表示学习的最新状态</strong>，并且其学习到的动作表示具有良好的可迁移性。此外，我们还展示了我们的<strong>框架对于半监督骨骼动作识别是有效的</strong>。我们的代码可以在https://github.com/HuiGuanLab/HiCo 上获得。</p><p>摘要总结： 这篇论文提出了一个新的Hierarchical Contrast（HiCo）框架，用于<strong>无监督骨骼动作表示学习</strong>。与传统的对比方法不同，<strong>HiCo将输入骨骼序列分层表示，并在多个层次进行对比。它通过序列到序列编码器和统一的下采样模块生成不同粒度的特征向量，然后在实例级、域级、剪辑级和部分级进行分层对比</strong>。实验结果表明，HiCo在动作识别和检索任务中实现了无监督骨骼动作表示学习的最新成果，并且具有良好的迁移性能，同时也在半监督骨骼动作识别方面表现出效果。其代码可在GitHub上找到。</p><h3 id="int">int</h3><p>这篇论文介绍了人类动作识别在人机交互、智能监控、视频内容分析、游戏控制等领域具有广泛应用。近年来，基于3D骨骼的动作识别在深度学习网络的推动下取得了显著进展。然而，如何学习更具区分性的骨骼表示仍然是骨骼动作识别的一个未解之谜。为了解决这个问题，早期的大多数工作采用了完全监督的方式来训练网络，这需要大量标记的3D骨骼数据，因此成本高昂且耗时。近年来，出现了一种新趋势，即提出无监督的基于骨骼的动作表示学习（SARL）方法，以减轻注释工作量。本文也关注无监督的SARL。</p><p>现有的无监督SARL方法大致分为三类：编码器-解码器方法、对比学习方法和混合方法。编码器-解码器方法首先将输入的骨骼序列编码为潜在特征，然后在各种手工制作的前提任务的指导下解码这些潜在特征，例如骨骼重建、骨骼上色预测和骨骼位移预测。对比学习方法通常将输入的骨骼序列增强为两个增强实例，并训练编码器使同一骨骼的实例具有比不同骨骼的实例更相似的表示。混合方法将编码器-解码器和对比学习的思想结合起来。近年来，由于其简单的机制和出色的性能，对比学习方法占据主导地位。</p><p>本文提出了一个新的Hierarchical Contrast（HiCo）框架，用于无监督SARL。HiCo具有分层编码器网络，将骨骼编码为不同粒度的部分级、剪辑级、域级和实例级表示，并进行分层对比。具体来说，分层编码器网络有两个分支，对应于时间域和空间域。对于时间分支，它将骨骼序列编码为不同时间粒度的多个剪辑级特征，通过提取不同长度的剪辑的特征。类似地，空间分支从不同大小的人体部位提取特征以获得部分级特征。此外，从时间和空间域导出的剪辑级特征和部分级特征逐渐用于组成域级表示和实例级表示。另外，鉴于多级表示，进行了四个级别的分层对比。我们的假设是这种分层对比与人体骨骼的分层性质一致，并提供更多对于无监督学习至关重要的监督。</p><p>总结： 本文提出了HiCo框架，该框架使用分层对比来进行无监督的骨骼动作表示学习。HiCo具有分层编码器网络，能够将骨骼序列编码为不同粒度的特征表示，并进行多级别的分层对比。实验结果表明，HiCo在无监督骨骼动作表示学习中取得了最新的最新成果，并具有良好的可迁移性能。此外，我们还展示了该方法对于半监督的基于骨骼的动作识别同样有效。</p><p>这篇论文旨在解决无监督骨骼动作表示学习的问题，并提出了一种新的<strong>分层对比（HiCo）框架</strong>。不同于现有的基于对比的解决方案通常将输入的骨骼序列表示为实例级特征并在整体上进行对比，我们提出的<strong>HiCo将输入表示为多级特征</strong>，<strong>并以分层方式进行对比</strong>。具体来说，对于给定的人体骨骼序列，我们<strong>通过序列到序列（S2S）编码器和统一的下采样模块</strong>，将其表示为来自时间和空间领域的不同粒度的多个特征向量。此外，分层对比从四个层面进行：实例级、域级、剪辑级和部分级。此外，==HiCo与S2S编码器是正交的==，这使我们能够灵活地应用最先进的S2S编码器。在四个数据集上进行的大量实验表明，HiCo在<strong>无监督骨骼动作表示学习的两个下游任务（动作识别和检索）中取得了最新的最新成果</strong>，并且其学到的动作表示具有良好的可迁移性。此外，我们还展示了我们的框架对于半监督骨骼动作识别同样有效。</p><h3 id="introduction">introduction</h3><p>1 引言 人体动作识别在人机交互、智能监视、视频内容分析、游戏控制等领域有广泛应用（Gao、Zhang和Xu 2019; Li等2019; Dong等2022）。近年来，基于3D骨架的动作识别在深度学习网络的推动下取得了显著进展（Zhang 2012; Liu等2019; Wang、Ni和Yang 2020）。然而，<strong>如何学习更具辨识性的骨骼表示仍然是骨骼动作识别领域的一个未解决问题。</strong></p><p>为了解决这个问题，早期的大多数工作（Ni、Wang和Moulin 2011; Wang等2012; Vemulapalli和Chellapa 2016; Zhang等2017; Liu等2019; Cheng等2020）采用了<strong>全面监督的方式训练网络</strong>，这需要大量标记的3D骨架数据，因此成本高昂且耗时。最近，我们注意到提出<strong>无监督骨骼动作表示学习（SARL）方法的新趋势</strong>，以减轻注释工作的负担（Zheng等2018; Su、Liu和Shlizerman 2020; Lin等2020）。本文也关注<strong>无监督的SARL</strong>。</p><p>目前的无监督SARL研究可以大致分为三类：<strong>编码器-解码器方法</strong>（Zheng等2018; Nie、Liu和Liu 2020），<strong>对比学习方法</strong>（Rao等2021; Thoker、Doughty和Snoek 2021; Guo等2022）和<strong>混合方法</strong>（Su、Lin和Wu 2021; Chen等2022）。==编码器-解码器方法首先将输入的骨骼序列编码成潜在特征，然后在各种手工制作的前提任务的指导下解码潜在特征，例如骨骼重建（Zheng等2018）、骨骼着色预测（Yang等2021）和骨骼位移预测（Kim等2022）。对比学习方法通常将输入的骨骼序列增强为两个增强实例，并训练编码器使相同骨骼的实例具有更相似的表示，而不同骨骼的实例则不然。混合方法融合了编码器-编码器和对比学习的思想==其中，对比学习方法在近年来占据主导地位，因为它们具有简单的机制和出色的性能（Li等2021; Guo等2022）。<strong>第一个无监督SARL的对比学习方法由（Rao等2021）提出，它改编了最初设计用于无监督图像表示学习的MoCo（He等2020）来进行无监督骨骼表示学习</strong>。此后，通过建模骨骼的不确定性（Su等2021）、探索更多正样本对（Li等2021）和利用更好的骨骼特定增强（Thoker、Doughty和Snoek 2021）等方式提出了==一些改进的对比学习方法==。正如图1(a)所示，这些对比学习方法<strong>通常首先将骨骼序列表示为实例级特征，然后以整体的方式执行实例级对比</strong>。尽管对比学习方法已经表现出更好的性能，但这种整体的实例级对比可能不够优化，考虑到人体骨架天然具有分层结构。<strong>骨骼序列可以在时间上常被看作是整个骨骼（帧）的列表，或在空间上被看作是骨骼关节的列表</strong>。此外，帧或关节都是时间或空间域中的基本元素，可以构建为较大粒度的元素，例如帧剪辑或身体部分。</p><p>受人体骨骼分层性质的启发，我们提出了一种<strong>新的无监督SARL框架</strong>——Hierarchical Contrast（HiCo）。HiCo具有一个分层编码器网络，用于<strong>将骨骼编码为部分级、剪辑级、域级和实例级表示，并进行多层次对比</strong>，如图1(b)所示。具体而言，分层编码器网络有两个分支，分别对应时间域和空间域。对于时间分支，<strong>它将骨骼序列编码为不同时间粒度的多个剪辑级特征，通过从不同长度的剪辑中提取特征</strong>。同样，<strong>空间分支从不同尺寸的人体部位提取特征以获得部分级特征。<strong>此外，从时间和空间领域导出的</strong>剪辑级特征和部分级特征逐渐用于构建域级表示和实例级表示</strong>。另外，<strong>考虑到多层次表示，进行了四个级别的分层对比</strong>。**我们的假设是，这种分层对比与人体骨架的分层性质一致，并为无监督学习提供了更多关键的监督。**总之，本文的贡献如下。</p><p>• 我们提出了一种分层编码器网络，通过序列到序列（S2S）编码器和统一的下采样模块，将骨骼序列表示为来自时间和空间领域的不同粒度的多个特征向量。此外，我们的框架与S2S编码器正交，允许我们灵活应用最先进的S2S编码器。</p><p>• 基于人体骨骼的多级表示，我们提出了一种新的分层对比损失，用于无监督学习，在无监督SARL的情况下，它比实例级表示上的简单对比损失更有效。</p><p>• 在四个数据集上进行的广泛实验表明，我们的HiCo在两个下游任务中取得了无监督SARL的最新成果，其学到的表示具有良好的可迁移性。此外，我们还展示了我们的方法对于半监督的基于骨骼的动作识别同样有效。</p><h2 id="related">related</h2><p><strong>Hierarchical Modeling on Skeletons 骨架的层次化建模</strong></p><ol><li>There are also some previous works that modeled human skeletons hierarchically. 也有一些以前的研究对人类骨架进行了分层建模。</li><li>For instance, (Du, Wang, and Wang 2015; Cheng et al. 2021; Wang, Ni, and Yang 2020) modeled the spatial hierarchical structure with hand-crafted designs, but they did not consider the temporal hierarchical structure. 例如，（杜，王和王2015年；程等人2021年；王，倪和杨2020年）使用手工设计对空间层次结构进行了建模，但没有考虑时间层次结构。</li><li>(Chen et al. 2022) exploited the temporal hierarchical clues from the frame, clip, and video levels. （陈等人2022年）从帧、剪辑和视频级别利用了时间层次线索。</li><li>By contrast, we jointly model the temporal and spatial hierarchical structure, which can be readily implemented using our proposed more unified framework. 相比之下，我们联合建模了时间和空间的层次结构，这可以很容易地使用我们提出的更统一的框架来实现。</li><li>Besides, different from the previous models that present skeletons at the instance level, we present skeletons at the instance level, domain level, clip level, and part level. 此外，不同于以前将骨架呈现在实例级别的模型，我们在实例级别、域级别、剪辑级别和部分级别呈现骨架。</li></ol><p>总结： 本章介绍了以前的一些关于对人体骨架进行分层建模的研究，其中一些模型侧重于空间层次结构，而另一些侧重于时间层次结构。与以前的模型不同，本研究提出了一个更统一的框架，可以同时对时间和空间的层次结构进行建模，并将骨架呈现在不同的层次上，包括实例级别、域级别、剪辑级别和部分级别。</p><h2 id="method">method</h2><p>3.1 分层编码器网络 分层编码器网络由一个时间分支和一个空间分支组成，分别从时间域和空间域对输入骨骼序列进行编码。<strong>时间分支</strong>对骨骼序列进行多个时间粒度的编码，产生一个<strong>剪辑级别的表示</strong>。类似地，<strong>空间分支</strong>对骨骼序列进行多个空间粒度的编码，产生一个<strong>部分级别的表示</strong>。<strong>剪辑级别的表示和部分级别的表示都逐渐用于组装域级别和实例级别的表示。</strong></p><p>总结：这一部分描述了分层编码器网络的结构，它包括时间分支和空间分支，用于对骨骼序列进行编码。时间分支和空间分支分别处理骨骼序列的时间和空间信息，产生剪辑级别和部分级别的表示。这些不同粒度的表示逐渐组合以构建域级别和实例级别的表示。这个网络的设计充分考虑了人体骨架的分层性质，有助于提高无监督学习的性能。</p><h4 id="Clip-level-Representation"><strong>Clip-level Representation</strong>.</h4><p>裁剪级别表示。 Given a skeleton sequence X ∈ RT×J×3 that has T frames with J joints, and each joint consists of three spatial coordinates, 我们首先将它在以时间为主的领域中重塑为一个帧列表 Xc = {xc i }T i=1 ∈ RT×3J, where xc i 表示实际上是一个完整的人体骨架的第i帧。此外，我们使用帧嵌入将输入投影到C维密集特征空间，通过两个全连接层。形式上，从xc i 获得的投影特征如下： x c i = W2(σ(W1xc i + b1)) + b2， 其中W1 ∈ RC×3J和W2 ∈ RC×C是变换矩阵，b1 ∈ RC和b2 ∈ RC表示偏置向量，σ是ReLU激活函数。所有投影特征联合表示为Xc 1 = { xc i }T i=1 ∈ RT×C。为了方便参考，我们将这些特征称为初始帧表示。==由于初始帧特征是独立表示的，它们自然缺乏对于动作表示至关重要的时间依赖性。因此，我们建议使用多个时间粒度来建模它们的时间依赖性。我们首先生成不同时间粒度的剪辑，然后利用S2S编码器来建模它们的时间依赖性==</p><p><strong>总结：这一部分介绍了剪辑级别的表示方法。首先，输入的骨骼序列被转化为帧列表，并通过帧嵌入映射到一个C维的特征空间，形成初始帧表示。然后，=== 为了引入时间依赖性===，作者生成不同时间粒度的剪辑，并使用S2S编码器来建模它们的时间依赖性。这样，得到了包含不同时间粒度信息的剪辑级别表示。</strong></p><h4 id="Clip-Construction-剪辑构建。"><strong>Clip Construction. 剪辑构建。</strong></h4><p>In order to generate clips of different temporal granularities, we propose a Unified Downsampling Module (UDM), which sequentially merges multiple consecutive frame/clip features to obtain more coarse-grained clips. 为了生成不同时间粒度的剪辑，我们提出了一个统一的下采样模块 (UDM)，它依次合并多个连续的帧/剪辑特征，以获得更粗粒度的剪辑。</p><p>The structure of UDM is illustrated in Figure 2(b), which can be formally defined as: UDM(·) = MaxPool1D(LN(σ(Conv1D(·)))), 其中UDM的结构如图2(b)所示，可以正式定义为：UDM(·) = MaxPool1D(LN(σ(Conv1D(·)))),， 这里 Conv1D 表示一个具有核大小为5和步幅大小为1的1D卷积层，LN表示层归一化，而MaxPool1D是具有核大小2的1D最大池化操作。这里的1D卷积层用于收集附近的上下文信息，而最大池化则用于信息聚合。需要注意的是，UDM可以串联堆叠，从而导致粒度更粗的剪辑。此外，我们将帧视为粒度为1的单帧剪辑，以方便描述。借助UDM的帮助，给定粒度为n的剪辑，可以轻松获得更大粒度的剪辑，如下所示： Xc n+1 = UDM(Xc n)。通过连续使用UDM L-1 次，我们能够获得不同时间粒度的剪辑{Xc 1, Xc 2, . . . , Xc L}。</p><p>Summary: This section discusses the construction of clips at different temporal granularities. The Unified Downsampling Module (UDM) is introduced, which merges consecutive frame/clip features to create coarser-grained clips. UDM consists of a 1D convolution layer, Layer Norm (LN), and 1D max pooling, and it can be stacked in series to obtain clips of varying granularities. This approach allows for the generation of clips with different levels of temporal detail.</p><p>摘要：本节讨论不同时间粒度的剪辑的构造。 引入了统一下采样模块（UDM），该模块合并连续的帧/剪辑特征以创建更粗粒度的剪辑。  UDM 由 1D 卷积层、Layer Norm (LN) 和 1D max pooling 组成，可以串联堆叠以获得不同粒度的剪辑。 这种方法允许生成具有不同时间细节级别的剪辑。</p><h4 id="Temporal-Dependency-Modeling-时间依赖性建模。"><strong>Temporal Dependency Modeling. 时间依赖性建模。</strong></h4><p>For each granularity of clips, we further model the temporal dependency by an S2S encoder. 具体来说，对于每个剪辑的粒度Xc n，我们将其馈送到一个S2S编码器，例如GRU，然后经过一个时间最大池化层（TMP）进行特征聚合，粒度为n的剪辑级特征vc n如下获取： vc n = TMP(S2S(Xc n))。 (4) 这里，使用了S2S编码器所有时间步的输出。因此，给定L个粒度的{Xc 1,Xc 2, . . . ,Xc L}，我们能够获得包含L个特征向量的最终剪辑级表示Vc如下： Vc = {vc 1, vc 2, . . . , vc L}。 (5) 需要注意的是，我们的网络与S2S编码器正交，理论上可以使用任何S2S编码器。在我们的实现中，我们尝试了GRU、LSTM和Transformer。</p><p>Summary: In this section, temporal dependency modeling is discussed. For each granularity of clips, a temporal dependency model is applied using an S2S encoder, which can be GRU, LSTM, or Transformer. This modeling captures the temporal relationships within the clips and results in clip-level features that are aggregated using a temporal max pooling layer (TMP). These features are then used to construct the final clip-level representation.</p><p>摘要：本节将讨论时态依赖关系建模。对于剪辑的每个粒度，使用 S2S 编码器（可以是 GRU、LSTM 或转换器）应用时态依赖关系模型。此建模可捕获剪辑内的时间关系，并生成使用时间最大池化图层 （TMP） 聚合的剪辑级要素。然后，这些特征用于构造最终的剪辑级表示。</p><h4 id="Part-level-Representation"><strong>Part-level Representation.</strong></h4><p>Obtaining the part-level representation is similar to obtaining the clip-level representation, so we mainly specify choices that are unique at the part-level. 获得部分级表示与获得剪辑级表示类似，因此我们主要说明在部分级别独特的选择。↳</p><p>Given a skeleton sequence X ∈ RT×J×3, here we reshape it in the space-m</p><p>Similarly, a joint embedding is employed to obtain the initial joint representation Xp 1 ∈ RJ×C. 类似地，使用关节嵌入来获取初始关节表示Xp 1 ∈ RJ×C。</p><p>Additionally, we generate human parts of different spatial granularities by UDM. 此外，我们通过UDM生成不同空间粒度的人体部分。</p><p>The UDM here is employed to merge nearby joints/parts to obtain parts of larger sizes. 这里的UDM被用来合并附近的关节/部分，以获取更大尺寸的部分。</p><p>Similarly, by jointly employing UDM L−1 times in series, we are able to obtain parts of different spatial granularities {Xp 1 ,Xp 2 , . . . ,Xp L}. 类似地，通过连续共同使用UDM L−1次，我们可以获得不同空间粒度的部分{Xp 1 ,Xp 2 , . . . ,Xp L}。</p><p>Furthermore, for each granularity, an S2S encoder with a max pooling layer is used to capture the spatial dependency. 此外，对于每个粒度，使用带有最大池化层的S2S编码器来捕捉空间依赖性。</p><p>Consequently, we are able to obtain the final part-level representation containing clues of multiple spatial granularities as Vp = {vp 1, vp 2, . . . , vp L}. 因此，我们能够获得包含多个空间粒度线索的最终部分级表示Vp = {vp 1, vp 2, . . . , vp L}。</p><p>Summary: In the Part-level Representation section, the process of obtaining part-level representation is explained. It involves reshaping the skeleton sequence to focus on joints, using a joint embedding for initial representation, and employing a Unified Downsampling Module (UDM) to generate parts of different spatial granularities. Multiple spatial granularities of parts are obtained, and for each granularity, an S2S encoder with a max pooling layer is used to capture spatial dependencies. This results in a final part-level representation that incorporates information from various spatial granularities.</p><p>摘要：在部件级表示部分中，解释了获取部件级表示的过程。 它涉及重塑骨架序列以关注关节，使用关节嵌入进行初始表示，并采用统一下采样模块（UDM）生成不同空间粒度的部分。 获得零件的多个空间粒度，并且对于每个粒度，具有最大池化层的S2S编码器用于捕获空间依赖性。 这会产生最终的部件级表示，其中包含来自各种空间粒度的信息</p><p>领域级别和实例级别表示 领域级别和实例级别表示。到目前为止，我们已经具有多个<strong>时间粒度的剪辑级别</strong>表示Vt以及多个<strong>空间粒度的部分级别表示</strong>Vs。我们进一步逐步将它们组合起来，以获得领域级别表示和实例级别表示。 对于领域级别表示，我们融合所有L个剪辑级别表示的特征向量，以得出时间域表示vt，并融合所有相应的部分级别表示的特征向量，以得出空间域表示vs： vt = F(Vt) = F(vc 1, vc 2, . . . , vc L), vs = F(Vp) = F(vp 1, vp 2, . . . , vp L), (6) 其中F(·)表示对多个特征向量的融合运算符。在我们的实现中，使用了串联操作。 此外，时间域和空间域表示被联合组合成实例级别表示vi： vi = F(vt, vs). (7) 值得指出的是，尽管已经获得了多级表示，但最终骨架序列表示仅使用实例级别表示，而其他表示用于分层对比，将在下一部分中描述。</p><p>总结：在领域级别和实例级别表示部分，我们将多个时间粒度的剪辑级别表示（Vt）和多个空间粒度的部分级别表示（Vs）逐步组合，以获得领域级别和实例级别表示。领域级别表示包括时间域（vt）和空间域（vs）表示。这些领域级别表示然后联合组合成实例级别表示（vi）。值得注意的是，对于最终的骨架序列表示，仅使用实例级别表示，而其他表示用于分层对比，将在接下来的部分中解释。</p><h4 id="Hierarchical-Contrast"><strong>Hierarchical Contrast</strong></h4><p>分层对比 不同于以前的对比学习方法，仅在实例级特征上进行对比，我们提出了一种分层对比方法，该方法在实例级、领域级、剪辑级和部分级特征上进行对比。对于对比的方式，我们采用了MoCo的方式（He等人，2020年），即查询编码器和关键编码器与动态字典队列和移动平均更新机制一起使用。此外，根据（Chen等人，2020年）的方法，我们在对比之前使用了一个两层的多层感知器进行特征投影。为了简洁描述，我们使用了与原始特征相同的投影特征的符号。</p><p>总结：在分层对比部分，作者介绍了一种不同于以前的对比学习方法，即分层对比方法，该方法在实例级、领域级、剪辑级和部分级特征上进行对比。他们采用了MoCo的方式，并使用多层感知器进行特征投影。这种方法旨在提高对比学习的效果，以更好地捕捉骨架序列的层次结构特征。</p><h4 id="Instance-level-Contrast">Instance-level Contrast</h4><p>根据（Rao等人，2021年）的方法，我们使用了噪声对比估计损失InfoNCE（Van den Oord等人，2018年）进行对比。在实例级别上，对比损失计算如下： L实例 = −log exp(vi · ˆvi/τ) exp(vi · ˆvi/τ) +  mi j∈Mi exp(vi · mi j/τ) ， (8) 其中τ是温度超参数，mi j表示来自先进先出队列Mi的前一次投影实例级别特征的第j个负样本。</p><p>总结：在实例级对比部分，作者采用了InfoNCE损失函数，用于衡量正样本和负样本之间的对比。这有助于训练模型，使其能够更好地区分不同的实例级特征。</p><p>无监督方法在各种任务中越来越受到关注（Yang等人，2018年；Deng等人，2019年；Gao等人，2020年；Liu等人，2022年），因为它具有在没有人工标注的情况下进行训练的良好特性。现有的无监督骨骼动作表示学习（SARL）方法可以大致分为三类：编码-解码方法、对比学习方法和混合方法。</p><h4 id="Hierarchical-Modeling-on-Skeletons">Hierarchical Modeling on Skeletons</h4><ol><li>There are also some previous works that modeled human skeletons hierarchically. 也有一些以前的研究对人类骨架进行了分层建模。</li><li>For instance, (Du, Wang, and Wang 2015; Cheng et al. 2021; Wang, Ni, and Yang 2020) modeled the spatial hierarchical structure with hand-crafted designs, but they did not consider the temporal hierarchical structure. 例如，（杜，王和王2015年；程等人2021年；王，倪和杨2020年）使用手工设计对空间层次结构进行了建模，但没有考虑时间层次结构。</li><li>(Chen et al. 2022) exploited the temporal hierarchical clues from the frame, clip, and video levels. （陈等人2022年）从帧、剪辑和视频级别利用了时间层次线索。</li><li>By contrast, we jointly model the temporal and spatial hierarchical structure, which can be readily implemented using our proposed more unified framework. 相比之下，我们联合建模了时间和空间的层次结构，这可以很容易地使用我们提出的更统一的框架来实现。</li><li>Besides, different from the previous models that present skeletons at the instance level, we present skeletons at the instance level, domain level, clip level, and part level. 此外，不同于以前将骨架呈现在实例级别的模型，我们在实例级别、域级别、剪辑级别和部分级别呈现骨架。</li></ol><p>总结： 本章介绍了以前的一些关于对人体骨架进行分层建模的研究，其中一些模型侧重于空间层次结构，而另一些侧重于时间层次结构。与以前的模型不同，本研究提出了一个更统一的框架，可以同时对时间和空间的层次结构进行建模，并将骨架呈现在不同的层次上，包括实例级别、域级别、剪辑级别和部分级别。</p><h1>Multilevel Spatial–Temporal Excited Graph Network for Skeleton-Based Action Recognition</h1><h2 id="1-Introduction">1 Introduction</h2><p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节 INTRODUCTION：</p><p>Although it improves the capacity of modeling the joint<br>relationships, the pipeline of GCN+TCN still suffers from<br>several problems. From the perspective of the GCN, the<br>physical skeleton graph is predefined according to the intrinsic<br>connectivity of the human body and thus lacks flexibility in<br>network training. It cannot handle the various joint relations<br>of different samples, especially when they are performing<br>different actions. For example, as Figure 1 shows, the relation<br>between ‘finger’ and ‘head’ is important for distinguishing<br>between ‘wave hand’ and ‘touch head’. However, it is difficult<br>for the fixed graph to explore the dependency between these<br>two parts. Moreover, most current methods [11], [12], [17]<br>utilize only joint-level relations to generate spatial features<br>but ignore high-level cues such as part-level and body-<br>level relations. Notice that such high-level interactions often</p><p>carry crucial information for identifying complicated actions.<br>For example, the motion of ‘jump up’ in Figure 1 is typically<br>interpreted based on the interplay of arms and legs at a<br>relatively high level, rather than the detailed locations of<br>fingers and toes.</p><p>尽管它增强了对关节关系建模的能力，但GCN+TCN的管道仍然存在一些问题。从GCN的角度看，<strong>物理骨骼图是根据人体内在连接性预定义的，因此在网络训练中缺乏灵活性。<strong>它</strong>不能处理不同样本的各种关节关系</strong>，尤其是<strong>当它们执行不同动作时</strong>。例如，正如图1所示，'手指’和’头部’之间的关系对于区分’挥手’和’触摸头’非常重要。然而，<strong>对于固定的图来探索这两部分之间的依赖关系是困难的。<strong>此外，大多数当前方法[11]、[12]、[17]仅</strong>利用关节级别的关系来生成空间特征</strong>，但忽略了<strong>高级别的线索，如</strong>部分级别和整体级别的关系。请注意，这种高级交互通常携带了识别复杂动作所需的重要信息。例如，图1中的’跳起’动作通常是基于相对较高级别上手臂和腿的相互作用来解释，而不是基于手指和脚趾的详细位置。</p><p><strong>总结：</strong></p><p>这段介绍指出了传统的GCN+TCN方法的局限性，包括固定的骨骼图和无法适应不同样本的不同关节关系。它还提到了目前的方法通常只使用关节级别的关系来生成空间特征，而忽略了高级别的部分和整体关系，而这些高级别的关系通常携带着识别复杂动作所需的重要信息。</p><p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节 INTRODUCTION： HUMAN action recognition becomes increasingly impor- tant in various application scenarios, such as video surveillance and human-computer interaction [1], [2], [3]. Many researchers exploit image-based methods to learn unique spatial-temporal contexts inherited in videos. Although great advances have been made, the RGB-based representations learned by these methods are not robust against changes in body scales, camera viewpoints, and background interference. In contrast to the RGB image, skeleton data only contain a time series of 2D or 3D positions of multiple human joints, providing extremely abstract and well-structured information. Moreover, this information can be easily acquired in many ways, such as human pose estimation methods [4] and mobile sensors such as Kinect [5], successfully meeting the requirement of real-time action recognition. The development of skeleton-based action recognition in deep learning can be roughly divided into two phases. The first type employs a Recurrent Neural Network (RNN) [6], [7], [8] or Convolutional Neural Network (CNN) [9], [10] to investigate short-term and long-term temporal dynamics. These networks treat skeleton data as joint-coordinate sequences or pseudoimages, which are then input into RNNs or CNNs to predict the action category. While effective to a certain extent, these approaches overlook the structural pattern of the human body as well as the spatial interactions between adjacent joints. The skeleton is naturally a graph structure in non-Euclidean space, with joints serving as nodes and their connections serving as edges. For this reason, recent works prefer to build skeleton graphs based on the physical links of human anatomy [11], [12], [13], [14], [15], [16]. The relations between human joints are modeled by graphs, and a ‘graph convolution network (GCN) + temporal convolution network (TCN)’ pipeline is often used to extract spatiotemporal features. Although it improves the capacity of modeling the joint relationships, the pipeline of GCN+TCN still suffers from several problems. From the perspective of the GCN, the physical skeleton graph is predefined according to the intrinsic connectivity of the human body and thus lacks flexibility in network training. It cannot handle the various joint relations of different samples, especially when they are performing different actions. For example, as Figure 1 shows, the relation between ‘finger’ and ‘head’ is important for distinguishing between ‘wave hand’ and ‘touch head’. However, it is difficult for the fixed graph to explore the dependency between these two parts. Moreover, most current methods [11], [12], [17] utilize only joint-level relations to generate spatial features but ignore high-level cues such as part-level and body- level relations. Notice that such high-level interactions often carry crucial information for identifying complicated actions. For example, the motion of ‘jump up’ in Figure 1 is typically interpreted based on the interplay of arms and legs at a relatively high level, rather than the detailed locations of fingers and toes.</p><p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节(不用展示英文原文) INTRODUCTION：  In temporal modeling, the action is described as a spatial human body moving along the temporal dimension. Since the motion changes are often concentrated in a few key areas, TCN, which simply uses temporal convolution, may not be able to fully explore the locations with significant changes. Some recent methods [16], [18] attempt to alleviate this issue by utilizing attention-based multistream models or 3D graph convolution. Although they improve the recognition accuracy, the high computational cost severely limits their real-world applications. To address the above limitations, we design a multilevel spatial-temporal excited graph network (ML-STGNet), which contains a multilevel GCN (ML-GCN) network, a spatial data- driven excitation (SDE) module, a temporal motion excitation (TME) module and a simplified multiscale TCN (MS-TCN) network. Note that when observing a comprehensive action, the human visual system usually first identifies its generality and then performs fine-grained recognition based on the personality of the action. To mimic this process, we decouple the learning of the skeleton graph into two modules, ML- GCN and SDE, which correspond to the general graph and individual graph, respectively. Specifically, ML-GCN leverages hierarchical graphs to represent both low-level joint linkages and high-level body interactions. These graphs are initialized based on the topological priors of the human body and have adaptive, trainable parameters.</p><p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节(不用展示英文原文) INTRODUCTION：</p><p>SDE further leverages a transformer-like structure to learn exclusive data- dependent topologies for different samples. Different from the self-attention mechanism, SDE fully exploits positional information, which is critical to capture spatial structures in vision tasks. Compared with the previous methods, this decoupling method is more flexible in graph construction and more adaptive to various samples. Furthermore, TME is also proposed to capture salient motion information by applying the concept of temporal difference in optical flow [19] into a systematic and efficient temporal module. Instead of treating the motion velocities as another input modality [20], our module could integrate the motion modeling into the whole feature learning process. Concretely, the feature-level motion difference is calculated between adjacent frames. These motion features are then utilized to produce weights for exciting the motion-sensitive features. In this way, the network is forced to enhance the informative features that capture salient information. Based on these features, we further leverage a simplified MS-TCN [16] to enrich the representation capacity of temporal features. To verify the effectiveness of ML-STGNet, we report experimental results on four popular skeleton-based action recognition datasets, including NTU-RGB+D 120 [21], NTU-RGB+D 60 [22], Kinetics Skeleton 400 [23] and Toyota Smarthome [24]. The experimental results show that ML-STGNet outperforms most state-of-the-art methods in terms of recognition accuracy. The main contributions of this paper are as follows: 1. We propose a novel ML-STGNet for skeleton-based action recognition, which attempts to exploit the comprehensive joint relations in various samples and enhance the motion dynamics. 2. We decouple the learning of the skeleton graph into the general and individual parts. ML-GCN is proposed to model the general graph, which is responsible for capturing the hierarchical joint relations in complicated action. SDE is proposed to model the individual graph, which is responsible for exploring the topological relations among skeleton joints of different samples. 3. We design an efficient temporal module TME to select the most salient information at the feature level. This module is easy to implement and facilitates temporal modeling. 4. Experimental results show that the proposed ML-STGNet achieves impressive performance on popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, Kinetics Skeleton 400 and Toyota Smarthome.</p><p>以下是对Introduction部分的逐句翻译和总结：</p><p><strong>Introduction引言：</strong></p><p><strong>In temporal modeling, the action is described as a spatial human body moving along the temporal dimension. Since the motion changes are often concentrated in a few key areas, TCN, which simply uses temporal convolution, may not be able to fully explore the locations with significant changes.</strong> 在时域建模中，动作被描述为在时间维度上移动的空间人体。<strong>由于运动变化通常集中在一些关键区域</strong>，==仅使用时间卷积的TCN可能无法充分探索具有显著变化的位置==。</p><p><strong>Some recent methods [16], [18] attempt to alleviate this issue by utilizing attention-based multistream models or 3D graph convolution. Although they improve the recognition accuracy, the high computational cost severely limits their real-world applications.</strong> 一些最近的方法<strong>尝试通过使用基于注意力的多流模型或3D图卷积来缓解这个问题</strong>。尽管它们提高了识别精度，但高昂的计算成本严重限制了它们在现实世界中的应用。</p><p><strong>To address the above limitations, we design a multilevel spatial-temporal excited graph network (ML-STGNet), which contains a multilevel GCN (ML-GCN) network, a spatial data-driven excitation (SDE) module, a temporal motion excitation (TME) module and a simplified multiscale TCN (MS-TCN) network.</strong> 为了解决上述限制，==我们设计了一个<strong>多级时空激发图网络</strong>（ML-STGNet），其中包含多级GCN（ML-GCN）网络、空间数据驱动激发（SDE）模块、时间运动激发（TME）模块和简化的多尺度TCN（MS-TCN）网络==</p><p><strong>Note that when observing a comprehensive action, the human visual system usually first identifies its generality and then performs fine-grained recognition based on the personality of the action. To mimic this process, we decouple the learning of the skeleton graph into two modules, ML-GCN and SDE, which correspond to the general graph and individual graph, respectively.</strong> 请注意，在观察综合动作时，人类视觉系统通常首<strong>先识别其一般性</strong>，然后根据动作的<strong>个性进行细粒度识别</strong>。为模仿这个过程，我们将骨骼图的学习解耦为两个模块，ML-GCN和SDE，分别对应于一般图和个体图。</p><p><strong>Specifically, ML-GCN leverages hierarchical graphs to represent both low-level joint linkages and high-level body interactions. These graphs are initialized based on the topological priors of the human body and have adaptive, trainable parameters. SDE further leverages a transformer-like structure to learn exclusive data-dependent topologies for different samples.</strong> 具体来说，ML-GCN利用分层图来表示低级关节连接和高级身体相互作用。这些图是基于人体的拓扑先验初始化的，并具有自适应的可训练参数。SDE进一步利用类似Transformer的结构来学习不同样本的独占数据依赖拓扑结构。</p><p><strong>Different from the self-attention mechanism, SDE fully exploits positional information, which is critical to capture spatial structures in vision tasks. Compared with the previous methods, this decoupling method is more flexible in graph construction and more adaptive to various samples.</strong> 与自注意机制不同，<strong>SDE充分利用位置信息</strong>，这对于捕捉视觉任务中的空间结构至关重要。<strong>与以前的方法相比，这种解耦方法在图构建方面更加灵活，对各种样本更具适应性。</strong></p><p><strong>Furthermore, TME is also proposed to capture salient motion information by applying the concept of temporal difference in optical flow [19] into a systematic and efficient temporal module. Instead of treating the motion velocities as another input modality [20], our module could integrate the motion modeling into the whole feature learning process.</strong> 此外，还提出了TME，通过<strong>将光流中的时间差概念</strong>应用到<strong>系统化和高效的时间模块中来捕获显著的运动信息</strong>。与将运动速度视为另一输入模态的方法不同，我们的模块可以<strong>将运动建模整合到整个特征学习过程中</strong>。</p><p><strong>Concretely, the feature-level motion difference is calculated between adjacent frames. These motion features are then utilized to produce weights for exciting the motion-sensitive features. In this way, the network is forced to enhance the informative features that capture salient information. Based on these features, we further leverage a simplified MS-TCN [16] to enrich the representation capacity of temporal features.</strong> 具体来说，<strong>计算相邻帧之间的特征级运动差异</strong>。然后<strong>利用这些运动特征生成权重来激发运动敏感特征</strong>。通过这种方式，<strong>网络被迫增强捕捉显著信息的信息特征</strong>。基于这些特征，我们进一步利用了<strong>简化的MS-TCN来丰富时间特征的表示能力</strong>。</p><p><strong>To verify the effectiveness of ML-STGNet, we report experimental results on four popular skeleton-based action recognition datasets, including NTU-RGB+D 120 [21], NTU-RGB+D 60 [22], Kinetics Skeleton 400 [23] and Toyota Smarthome [24]. The experimental results show that ML-STGNet outperforms most state-of-the-art methods in terms of recognition accuracy.</strong> 为了验证ML-STGNet的有效性，我们报告了在四个常见的基于骨骼的动作识别数据集上的实验结果，包括NTU-RGB+D 120、NTU-RGB+D 60、Kinetics Skeleton 400和Toyota Smarthome。实验结果表明，ML-STGNet在识别准确性方面优于大多数最先进的方法。</p><p><strong>The main contributions of this paper are as follows:</strong> 本文的主要贡献如下：</p><ol><li>We propose a novel ML-STGNet for skeleton-based action recognition, which attempts to exploit the comprehensive joint relations in various samples and enhance the motion dynamics.</li><li>我们提出了一种新颖的ML-STGNet，用于基于骨骼的动作识别，旨在<strong>利用各种样本中的全面关节关系并增强运动动态性</strong>。</li><li>We decouple the learning of the skeleton graph into the general and individual parts. ML-GCN is proposed to model the general graph, which is responsible for capturing the hierarchical joint relations in complicated action. SDE is proposed to model the individual graph, which is responsible for exploring the topological relations among skeleton joints of different samples.</li><li>我们将骨骼图的学习<strong>解耦为一般和个体部分</strong>。ML-GCN被用来模拟通用图，负责捕捉复杂动作中的分层关节关系。SDE被用来模拟个体图，负责探索不同样本的骨骼关节之间的拓扑关系。</li><li>We design an efficient temporal module TME to select the most salient information at the feature level. This module is easy to implement and facilitates temporal modeling.</li><li>我们设计了一个<strong>高效的时间模块TME</strong>，以在特征级别选择最显著的信息。该模块易于实施，并有助于时间建模。</li><li>Experimental results show that the proposed ML-STGNet achieves impressive performance on popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, Kinetics Skeleton 400 and Toyota Smarthome.</li><li>实验结果表明，所提出的ML-STGNet在流行数据集（NTU-RGB+D 60、NTU-RGB+D 120、Kinetics Skeleton 400和Toyota Smarthome）上取得了令人印象深刻的性能。</li></ol><h2 id="2-related-work">2.related work</h2><h3 id="A-Skeleton-Based-Action-Recognition">A  Skeleton-Based Action Recognition</h3><p>章节 基于骨架的动作识别 针对骨架数据的传统动作识别方法通常从骨架序列中提取手工设计的特征[25]，[26]，[27]。然而，它们主要依赖于关节之间的旋转或平移，并且特征设计复杂。最近，随着深度学习的发展，基于神经网络的方法在基于骨架的动作识别中展现出强大的能力，可以根据网络架构归纳为三类：基于RNN的[6]，[7]，[8]，基于CNN的[9]，[10]，[28]，以及基于GCN的[11]，[12]，[13]，[14]，[15]，[16]，[18]，[29]，[30]，[31]，[32]。 基于RNN的方法通常将骨架数据建模为矢量序列，其中每个矢量代表人体关节的坐标。杜等人[6]将人体骨架划分为不同部分，然后将它们分别馈送到分层RNN中。张等人[7]设计了一个自适应RNN架构，使网络能够确定最佳视点。 基于CNN的方法通常将骨架转化为伪图像，并利用CNN将图像分类为动作类别。李等人[9]设计了一个新的骨架变换模块，并将关节坐标以及骨架运动馈送到一个7层CNN中，以进行双流预测。尽管CNN比RNN更受欢迎且更容易训练，但它们都无法描述相关关节之间的空间依赖性，因为骨架数据自然表示图拓扑而不是基于网格的图像或序列。</p><p>【</p><p>受益于图卷积在结构化数据建模方面的强大能力，Yan等人首次将图卷积网络（GCN）引入骨架动作识别中，提出ST-GCN，以同时建模关节的空间和时间关系。在这之后，2s-AGCN结合了固定物理图和可学习掩码，自适应地学习不同骨架样本的拓扑结构。SGN则将高层语义信息（关节类型和帧索引）作为网络输入的一部分，以提高特征学习能力。而MS-G3D进一步提出了一个统一的多尺度时空模块，结合多邻接GCN和MS-TCN，以利用时空中的直接信息流。然而，上述方法多多少少存在以下问题：它们可能无法灵活有效地学习骨架图结构，无法建模高层身体部位之间的连接，以及可能存在昂贵的计算成本。为了解决这些问题，我们首先正式将骨架图的学习分为两个部分：通用部分和个体部分。我们使用ML-GCN在通用图中捕获复杂动作中的分层关节关系，并使用类似于变换器的结构在个体图中学习独特的数据相关拓扑结构。这种实现方式模块化且即插即用，易于构建且不需要大量参数。</p><p>】</p><p>该章节讨论了基于骨架的动作识别方法。传统方法通常从骨架序列中提取手工设计的特征，但这些方法主要依赖于关节之间的旋转或平移，存在复杂的特征设计。近年来，深度学习方法在骨架动作识别中取得了显著进展，主要分为基于RNN、CNN和GCN的三种网络架构。</p><ul><li>基于RNN的方法将骨架数据视为矢量序列，每个矢量代表关节的坐标。这些方法通常使用分层RNN或自适应RNN架构。</li><li>基于CNN的方法将骨架数据转化为伪图像，然后使用CNN分类器。尽管CNN更容易训练，但它们难以描述关节之间的空间依赖性。</li><li>基于GCN的方法利用图卷积网络的能力，首次引入GCN用于骨架动作识别，以同时建模关节的空间和时间关系。后续的研究进一步改进了GCN的性能，例如2s-AGCN、SGN和MS-G3D。但这些方法存在某些缺点，如难以灵活有效地学习骨架图、难以建模高级身体部件之间的连接以及计算成本较高。</li></ul><p>为解决这些问题，该研究采用两部分模块化方法来学习骨架图：通用部分和个体部分。通用部分使用ML-GCN捕获复杂动作中的层次关系，而个体部分使用类似Transformer的结构为不同样本学习独立的数据相关拓扑结构。这种实现方式简单且不需要大量参数，使模型更具灵活性和效率。</p><h3 id="B-Attention-Mechanism-注意力机制">B. Attention Mechanism 注意力机制</h3><p>这一章节讨论了注意力机制。注意力机制可视为一种基于输入图像特征的动态权重调整过程，在各种视觉任务中取得了巨大成功。SENet首次提出了通道注意力，提出了一个压缩激发块来收集通道级全局信息。随着自注意力在自然语言处理中的流行，Wang等人提出了非局部操作以捕获像素对之间的长程空间依赖关系。在骨架动作识别领域，AGC-LSTM提出了增强型注意力图卷积LSTM来有效捕获有区别的时空特征。Chen等人设计了交叉头部注意力机制，使模型能够聚焦于关键运动信息。然而，纯粹的自注意力结构缺乏位置信息，而在视觉任务中描述空间结构非常重要。最近，Dosovitskiy等人通过引入视觉变换器（ViT）来解决这个问题。受到ViT的启发，该研究设计了SDE来模拟个体图。此外，还将查询位置编码的思想扩展到所有键-值对，以获取更精确的位置信息。</p><p>总结该章节： 该章节讨论了注意力机制在骨架动作识别中的应用。传统的注意力机制已在视觉任务和自然语言处理中取得成功。针对骨架动作识别，之前的研究已提出各种不同的注意力机制，如通道注意力、自注意力、交叉头部注意力等。然而，这些方法在描述空间结构时缺乏位置信息。因此，该研究受到视觉变换器（ViT）的启发，设计了SDE模型，并引入了查询位置编码，以提供更精确的位置信息。</p><h3 id="C-Temporal-Difference-Modeling">C. Temporal Difference Modeling</h3><p>时间差异建模 时间差异操作在动作识别中用于提取运动信息，例如RGB差异和特征差异。前者计算输入级别图像之间的差异以提供运动信息。ResGCN将这一思想扩展到骨架领域，并设计了一个多分支架构，分别从关节、速度和骨骼模态接收输入。然而，这些方法的主要缺点在于它们将RGB差异仅视为另一种视频模态，并训练一个独立的网络将其与主流网络融合。后者采用差异操作进行网络设计。例如，STM提出了一个逐通道的运动模块来编码运动特征。TEA设计了一个运动激发模块，以促使网络发现信息丰富的时间瞬间。受到TEA的启发，我们将特征差异操作扩展到基于骨架的动作识别中，并设计TME来激发显著的运动敏感特征。</p><p>总结：本节介绍了在动作识别中的时间差异建模。时间差异操作可用于提取运动信息，包括RGB差异和特征差异。先前的方法将RGB差异视为另一种视频模态，与主流网络分开训练。而后者使用差异操作来设计网络，例如通过逐通道运动模块或运动激发模块来编码运动特征。本文受到后者的启发，将特征差异操作引入基于骨架的动作识别中，设计了TME来激发显著的运动敏感特征。</p><h2 id="III-MULTILEVEL-SPATIAL-TEMPORAL-EXCITED-GRAPH-NETWORK">III. MULTILEVEL SPATIAL-TEMPORAL EXCITED GRAPH NETWORK</h2><p>跟随“GCN+TCN”流程，我们提出的ML-STGNet是GC-Blocks的集成，每个块包含四个组件：<strong>多级GCN（ML-GCN）网络，空间数据驱动激发（SDE）模块，时间运动激发（TME）模块和简化的多尺度TCN（MS-TCN）网络。</strong></p><p>首先，我们将<strong>骨架图的学习</strong>分为==通用部分和个体部分==，<strong>以模拟人类视觉系统</strong>。<strong>ML-GCN被引入作为通用图</strong>，用于探索关节之间复杂动作的层次关联。如图2所示，我们基于人体拓扑结构启发式地设计了三个层次的图，即<strong>低级关节图，中级部位图和高级人体图</strong>。为了促进网络的灵活性，这些图都有可学习的参数。<strong>SDE进一步利用类似变换器的结构充分探索不同样本的个体图</strong>。与普通的自注意力或变换器结构相比，<strong>SDE具有更精确的位置信息</strong>。此外，我们<strong>引入了TME来突出显示对运动敏感的特征，这易于实现，并有助于MS-TCN中的时间建模</strong>。总体流程如图3所示。在接下来的章节中，我们介绍一些相关符号。然后，我们提供了ML-STGNet框架的详细技术描述。</p><p>总结：本章介绍了ML-STGNet的结构，它采用了多级GCN网络、空间数据驱动激发模块、时间运动激发模块和多尺度TCN网络。ML-GCN用于捕捉复杂动作之间的关联，SDE用于探索不同样本的个体图，TME用于突出显示运动敏感特征。整体框架分为通用部分和个体部分，以模拟人类视觉系统。这一结构的设计旨在提高网络的灵活性和性能。</p><h3 id="A-Preliminaries">A. Preliminaries</h3><ol><li>符号：人体骨架可以表示为一个图拓扑结构。我们可以将其形式化为G = (V, E)，其中V = {v1, v2, . . . , vN} 是表示关节的N个节点的集合。E以邻接矩阵A ∈ RN×N的形式给出了边的代表性描述，其中元素ai j取1或0的值，指示ai和aj是否相邻。给定一个骨架序列，我们首先计算可以表示为X ∈ RC×T×N的节点特征X = {xt,n|1 ≤ t ≤ T, 1 ≤ n ≤ N; n, t ∈ Z}，其中xt,n = Xt,n表示帧t上节点vn的C维特征。</li><li>图卷积网络：每个i-th关节在t-th帧上的层次GCN的更新规则可以写成： fout (vti ) = X vt j ∈B(vti ) 1 Z[l(vt j )] fin(vt j )w[l(vt j )]， 其中fin和fout代表特征映射。B(vti ) = {vti |d(vti , vt j ) ≤ 1}表示vti的1距离邻居集。<strong>l(·)是为B(vti)中的每个顶点分配从1到K的标签的标签函数</strong>。按照ST-GCN [11]中提出的空间配置分割策略，K被设置为3，将B(vti )分为3个子集：顶点本身、向心子集和离心子集，如图2（a）所示。w(·)是可学习的1×1卷积的权重。Z是归一化项。</li></ol><p>为了便于计算，方程1可以转换成矩阵计算： Xt = σ Kv X k AkXtWk ， 其中Ak = 3 −1 2 k Ak c k3 −1 2 k，B(vti )的每个k-th分区表示为cAk，3i</p><p><strong>总结：</strong> 本章介绍了关于骨架的预备知识，包括符号表示和图卷积网络。人体骨架被表示为一个图，其中节点表示关节，边表示它们之间的连接。图卷积网络用于对骨架数据进行特征提取，其中每个关节的特征是通过与邻近关节的特征加权组合得到的。矩阵形式的计算更有利于处理这些图卷积操作。</p><h3 id="B-Multilevel-Graph-Convolution">B. Multilevel Graph Convolution</h3><p>通用图表捕获了包含在动作中的常见模式。如图2所示，为了充分挖掘综合性动作，我们定义了三个分层图Ajoint、Apart和Abody。在关节级别图中，Ajointk与ST-GCN [11]中的Ak完全相同。在部位级别图中，我们根据人体拓扑结构将关节手动分类为<strong>10个类别</strong>。然后，我们定义Apart = 3 −1 2 partRpart[ApartRT part3 −1 2 part，其中Rpart是一个25×10的关系矩阵，<strong>描述了新的部位级别图中每个关节属于哪个类别</strong>（如果第i个关节属于第j个部位，则Ri j设置为1；否则，Ri j设置为0），RT part是它的转置。[Apart是一个10×10的部位级别邻接矩阵，显示了不同部位之间是否存在连接。3ii part = P j Ai j part是一个对角线矩阵。类似于部位级别图，我们将关节分成5个不同的身体部分，并将身体级别图定义为Abody = 3 −1 2 bodyRbody\AbodyRT body3 −1 2 body。在训练过程中，所有这些图都会自适应调整以捕捉灵活的多级身体关系。 然后，如图4（a）所示，给定X ∈ RC×T×N，我们首先基于上述图计算图特征： Xjoint = Kv X k Ajointk XWk， Xpart = ApartXWpart， Xbody = AbodyXWbody， (3) 我们选择1×1卷积层作为嵌入函数，其中Wk、Wpart和Wbody∈ RCout×Cin。为简化起见，我们假设Cout = Cin = C。</p><p><strong>总结： 本章介绍了一种通用图表，该图表捕获了动作中的常见模式。作者通过定义关节级别、部位级别和身体级别的分层图来捕获多级身体关系。在训练过程中，这些图会自适应调整以更好地适应多级关系。作者还介绍了如何计算图特征，这些特征用于后续的分析。</strong></p><h3 id="C-Spatial-Data-Driven-Excitation">C. Spatial Data-Driven Excitation</h3><p>个体图<strong>以数据驱动的方式处理不同样本的多样化关节关系</strong>。如图4（b）所示，我们设计了一个类似于Transformer的结构来利用这些信息。让我们<strong>考虑编码后的通用图特征X ∈ RCin×T×N</strong>。首先，<strong>为了减少计算成本并更加强调空间信息</strong>，我们使用<strong>时间池化来压缩时间维度</strong>：Xs ∈ RCin×1×N。然后，我们从自注意力开始，介绍了如何构建我们的SDE模块。在自注意力中，输出Y ∈ RCout×1×N是通过<strong>对投影输入进行池化计算得到的</strong>： Yi = XN p=1 softmax(qT i kp)vp， (6) 其中，查询项qi = WqXs i，键项ki = WkXs i，值项vi = WvXs i都是输入Xs i的线性投影，其中i ∈ {1, 2, . . . N}。Wq，Wk ∈ RCq×Cin 和Wv ∈ RCout×Cin 都是可学习参数。然而，这个机制在计算非局部上下文时没有使用任何位置信息。在视觉模型中，位置信息通常有助于捕捉对象的结构。</p><p><strong>总结： 本章讨论了个体图的构建，该图以数据驱动的方式处理不同样本的关节关系。为了更好地处理这些关系，作者介绍了一个自注意力机制，但指出这一机制在计算非局部上下文时未使用任何位置信息。此位置信息在视觉模型中通常用于捕捉对象的结构。</strong></p><p>变换器模块通过以下公式缓解了这个问题： Yi = XN p=1 softmax(qT i kp + qT i rp−i )vp， (7) 其中rp−i是<strong>添加的相对位置编码</strong>。内积qT i rp−i测量从第p个关节到第i个关节的兼容性。请注意，此结构的位置信息仅取决于查询，但键和值也传达关于要关注的位置的信息。<strong>将变换器模块应用于语义信息丰富的RGB数据时，单一的查询位置编码可能足以描述对象的结构</strong>。然而，骨架数据要稀疏得多，我们可能<strong>需要更多的位置编码以协作更好地描述关节的空间位置</strong>。 因此，在我们的SDE中，我们<strong>将查询位置编码扩展到所有键-值对</strong>： Yi = XN p=1 softmax(qT i kp + qT i rq p−i + kT p rk p−i )(vp + rv p−i )， (8) 其中rk p−i和rv p−i<strong>是键和值的位置编码。<strong>在实践中，可以将</strong>等式8中的单头注意力扩展为多头注意力</strong>，以捕获各种相关性。多头注意力通过将G个单头注意力应用于Xs i（每个头的g-th head采用不同的Wg q，Wg k，Wg v∀g ∈ {1, 2, . . . G}），然后<strong>通过连接每个头部（G经验性地设置为8）获得输出Yi</strong>。最后，<strong>Y被映射回T帧，被视为激发X的注意权重</strong>。在SDE中，采用了残差连接以稳定训练。</p><p><strong>总结：</strong> 本章介绍了一个SDE模块，该模块使用Transformer结构来处理骨骼数据中的位置信息。作者指出，与RGB数据不同，骨骼数据更稀疏，因此<strong>需要更多的位置编码来更好地描述关节的空间位置</strong>。在该模块中，查询位置编码被扩展到所有键-值对，并采用了多头注意力以捕获多种相关性。残差连接被用于稳定训练。</p><h3 id="D-Temporal-Motion-Excitation">D. Temporal Motion Excitation</h3><p>运动是<strong>两个连续帧之间的位移测量，反映了真实的动作</strong>。如图4（c）所示，我们设计了TME<strong>以突出运动敏感特征</strong>。给定编码的图形特征X ∈ RCin×T×N，首先使用<strong>1×1卷积层来提高效率的降低特征通道数</strong>： Xr = WrX，其中Wr ∈ R Cin η ×Cin <strong>是一个可学习的卷积权重</strong>，η经验性地设置为4。在<strong>时间步骤t上计算特征级运动表示</strong>，它是<strong>相邻两帧Xr t和Xr t+1之间的差异</strong>。如TEA [42]中的实践，对（t + 1）-th帧应用了一个<strong>通道级的1×1卷积来减轻不匹配问题</strong>： Mt = WmXr t+1 −Xr t 1 ≤ t ≤ T − 1， 其中Wm是1×1通道级的转换，<strong>Mt ∈ R Cin η ×N是时间t的运动特征</strong>。为了对齐时间维度，我们<strong>将MT = 0表示最后时间步的运动特征</strong>，最后将这些特征连接成M ∈ R Cin η ×T×N。 然后，我们使用全局平均池化（GAP）来总结空间信息，并利用另一个1×1卷积层将通道维度扩展到原始大小Cin。 可以进一步计算运动关注权重β ∈ RCin×T×1如下： β = sigmoid(WeGAP(M))，</p><p><strong>总结：</strong> 本章节介绍了关于运动特征建模的部分。运动在骨骼动作识别中起到关键作用，因为它是连续帧之间的位移测量，反映了真实的动作。TME模块用于突出运动敏感特征。该模块通过1x1卷积层降低特征通道数，计算相邻帧之间的运动差异，最后使用全局平均池化和卷积层处理运动特征，以计算运动注意权重。这有助于提取并强调骨骼动作中的运动信息。</p><p>最后，<strong>该模块的目标是突出运动敏感特征：</strong> X ← βX+X。 这里使用了残差连接以稳定训练。</p><p>基于运动敏感特征，我们<strong>进一步利用TCN来学习最终的表示</strong>。我们的TCN是MS-TCN [16]的简化版本，主要区别在于我们<strong>使用较少的分支</strong>（dilations={1, 2}）和<strong>更大的时间核大小（核大小=5）以实现高效的时间建模</strong>。</p><p>在TCN之后，使用GAP（全局平均池化）层来总结时空信息，然后使用FC（全连接）层将隐藏空间映射到类别概率。ML-STGNet的损失函数采用了预测值和真实标签之间的交叉熵损失。</p><p>需要注意的是，这些模块都设计为灵活的组件，可以轻松嵌入到GC-Block中。如图5所示，我们主要探讨了三个示例：ML-STGNet-c，ML-STGNet-p和ML-STGNet-r。这些组合将在消融研究中进行详细讨论，如果没有特别说明，所有实验都使用ML-STGNet-c。</p><p>这一章节描述了一个模块，其目标是突出运动敏感特征，然后通过TCN和其他层来学习最终的表示。不同的模块组合可以用于不同的实验设置。</p><h1>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</h1><h2 id="1-Introduction-2">1. Introduction</h2><p>这一章节是论文的引言，讨论了以下主要内容：</p><ul><li>骨骼动作识别是一个有广泛应用领域的重要研究领域，包括人机交互、智能安全监控和视频理解。</li><li><strong>深度神经网络</strong>在骨骼序列中<strong>学习区分性空间和时间特征方面</strong>取得了显著进展。然而，这些方法通常依赖于集中收集人体骨骼视频，存在<strong>隐私问题</strong>。</li><li>论文提出了<strong>利用联邦学习（<strong>Federated Learning）进行骨骼动作识别</strong>以保护骨骼数据隐私</strong>的方法。</li><li><strong>==联邦学习是一种协作学习方法，旨在从多个分布式的边缘设备或客户端共同学习，同时保护本地数据的安全和隐私==</strong>。</li><li>然而，现有的联邦学习方法主要针对图像任务，鲜有应用于骨骼动作识别。所以，==本文尝试将联邦学习应用于骨骼动作识别任务==。</li><li>骨骼数据的非欧几里得性质使得图卷积网络（Graph Convolution Networks）成为骨骼任务的有效模型。</li><li>论文介绍了 Federated Skeleton-based Action Recognition (FSAR) 作为一个==新的联邦学习骨骼动作识别任务的基准==。FSAR 使用自适应拓扑结构（ATS）来提高训练的稳定性，并采用==多粒度知识蒸馏==（MKD）机制降低客户端模型的差异。</li><li>作者通过ATS和MKD取得了显著的性能，并提供了骨骼动作识别中联邦学习的实际解决方案。</li><li>论文的贡献总结如下：1）首次将联邦学习引入骨骼动作识别领域；2）提出自适应拓扑结构（ATS）来解决骨骼<strong>数据的训练不稳定性</strong>问题；3）开发多粒度知识蒸馏（MKD）机制来<strong>提高准确性</strong>；4）通过大量实验证明了FSAR的有效性。</li></ul><p>这一章节主要介绍了论文的背景、研究问题、方法和贡献，为后续章节提供了基础。</p><h2 id="逐句翻译">逐句翻译</h2><p>Introduction 骨骼动作识别是一个有广泛应用领域的重要研究领域，包括人机交互、智能安全监控和视频理解。</p><p>Recent advancements in deep neural networks [41, 38] have shown significant progress in learning discriminative spatial and temporal features from skeleton sequences. 最近深度神经网络的进展（引用了参考文献41和38）在从骨骼序列中学习区分性的时空特征方面取得了显著进展。</p><p>Though successful, these methods heavily rely on massively centralizing human skeleton videos, which directly and effectively pose privacy concerns due to the exposure of human-related formation, like motion patterns, behavior tendencies, and personal identity. 尽管这些方法取得了成功，但它们严重依赖于集中收集人体骨骼视频，这直接而有效地引发了隐私问题，因为这些==视频暴露了与人相关的信息，如运动模式、行为倾向和个人身份==。</p><p>For instance, Liao et al. [24, 23] have developed algorithms to identify individuals based on their body structures and walking styles. 例如，Liao等人已经开发了一种算法，可以<strong>根据个体的身体结构和步行风格来识别个人</strong>。</p><p>Therefore, centralized collection of such sensitive data exacerbates the risk of privacy disclosure for each local user site. 因此，对这种敏感数据进行集中收集增加了每个本地用户站点的隐私披露风险。</p><p>In response to the increasing awareness of personal data protection, decentralized training techniques are being developed, with federated learning being a powerful approach. 为了响应对个人数据保护意识的增强，==正在开发分布式训练技术，而联邦学习是一个强大的方法==。</p><blockquote><p>现在的骨架动作识别基于骨骼数据，但是现有的方法依赖的这些数据会导致个人信息的暴露</p><p>同时又有技术已经开始针对这些数据进行个人信息的识别</p><p>所以要倾向于保户个人数据 利用分布式训练技术 获取数据 引出连邦学习</p><p>连邦学习是一个非常强大的分布式训练服方法</p></blockquote><p>Thus in this work, we explore the application of federated learning to skeleton-based action recognition for privacy-preserving of skeleton data, which has rarely been investigated before. 因此，在这项研究中，我们探索了==将联邦学习应用于骨骼动作识别，以保护骨骼数据的隐私，这之前很少有研究==</p><p>Federated Learning (FL) is a collaborative learning approach that aims to collectively learn from multiple decentralized edge devices or clients while preserving the security and privacy of local data [30, 22]. 联邦学习（FL）是一种协作学习方法，旨在通过保护本地数据的安全和隐私，从多个分布式的边缘设备或客户端集体学习[30, 22]。</p><p>However, many effective FL techniques are image-based tasks, such as person re-identification [44], medical image segmentation [8], and vision language navigation [48]. 然而，许多有效的FL技术是基于图像的任务，如人员再识别[44]、医学图像分割[8]和视觉语言导航[48]。</p><p>To introduce FL schemes into the skeleton-based action recognition task, we follow the standard client-server architecture [30, 12] and construct a vanilla Federated Skeleton-based Action Recognition (Vanilla FSAR) paradigm. 为了将FL方案引入基于骨骼的动作识别任务，我们<strong>遵循标准的客户端-服务器架构[30, 12]，并构建了一个普通的基于骨骼的联邦动作识别（Vanilla FSAR）范例</strong>。</p><p>As shown in Fig. 1, we consider multiple local clients having independent and non-overlapping category labels. 如图1所示，我们考虑了<strong>多个具有独立和非重叠类别标签的本地客户端</strong>。</p><p>Each client is optimized with non-shared local data under the orchestration of a central server. 在中央服务器的协调下，每个客户端都<strong>使用非共享的本地数据进行优化</strong>。</p><p>The generalized server model is learned by aggregating the local model parameters without accessing sensitive local data. 综合的服务器模型是<strong>通过聚合本地模型参数学习的，而不访问敏感的本地数据。</strong></p><p>Under this paradigm, vanilla FSAR optimizes a generic feature space from multiple and non-shared data silos while maintaining data privacy. 在这种范例下，普通FSAR==从多个非共享的数据储存空间中优化了通用的特征空间，同时保护数据隐私==</p><p>Due to the non-euclidean nature of skeleton data, Graph Convolution Networks (GCNs) have been effective models for skeleton-based tasks. 由于骨骼数据的非欧几里得性质，图卷积网络（Graph Convolution Networks）已经成为骨骼任务的有效模型。</p><p>One of the notable efforts is ST-GCN [46] which models spatial temporal graph of skeleton sequences based on human topology structures. 其中一项值得注意的工作是ST-GCN[46]，它基于人体拓扑结构对骨骼序列的时空图进行建模。</p><p>Compared to CNN- or RNN-based methods, ST-GCN and its variants [3, 19, 28] achieve superior performance on skeleton-based action recognition. 与基于CNN或RNN的方法相比，<strong>ST-GCN及其变种</strong>[3, 19, 28]在基于骨骼的动作识别上实现了更出色的性能。</p><p>Without loss of generality, we apply FL approaches to the ST-GCN model via the above client-server collaboration training paradigm as the straightforward solution to address the privacy and security concerns. 毫不失一般性，我们<strong>通过上述客户端-服务器协作培训范例将FL方法应用于ST-GCN模型，作为解决隐私和安全问题的直接方法</strong>。</p><p>Nevertheless, the direct combination can suffer from slow convergence and considerable fluctuations, as shown in Fig. 2, which obstructs the model from generating feature representation suitable for efficient deployment with privacy protection. 然而，直接组合可能会导致收敛速度较慢和相当大的波动，如图2所示，这<strong>阻碍了模型生成适用于隐私保护的高效部署的特征表示</strong>。</p><p>Apart from the non-Independently Identically Distribution (non-IID) [47] of data from different clients, we identify the heterogeneous graph topologies across clients as a critical trigger for this phenomenon. 除了来自不同客户端的数据的非独立同分布性（non-IID）[47]，我们确定<strong>不同客户端之间的异构图拓扑</strong>是这一现象的重要触发因素。</p><p>This structured element is dataset-specific, causing local models of different clients to gradually diverge from each other during local training. 这种结构化元素是特定于数据集的，<strong>导致不同客户端的本地模型在本地训练期间逐渐发散</strong>。</p><p>It is known as the client drift problem in traditional FL [13], which can drastically damage the training performance of the global model when the data similarity decreases. 这在传统的FL[13]中被称为客户端漂移问题，当数据相似性降低时，它可能会严重损害全局模型的训练性能。</p><p>In light of this, we introduce FSAR, a novel benchmark for Federated Skeleton-based Action Recognition, to address the aforementioned client drift issues, especially for skeleton data. 鉴于此，我们引入了FSAR，这是一个新的联邦骨骼动作识别的基准，以解决前述的客户端漂移问题，特别是针对骨骼数据。</p><p>In FSAR, instead of finding one global model that fits the data distribution of all clients, an Adaptive Topology Structure (ATS) is proposed to inject modulated and customized elements into each client model. 在FSAR中，我们<strong>不是寻找适应所有客户端数据分布的全局模型，而是提出了一个自适应拓扑结构（ATS），将调制和定制的元素注入到每个客户端模型中</strong>。</p><p>The ATS learns the commonality of shared structure to improve the stability of FL training, and preserves the unique structure of their own data to prevent current clients from being affected by other clients with different dataset scales, respectively. ATS学习共享结构的共性，以提高FL培训的稳定性，并保留自己数据的独特结构，以防止当前客户端受到其他具有不同数据集规模的客户端的影响。</p><p>Moreover, we adopt learnable factors to automatically balance both topology structures on each client data since smaller datasets are more susceptible to large-scale datasets. 此外，我们采用<strong>可学习的因子来自动平衡每个客户端数据上的两种拓扑结构</strong>，因为较小的数据集更容易受到大规模数据集的影响。</p><p>Apart from this, data heterogeneity across clients caused by various source domains also jeopardize the training stability and accuracy. 除此之外，由<strong>各种来源领域引起的客户端之间</strong>的<strong>数据异质性也危害了培训的稳定性和准确性</strong>。</p><p>Generally, the features extracted by the shallower layer contain universal information (e.g., the connection between different joints), and the deeper features hold semantic information related to action labels (which is personalized and client-specific). 通常，浅层提取的特征包含通用信息（例如不同关节之间的连接），而深层特征包含与动作标签相关的语义信息（这是个性化的和特定于客户端的）。</p><p>Therefore, a Multi-grain Knowledge Distillation (MKD) mechanism is further developed to reduce the feature variation of shallow layers, which decreases client divergences with respect to the server model and facilitates client-server communication on generalized messages. 因此，我们进一步开发了<strong>多粒度知识蒸馏（MKD）机制</strong>，<strong>以减少浅层特征的变化，从而减小了客户端与服务器模型的差异，促进了客户端-服务器上的广义消息通信</strong>。</p><p>By leveraging both ATS and MKD, FSAR achieves remarkable performance and provides practical solutions for federated learning in skeleton-based action recognition. 通过充分利用ATS和MKD，FSAR取得了显著的性能，并为基于骨骼的动作识别的联邦学习提供了实际的解决方案。</p><blockquote><p>ATS 自适应的异质图拓扑结构 保证所有客户端之间的互相影响   解决了客户端飘移问题</p><p>多粒度的知识蒸馏机制  减少浅层的 类似于节点间连接  和 特征变换等类似问题  减少客户端和服务器间模型差异  <strong>从而减小了客户端与服务器模型的差异，促进了客户端-服务器上的广义消息通信</strong></p></blockquote><p>The contributions are summarized as follows: 总结贡献如下：</p><p>• We take the lead in introducing federated learning into skeleton-based action recognition and present a novel benchmark FSAR to address privacy concerns in this field for the first time. • 我们引领将联邦学习引入基于骨骼的动作识别，并首次提出了一个新的基准FSAR，以解决这一领域的隐私问题。</p><p>• We identify the heterogeneous graph topology structure as a major obstacle that causes training instability for skeleton data, and explore a novel Adaptive Topology Structure (ATS) to facilitate collaborative training between decentralized clients by learning domain-invariant and domain-specific topologies. • 我们确定<strong>异构图拓扑结构是导致骨骼数据训练不稳定性的主要障碍</strong>，并探索了一<strong>种新颖的自适应拓扑结构（ATS），通过学习领域不变和领域特定的拓扑结构来</strong>促进分散客户端之间的协作培训。</p><p>• The innovative Multi-grain Knowledge Distillation (MKD) mechanism is then explored by aligning shallow features to mitigate client-server divergence and further boost the accuracy. • 通过将浅层特征与客户端-服务器的差异来减少差异，从而进一步提高精度。↳</p><p>• Extensive experiments validate the effectiveness of FSAR and demonstrate that it achieves significant improvements over SOTA FL-based methods on several benchmark datasets with local data privacy protected. • 大量实验证实了FSAR的有效性，并表明它在多个具有本地数据隐私保护的基准数据集上取得了显著的改进。</p><h1>LAC - Latent Action Composition for Skeleton-based Action Segmentation</h1><ul><li>Human-centric activity recognition is a crucial task in real-world video understanding. 人类中心的活动识别是实际视频理解中的重要任务。↳</li><li>In this context, skeleton data that can be represented by 2D or 3D human keypoints plays an important role, as it is complementary to other modalities such as RGB and optical flow. 在这一背景下，可以<strong>由2D或3D人体关键点表示的骨骼数据起着重要作用</strong>，因为它是与其他形式（如RGB和光流）互补的。</li><li>The study of recognizing activities directly from 2D/3D skeletons has gained increasing attention. 直接从2D/3D骨骼中识别活动的研究引起了越来越多的关注。</li><li>In untrimmed videos, activities are composable, and expressive skeleton features are required to model long-term dependency among different actions. 在未经修剪的视频中，<strong>活动是可组合的</strong>，需要有表现力的骨骼特征来建模不同活动之间的长期依赖性。</li><li>Current approaches obtain such features through visual encoders pre-trained on trimmed datasets, but their performance is not satisfactory for classifying complex actions. <strong>==当前的方法通过在经过修剪的数据集上预训练的视觉编码器获得这些特征，但对于分类复杂的动作，它们的性能不令人满意==</strong></li><li>To address this issue, the paper proposes to construct synthesized composable skeleton data for training a more effective visual encoder. 为了解决这个问题，本文提出<strong>构建合成可组合骨骼数据，以训练更有效的视觉编码器</strong>。</li><li>The proposed framework, Latent Action Composition (LAC), leverages synthesized composable motion data for self-supervised action representation learning. 提出的框架，潜在动作合成（LAC），利用合成的可组合动作数据进行自监督的动作表示学习。</li><li>LAC learns action representations in two steps: a first action composition step is followed by a contrastive learning step. LAC分两步学习动作表示：==首先是动作合成步骤，然后是对比学习步骤==。</li><li>Action composition involves training a generative module to generate new skeleton sequences by combining multiple videos using a Linear Action Decomposition (LAD) mechanism. 动作合成包括训练一个生成模块，使用线性动作分解（LAD）机制结合多个视频生成新的骨骼序列。</li><li>The contrastive learning step is designed to train a skeleton visual encoder in a self-supervised manner, maximizing the similarity of different skeleton sequences across datasets. 对比学习步骤旨在以自监督方式训练骨骼视觉编码器，最大化不同骨骼序列之间的相似性。</li><li>Experimental results show that LAC significantly improves the expressive power of the visual encoder for action segmentation. 实验结果表明，LAC显著提高了用于动作分割的视觉编码器的表现力。</li></ul><p>总结： 该章节介绍了文章的背景，指出了骨骼数据在人类中心活动识别中的重要性。文章提出了一种新的方法，即Latent Action Composition（LAC），用于自监督的动作表示学习。LAC包括两个步骤：动作合成和对比学习。动作合成利用线性动作分解（LAD）机制合成可组合的骨骼数据，对比学习用于训练骨骼视觉编码器。实验结果表明，LAC显著提高了骨骼数据的表现能力，特别是在动作分割任务中。</p><ol><li><p>Introduction 人类中心的活动识别在实际视频理解中是一项关键任务。</p><p>在这个背景下，可以由2D或3D人类关键点表示的骨骼数据发挥着重要作用，因为它与其他类型，如RGB和光流，具有互补性。</p><p>随着人类骨骼模态在相机视角和主体外观相关内容变化方面的稳健性大幅提升，直接从2D/3D骨架中识别活动的研究引起了越来越多的关注。</p><p>虽然先前的方法取得了显著的成功，但这些方法通常专注于包含单个动作的经修剪视频，这构成了一种高度简化的情境。与此不同，本文致力于基于未经修剪的视频中的骨骼序列进行动作分割，这是一种具有挑战性的设置。</p><p>在未经修剪的视频中，活动是可组合的，也就是说，一个人的动作通常包括多个动作（共现），每个动作持续几秒钟。</p><p>为了建模不同动作之间的长期依赖性，需要具有表现力的骨骼特征。</p><p>目前的方法通过视觉编码器（例如AGCNs）获得这些特征，这些编码器在经修剪的数据集上进行了预训练。然而，由于修剪样本中的<strong>动作信息有限</strong>，这些特征在分类复杂动作方面的性能远未令人满意。</p><p>为了解决这个问题，我们提出构建综合可组合的骨骼数据，以训练具有强大表现力的视觉编码器，用于动作分割。</p></li><li><p>总结 本文介绍了一种名为<strong>LAC的新框架</strong>，旨在利用<strong>综合可组合的运动数据进行自监督动作表示学习</strong>。</p><p>与当前的自监督方法不同，LAC分两步学习动作表示：首先进行动作合成，然后进行对比学习。</p><p>动作合成是一种新颖的初始化步骤，用于<strong>训练生成模块，该模块可以通过组合多个视频生成新的骨骼序列</strong>。</p><p>高级运动很难通过联合坐标（例如“喝水”和“坐下”）直接组合，因此LAC在自动编码器内部==引入了一种新颖的线性动作分解（LAD）机制==。</p><p>LAD旨在学习一个<strong>动作字典，以离散方式表达微妙的动作分布。这种动作字典在潜在编码空间中包含两组方向</strong>。</p><p>第一组称为“静态”，包括代表骨骼序列的静态信息的方向，例如视角和体型。</p><p>另一组称为“运动”，包括代表骨骼序列的时间信息的方向，例如主体执行的动作的基本动态。</p><p>新的骨骼序列通过线性组合学习的“静态”和“运动”方向来生成。我们<strong>采用运动重定向来训练自动编码器和字典</strong>，使用了从3D合成数据构建的**“静态”和“运动”信息的骨骼序列**。</p><p>一旦构建了动作字典，在接下来的对比学习步骤中，不需要“静态”/“运动”信息和动作标签，可以从多个输入骨骼序列中生成可组合的动作，通过组合它们的潜在“运动”集。</p><p><strong>对比学习步骤的目标是以自监督方式训练骨骼视觉编码器</strong>，如UNIK，无需动作标签。</p><p>它旨在<strong>使最终的视觉编码器能够最大程度地增加来自相同原始序列的数据增强获取的不同骨骼序列的相似性</strong>，</p><blockquote><p>对比学习 增大 两个来自相同的原始骨骼序列的相似性  但是两个序列经过不同的数据增强后变的不一样 但经过对比学习后会增强两个骨架序列的相似性</p></blockquote><p>这些数据增强涵盖了大规模数据集。与目前的方法不同，<strong>这里的对比学习还在帧空间中执行</strong>，<strong>以精细地增加正样本之间的每帧相似性。</strong></p><p>随后，<strong>在动作分割数据集上对经过此培训的帧级骨骼视觉编码器进行转移和重新训练。</strong></p><p>为了评估LAC的性能，我们在大规模数据集Posetics上训练骨骼视觉编码器，并通过对未经修剪的动作分割数据集（如TSU、Charades、PKU-MMD）进行微调来评估学到的骨骼表示的质量。实验分析证实，<strong>动作合成和对比学习可以显著增强视觉编码器的表现力。</strong></p><p>总之，本文的贡献包括：（i）我们引入了LAC，这是一种新颖的<strong>生成和对比框架</strong>，旨在综合复杂的运动并提高骨骼动作表示的能力。（ii）在生成步骤中，我们<strong>引入了一种新颖的线性动作分解（LAD）机制</strong>，通过<strong>正交基表示高级运动特征</strong>。因此，可以<strong>通过潜在空间操作线性组合多个骨骼序列</strong>。（iii）在对比学习步骤中，我们建议<strong>在视频和帧空间中学习骨骼表示</strong>，以<strong>提高面向帧的动作分割任务的泛化能力</strong>。（iv）我们进行实验分析，并显示在Posetics上进行预训练并将其传输到未经修剪的目标视频数据集是一种用于动作分割的通用且有效的方法。</p></li></ol><h1>Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</h1><p>当然，我会逐句为您翻译这一段文字并总结 Introduction（引言）章节：</p><ol><li><p>骨骼动作识别[48, 2]在近年来引起了越来越多的关注，它是许多领域的主要话题，从人机互动到虚拟现实，由于其聚焦于动作本质和紧凑性而备受青睐[6]。</p><p>这一句说明了骨骼动作识别的重要性和它在多个领域的广泛应用，强调了它引起了人们的关注。</p></li><li><p>然而，<strong>如何识别新颖的动作</strong>仍然是一个未解决的问题。为了克服这个问题，越来越多的工作已经集中在<strong>少样本动作识别上</strong>，这可以<strong>减轻对稀有类别造成的性能下降问题</strong>[10, 27, 20, 43, 12]。特别是，这些方法探讨了<strong>无标签查询和标记支持集</strong>，以便<strong>学习具有区分性的特征表示</strong>，将<strong>查询动作与由少数支持样本表示的类别相匹配</strong>[35, 9]。</p><p>这段讨论了关于如何应对新颖动作识别问题以及少样本动作识别的方法，特别是强调了它们在解决性能下降问题上的作用。</p><blockquote><p>这些方法同时探索了无标签查询和有标签支持集，从而学习一种判别特征表示，将查询动作与由少数支持样本表示的类别相匹配</p></blockquote></li><li><p>如图1所示，现有方法侧重于<strong>如何利用骨骼内部或骨骼间的关系，而忽略了这两种范例之间的互补性</strong>。他们在挑战性场景中容易失败，例如<strong>相似的空间外观或不一致的时间依赖性</strong>。</p><p>这部分提到了现有方法的局限性，即它们倾向于忽略了骨骼内部和骨骼间关系的互补性，导致在特定场景下的失败。</p></li><li><p>尽管取得了显著进展，但我们仍然认为只考虑序列内部依赖性或序列间关联性是不足够的，这需要并行对齐上述样本。</p><p>这句指出了作者的观点，即现有方法还有改进的空间，特别是需要在并行方面进行样本的对齐。</p></li><li><p>受此启发，我们提出了一种新的少样本骨骼动作识别框架，称为Parallel Attention Interaction Network（PAINet）。我们认为，适应骨骼内部和骨骼间的本地关节特征是完善空间匹配的不可或缺的方式。</p><p>这句说明了作者提出的新方法，PAINet，以及他们认为的改进方法。</p></li><li><p>我们的贡献可以总结如下：</p><ul><li>我们提出了一种新颖的PAINet，用于少样本骨骼动作识别，在匹配过程中减轻了类似空间外观和不一致时间依赖性等挑战。</li><li>我们进一步设计了一个拓扑编码模块，以捕获关节和身体部位之间的协同运动，以及关节之间的内在语义关系。此外，我们提出了一个方向平均对称表面度量，用于发现最接近的时间关系。</li><li>我们在NTU-T、NTU-S和Kinetics等数据集上进行了广泛的实验，结果表明我们的模型明显优于现有的最先进方法。</li></ul><p>这段总结了作者的贡献，说明了他们的新方法PAINet的优势，以及他们的实验结果支持了他们的观点。</p></li></ol><p>总的来说，引言章节强调了骨骼动作识别的重要性和当前的挑战，然后引入了作者提出的新方法PAINet，以解决这些挑战。作者还指出了他们的贡献和实验结果。</p><p>正如图1所示，当前的方法主要关注如何利用骨架内部或骨架间的关系，同时忽略了这两种范例之间的互补性。它们容易在具有挑战性的情境中失败，例如相似的空间外观或不一致的时间依赖性。</p><p>一方面，一些研究尝试<strong>学习序列内的区分性特征</strong>。例如，==[3, 52, 26, 21, 31] 利用<strong>骨架内的局部关节特征来捕捉不同的模式</strong>==。特别是，[3] 提出了一个<strong>基于部分的空间区域聚合</strong>【Part-aware prototypical graph network for one-shot skeleton-based action recognition】，而[52]【Adaptive local-component-aware graph convolutional network for one-shot skeleton-based action recognition.】 采用了<strong>基于身体部位的局部嵌入的选择性求和以获得个体表示</strong>。</p><p>另一方面，一些研究探讨了跨序列调整局部特征以识别相似性和差异性。例如，[25] 利用交叉注意机制来激活它们的空间信息，而[40] 在查询和支持之间的时间和相机视角空间中实现了最佳对齐。尽管已经取得了显著的进展，我们仍然认为仅考虑序列内的依赖性或序列间的关联性是不足够的，这需要同时对齐上述样本。</p><p>如图1(a)所示，当分类具有相似外观的样本，如摘下耳机和摘下眼镜时，手部和肘部关节的空间位置中微差异有助于识别。序列间的交互可以通过优先考虑区分性关节来放大类别特定信息，因此只需要进行最佳的时间集匹配来进行识别。另外，如图1(b)中所示，当分类具有不一致运动模式的样本，如举手，肘部和躯干关节在两个图中具有不同的路径，但具有坚实的语义连接。在这种情况下，需要通过序列内的上下文聚合来增强实例特定信息。随着动作特征的丰富，序列之间的时间自适应交互可以进一步改善对齐。</p><p>受此启发，我们提出了一种新颖的少样本骨架动作识别框架，称为“Parallel Attention Interaction Network”（PAINet）。我们认为，适应骨架内部和骨架间的局部关节特征是完善空间匹配的不可或缺的方法。与以前的方法相比，我们的方法涉及了两个并行分支内的空间和时间域的对齐，从而实现了对骨架序列中信息丰富区域的互补注意。具体来说，我们提出了一个拓扑编码模块，利用拓扑和物理信息来增强两个分支中交互部分和关节对的建模。在交叉空间对齐分支中，我们使用了一个空间交叉注意模块来建立跨序列的关节关联。随后，我们引入了一个方向平均对称表面度量，考虑了所有可能的子序列对，并选择具有最大相似性的对。在交叉时间对齐分支中，我们提出了一个空间自注意模块，用于在序列内汇总空间上下文。随后，我们遵循基于视频的方法TRX [27]，通过时间交叉注意匹配器来汇总对齐的距离。我们的贡献总结如下：</p><p>• 我们提出了一种新颖的PAINet框架，用于少样本骨架动作识别，从而在匹配过程中解决了相似的空间外观和不一致的时间依赖性所带来的挑战。</p><p>• 我们进一步设计了一个拓扑编码模块，以捕捉关节和身体部分之间的共同运动，以及关节之间的内在语义关系。此外，我们提出了一种方向平均对称表面度量，用于发现最接近的时间关系。</p><p>• 对NTU-T、NTU-S和Kinetics等广泛的实验结果表明，我们的模型在性能上明显优于现有的方法。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文献略读6</title>
      <link href="/project/posts/ca7eff4.html"/>
      <url>/project/posts/ca7eff4.html</url>
      
        <content type="html"><![CDATA[<h1>6 STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action RecognitionSTMT：一种用于MoCap动作识别的时空网格Transformer</h1><h2 id="abstract">abstract</h2><p>本研究探讨了使用运动捕捉（MoCap）序列进行人体动作识别的问题。与现有技术不同，这些技术需要多个手动步骤来导出标准化的骨骼表示作为模型输入，我们提出了一种全新的空时网格变换器（STMT），用于直接建模网格序列。该模型使用分层变换器，具有帧内偏移关注和帧间自关注。这种关注机制允许模型在空时领域中自由关注任意两个顶点块，以学习非局部关系。我们使用遮蔽的顶点建模和未来帧预测作为两个自监督任务，以充分激活我们分层变换器中的双向和自回归关注。所提出的方法在常见的MoCap基准测试中相较于基于骨骼和基于点云的模型实现了最先进的性能。代码可在 <a href="https://github.com/zgzxy001/STMT">https://github.com/zgzxy001/STMT</a> 上找到。</p><p>要点总结：</p><ul><li>本研究关注使用MoCap序列进行人体动作识别。</li><li>与现有技术不同，该研究提出了STMT模型，用于直接建模网格序列，而不需要多个手动步骤来导出标准化的骨骼表示。</li><li>STMT模型采用了分层变换器，包括帧内偏移关注和帧间自关注，以学习空时领域中的非局部关系。</li><li>为了激活分层变换器中的双向和自回归关注，采用了遮蔽的顶点建模和未来帧预测作为两个自监督任务。</li><li>该方法在MoCap基准测试中取得了与基于骨骼和基于点云的模型相比的最先进性能。</li></ul><h3 id="总结">总结</h3><ul><li>本研究旨在解决使用MoCap（运动捕捉）序列进行人体动作识别的问题。</li><li>与现有技术不同，提出了一种名为STMT的新型模型，该模型直接处理网格序列，无需多个手动步骤来导出标准化的骨骼表示作为模型输入。</li><li>STMT模型采用了分层变换器，包括帧内偏移关注和帧间自关注。这些机制允许模型在空间和时间维度上自由关注任意两个顶点块，以学习空时领域中的非局部关系。</li><li>为了增强模型的性能，采用了两个自监督任务：遮蔽的顶点建模和未来帧预测。这些任务激活了模型中的双向和自回归关注。</li><li>所提出的STMT方法在MoCap基准测试中表现出卓越的性能，相较于基于骨骼和基于点云的模型，实现了最先进的成果。</li></ul><p>文章代码可在https://github.com/zgzxy001/STMT上找到。</p><h2 id="introduction">introduction</h2><ol><li><strong>MoCap和其重要性</strong>：Motion Capture（MoCap）是数字记录人体运动的过程，对于多个研究领域具有关键作用，包括动作识别、跟踪、姿势估计、模仿学习和运动合成等。此外，在医院和制造环境等各种实际场景中，MoCap也是增强人机交互的基本技术之一。文章强调从MoCap数据中理解人类行为对于机器人感知、规划和控制至关重要。</li><li><strong>骨架表示</strong>：通常使用骨架表示来建模MoCap序列。过去的方法通常直接使用身体标记和它们之间的连接关系来构建骨架图。然而，标记位置依赖于每个受试者，这会在每个数据集中引入样本方差。而且，不同的MoCap数据集通常具有不同数量的身体标记，这阻止了模型在统一框架上进行训练和测试。为了使用标准的骨架表示（如NTU RGB+D），一些方法需要多个手动步骤来将网格适应到骨架上。然而，这些方法具有以下缺点：需要多个手动步骤来将网格适应到骨架上，骨架表示丢失了原始MoCap数据提供的信息（即表面运动和身体形状知识）。</li><li><strong>基于网格的动作识别方法</strong>：为了克服上述缺点，文章提出了一种基于网格的动作识别方法，直接对原始网格序列中的动态变化建模。网格表示提供了精细的身体信息，但将高维网格序列分类为不同的动作是具有挑战性的。为了解决这些挑战，文章引入了一种新的空间-时间网格变换器（STMT）。</li><li><strong>STMT模型</strong>：STMT利用网格连接信息在帧级别构建补丁，并使用分层变压器。分层变压器能够自由关注任何帧内和帧间的补丁，以学习空间-时间关联性。分层的关注机制允许模型学习整个序列的补丁相关性，减轻了对显式顶点对应的要求。文章还定义了两个自监督学习任务，即遮挡顶点建模和未来帧预测，以增强顶点补丁之间的全局交互。</li><li><strong>贡献</strong>：文章的贡献包括引入了新的分层变压器架构，设计了有效且高效的预训练任务，并在常见的MoCap基准数据集上实现了超越状态-of-the-art点云和骨架模型的性能。</li></ol><p>这些要点总结了文章的介绍部分，强调了MoCap的重要性，骨架表示的局限性，以及基于网格的动作识别方法（STMT）的潜在优势和贡献。</p><h3 id="总结-2">总结</h3><p>这篇文章的介绍部分强调以下关键要点：</p><ol><li><strong>MoCap的重要性</strong>：Motion Capture（MoCap）是数字记录人体运动的技术，对多个领域如动作识别、跟踪、姿势估计、模仿学习和运动合成至关重要。MoCap还在实际应用场景中提供了机器人感知、规划和控制的关键信息。</li><li><strong>骨架表示的问题</strong>：传统上，骨架表示是处理MoCap数据的常用方法。然而，标记位置在不同受试者之间不同，导致了样本差异。不同MoCap数据集的标记数量也不同，难以在统一框架下进行建模。现有方法通常需要多个手动步骤来将网格数据映射到标准骨架上，同时也会丢失原始MoCap数据的信息。</li><li><strong>基于网格的动作识别</strong>：为了克服这些问题，文章提出了一种新的方法，直接模拟原始网格序列的动态变化。虽然网格表示提供了详细的身体信息，但将高维网格序列分类为不同的动作具有挑战性。</li><li><strong>STMT模型</strong>：文章引入了Spatial-Temporal Mesh Transformer（STMT）模型，该模型利用网格连接信息在帧级别构建补丁，并使用分层变压器进行建模。分层变压器允许模型自由关注不同补丁，学习空间-时间关联性，解决了网格数据中的挑战。</li><li><strong>自监督学习任务</strong>：为了增强全局交互，文章定义了两个自监督学习任务，包括遮挡顶点建模和未来帧预测。</li><li><strong>贡献</strong>：文章的贡献包括引入了新的分层变压器架构，设计了有效的自监督学习任务，并在MoCap基准数据集上取得了领先于其他点云和骨架模型的性能。</li></ol><h3 id="详细">详细</h3><p><strong>1. 引言</strong>：</p><ul><li>介绍了MoCap（Motion Capture）技术的定义，即数字记录人体运动的过程。</li><li>指出MoCap在多个领域中的广泛应用，包括动作识别、跟踪、姿势估计、模仿学习和运动合成。</li><li>强调MoCap对于增强人机交互在各种实际场景中的重要性，如医院和制造环境。</li></ul><p><strong>2. 骨架表示的问题</strong>：</p><ul><li>提出了传统骨架表示方法的问题，这些方法通常涉及将MoCap数据中的标记位置映射到骨架图上。</li><li>指出标记位置在不同受试者之间不同，导致了在数据集内的样本差异。</li><li>提到不同MoCap数据集通常具有不同数量的标记点，因此很难在一个统一的骨架模型上进行训练和测试。</li><li>指出传统骨架表示方法需要多个手动步骤来将网格数据映射到标准骨架上，同时丧失了原始MoCap数据的信息。</li></ul><p><strong>3. 基于网格的动作识别</strong>：</p><ul><li>引入了一种新的方法，即直接模拟原始网格序列的动态变化。</li><li>提出了使用网格表示进行动作识别的挑战，因为需要将高维网格序列分类为不同的动作。</li></ul><p><strong>4. STMT模型</strong>：</p><ul><li>介绍了Spatial-Temporal Mesh Transformer（STMT）模型，这是文章提出的关键方法。</li><li>提到STMT模型利用网格连接信息在帧级别构建补丁（patches）。</li><li>描述STMT模型的分层变压器结构，该结构允许模型自由关注任何两个帧内和帧间的补丁，以学习空间-时间关联性。</li></ul><p><strong>5. 自监督学习任务</strong>：</p><ul><li>详细解释了两个自监督学习任务的设计，包括遮挡顶点建模（masked vertex modeling）和未来帧预测（future frame prediction）。</li><li>指出这些自监督任务的目的是增强模型对顶点补丁的全局交互，帮助模型更好地理解网格数据的空间和时间特性。</li></ul><p><strong>6. 贡献</strong>：</p><ul><li>总结了文章的主要贡献，包括引入新的分层变压器架构，设计有效的自监督学习任务，并在MoCap基准数据集上取得了领先于其他点云和骨架模型的性能。</li></ul><h1>7 Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition基于骨架的分层分解图卷积网络动作识别</h1><h2 id="abstract-2">abstract</h2><p>摘要：骨架动作识别常使用图卷积网络（GCN），取得了显著的性能。在这一任务中，生成具有语义含义的邻接矩阵尤为重要，但提取这样的边是具有挑战性的。为解决这一问题，我们提出了一种分层分解的图卷积网络（HD-GCN）架构，采用新颖的分层分解图（HD-Graph）。提出的HD-GCN将每个关节节点有效分解为多个集合，以提取主要的结构邻接和远程边，并将它们用于构建一个包含这些边的HD-Graph，其语义空间与人体骨架相同。此外，我们引入了一个注意力引导的分层聚合（A-HA）模块，以突出显示HD-Graph的主导分层边集。此外，我们还应用了一种新的六路集成方法，仅使用关节和骨流，而不使用任何运动流。我们的模型在四个大型热门数据集上进行了评估，并取得了最先进的性能。最后，我们通过各种比较实验证明了我们模型的有效性。</p><p>要点总结：</p><ol><li>骨架动作识别常使用图卷积网络（GCN）方法，并取得了显著的性能。</li><li>生成带有语义含义的邻接矩阵在任务中至关重要，但提取这些边是一项具有挑战性的任务。</li><li>为解决这一问题，提出了一种新的架构，即分层分解的图卷积网络（HD-GCN）。</li><li>HD-GCN有效地将关节节点分解为多个集合，以提取主要的结构邻接和远程边，并构建HD-Graph来表示这些边。</li><li>引入了注意力引导的分层聚合模块（A-HA），以突出显示HD-Graph的主导分层边集。</li><li>应用了一种新的六路集成方法，只使用关节和骨流，而不使用任何运动流。</li><li>该模型在四个大型数据集上取得了最先进的性能，并通过比较实验证明了其有效性</li></ol><h2 id="总结-3">总结</h2><ol><li>骨架动作识别通常使用图卷积网络（GCN）方法，但生成带有语义含义的邻接矩阵对任务至关重要。</li><li>提取具有语义含义的邻接边是一项具有挑战性的任务。</li><li>为解决这个问题，作者提出了一种新的架构，即分层分解的图卷积网络（HD-GCN）。</li><li>HD-GCN有效地将每个关节节点分解为多个集合，以提取主要的结构性邻接边，并构建HD-Graph来表示这些边。</li><li>引入了一个注意力引导的分层聚合模块（A-HA），以突出显示HD-Graph的主导分层边集。</li><li>还采用了一种新的六路集成方法，只使用关节和骨流，而不使用任何运动流。</li><li>该模型在四个大型数据集上取得了最先进的性能，并通过比较实验证明了其有效性。</li></ol><h2 id="introduction-2">introduction</h2><p><strong>摘要：</strong></p><ul><li><strong>问题背景：</strong> 文章探讨了基于骨骼的人体动作识别（HAR）问题，其中通过视频数据对动作类别进行分类。</li><li><strong>技术背景：</strong> 深度学习技术的发展使得基于RGB和基于骨骼的HAR方法得以提出。</li><li><strong>问题：</strong> 基于RGB的方法容易受到背景颜色、光线亮度和服装等环境噪声的干扰，因此骨骼模态方法因不受这些噪声的影响而备受关注。</li><li><strong>方法限制：</strong> 现有的基于图卷积网络（GCN）的方法存在局限性：1) 不能识别远程关节点之间的关系，因为它们仅使用人体骨骼中的PC边的关系。2) 有些方法只是简单地汇总了边缘特征，忽略了每个边的贡献。</li><li><strong>解决方法：</strong> 为解决这些问题，文章提出了一种分层分解的图卷积网络（HD-GCN）架构，使用分层分解的图（HD-Graph）和注意力引导的层次聚合（A-HA）模块。</li><li><strong>性能提升：</strong> 提出的方法在四个大型流行数据集上取得了最先进的性能。</li></ul><p><strong>介绍：</strong></p><ul><li><strong>HAR背景：</strong> 介绍了HAR的背景和在人机交互和虚拟现实等领域的应用。</li><li><strong>方法选择：</strong> 介绍了基于RGB和基于骨骼的HAR方法，强调了基于骨骼的方法因不受环境噪声干扰而受到关注。</li><li><strong>GCN方法限制：</strong> 分析了现有的GCN方法的局限性，包括不能处理远程关节点之间的关系和不能充分考虑每个边的贡献。</li><li><strong>解决方法：</strong> 提出了HD-GCN方法，包括HD-Graph和A-HA模块，以解决现有方法的问题。</li><li><strong>六通道集成：</strong> 介绍了一种新的六通道集成方法，可以在没有任何动作数据的情况下提高性能。</li><li><strong>贡献：</strong> 总结了文章的主要贡献，包括HD-Graph的引入、A-HA模块的设计和性能改进。</li></ul><p><strong>摘要：</strong></p><ul><li>研究问题：骨骼动作识别在视频理解中起着重要作用。</li><li>挑战：构建具有语义含义的邻接矩阵以准确表示骨骼关系是困难的。</li><li>提出的方法：引入了分层分解图卷积网络（HD-GCN）和分层分解图（HD-Graph），以及注意力引导的层次聚合（A-HA）模块。</li><li>创新点：HD-Graph能够准确识别不同层次的关节点之间的关系。A-HA模块突出显示了关键关系，有助于更好地识别重要的边缘。</li><li>实验结果：在四个常用数据集上，该方法表现出色，实现了领先水平。</li></ul><p><strong>介绍：</strong></p><ul><li>背景：介绍了人体动作识别（HAR）的背景和应用领域。</li><li>问题：阐述了RGB和骨骼HAR方法的选择，指出RGB方法容易受到环境噪声的干扰，而骨骼方法不受其影响。</li><li>GCN方法限制：解释了现有GCN方法的问题，包括不能处理关节点之间的远程关系和不能全面考虑每个边的贡献。</li><li>解决方案：提出了HD-GCN方法，通过HD-Graph和A-HA模块解决了这些问题。</li><li>六通道集成：介绍了一种新的六通道集成方法，提高了性能。</li><li>贡献：总结了文章的主要贡献，包括HD-Graph的引入、A-HA模块的设计和性能改进。</li></ul><h3 id="详细示例">详细示例</h3><p><strong>摘要：</strong></p><ul><li><strong>研究背景：</strong> 骨架动作识别一直是视频理解领域的重要问题，取得了显著的进展。</li><li><strong>关键挑战：</strong> 在这一任务中，生成具有语义含义的邻接矩阵尤为关键，但提取这样的关系是一个具有挑战性的问题。</li><li><strong>提出的方法：</strong> 为了解决这一问题，我们提出了一种分层分解图卷积网络（HD-GCN）架构，配以新颖的分层分解图（HD-Graph）。HD-GCN能够将每个关节点有效地分解成多个集合，以提取主要的结构上相邻和相距较远的关系，并使用它们来构建一个包含这些关系的HD-Graph，同时保持人体骨架的语义空间。此外，我们引入了一个基于注意力的层次聚合（A-HA）模块，以突出HD-Graph的主要层次关系集。</li><li><strong>创新点：</strong> HD-Graph能够准确识别不同层次的关节点之间的关系。A-HA模块有助于强调HD-Graph中最重要的关系，从而更好地识别关键边缘。</li><li><strong>实验结果：</strong> 我们在四个广泛使用的动作识别数据集上对我们的模型进行了评估，取得了领先水平的性能。</li></ul><p><strong>介绍：</strong></p><ul><li><strong>人体动作识别背景：</strong> 人体动作识别（HAR）是通过接收视频数据来对动作类别进行分类的任务，已广泛应用于多个领域，如人机交互和虚拟现实。</li><li><strong>问题阐述：</strong> 最近，随着深度学习技术的发展，提出了几种基于RGB和基于骨架的HAR方法。然而，RGB方法容易受到背景颜色、光亮度和服装等环境噪声的影响，因此基于骨架的方法更受关注，因为它们不受这些噪声的影响。</li><li><strong>GCN方法限制：</strong> 已有的GCN方法存在一些限制，包括不能处理关节点之间的远程关系，以及不能充分考虑每个边的贡献。</li><li><strong>解决方案：</strong> 为解决这些问题，我们提出了HD-GCN方法，通过HD-Graph和A-HA模块解决了这些问题。</li><li><strong>六通道集成：</strong> 我们提出了一种新的六通道集成方法，通过设置三个具有不同质心节点的HD-Graph来提高性能。</li><li><strong>贡献：</strong> 我们的主要贡献总结如下，包括引入HD-Graph、设计A-HA模块以及提高性能。</li></ul><h2 id="详细介绍">详细介绍</h2><p><strong>1. 引言</strong></p><ul><li><p><strong>人体动作识别的背景：</strong> 人体动作识别（HAR）是将视频数据进行分类以识别人类动作类别的任务。这在许多领域中都有应用，如人机交互和虚拟现实。</p></li><li><p><strong>问题的关键：</strong> 传统的基于RGB的方法容易受到环境因素的影响，如背景颜色、光线亮度和服装等。因此，骨架模态成为了研究重点，因为它们不受这些因素的干扰。这些方法通过接收主要人体关节的2D或3D坐标作为时间序列输入来进行动作识别。</p></li><li><p>GCN方法的问题：</p><p>近年来，一些方法采用了图卷积网络（GCN）将人体骨架图应用到卷积层。然而，现有的GCN方法存在问题：</p><ol><li>他们无法充分识别远距离关节点之间的关系。</li><li>他们忽略了每个边的贡献，导致不完全识别哪些边对于每个骨架样本是重要的。</li></ol></li><li><p><strong>研究动机：</strong> 以上问题的存在激发了这项研究的动力，研究人员提出了一种新方法，旨在改进GCN方法以更好地处理骨架数据。</p></li></ul><p><strong>2. 提出的方法</strong></p><ul><li><strong>HD-GCN架构：</strong> 研究人员提出了HD-GCN，这是一种新型的图卷积网络架构，用于骨架动作识别。</li><li><strong>HD-Graph：</strong> HD-GCN使用HD-Graph，这是一种层次分解的图形结构，将每个关节节点分解成多个集合，以提取主要的结构相邻和相距较远的关系。这有助于捕捉具有语义意义的骨架关系。</li><li><strong>A-HA模块：</strong> 为了强调HD-Graph中的关键层次关系集，研究人员引入了一个基于注意力的层次聚合（A-HA）模块。这个模块包括代表性的空间平均池化（RSAP）和层次边缘卷积（H-EdgeConv）。RSAP用于提取代表性节点，并解决了不同节点具有不同数量相邻节点的问题。而H-EdgeConv用于管理分层特征并确定哪些层次边缘集应该在输入特征中突出显示。</li><li><strong>六通道集成：</strong> 文章还介绍了一种新的六通道集成方法，该方法使用了三个HD-Graphs，并且每个图具有不同的质心节点，以提取不同语义空间的特征。</li></ul><p><strong>3. 实验结果</strong></p><ul><li><p><strong>实验数据集：</strong> 研究人员在四个常用的动作识别数据集上进行了广泛的实验评估，这些数据集包括NTU-RGB+D 60、NTU-RGB+D 120、Kinetics-Skeleton和Northwestern-UCLA。</p></li><li><p>主要贡献总结：</p><p>研究人员的主要贡献总结如下：</p><ol><li>提出了HD-Graph来充分识别相同层次节点集之间的重要远程边缘。</li><li>引入了A-HA模块，使用RSAP和H-EdgeConv，以突出显示关键边缘集。</li><li>提出了一种新的六通道集成方法，用于骨架动作识别，可以在没有任何运动数据的情况下提高性能。</li><li>HD-GCN在四个骨架动作识别数据集上均取得了领先的性能。</li></ol></li></ul><h1>8 Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition跨领域三维动作识别的领域共享和目标特定特征聚类协同算法</h1><h2 id="abstract-3">abstract</h2><p>摘要：本研究探讨了罕见的跨领域3D动作识别问题，特别是在开放集设置下。具体来说，存在一个包含不同风格和类别骨骼序列的源域和目标域，我们的目标是利用标记的源数据和未标记的目标数据来对目标数据进行聚类。针对这一具有挑战性的任务，本文提出了一种名为CoDT的新方法，以协作地聚类域共享特征和目标特定特征。CoDT包括两个并行分支。一个分支旨在在源域中使用监督学习学习域共享特征，而另一个分支则在目标域中使用对比学习学习目标特定特征。为了聚类这些特征，我们提出了一种在线聚类算法，可以同时促进稳健的伪标签生成和特征聚类。此外，为了利用域共享特征和目标特定特征的互补性，我们提出了一种新颖的协作聚类策略，以强制执行两个分支之间的成对关系一致性。我们在多个跨领域3D动作识别数据集上进行了大量实验，结果证明了我们方法的有效性。代码可在CoDT中找到。</p><p>要点总结：</p><ul><li>本研究探讨了跨领域3D动作识别问题，即在不同风格和类别的骨骼序列之间进行开放集聚类。</li><li>提出了一种名为CoDT的新方法，用于协作地聚类域共享特征和目标特定特征。</li><li>CoDT包括两个并行分支，一个用于源域中的监督学习，另一个用于目标域中的对比学习，以学习相应的特征。</li><li>引入了在线聚类算法，同时促进伪标签生成和特征聚类，以便对目标数据进行聚类。</li><li>提出了协作聚类策略，以确保两个分支之间的成对关系一致性，充分利用了域共享和目标特定特征的互补性。</li><li>通过在多个跨领域3D动作识别数据集上进行实验证明了方法的有效性。</li></ul><p>以下是文章摘要的要点总结：</p><ol><li>本研究关注跨领域3D动作识别问题，即在不同风格和类别的骨骼序列中进行开放集聚类。</li><li>提出了一种名为CoDT的新方法，用于协作地聚类域共享特征和目标特定特征。</li><li>CoDT包括两个并行分支，一个用于源域中的监督学习，另一个用于目标域中的对比学习，以学习相应的特征。</li><li>引入了在线聚类算法，同时促进伪标签生成和特征聚类，以便对目标数据进行聚类。</li><li>提出了协作聚类策略，以确保两个分支之间的成对关系一致性，充分利用了域共享和目标特定特征的互补性。</li><li>通过在多个跨领域3D动作识别数据集上进行实验证明了方法的有效性</li></ol><h2 id="introduction-3">introduction</h2><p>这篇论文探讨了骨架数据的跨领域三维动作识别问题，特别是在开放集合环境中，这是一个相对较少研究的领域。研究中有一个源领域和一个目标领域，它们包含具有不同风格和类别的骨架序列，研究的目的是利用已标记的源数据和未标记的目标数据对目标数据进行聚类。为了解决这个具有挑战性的任务，论文提出了一种名为CoDT的新方法，用于协同聚类领域共享特征和目标特定特征。CoDT包括两个并行分支，一个分支旨在在源领域进行监督学习，学习领域共享特征，而另一个分支旨在在目标领域进行对比学习，学习目标特定特征。为了对特征进行聚类，论文提出了一种在线聚类算法，可以同时促进稳健伪标签生成和特征聚类。此外，为了利用领域共享特征和目标特定特征的互补性，论文提出了一种新颖的协同聚类策略，以强制执行两个分支之间的成对关系一致性。论文在多个跨领域三维动作识别数据集上进行了大量实验，结果表明了方法的有效性。</p><p>以下是文章介绍部分的详细要点：</p><ol><li>引言：<ul><li>3D深度摄像头和姿势估计算法的最新进展使得快速准确地估计3D骨架成为可能。</li><li>骨架数据只包含人体关键点的坐标，提供了高度抽象和与环境无关的信息。</li><li>3D动作识别引起了越来越多的关注，但现有方法主要集中在传统的监督分类上。</li><li>实际上，源数据和目标数据之间通常存在差异，这种差异被称为领域差距，由于设备、摄像机设置、场景等因素引起。</li><li>论文首次系统研究了骨架数据中的领域差距问题，其中源和目标数据具有不同的风格和类别，这是一个开放集问题，预期算法可以自动将目标样本聚类到潜在类别中。</li></ul></li><li>问题描述：<ul><li>已标记的源数据有助于开发领域共享表示，可以从源领域传递到目标领域，但通常由于领域差距导致模型对目标领域的泛化能力较差。</li><li>目标领域具有许多未标记样本，可以用于学习更多的目标特定表示，揭示目标领域的内在特征。</li><li>论文建议将监督学习和对比学习结合起来，以整合源数据和目标数据的信息。</li></ul></li><li>方法概述：<ul><li>论文提出了CoDT网络，用于协同领域共享和目标特定特征的聚类。</li><li>使用在线聚类算法生成稳健的伪标签，以指导特征聚类。</li><li>引入协同聚类策略，以确保两个分支之间的成对关系一致性。</li></ul></li><li>贡献：<ul><li>提供了CD-SAR任务的基准，提出了一种新的方法CoDT，用于协同领域共享和目标特定特征的聚类。</li><li>提出了一种在线聚类算法，可实现稳健的伪标签生成和特征聚类。</li><li>提出了一种协同聚类算法，可实现两个模型之间的一致性学习。</li><li>在不同的跨领域任务上进行了评估，并证明了方法的有效性。</li></ul></li></ol><h2 id="实例">实例</h2><ol><li><p>引言：</p><ul><li>最近的技术进步使得快速准确地估计3D骨架成为可能。</li><li>骨架数据提供高度抽象和与环境无关的信息，因此引起了3D动作识别的兴趣。</li><li>现有方法主要集中在传统的监督分类上，但实际中很难保持源和目标数据具有相同的分布。</li><li>论文首次系统研究了骨架数据中的领域差距问题，即源和目标数据具有不同的风格和类别，这是一个开放集问题。</li></ul></li><li><p>问题描述：</p><ul><li>源数据和目标数据之间存在领域差距，由于设备、摄像机设置、场景等因素引起。</li><li>论文考虑源数据对开发领域共享表示有帮助，但模型泛化到目标领域通常较差。</li><li>目标领域具有许多未标记样本，可以用于学习目标特定表示。</li><li>论文提出将监督学习和对比学习结合起来，以整合源数据和目标数据的信息。</li></ul></li><li><p>方法概述：</p><ul><li>论文提出了CoDT网络，用于协同领域共享和目标特定特征的聚类。</li><li>引入在线聚类算法生成伪标签，以指导特征聚类。</li><li>提出协同聚类策略，确保两个分支之间的成对关系一致性。</li></ul></li><li><p>贡献：</p><ul><li>提供了CD-SAR任务的基准，提出了一种新的方法CoDT，用于协同领域共享和目标特定特征的聚类。</li><li>引入在线聚类算法，实现稳健的伪标签生成和特征聚类。</li><li>提出协同聚类算法，使两个模型能够共同学习以保持特征的一致性。</li><li>在不同的跨领域任务上进行了评估，证明了方法的有效性。</li></ul></li><li><p><strong>引言</strong>：</p><ul><li>介绍了最近3D深度摄像头和姿势估计算法的进步，这使得能够快速准确地估计3D骨架。</li><li>对比RGB图像，骨架数据只包含人体关键点的坐标，提供了高度抽象和与环境无关的信息。</li><li>引出了3D动作识别的背景和重要性。</li></ul></li><li><p><strong>问题描述</strong>：</p><ul><li>指出了传统的监督学习方法，其中假设标记的源培训数据和未标记的目标测试数据具有相同的分布。这是一个理论上的假设，但在实际情况下很难实现，因为标记目标数据是一项繁重的任务。</li><li>引入了领域差距问题（domain gap），这是因为源和目标数据可能由不同的设备、摄像机设置和场景捕获，导致它们的骨架数据具有不同的风格和行为类别。</li><li>以一个实际的案例说明了领域差异问题，即需要识别来自不同摄像机和姿势估计方法的数据的动作类别。</li></ul></li><li><p><strong>方法概述</strong>：</p><ul><li>论文提出了CoDT（Collaborating Domain-shared and Target-specific features clustering）网络，旨在解决跨领域骨架动作识别问题。</li><li>CoDT包括两个平行分支，一个用于在源域中进行监督学习以学习领域共享特征，另一个用于在目标域中进行对比学习以学习目标特定特征。</li><li>引入了在线聚类算法，用于同时生成健壮的伪标签和进行特征聚类。</li><li>提出了协同聚类策略，以确保两个分支之间的特征聚类在一对一关系上是一致的，尽管它们的聚类方法不同。</li></ul></li><li><p><strong>贡献</strong>：</p><ul><li>提供了一个具有领域差异的骨架动作识别任务的基准，并引入了新的方法CoDT来解决这个问题。</li><li>引入在线聚类算法，以实现稳健的伪标签生成和特征聚类。</li><li>提出协同聚类策略，通过对特征的一对一关系进行共同训练，以确保它们的一致性。</li><li>通过不同的跨领域任务进行了广泛评估，证明了方法的有效性。</li></ul></li></ol><p>这些详细介绍有助于理解文章的主要思想、方法和贡献。文章的核心思想是通过协同领域共享和目标特定特征来解决跨领域骨架动作识别问题，这是一个具有挑战性的任务。</p><h1>9 Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning 基于无监督骨架的动作学习全局局部运动Transformer</h1><h2 id="abstract-4">abstract</h2><p>摘要：我们提出了一种新的变换器模型，用于无监督学习骨骼运动序列。现有的用于无监督骨骼动作学习的变换器模型从相邻帧中学习每个关节的瞬时速度，而没有全局运动信息。因此，该模型在学习整体身体动作和时间上的远距离关节时存在困难。此外，人与人之间的互动也未在模型中考虑。为了解决学习整体身体动作、长时间动态和人际互动的问题，我们设计了一种全局和局部注意机制，其中全局身体动作和局部关节动作互相关注。此外，我们提出了一种新的预训练策略，多区间姿势位移预测，以在不同时间范围内学习全局和局部关注。所提出的模型成功地学习了关节的局部动态并从运动序列中捕捉了全局背景。我们的模型在代表性基准测试中表现优越，超过了最先进的模型很大幅度。代码可在https://github.com/Boeun-Kim/GL-Transformer 上找到。</p><p>要点总结：</p><ul><li>提出了一种新的变换器模型，用于无监督学习骨骼运动序列。</li><li>传统变换器模型难以学习全局运动信息、长时间关节动态和人际互动。</li><li>引入全局和局部关注机制，使全局身体动作和局部关节动作可以相互关注。</li><li>提出了一种多区间姿势位移预测的预训练策略，有助于在不同时间范围内学习全局和局部关注。</li><li>模型成功学习了关节的局部动态并从运动序列中捕捉了全局背景。</li><li>在代表性基准测试中，该模型表现优越，超过了最先进的模型</li></ul><h2 id="总结-4">总结</h2><ul><li>提出了新的变换器模型，用于无监督学习骨骼运动序列。</li><li>传统的变换器模型仅从相邻帧中学习骨骼关节的瞬时速度，缺乏全局运动信息，无法处理全身动作和时间上的远距离关节。</li><li>引入了全局和局部注意机制，以允许全局身体动作和局部关节动作之间相互关注和交互。</li><li>提出了一种多区间姿势位移预测的预训练策略，旨在在不同时间范围内学习全局和局部的关注。</li><li>该模型成功地学习了骨骼关节的局部动态，并能够捕捉骨骼运动序列中的全局背景信息。</li><li>在代表性基准测试中，该模型相较于最先进的模型取得了显著的性能提升。</li><li>项目代码可在https://github.com/Boeun-Kim/GL-Transformer 上找到。</li></ul><h2 id="introduction-4">introduction</h2><p><strong>1. 引言</strong></p><ul><li>近年来，骨架动作识别领域的研究已经转向了无监督学习技术，以避免昂贵和耗时的监督学习注释。</li><li>为了适用于动作识别的无监督预训练，需要学习整个动作序列的全局上下文，以及学习局部关节动态和拓扑。</li><li>然而，现有方法在有效捕捉全局上下文和局部关节动态方面存在局限性。</li></ul><p><strong>2. 现有方法的局限性</strong></p><ul><li>一些方法采用了基于RNN的编码器-解码器模型，但由于长期依赖性问题，这些方法难以提取全局上下文。</li><li>其他方法使用对比学习方案，但性能高度依赖于编码器模型的选择。</li><li>第一个应用于无监督预训练的transformer模型是H-transformer，但其仍然存在学习全局注意力的局限性，因为它只预测每帧关节的瞬时速度。</li></ul><p><strong>3. GL-Transformer：全局和局部注意力机制</strong></p><ul><li>为了解决全局上下文、长期时间动态和人际互动的学习问题，提出了GL-Transformer模型。</li><li>GL-Transformer采用全局和局部注意力（GLA）机制，包括空间多头注意力（spatial-MHA）和时间多头注意力（temporal-MHA）模块。</li><li>空间多头注意力模块执行三种类型的注意力：局部（关节之间）注意力，全局（整体）从/到局部（关节）注意力，以及全局（人际）到全局（人际）注意力。</li><li>时间多头注意力模块在每两帧之间执行全局和局部注意力，适用于每个人的序列。</li></ul><p><strong>4. 新的预训练策略：多区间姿势位移预测（MPDP）</strong></p><ul><li>引入了一种新的预训练策略，称为多区间姿势位移预测（MPDP），以在不同时间范围内学习注意力。</li><li>MPDP通过训练GL-Transformer来同时预测多个关节的多个姿势位移（角度和运动距离）来实现预训练。</li><li>通过MPDP，GL-Transformer可以从较小的时间间隔中学习局部注意力，同时也可以从较大的时间间隔中学习全局注意力。</li></ul><p><strong>5. 实验结果</strong></p><ul><li>在广泛使用的数据集（NTU-60、NTU-120和NW-UCLA）上进行了大量实验评估。</li><li>在线性评估协议中，GL-Transformer的性能明显超过了H-transformer和其他最先进的方法。</li><li>在半监督设置下，我们的方法甚至在性能上超越了最先进的方法。</li></ul><p>这篇文章介绍了GL-Transformer，这是一个新型的transformer模型，用于无监督学习骨架运动序列。GL-Transformer旨在解决全局上下文、长期时间动态和人际互动的学习问题，并引入了多区间姿势位移预测（MPDP）策略来加强预训练。实验结果表明，GL-Transformer在骨架动作识别方面的性能明显优于现有方法。</p><h2 id="总结实力">总结实力</h2><p><strong>1. 引言</strong></p><ul><li>骨架动作识别正在采用无监督学习技术，避免了昂贵且耗时的监督注释。</li><li>为了进行动作识别的无监督预训练，需要同时学习整个动作序列的全局上下文和局部关节动态。</li><li>然而，现有方法难以有效捕获全局上下文和局部关节动态。</li></ul><p><strong>2. 现有方法的局限性</strong></p><ul><li>一些方法使用RNN-based编码器-解码器模型，但由于长期依赖性问题，难以提取全局上下文。</li><li>其他方法使用对比学习方案，但性能高度依赖于编码器模型的选择。</li><li>第一个尝试无监督预训练的transformer模型是H-transformer，但其仍然有全局注意力的限制，因为它只预测每帧关节的瞬时速度。</li></ul><p><strong>3. GL-Transformer：全局和局部注意力机制</strong></p><ul><li>为了解决全局上下文、长期时间动态和人际互动的学习问题，提出了GL-Transformer模型。</li><li>GL-Transformer采用全局和局部注意力（GLA）机制，包括空间多头注意力（spatial-MHA）和时间多头注意力（temporal-MHA）模块。</li><li>空间多头注意力模块执行三种类型的注意力：局部、全局从/到局部以及全局人际注意力。</li><li>时间多头注意力模块用于处理每两帧之间的全局和局部注意力。</li></ul><p><strong>4. 新的预训练策略：多区间姿势位移预测（MPDP）</strong></p><ul><li>引入了MPDP策略，通过多任务学习方式，在不同时间范围内学习关节的位移预测，以增强GL-Transformer的性能。</li><li>MPDP通过训练GL-Transformer来同时预测多个关节的多个姿势位移，从小时间间隔学习局部关节动态，从大时间间隔学习全局上下文。</li></ul><p><strong>5. 实验结果</strong></p><ul><li>在NTU-60、NTU-120和NW-UCLA等广泛使用的数据集上进行了广泛的实验评估。</li><li>在线性评估协议中，GL-Transformer的性能显著超越了H-transformer和其他最先进的方法。</li><li>在半监督设置下，GL-Transformer的性能甚至超越了最先进的方法。</li></ul><p>这篇文章详细介绍了GL-Transformer模型的设计，包括全局和局部注意力机制以及多区间姿势位移预测（MPDP）策略，以解决骨架动作识别中的关键问题。实验证明，GL-Transformer在多个数据集上表现出色，超越了现有方法。</p><h2 id="总结-5">总结</h2><p><strong>1. 引言</strong></p><ul><li><strong>研究背景</strong>：文章开篇介绍了骨架动作识别的研究背景，指出无监督学习方法的重要性，因为传统的监督学习需要昂贵和耗时的标注工作。</li><li><strong>学习目标</strong>：指出在无监督学习中，除了学习局部关节动态和拓扑，还需要学习整个运动序列的全局上下文。</li></ul><p><strong>2. 现有方法的局限性</strong></p><ul><li><strong>RNN-based方法</strong>：介绍现有方法中使用的RNN-based编码器-解码器模型，但强调它们在提取全局上下文方面的困难，因为它们受到长期依赖性问题的限制。</li><li><strong>对比学习方法</strong>：提到一些方法使用对比学习方案，但指出它们高度依赖于编码器模型的选择。</li></ul><p><strong>3. GL-Transformer：全局和局部注意力机制</strong></p><ul><li><strong>GL-Transformer模型</strong>：详细介绍了GL-Transformer模型的设计，包括全局和局部注意力（GLA）机制。</li><li><strong>GLA的组成</strong>：解释GLA机制如何由空间多头注意力（spatial-MHA）和时间多头注意力（temporal-MHA）模块组成。</li><li><strong>GLA的功能</strong>：说明GLA机制的作用，包括局部关节动态的学习、全局上下文的捕获以及人际互动的处理。</li></ul><p><strong>4. 新的预训练策略：多区间姿势位移预测（MPDP）</strong></p><ul><li><strong>MPDP策略</strong>：介绍了MPDP策略，它是一种多任务学习策略，用于训练GL-Transformer以学习不同时间范围内的关节位移预测。</li><li><strong>MPDP的优点</strong>：强调了MPDP策略的优点，它可以同时学习局部关节动态和全局上下文，以提高模型性能。</li></ul><p><strong>5. 实验结果</strong></p><ul><li><strong>实验数据集</strong>：列出了在哪些广泛使用的数据集上进行了实验评估，包括NTU-60、NTU-120和NW-UCLA。</li><li><strong>实验协议</strong>：提到了实验协议，特别是线性评估协议，以及模型性能评估的方法。</li><li><strong>结果总结</strong>：总结了实验结果，强调GL-Transformer在多个数据集上表现卓越，超越了H-transformer和其他最先进的方法。</li></ul><p>这篇文章详细阐述了GL-Transformer模型的设计，以及如何解决骨架动作识别中的全局上下文、长期时间动态和人际互动等关键问题。它还介绍了MPDP策略，该策略有助于提高模型性能，并通过实验证明了GL-Transformer在多个数据集上的卓越表现。</p><h1>10 Zoom Transformer for Skeleton-Based Group Activity Recognition基于骨架的群体活动识别的缩放Transformer</h1><h2 id="abstract-5">abstract</h2><p>摘要：骨架基础的人体动作识别引起了越来越多的关注，已经提出了许多方法来提高性能。然而，这些方法仍然面临三个主要限制：1）侧重于单人动作识别，而忽略了多人（超过5人）的群体活动。实际上，通过骨架数据进行多人群体活动识别也是一个有意义的问题。2）无法从骨架数据中挖掘高级语义信息，例如多人之间的互动和他们的位置关系。3）用于多人群体活动识别的现有数据集都涉及RGB视频，不能直接应用于基于骨架的群体活动分析。为解决这些问题，我们提出了一种新的Zoom Transformer，以统一的模型结构和精心设计的关系感知映射，利用了低级单人运动信息和高级多人互动信息。此外，我们从现有的真实世界视频数据集（如Kinetics和排球活动）中估算了多人骨架，并发布了两个新的基准数据集来验证我们的Zoom Transformer的有效性。大量实验证明，我们的模型能够有效处理基于骨架的多人群体活动。此外，对大规模的NTU-RGB+D数据集的实验验证了我们的模型在单人动作识别方面也取得了显著的性能。代码和骨架数据可在https://github.com/Kebii/Zoom-Transformer 上公开获取。</p><p>要点总结：</p><ol><li>骨架基础的人体动作识别受到广泛关注，但现有方法存在三个主要限制：单人动作、无法挖掘高级语义信息、现有数据集为RGB视频。</li><li>为解决这些问题，提出了Zoom Transformer模型，能够统一处理低级单人运动信息和高级多人互动信息。</li><li>创建了两个新的基准数据集，用于验证Zoom Transformer的有效性。</li><li>大量实验证明Zoom Transformer可以有效处理基于骨架的多人群体活动，并在单人动作识别方面表现出色。</li></ol><h2 id="总结-6">总结</h2><ol><li>骨架基础的人体动作识别是一个备受关注的领域，已经有很多方法来提高性能。</li><li>现有方法存在三个主要限制：<ul><li>侧重于单人动作识别，而忽略了多人（超过5人）的群体活动。</li><li>无法从骨架数据中挖掘高级语义信息，如多人之间的互动和位置关系。</li><li>用于多人群体活动识别的现有数据集都是RGB视频，不能直接应用于基于骨架的群体活动分析。</li></ul></li><li>为解决这些问题，提出了Zoom Transformer模型，该模型能够以统一的模型结构处理低级单人运动信息和高级多人互动信息。</li><li>为验证Zoom Transformer的有效性，从现有的真实世界视频数据集（如Kinetics和排球活动）估算了多人骨架，并创建了两个新的基准数据集。</li><li>大量实验证明，Zoom Transformer可以有效处理基于骨架的多人群体活动，并在单人动作识别方面也表现出色。</li><li>作者提供了代码和骨架数据的公开获取链接。</li></ol><h2 id="introduction-5">introduction</h2><p><strong>1. 引言</strong></p><ul><li>介绍了动作识别在计算机视觉领域的重要性和广泛应用。</li><li>指出了使用视频数据进行动作识别的一些不足，如数据量大、冗余高、存储和传输效率低。</li><li>引入了骨架数据作为一种替代，因其对摄像机视角的变化和背景干扰具有较强的鲁棒性。</li><li>提到了最近提出的骨架数据动作识别方法，强调这些方法的优势。</li></ul><p><strong>2. 不足之处</strong></p><ul><li>指出了现有骨架数据动作识别方法的局限性。</li><li>现有研究主要集中在单人或双人动作识别上，忽略了多人的群体活动。</li><li>强调了多人群体活动识别的重要性，并提出这个问题在骨架数据上尚未得到充分研究。</li><li>指出现有数据集通常只包含最多两个人的骨架数据，不适用于多人群体活动识别。</li><li>引出了一种新的Transformer模型（Zoom Transformer）以及两个骨架数据多人群体活动数据集（K-SA和V-SA）的提出，旨在解决这些问题。</li></ul><p><strong>3. Zoom Transformer</strong></p><ul><li>介绍了Zoom Transformer模型，它包括Zoom-in Transformer（ZiT）和Zoom-out Transformer（ZoT）两个部分。</li><li>ZiT部分用于提取单个人的骨架动作信息，其中使用了多头关系感知注意力和多尺度时间卷积。</li><li>ZoT部分则用于处理多人群体的骨架数据，它通过关系感知注意力捕获了多人之间的时空交互信息。</li><li>引入了关系感知注意力机制，以充分利用人体结构和全局动作特征来提取多层次的群体活动特征。</li></ul><p><strong>4. 骨架数据多人群体活动数据集</strong></p><ul><li>介绍了两个新的骨架数据多人群体活动数据集：K-SA和V-SA。</li><li>K-SA包含从Kinetics视频数据中提取的最多5个人的骨架数据，用于验证Zoom Transformer模型的效果。</li><li>V-SA包含从Volleyball Activity视频数据中估算的最多12个人的骨架数据，同样用于评估Zoom Transformer模型的性能。</li><li>指出这些数据集的骨架数据质量有限，存在噪音，使得群体活动识别更加具有挑战性，更符合实际情况。</li></ul><p><strong>5. 主要贡献</strong></p><ul><li>扩展了骨架数据动作识别领域，将其应用于多人群体活动，具有广泛的实际应用前景。</li><li>引入Zoom Transformer模型，有效地提取了单人和多人群体活动的特征信息。</li><li>设计并发布了两个新的骨架数据多人群体活动数据集，以验证提出的模型的有效性，并促进未来的研究。</li></ul><p>文章的主要贡献在于提出了一个新的Transformer模型和两个骨架数据多人群体活动数据集，以弥补现有骨架动作识别方法的不足，并促进了多人群体活动识别的研究和应用。</p><h2 id="总结-7">总结</h2><p><strong>1. 引言</strong></p><ul><li>介绍了计算机视觉中动作识别的重要性和应用领域。</li><li>指出视频数据在动作识别中存在的问题，如数据量大、冗余高、存储和传输效率低。</li><li>引入骨架数据作为一种替代，因其对摄像机视角的变化和背景干扰具有鲁棒性。</li><li>提到了最近提出的骨架数据动作识别方法，强调这些方法的优势。</li></ul><p><strong>2. 不足之处</strong></p><ul><li>现有骨架数据动作识别方法的局限性。</li><li>现有研究主要集中在单人或双人动作识别上，忽略了多人的群体活动。</li><li>强调了多人群体活动识别的重要性，并提出这个问题在骨架数据上尚未得到充分研究。</li><li>现有数据集通常只包含最多两个人的骨架数据，不适用于多人群体活动识别。</li><li>引出了一种新的Transformer模型（Zoom Transformer）以及两个骨架数据多人群体活动数据集（K-SA和V-SA）的提出，旨在解决这些问题。</li></ul><p><strong>3. Zoom Transformer</strong></p><ul><li>介绍了Zoom Transformer模型，它包括Zoom-in Transformer（ZiT）和Zoom-out Transformer（ZoT）两个部分。</li><li>ZiT部分用于提取单个人的骨架动作信息，其中使用了多头关系感知注意力和多尺度时间卷积。</li><li>ZoT部分则用于处理多人群体的骨架数据，它通过关系感知注意力捕获了多人之间的时空交互信息。</li><li>引入了关系感知注意力机制，以充分利用人体结构和全局动作特征来提取多层次的群体活动特征。</li></ul><p><strong>4. 骨架数据多人群体活动数据集</strong></p><ul><li>介绍了两个新的骨架数据多人群体活动数据集：K-SA和V-SA。</li><li>K-SA包含从Kinetics视频数据中提取的最多5个人的骨架数据，用于验证Zoom Transformer模型的效果。</li><li>V-SA包含从Volleyball Activity视频数据中估算的最多12个人的骨架数据，同样用于评估Zoom Transformer模型的性能。</li><li>指出这些数据集的骨架数据质量有限，存在噪音，使得群体活动识别更加具有挑战性，更符合实际情况。</li></ul><p><strong>5. 主要贡献</strong></p><ul><li>扩展了骨架数据动作识别领域，将其应用于多人群体活动，具有广泛的实际应用前景。</li><li>引入Zoom Transformer模型，有效地提取了单人和多人群体活动的特征信息。</li><li>设计并发布了两个新的骨架数据多人群体活动数据集，以验证提出的模型的有效性，并促进未来的研究。</li></ul><h2 id="实例-2">实例</h2><p><strong>1. 引言</strong></p><ul><li>介绍了计算机视觉领域中的动作识别问题，以及该问题在多个应用领域中的重要性，如人类行为分析、智能视频监控、人机交互等。</li><li>指出了视频数据在动作识别中的不足，包括数据量大、冗余高、存储和传输低效。</li><li>引入了骨架数据的概念，强调骨架数据对摄像机视角变化和背景干扰的鲁棒性以及其适用于长期存储和快速传输的特点。</li><li>提到最近的骨架数据动作识别方法，这些方法专注于提取骨架序列的时空运动信息，探索骨架图的物理结构，并显著提高了动作分类的准确性。</li></ul><p><strong>2. 不足之处</strong></p><ul><li>阐述了现有骨架数据动作识别方法的限制：<ul><li>多数研究集中在单人或双人动作识别上，而忽略了多人参与的群体活动。然而，在实际视频中，人类活动通常包括多人的独立动作、位置关系和相互作用，形成了复杂的群体活动。</li><li>多人群体活动识别与单人动作识别同样重要，但在骨架数据上尚未得到足够研究。对于视频监控等应用，快速识别人群斗殴或拥挤踩踏等群体活动是及时采取行动的关键。</li><li>现有数据集通常只包含最多两个人的骨架数据，不适用于多人群体活动识别。而且，用于多人群体活动识别的现有数据集只包括RGB视频数据，不能直接应用于骨架数据的分析。</li></ul></li></ul><p><strong>3. Zoom Transformer</strong></p><ul><li>介绍了Zoom Transformer模型，该模型由Zoom-in Transformer（ZiT）和Zoom-out Transformer（ZoT）两部分组成。</li><li>ZiT部分用于提取单个人的骨架动作信息，使用多头关系感知注意力和多尺度时间卷积来学习单个人的运动特征。</li><li>ZoT部分用于处理多人群体的骨架数据，通过关系感知注意力来捕获多人之间的时空交互信息。</li><li>引入了关系感知注意力机制，充分利用人体结构和全局动作特征，提取多层次的群体活动特征。</li></ul><p><strong>4. 骨架数据多人群体活动数据集</strong></p><ul><li>详细描述了两个新的骨架数据多人群体活动数据集：K-SA和V-SA。</li><li>K-SA数据集包含从Kinetics视频数据中提取的最多5个人的骨架数据，用于验证Zoom Transformer模型的效果。</li><li>V-SA数据集估算了最多12个人的骨架数据，来源于流行的Volleyball Activity视频数据。</li><li>指出这些数据集的骨架数据质量有限，存在噪音，使得群体活动识别更具挑战性，更符合实际场景。</li></ul><p><strong>5. 主要贡献</strong></p><ul><li>总结了研究的三个主要贡献：<ul><li>将骨架数据动作识别扩展到多人群体活动识别领域，具有广泛的实际应用前景。</li><li>探索Zoom Transformer模型，通过Zoom-in和Zoom-out Transformer有效地提取单人和多人群体活动的特征信息。</li><li>设计并公开了两个新的骨架数据多人群体活动数据集，K-SA和V-SA，以验证提出的模型的有效性并促进未来的研究。</li></ul></li></ul><p>这些详细的介绍可以帮助您更深入地理解文章的内容和贡献。</p><h1>11 Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition基于骨架的动作识别时空元组Transformer</h1><h2 id="abstract-6">abstract</h2><p>Abstract: Capturing the dependencies between joints is critical in skeleton-<br>based action recognition task. Transformer shows great potential to model the<br>correlation of important joints. However, the existing Transformer-based meth-<br>ods cannot capture the correlation of different joints between frames, which the<br>correlation is very useful since different body parts (such as the arms and legs<br>in ”long jump”) between adjacent frames move together. Focus on this prob-<br>lem, A novel spatio-temporal tuples Transformer (STTFormer) method is pro-<br>posed. The skeleton sequence is divided into several parts, and several consecu-<br>tive frames contained in each part are encoded. And then a spatio-temporal tuples<br>self-attention module is proposed to capture the relationship of different joints in<br>consecutive frames. In addition, a feature aggregation module is introduced be-<br>tween non-adjacent frames to enhance the ability to distinguish similar actions.<br>Compared with the state-of-the-art methods, our method achieves better perfor-<br>mance on two large-scale datasets.</p><p>摘要：在基于骨骼的动作识别任务中，捕捉关节之间的依赖关系至关重要。Transformer模型显示出模拟重要关节之间的相关性的巨大潜力。然而，现有的基于Transformer的方法不能捕捉帧与帧之间不同关节的相关性，而这种相关性非常有用，因为相邻帧之间的不同身体部位（例如&quot;跳远&quot;中的手臂和腿部）会一起移动。针对这个问题，提出了一种新颖的时空元组Transformer（STTFormer）方法。骨骼序列被分成若干部分，每部分包含若干连续帧，然后对每部分进行编码。接着，引入了一个时空元组自注意力模块，用于捕捉相邻帧中不同关节的关系。此外，还引入了一个特征聚合模块，用于增强区分相似动作的能力。与最先进的方法相比，我们的方法在两个大规模数据集上实现了更好的性能。</p><p>要点总结：</p><ul><li>在骨骼动作识别任务中，关节之间的依赖关系至关重要。</li><li>Transformer模型具有捕捉关节之间相关性的潜力，但现有方法不能捕捉帧与帧之间不同关节的相关性。</li><li>提出了STTFormer方法，将骨骼序列分成部分，然后引入时空元组自注意力模块来捕捉不同关节之间的关系。</li><li>引入了特征聚合模块，用于增强区分相似动作的能力。</li><li>在两个大规模数据集上，STTFormer方法表现出比最先进方法更好的性能。</li></ul><h2 id="总结-8">总结</h2><ol><li><strong>背景和问题</strong>：在骨骼动作识别任务中，捕捉不同关节之间的依赖关系对于准确性很重要。Transformer模型在此任务中显示出巨大潜力，但现有的Transformer方法无法捕捉帧与帧之间不同关节的相关性，这在某些情况下非常有用。</li><li><strong>方法介绍</strong>：提出了一种新颖的方法，即时空元组Transformer（STTFormer）。该方法首先将骨骼序列分成若干部分，并对每个部分中的连续帧进行编码。然后引入了一个特殊的自注意力模块，用于捕捉相邻帧中不同关节之间的关系。</li><li><strong>特征聚合</strong>：此方法还引入了一个特征聚合模块，用于增强模型区分相似动作的能力。这有助于提高模型的性能。</li><li><strong>性能优势</strong>：与目前最先进的方法相比，STTFormer在两个大规模数据集上实现了更好的性能。这表明该方法在骨骼动作识别中具有潜力。</li></ol><p>这些要点总结了文章的主要贡献和方法，强调了STTFormer方法在解决骨骼动作识别中的问题时的有效性。</p><h2 id="introduction-6">introduction</h2><p><strong>1. 引言</strong></p><ul><li>骨架数据在动作识别领域引起了广泛关注，因为其紧凑的数据表示使模型更加高效且对复杂背景、光照条件、视点变化和其他环境噪声更加鲁棒。</li><li>深度学习网络通常用于从骨架数据中提取特征，但现有的方法存在限制，因为它们只能捕捉到短程关联。</li><li>基于Transformer的方法可以更好地捕捉骨架数据中的关联信息，但如何有效地使用Transformer来建模时空相关性非常关键。</li></ul><p><strong>2. 问题和挑战</strong></p><ul><li>传统的骨架动作识别方法通常处理整个骨架序列，无法区分不同帧之间的关联。</li><li>过去的Transformer方法也未充分解决这个问题，它们在捕捉帧内和帧间关联方面仍然存在不足。</li></ul><p><strong>3. 提出的方法</strong></p><ul><li>作者提出了一种新的时空元组Transformer（STTFormer）模型，用于骨架动作识别。</li><li>STTFormer将骨架序列划分为多个不重叠的部分，每个部分称为“元组”，包含连续的帧。</li><li>然后，使用时空元组自注意（STTA）模块来简单而有效地提取每个短序列中不同关节的相关特征。</li><li>此方法不仅捕捉了相邻帧之间不同关节的关联，还在计算成本上几乎没有增加。</li><li>此外，还引入了跨帧特征聚合（IFFA）模块，以帮助区分相似的动作。</li></ul><p><strong>4. 实验和结果</strong></p><ul><li>在两个具有挑战性的基准测试中进行了广泛的实验，通过消融实验验证了模型各部分的有效性，并且模型的性能优于现有的最先进方法。</li></ul><p><strong>5. 结论</strong></p><ul><li>本研究的主要贡献在于提出了一种能够捕捉不同关节在相邻帧之间的关联信息的STTFormer模型，为骨架动作识别提供了新的方法。</li><li>实验结果表明，该模型在两个基准测试上的性能明显优于现有方法。</li></ul><p>这篇文章介绍了一种用于骨架动作识别的新型Transformer模型，该模型能够有效地捕捉不同关节在相邻帧之间的相关性，从而提高了动作识别的性能。作者通过实验验证了模型的有效性，取得了令人满意的结果。</p><h2 id="详细介绍-2">详细介绍</h2><p><strong>1. 引言</strong></p><p>文章引言了骨架数据在动作识别中的重要性。骨架数据以其紧凑的形式在处理复杂背景、光照变化、视角变化等方面表现出色。此外，随着成本效益高的深度摄像头和人体姿态估计方法的发展，获取人体骨架信息变得更加容易。</p><p><strong>2. 问题和挑战</strong></p><p>在介绍部分，文章明确了传统方法和现有深度学习网络的不足之处。传统方法主要问题在于它们无法很好地处理骨架序列中不同帧之间的关联。深度学习网络如RNN仅能捕捉到短程相关性，而CNN只能进行局部操作。而GCN依赖于骨架的图拓扑结构，无法充分利用未连接关节之间的关系。</p><p><strong>3. 提出的方法</strong></p><p>在这一部分，文章详细介绍了作者提出的STTFormer模型。该模型通过将骨架序列分割成多个不重叠的部分，每个部分称为“元组”。每个元组都包含一系列连续的帧。这样的划分使得不同帧之间的关节之间的关联更加明显。</p><p>作者还介绍了STTFormer中的关键组成部分，包括：</p><ul><li><strong>时空元组自注意（STTA）模块</strong>：这个模块用于在每个短序列内部捕捉不同关节之间的相关特征。这有助于模型理解不同帧之间的关系。</li><li><strong>跨帧特征聚合（IFFA）模块</strong>：该模块有助于在时间维度上聚合不同元组中的特征。如果将元组视为子动作，IFFA可以看作是整合一系列子动作的过程。</li></ul><p><strong>4. 实验和结果</strong></p><p>在这一部分，文章讨论了作者进行的实验以及实验结果。作者在两个具有挑战性的基准测试上进行了广泛的实验，以验证STTFormer模型的有效性。实验结果表明，STTFormer的性能优于现有方法。</p><p><strong>5. 结论</strong></p><p>文章总结了STTFormer模型的主要贡献和优势。它能够有效地捕捉不同关节在相邻帧之间的相关性，从而提高了动作识别性能。这一模型的引入对骨架动作识别领域具有重要意义。</p><h1><strong>12Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition基于骨骼的动作识别的局部和全局时空Transformer</strong></h1><p>摘要尽管变形器在各种视觉任务中取得了很大的进展，但基于骨骼的 动作识别仍有待探索，只有很少的尝试。此外，这些方法在空间和时 间维度上均直接计算所有关节的成对全局自注意，低估了判别局部关 节和短期时间动态的影响。本文提出了一种新型的焦域和全局时空变 流器网络(FG- STFormer)，该网络由两个关键部件组成:</p><p>(1)FG- sformer: 焦域节点和全局部件耦合的空间变流器。它迫使网络分别关注学习到 的判别空间关节和人体部位的建模相关性。选择性焦结消除了非信息 焦结在相关累积过程中的负面影响。同时，引入关节与身体部位之间 的相互作用，通过相互交叉注意增强空间依赖性。</p><p>(2) FG-TFormer:聚 焦和全局时间转换器。将扩展时间卷积集成到全局自注意机制中，明 确地捕获关节或身体部位的局部时间运动模式，这对时间变换器的工 作至关重要。在NTU-60、NTU-120 和NW-UCLA三个基准上的大量 实验结果表明，我们的FG-STFormer 超越了现有的所有基于变压器的 方法，并与最先进的基于gcn的方法进行了比较。</p><p><img src="../assets/image-20231030144053157.png" alt="image-20231030144053157"></p><p>近年来，基于骨架的动作识别引起了越来越多的关注，这是因为出现了像微软Kinect这样成本效益高的深度相机以及先进的姿势估计技术，这些技术使骨架数据更准确和易于获取。通过将动作表示为人体关节坐标的序列，高度抽象的骨架数据具有紧凑性和鲁棒性，不受光照、人体外貌变化和背景噪音的影响。</p><p>为了有效地建模关节的时空相关性和动态特性对于骨架序列中的动作识别至关重要。近年来，主要的解决方案是图卷积网络（GCNs），因为它们能够建模人体骨架的不规则拓扑。通过设计高级图拓扑或遍历规则，基于GCN的方法极大地提高了识别性能。同时，Transformer最近的成功在各种计算机视觉任务中引起了广泛的兴趣和性能提升。对于基于骨架的动作识别，人们期望Transformer中的自注意机制应该自然地捕获关节在空间和时间维度上的有效相关性，而不像GCN那样强制规定人体的关节约束。然而，目前只有少数基于Transformer的尝试，并且它们通常设计为GCN和Transformer的混合模型或多任务学习框架。如何利用自注意机制来学习关节的有效时空关系和代表性运动特征仍然是一个棘手的问题。此外，大多数基于Transformer的方法直接计算了关节在空间和时间维度上的全局一对一关系。这种策略低估了具有辨别能力的局部关节之间的空间相互作用以及用于识别关键动作相关模式的短期时间动态。另一方面，由于并非所有关节对于识别动作都有信息价值，这些方法受到与它们通过注意机制积累相关性的无关或嘈杂关节的影响，这可能会损害识别。此外，由于纯Transformer缺乏捕捉时间结构数据的局部性的归纳偏差，因此这些方法很难直接在长输入序列上全局地建模关节的有效时空关系。</p><p>为了解决这些问题，研究提出了一种新的端到端的Focal and Global Spatial-Temporal Transformer网络，称为FG-STFormer，以有效地捕获关键局部关节和空间和时间维度上的全局背景信息之间的关系，用于骨架动作识别。它由两个部分组成：FG-SFormer和FG-TFormer。在网络的后期阶段，通过自适应采样每种动作的信息空间局部关节（焦点关节），并强制网络通过多头自注意力来模拟它们之间的相关性，而不涉及无关的关节。与此同时，为了弥补全局运动和空间结构信息的缺失，通过自注意机制结合了人体部位之间的依赖关系。此外，通过相互交叉关注明确建模了身体部位和焦点关节之间的互动，以增强它们的空间协作。所有这些都是通过FG-SFormer实现的。</p><p>总之，这项工作的贡献有四个方面：</p><ol><li>我们提出了一种新颖的FG-STFormer网络，用于骨架动作识别，可以有效地捕获关键局部关节的相关性以及空间和时间维度中的全局背景运动信息。</li><li>我们设计了焦点关节和全局部位耦合的空间变换器，即FG-SFormer，用于模拟自适应选择的关键局部关节和人体部位之间的相关性，同时集成了关节部位的相互交叉关注以增强空间协作。</li><li>我们引入了FG-TFormer，以明确地捕获关节和身体部位的短期和长期时间依赖关系。</li><li>在三个数据集上的广泛实验结果突显了我们的方法的有效性，超越了所有现有的基于Transformer的方法。</li></ol><hr><p>FG-TFormer被设计用于建模关节或身体部位的时间动态。研究发现，直接使用传统的时间Transformer会导致时间关系不明显和识别性能差的问题。其中一个关键原因是缺乏局部偏差，这使得Transformer难以专注于长输入中的有效时间运动模式。考虑到这些因素，将扩展的时间卷积集成到多头自注意机制中，明确地对关节或部位的短期时间运动进行编码，从而为Transformer提供了局部归纳偏差。所有帧的短程特征表示进一步由全局自注意权重进行融合，以将全局上下文运动信息融入表示中。因此，这一设计策略使Transformer能够在统一结构中学习关节和人体部位的重要局部和有效全局时间关系，这对于使时间Transformer发挥作用至关重要。</p><p>总结一下，这项工作的贡献有四个方面：</p><ol><li>我们提出了一种新颖的FG-STFormer网络，用于基于骨架的动作识别，能够有效地捕捉关键局部关节以及空间和时间维度中的全局上下文运动信息之间的区别性相关性。</li><li>我们设计了焦点关节和全局部位耦合的空间变换器，即FG-SFormer，用于模拟自适应选择的焦点关节和人体部位之间的关系。关节部位的相互交叉关注被整合到其中，以增强空间协作。</li><li>我们引入了FG-TFormer，明确地捕捉了关节和身体部位的短期和长期时间依赖关系。</li><li>在三个数据集上进行的大量实验证明了我们的方法的有效性，超越了所有现有基于Transformer的方法。</li></ol><hr>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读5</title>
      <link href="/project/posts/b8cef9c9.html"/>
      <url>/project/posts/b8cef9c9.html</url>
      
        <content type="html"><![CDATA[<h1>Self-Adaptive Graph With Nonlocal Attention Network for Skeleton-Based Action Recognition基于非局部注意力网络的自适应图骨架动作识别</h1><h2 id="abstract">abstract</h2><p>这篇文章介绍了一个名为SAGGAN的创新性的空间-时间模型，旨在解决图卷积网络（GCNs）在建模人体骨骼时存在的问题。传统方法存在两个固有缺陷：首先，这些模型基于人体的物理结构处理输入数据，导致一些潜在的关节之间的相关性被忽略。其次，它们忽视了非相邻帧之间的关键时间关系，无法充分学习骨骼关节沿时间维度的变化。为了解决这些问题，文章提出了SAGGAN模型，包括自适应GCN（SAGCN）和全局注意力网络。具体来说，SAGCN模块构建了两个额外的动态拓扑图，分别用于学习所有数据的共同特征和为每个样本表示唯一模式。同时，全局注意力模块（空间注意力和时间注意力模块）用于提取单帧中不同关节之间的全局连接，并建模相邻和非相邻帧之间的时间关系。通过这种方式，网络能够捕捉更丰富的动作特征，实现准确的动作识别，克服了标准图卷积的缺陷。在三个基准数据集上的广泛实验证明了我们提出的方法的卓越性能。</p><p>总结：</p><ul><li><p>传统的GCN方法在建模人体骨骼时存在问题，忽略了关节之间的潜在关联和时间关系。</p></li><li><p>SAGGAN引入了SAGCN模块和全局注意力网络，以解决这些问题。</p></li><li><p>SAGCN构建了动态拓扑图，学习了所有数据的共同特征和每个样本的唯一模式。</p></li><li><p>全局注意力模块用于提取单帧中不同关节之间的全局连接和建模时间关系。</p></li><li><p>实验证明，SAGGAN在动作识别方面具有卓越性能。</p></li><li><p>传统图卷积网络（GCNs）在建模人体骨骼时存在问题，因为它们忽略了骨骼关节之间的潜在相关性和<strong>非相邻帧之间的时间关系</strong>。</p></li><li><p>为了解决这些问题，文章提出了SAGGAN，一种创新的空间-时间模型。</p></li><li><p>SAGGAN包括两个主要组成部分：SAGCN模块和全局注意力网络。</p></li><li><p>SAGCN模块用于构建两个额外的动态拓扑图，一个用于学习所有数据的共同特征，另一个用于为每个样本表示唯一的模式。</p></li><li><p>全局注意力网络包括空间注意力和时间注意力模块，用于提取单帧中不同关节之间的全局连接以及建模相邻和非相邻帧之间的时间关系。</p></li><li><p>SAGGAN的设计旨在捕捉更丰富的动作特征，以实现准确的动作识别，并克服标准图卷积的缺陷。</p></li><li><p>文章通过在三个基准数据集上进行广泛的实验，展示了SAGGAN方法的卓越性能。</p></li></ul><h2 id="Introduction">Introduction</h2><h3 id="翻译">翻译</h3><ol><li><strong>背景介绍</strong>：人类动作识别经过多年的深入研究，因其在智能安全、医疗辅助和人机交互等广泛应用领域而备受关注。准确的动作识别的主要挑战在于如何有效地提取独特且丰富的特征，以描述动作的空间和时间动态。</li><li><strong>骨骼数据的优势</strong>：相比之下，以往基于RGB的方法侧重于从整个图像帧中提取基于像素的外观和运动特征。然而，这些方法容易受到图像背景和视角的影响，导致动作信息不够精确。相反，骨骼数据作为对人体动作的简洁且高层次的抽象，在视角或外观上是不变的。此外，人类骨骼具有图形结构，因此在骨骼数据的动作识别任务中应用图卷积网络（GCNs）是自然而然的选择。</li><li><strong>ST-GCN的不足</strong>：自然-时空GCN（ST-GCN）[6]是一种先驱网络，用于根据关节之间的自然连接和相邻帧之间的时间边来建模骨骼数据。随后，一些ST-GCN的变种也采用了这一策略。通过这种方式，人体始终被表示为一个针对所有动作固定的预定义图形，忽略了非邻接关节之间的某些隐含关系。例如，在“鼓掌”和“摸鼻子”动作中，两只手之间存在连接，但对于“摸鼻子”动作，两只手之间的连接不存在。通过这一观察，有人认为固定的图形结构可能不适用于所有动作。此外，现有方法主要关注如何捕获相邻关节之间的局部关系，而忽略了非相邻关节之间的全局关系，这在动作识别中也起着关键作用。以“鼓掌”动作为例，动作“鼓掌”中的两只手不是直接连接的，但它们之间的互动实际上对于识别动作非常有用。</li><li><strong>时间动态特征的挑战</strong>：为了解决上述挑战，我们提出了一种创新框架，称为自适应图卷积与全局关注网络（SAGGAN）。我们的网络在时间和空间维度上均有操作。在时间维度上，我们利用时间卷积网络（TCN）模块提取低级时间特征，并引入了时间关注（TA）模块，以自动构建跨不同帧的远程关系。在空间维度上，它包含自适应GCN（SAGCN）模块和空间关注（SA）模块。具体而言，SAGCN模块用于基于三种图形捕获每一帧中的关系。一种是表示人体物理结构通用模式的固定图，另外两种是应用于学习所有数据的共同特征和分别表示每个样本的动态图。虽然动态图包含丰富的信息，但它仍然面临捕获远程关系的挑战和由于层叠过多导致的平滑过度问题。因此，我们引入了SA模块以增强局部关节之间的连接，并独立评估每一帧中任意关节对之间的关系，这可以使模型对各种数据样本更具通用性。</li><li><strong>主要贡献</strong>：我们工作的主要贡献可总结如下。<ul><li>我们提出了一种名为SAGGAN的创新框架，用于描述包含时间和空间维度中一些附加的非物理连接的时空特征。</li><li>我们引入了SAGCN模块，以自适应地捕获所有数据的共同特征和每个样本的独特模式，这可以为动作识别提供更丰富和有用的信息。</li><li>我们设计了全局关注模块，以捕获任意关节对之间的连接，包括SA模块和TA模块。SA模块可以有效避免平滑过度，并在单帧中捕获远程连接。TA模块可以获取时间序列中的判别性特征。</li><li>我们在三个基准数据集上进行了广泛的实验，以评估我们提出的动作识别任务技术。</li></ul></li></ol><h3 id="总结">总结</h3><ol><li><strong>背景介绍</strong>：动作识别在智能安全、医疗辅助和人机交互等领域有广泛的应用。关键挑战是如何有效提取具有差异性和丰富性的特征，以描述动作的时空动态。</li><li><strong>骨骼数据的优势</strong>：与基于RGB的方法相比，骨骼数据更适合动作识别，因为它不受视角或外观的影响，同时以图形形式表示，适合应用图卷积网络（GCNs）。</li><li><strong>ST-GCN的不足</strong>：ST-GCN等现有方法采用固定的图结构表示人体，忽略了一些非邻接关节之间的潜在关系。同时，它们主要关注如何捕捉邻接关节之间的局部关系，而忽略了非邻接关节之间的全局关系。</li><li><strong>时间动态特征的挑战</strong>：以前的方法使用2D卷积来获取骨骼序列中相邻帧之间的时间关联，但2D卷积受卷积核大小的限制，难以捕捉全局时间信息。此外，长序列可能导致梯度消失或梯度爆炸问题。</li><li><strong>SAGGAN框架</strong>：为了解决上述挑战，提出了自适应图卷积与全局关注网络（SAGGAN）框架。该框架在时空维度上进行操作，包括使用时间卷积网络（TCN）模块提取低级时间特征，引入时间关注（TA）模块自动构建跨不同帧的远程关系，以及使用自适应GCN（SAGCN）模块和空间关注（SA）模块捕获关节之间的关系。</li><li><strong>主要贡献</strong>：SAGGAN的主要贡献包括引入了SAGCN模块来自适应捕获所有数据的共同特征和每个样本的独特模式，设计了全局关注模块以捕获任意关节对之间的关系，并在三个基准数据集上进行了广泛的实验验证。</li></ol><p>这些要点概括了介绍部分的主要内容，突出了作者的研究动机、挑战、方法和贡献。</p><h2 id="related-work">related work</h2><h3 id="图神经网络">图神经网络</h3><ol><li><strong>神经网络在图上的应用</strong>：<ul><li>卷积神经网络（CNNs）在欧几里得数据，如图像和视频上取得了巨大成功。</li><li>但是，它们无法直接处理图数据，需要将图数据转换为伪图像数据，这可能会丢失图数据中隐藏的有用信息。</li><li>递归神经网络（RNNs）在处理时间序列方面具有更多优势。它们将图数据转换为包含一个帧中所有关节位置信息的向量序列，但不能感知人体骨骼的空间相关性。因此，保留图数据结构可以探索更多隐藏信息。</li></ul></li><li><strong>图神经网络（GNNs）</strong>：<ul><li>图神经网络在从图结构数据中提取特征方面取得了巨大进展。</li><li>提出的GNNs可以分为两大类：频谱GNNs和空间GNNs。</li><li>频谱GNNs将输入图形转换为图傅里叶域，并使用一系列滤波器。但频谱GNNs需要将整个图形加载到内存中，处理大型图形时效率低下。</li><li>相反，空间GNNs通常使用一些函数为每个节点完成逐层更新，如邻域函数、聚合函数和激活变换。</li><li>邻域函数用于选择邻居，如相邻节点。聚合函数用于合并所选邻居和自身的特征，如均值池化。</li><li>激活变换将非线性激活函数（如ReLU或softmax）应用于聚合特征向量。</li></ul></li><li><strong>基于GCN的方法</strong>：<ul><li>在不同的GNN变种中，处理图数据的最有效方法是基于GCN的方法。这些方法将CNNs推广为处理具有任意结构和形状的图形。</li><li>为了捕获来自非局部邻居的特征，一些多尺度GCNs被提出。</li><li>在一些研究中，他们使用高阶邻接矩阵来获取远距离节点之间的关系。</li><li>为了更好地表示GCN，引入了图注意力网络（GAT）的注意力机制。GAT从所有输入中选择相对关键的信息。</li><li>在此启发下，[30]在空间和时间维度上都使用了自注意力机制。我们的工作基于空间视角方法进行了一些改进。</li></ul></li></ol><p>这段文字介绍了神经网络在处理图数据时所面临的挑战，以及图神经网络（GNNs）在处理图结构数据中的进展，特别是基于GCN的方法的有效性。此外，它提到了一些关于多尺度GCNs和图注意力网络（GAT）的方法，以及自注意力机制在空间和时间维度上的应用。作者指出他们的工作是在空间角度上对这些方法进行改进的。</p><h2 id="method">method</h2><h3 id="GNN机制">GNN机制</h3><ol><li><strong>GCN建立空间图的过程</strong>：使用GCN建立空间图的过程是通过使用边信息来聚合节点信息，以生成新的节点表示。人体关节被表示为由编码在邻接矩阵A ∈ Rn×n中的边连接的图节点。输入的骨骼序列被表示为Xin ∈ RCin×T×N，其中Cin是输入骨骼序列的通道数，T是序列的长度，N是每帧中的节点数。每个向量Xt in = {vt1, vt2, . . . , vtn}表示在时间t时的n个人体关节的坐标。</li><li><strong>基本的空间图卷积操作</strong>：在第l层上对骨骼序列进行的基本空间图卷积操作可以表示为Xl+1 = W * Xl * A（式1），其中W和A分别是权重矩阵和邻接矩阵。</li><li><strong>连接强度的差异</strong>：尽管人们在执行动作时多个关节一起移动，但每个关节与其他关节之间的连接强度是不同的。为了探索这些连接强度的差异，ST-GCN实施了可学习的重要性加权。具体来说，每个邻接矩阵对应于一个可学习的权重矩阵M，它是一个N × N矩阵，表示每个顶点的重要性。学习过程可以表示如下（式2）：<ul><li>fout表示输出特征映射，fin表示输入特征映射。</li><li>Kv表示应用于ST-GCN的分区策略。</li><li>Ak是用于提取特定子集中连接节点的邻接矩阵。</li><li>Mk确定连接的强度。</li><li>⊗表示两个矩阵之间的逐元素乘法运算。</li><li>Ak中的每个元素的值为0或1，Mk的所有元素都初始化为1。</li><li>Wk表示权重矩阵，起到与卷积核相同的作用。</li></ul></li></ol><p>在这里，GCN（图卷积网络）被用于处理骨骼序列数据，其中邻接矩阵和权重矩阵用于建模关节之间的连接和权重。这有助于捕获动作中不同关节之间的关系和强度差异。这种方法允许模型自动学习关节之间的连接强度，以更好地理解动作。</p><h3 id="Transformer机制">Transformer机制</h3><p>以下是段落的逐句翻译和Transformer机制的解释：</p><ol><li><strong>Transformer：</strong> Transformer自注意力机制的主要思想是基于短距离和长距离的相关性来丰富单词的嵌入，最初设计用于自然语言处理（NLP）任务。具体来说，输入序列通过三个可训练的线性变换（Wq，Wk和Wv）编码为三个矩阵，即查询矩阵q ∈ Rdq，键矩阵k ∈ Rdk和值矩阵v ∈ Rdv。然后，通过查询矩阵和键矩阵之间的点积来计算分数，以表示单词j对单词i的注意力。<ul><li>公式：αi j = qi · kTj，其中qi表示单词i的查询矩阵，kj表示单词j的键矩阵，n是单词的总数。</li></ul></li><li><strong>缩放点积注意力：</strong> 单词i的最终嵌入是通过缩放点积注意力生成的。<ul><li>公式：Attention(qi, ki, vi) = ∑j softmax(αi j) / √dk * vi，其中qi表示单词i的查询矩阵，ki表示单词i的键矩阵，vi表示单词i的值矩阵，n是单词的总数，dk是键向量的通道维度。除以√dk的目的是增加梯度的稳定性。</li></ul></li><li><strong>多头注意力：</strong> 通常采用多头注意力方式，多次应用具有不同可学习参数的注意力。每个注意力头的输出被串联并投影以获得最终值。<ul><li>公式：Vfinal = Concat(V1, . . . , Vh)WO，其中Vh表示第h个注意力头的输出，WO是投影参数矩阵，dq = dk = dv = dmodel/h。</li></ul></li></ol><p><strong>Transformer机制解释：</strong> Transformer是一种用于处理序列数据的深度学习模型，最初设计用于自然语言处理。其中的关键部分是自注意力机制，它允许模型根据输入序列中不同位置的相关性来动态调整每个元素的权重。这可以用于丰富嵌入表示，从而更好地捕获序列中的信息。多头注意力允许模型以不同的方式关注序列中的不同部分，以提高性能。 Transformer已经在各种领域取得了卓越的成果，不仅限于自然语言处理，还在计算机视觉等领域广泛应用。</p><h3 id="SAGCN-Module机制">SAGCN Module机制</h3><p>以下是段落的逐句翻译和SAGCN Module机制的解释：</p><ol><li><strong>当处理骨骼数据时，大多数时空图卷积网络始终在预定义的图上操作，使用邻接矩阵A(i, j)表示拓扑关系，其中A(i, j) = 0表示关节vi和v j不相邻（否则相邻）。然而，这种策略只能描述人体骨架的物理结构，可能忽略了对动作识别有用的某些信息。这有两个主要原因。首先，由于每一层的图的拓扑结构相同，它缺乏对包含在所有GCN层中的多重语义信息的建模能力。其次，它未能捕捉到额外的非物理连接，因为如果两个关节之间没有连接，则经过多次乘法运算后A(i, j)仍然为0。此外，每个动作都具有在识别任务中起关键作用的多个独特特征。因此，需要构建不同的图来描述每个样本的独特特征。</strong></li><li><strong>为了解决上述问题，我们提出了一种新颖的GCN模块，采用自适应策略构建骨架图。我们引入了一个相似性矩阵，通过计算样本之间的差异来确定两个关节之间的连接强度。与原始的ST-GCN不同，我们的SAGCN通过逐元素相加邻接矩阵、掩码矩阵和相似性矩阵来定义。</strong></li><li><strong>公式：fout = ∑k (Ak + Mk + Sk) * fin * W，其中k表示卷积核的数量，Ak表示表示人体物理结构的邻接矩阵，Mk是大小为N×N的掩码矩阵，Sk表示两个节点的相似性。</strong></li><li><strong>与原始ST-GCN一样，我们提出的SAGCN中的掩码矩阵Mk用于确定连接的强度。与Ak不同，这些元素的值在模型训练过程中与其他参数一起更新。元素的值越大，连接关系就越强。</strong></li><li><strong>相似性矩阵Sk表示特征向量之间的相似性，每个元素Si j k表示节点vi和节点v j的相似性。这些元素的值在0到1之间。</strong></li><li><strong>最终的SAGCN通过将三个矩阵Ak、Mk和Sk相加来更新骨架图的结构，这与元素级的相乘不同。</strong></li><li><strong>总的来说，SAGCN相对于常规的时空图卷积可以更好地更新和优化骨架图的结构。此外，每个样本对应一个不同的图，使模型更加灵活。</strong></li></ol><p><strong>SAGCN Module机制解释：</strong> 这一段落描述了SAGCN（Self-Adaptive Graph Convolution Network）模块的机制。SAGCN的目标是更好地捕获骨骼数据中的信息，并构建不同的骨架图来适应不同样本的特征。</p><ul><li><strong>邻接矩阵Ak</strong>：表示人体骨架的物理结构，通常是一个固定的预定义矩阵。</li><li><strong>掩码矩阵Mk</strong>：用于确定连接的强度，其值在训练过程中会更新，以适应不同的层次和语义特征。</li><li><strong>相似性矩阵Sk</strong>：表示节点之间的相似性，根据节点的特征向量计算得出，用于捕获不同节点之间的关系。</li></ul><p>SAGCN通过将这三个矩阵相加而不是相乘来构建图，以确保在不存在连接的情况下不会消失。这允许模型根据数据动态构建图，以更好地适应不同样本的特征。此外，SAGCN采用了一种自适应策略，允许模型根据训练数据的不同层次和特征来更新掩码矩阵Mk的值，从而提高了模型的灵活性。这种方法有助于更好地捕获骨架数据中的关系，并改善动作识别性能。</p><h3 id="SA-Module机制">SA Module机制</h3><p>虽然SAGCN模块可以弥补无法在乘法操作中替代A(i, j) = 0的缺陷，但远距离的连接仍然容易被低估。此外，随着层叠的增加，图卷积的过度平滑风险会增加。因此，我们引入SA模块，以自动发现每个帧内的关节关系，舍弃任何预定义的骨架结构。对于帧t中骨架的每个节点nt i，可以通过应用具有三个参数矩阵Wq ∈ RCin×dq、Wk ∈ RCin×dk和Wv ∈ RCin×dv的可训练线性变换来计算查询向量qt i、键向量kt i和值向量vt i，这些参数矩阵是所有节点共享的。使用这些向量，计算每对节点nt i和nt j之间的相关分数αt i j，公式如下：</p><p>αt i j = qt i · ktT j (9)</p><p>其中，qt i表示帧t中节点i的查询向量，kt j表示同一帧t中节点j的键向量。然后，相关分数αt i j被用作对应于查询向量的节点vt i的权重。节点nt i的最终嵌入通过加权求和计算如下：</p><p>zt i = Σ j softmax(αt i j) * √dk * vt i (10)</p><p>其中，zt i表示节点nt i的新嵌入，dk表示查询向量和键向量的维度。</p><p>与原始的Transformer类似，我们还使用多头注意力，通过多次重复嵌入过程Nh次来捕获多重关注信息。每个头使用不同组的可学习参数来获取节点嵌入(zt i1, zt i2, . . . , zt i H)。最终的表示通过将它们连接在一起获得。</p><p>此外，我们选择了五个关键关节点来展示SA模块在空间维度上对骨架关节的学习过程，如图3所示。首先，我们为每个关节计算查询向量q、键向量k和值向量v。然后，通过查询向量与所有其他关节点的键向量的点积来获取每对关节的连接强度。根据连接强度，每个关节将与查询关节进行缩放。请注意，如果与查询关节存在强连接，那么关键关节将被赋予更高的权重。最后，计算查询关节的新表示，通过加权关节的求和来完成。</p><h3 id="TA-Module机制">TA Module机制</h3><p>这段文字讲述了在视频中获得动作的判别性特征，这些特征主要包括短期时间上下文和长期依赖关系，这些特征主要来自时间序列。大多数现有的基于GCN的方法使用2D时间卷积来关注短期时间上下文，以建模骨架序列中的时间动态。然而，2D卷积的操作范围受到卷积核的限制，忽略了一些长期依赖关系。</p><p>为了探索两个远距离帧中非相邻关节之间的关系，引入了TA模块，以获取相邻和非相邻帧之间的短程和长程相关性。更具体地说，我们使用不同帧中相同节点的嵌入在时间维度上的变化来获取不同帧之间的相关性。</p><p>首先，对每个时间t处的帧内节点nti应用线性变换，使用参数Wq、Wk和Wv，这类似于SA模块，以获得查询向量qit、关键向量kit和值向量vit。然后，我们计算用于确定一个节点向其他节点支付多少关注的分数si tu，这是通过查询-关键点乘积得出的：</p><p>si tu = qi t · ki u T (11)</p><p>其中qi t表示t帧中第i个节点，kiu表示u帧中第i个节点。</p><p>最后，获取节点nti的嵌入zi t，通过对所有分数进行求和获得：</p><p>zi t = Σ j softmax(si tu) * √dk * vi t (12)</p><p>其中，dk表示关键向量的嵌入维度，除以（dk）1/2的函数是为了防止梯度消失。</p><p>为了直观展示TA模块在时间维度上的操作，图4中提供了TA模块的示例。具体来说，计算了每个关节的查询向量q、关键向量k和值向量v。然后计算了每对关节之间的相似性分数，如q1kT3、q2kT4和q4kT1。这些分数可以衡量每个身体关节nti的影响，表明节点之间的相关性。</p><p>总之，TA模块提取了不同帧中节点之间的关系，以学习帧之间的相关性。通过这种方式，我们可以捕获比使用2D卷积获得的更丰富的特征，这对于动作识别可能至关重要。</p><h1>Hypergraph Transformer for Skeleton-based Action Recognition基于骨架的动作识别超图Transformer</h1><h2 id="abstract-2">abstract</h2><p>翻译： 基于骨骼的动作识别旨在通过具有骨骼相互连接的人体关节坐标来识别人类动作。通过将关节作为顶点，它们的<strong>自然连接作为边来定义图形</strong>，先前的研究成功地采用了图卷积网络（GCNs）来模拟关节共现情况，并取得了卓越的性能。最近，GCNs 的一个局限性被识别出来，即拓扑结构在训练后是固定的。为了放宽这种限制，引入了<strong>自注意力机制（SA）</strong>，使GCNs 的拓扑结构能够适应输入，从而产生了最先进的混合模型。同时，也尝试使用<strong>普通的Transformer 模型</strong>，但由于缺乏结构先验知识，它们仍然落后于最先进的基于GCN 的方法。与混合模型不同，我们提出了一种更加优雅的解决方案，通过<strong>图距离嵌入将骨骼连接性融入到Transformer 中</strong>。我们的嵌入在训练过程中保留了骨骼结构的信息，而GCNs 仅在初始化阶段使用它。更重要的是，我们揭示了==图形模型的一个潜在问题，即成对聚合本质上忽略了身体关节之间的高阶运动依赖关系==为了填补这一差距，我们提出了一种新的<strong>超图自注意力（SA）机制</strong>，称为超图自注意力（HyperSA），以<strong>将内在的高阶关系融入模型</strong>中。我们将结果模型命名为Hyperformer，它在NTURGB+D、NTURGB+D 120 和Northwestern-UCLA 数据集上的准确性和效率方面击败了最先进的图形模型。我们的代码可在Hyperformer Github 存储库中找到。</p><p>总结： 这篇论文关注骨骼基础的动作识别，通过使用<strong>人体关节坐标以及它们之间的连接关系</strong>来识别人类动作。以前的研究使用图卷积网络（GCNs）来处理这一问题，并取得了出色的成绩。然而，GCNs 存在一个限制，即它们在训练后不能调整拓扑结构。为了解决这个问题，研究引入了自注意力（SA）机制，以使GCNs 的拓扑结构能够自适应输入数据，这导致了最先进的混合模型。与此同时，一些研究尝试使用普通的Transformer 模型，但由于缺乏结构信息，它们的性能落后于GCN 方法。</p><p>本文提出了一种更优雅的解决方案，将<strong>骨骼连接性融入到Transformer 模型</strong>中，通过<strong>图距离嵌入</strong>在训练期间保留了骨骼结构的信息，而不仅仅在初始化阶段使用。此外，文章揭示了图形模型的一个潜在问题，即现有方法在处理身体关节之间的高阶运动依赖关系时存在不足。为了解决这个问题，研究提出了一种新的自注意力机制，称为Hypergraph Self-Attention（HyperSA），以引入更高阶的关系。最终的模型被命名为Hyperformer，它在多个数据集上的性能超越了最先进的图形模型。这项研究的代码可在Hyperformer Github 存储库中找到。</p><h3 id="总结-2">总结</h3><ul><li>本文关注骨骼基础的动作识别，使用人体关节坐标和连接关系来进行动作识别。</li><li>以前的方法主要采用图卷积网络（GCNs）来模拟关节共现情况，取得了卓越的性能。</li><li>GCNs 存在的一个问题是，在训练后不能调整拓扑结构，限制了其灵活性。</li><li>为了解决这个问题，引入了自注意力（SA）机制，使GCNs 的拓扑结构能够自适应输入数据。</li><li>最近还尝试使用普通的Transformer 模型，但由于缺乏结构信息，性能较差。</li><li>本文提出了一种更优雅的解决方案，通过图距离嵌入将骨骼连接性融入到Transformer 模型中，保留了结构信息。</li><li>文章还揭示了图形模型的一个潜在问题，即忽略了高阶运动依赖关系。</li><li>为了解决这个问题，提出了一种新的自注意力（SA）机制，称为Hypergraph Self-Attention（HyperSA），以引入更高阶的关系。</li><li>最终的模型被命名为Hyperformer，在多个数据集上的性能超越了最先进的图形模型。</li></ul><h2 id="introduction">introduction</h2><ol><li>Introduction 骨架基础的人体动作识别由于其计算效率和对环境变化以及摄像机视角的鲁棒性而受到越来越多的关注。骨架基础动作识别的关键优势之一是可以使用传感器（如Kinect [53]）或可靠的姿势估计算法[2]轻松获取身体关键点。这为传统的基于RGB或深度的方法提供了更可靠的替代方案，使其成为各种现实世界应用的有前途的解决方案。</li><li>Graph Convolution Networks (GCNs) have been widely used for modeling off-grid data. To our knowledge, Yan et al. [46] were the ﬁrst to treat joints and their natural connec- tions as nodes and edges of a graph, and employ a GCN [20] on such a predeﬁned graph to learn joint interactions. Since then, GCNs have become the de facto standard of choice for skeleton-based action recognition. To further capture the in- teractions between physically unconnected joints, state-of- the-art GCNs [7, 3, 47, 31, 35, 36] adopt a learnable topol- ogy which only uses the physical connections for initializa- tion. Even so, they still need to rely on attention mecha- nisms to relax the restriction of the ﬁxed topology, which is the key to the improved performances, as shown in their ablation studies. 图卷积网络（GCNs）已广泛用于建模离散数据。据我们所知，Yan等人[46]是第一个将关节及其自然连接视为图的节点和边，并在这样的预定义图上使用GCN [20]来学习关节交互的人。自那以后，GCNs已成为骨架基础动作识别的事实上的首选标准。为了进一步捕捉物理上不相连的关节之间的相互作用，最先进的GCNs [7, 3, 47, 31, 35, 36]采用了可学习的拓扑结构，仅使用物理连接进行初始化。即便如此，它们仍需要依赖注意力机制来放宽固定拓扑结构的限制，这是提高性能的关键，正如在它们的消融研究中所示。</li><li>Given these facts, it is natural to question whether a purely attention-based Transformer model would be a better candidate for skeleton-based action recognition. However, current research [28, 32] has shown that the performance of such models is far from satisfactory. This can be at- tributed to the fact that the formulation of the vanilla Trans- former ignores the unique characteristics of skeleton data, i.e., the permutation invariant attention operation is agnostic to the bone connectivity between human body joints. To ad- dress this issue, absolute positional embeddings have been used [39, 11, 38] , but they still lack the necessary struc- tural information. In contrast, relative positional embed- dings have been shown to be more effective for Transform- ers in various tasks, involving language [30, 9, 15], vision [25, 55, 42, 55], and graph data [48]. To incorporate the information of bone connectivity of human body as well, we also introduce a powerful relative positional embedding based on graph distance, inspired by Spatial Encoding in [48]. Our embedding retains the information of skeletal structure during the entire training process, whereas GCNs merely use it for initialization. 鉴于这些事实，自然会有疑问，纯粹基于注意力的Transformer模型是否更适合用于骨架基础的动作识别。然而，当前的研究[28, 32]表明，这种模型的性能远未令人满意。这可以归因于普通Transformer的构建忽略了骨架数据的独特特性，即排列不变的注意力操作不考虑人体关节之间的骨连接。为了解决这个问题，已经使用了绝对位置嵌入[39, 11, 38]，但它们仍然缺乏必要的结构信息。相反，相对位置嵌入已经被证明对于Transformer在各种任务中更加有效，包括语言[30, 9, 15]、视觉[25, 55, 42, 55]和图数据[48]。为了同时整合人体骨骼的连接信息，我们还引入了一种基于图距离的强大相对位置嵌入，灵感来自[48]的空间编码。我们的嵌入在整个训练过程中保留了骨骼结构的信息，而GCNs仅在初始化阶段使用它。</li><li>More importantly, we reveal an underlying issue of graph models for this task in general. For human actions, each type of body joint has its own unique physical functional- ity. As a result, certain re-occurring groups of body joints are often involved in specific actions, such as the subcon- scious hand movement for maintaining balance. Vanilla at- tention is not capable of capturing these underlying rela- tionships that are independent from joint coordinates and go beyond pair-wise interactions. To compensate for this, we employ the concept of hypergraph [54, 13, 1] to ac- commodate the higher-order relations of body joints. With the hypergraph representation, we propose a novel variant of Self-Attention (SA) called Hypergraph Self-Attention (HyperSA), which considers both pair-wise and high-order relations. As shown in Fig. 1, given a partition of the human body joints into different groups, a representation of each group is derived based on its assigned joints. The group representation is then linearly transformed and multiplied with joint queries, allowing joint-to-group interactions in addition to the vanilla joint-to-joint SA. Though HyperSA works well with empirical partitions, we additionally pro- pose an approach to search the optimal partition strategy automatically, further improving its performance. 更重要的是，我们揭示了一般情况下用于这项任务的图模型的一个潜在问题。对于人体动作，每一种类型的身体关节都有其独特的物理功能。因此，某些反复出现的身体关节组通常参与特定的动作，例如保持平衡的下意识手部运动。普通的注意力无法捕捉到这些与关节坐标无关且超越成对交互的基础关系。为了弥补这一点，我们采用了超图的概念[54, 13, 1]来容纳身体关节的高阶关系。通过超图表示，我们提出了一种称为超图自注意力（HyperSA）的新型自注意力（SA）变体，考虑了成对和高阶关系。如图1所示，给定人体关节划分为不同组，根据其分配的关节派生出每个组的表示。然后对该组表示进行线性变换并与关节查询相乘，除了基本的关节对关节SA外，还允许关节对组的相互作用。虽然HyperSA在经验划分中表现良好，但我们另外提出了一种自动搜索最佳划分策略的方法，进一步提高了其性能。</li><li>At the same time, Transformers spend a large portion of their capacity on intra-token modeling in their feed-forward layer. While this is important for complex tokens such as image patches or word embeddings, we analyze that such an expensive step is not necessary for joint coordi- nates which are merely three-dimensional. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key to successful action recognition. We thus suggest removing MLP layers for computation and memory reduction, and show in Sec. 5.3.1 that MLP lay- ers are indeed negligible. This leads to a light-weighted Transformer which is comparable to GCNs in model size and computation cost. 与此同时，Transformer在其前向传递层中将其大部分容量用于对令牌的内部建模。虽然这对于复杂的令牌（如图像块或单词嵌入）很重要，但我们分析发现，对于仅为三维的关节坐标来说，这样的昂贵步骤是不必要的。这意味着建模令牌间关系，或者所谓的关节共现，是成功的动作识别的关键。因此，我们建议去除MLP层以进行计算和内存减少，并在第5.3.1节中显示MLP层确实可以忽略不计。这导致了一个轻量级的Transformer，其模型大小和计算成本与GCNs相当。</li><li>Our main contributions can be summarized as follows: 我们的主要贡献可以总结如下：</li></ol><p>• We propose to incorporate the structural information of human skeleton into Transformer via a relative po- sitional embedding based on graph distance, leverag- ing the gap between Transformer and state-of-the-art hybrid models. • 我们提出通过基于图距离的相对位置嵌入，将人体骨骼的结构信息整合到Transformer中，弥合了Transformer和最先进的混合模型之间的差距。</p><p>• We devise a novel variant of Self-Attention (SA) called Hypergraph Self-Attention (HyperSA), based on the hypergraph representation. To our best knowledge, our work is the first to apply the hypergraph representation for skeleton-based action recognition, which considers the pair-wise as well as high-order joint relations, go- ing beyond current state-of-the-art methods. • 我们设计了一种称为Hypergraph Self-Attention（HyperSA）的新型自注意力（SA）变体，基于超图表示。据我们所知，我们的工作是第一个将超图表示应用于基于骨架的动作识别的工作，它考虑了成对和高阶关节关系，超越了当前最先进的方法。</p><p>• We construct a light-weighted Transformer based on our proposed relative positional embedding and HyperSA. It beats state-of-the-art graph models on skeleton-based action recognition benchmarks w.r.t. both efficiency and accuracy. • 我们构建了一个基于我们提出的相对位置嵌入和HyperSA的轻量级Transformer。在骨架基础的动作识别基准测试中，它在效率和准确性方面超越了最先进的图模型。</p><h2 id="Related-work">Related work</h2><ol><li><strong>Graph Representation（图表示）</strong>：图是用于表示非欧几里得数据的最常见选择，而人类骨骼可以自然地表示为图。与其他图模型相比，Kipf等人提出的图卷积网络（Graph Convolutional Network，GCN）因其简单性和更高的防过拟合性而被广泛用于动作识别。最近，Graphformer在通用图数据上实现了最先进的性能。它揭示了有效地将结构信息编码到Transformer中的必要性。他们的发现与我们关于拓扑结构方面的分析一致，因此我们提出了适用于我们的Hyperformer的强大的k-Hop RPE（相对位置嵌入）。</li><li><strong>Hypergraph Representation（超图表示）</strong>：在现实世界的情景中，关系可能超出了成对的关联。超图进一步考虑了数据中的高阶关联。虽然可以通过诸如团扩展等技术将超图近似建模为图，但这种近似无法捕获数据中的高阶关系，导致性能不可靠。这激发了对超图上的学习的研究。已经提出了基于注意力的超图模型，用于多模态学习和归纳文本分类。我们提出的HyperSA是专为基于骨骼的动作识别而设计的第一个超图注意力模型。</li></ol><p><strong>Representation of skeleton data（骨骼数据的表示）</strong>：这段文字介绍了两种方法来表示骨骼数据，即图表示和超图表示。图表示使用简单的图模型（如GCN）来表示骨骼数据，这种方法在动作识别中广泛使用。超图表示考虑了数据中更高阶的关联关系，这对于某些复杂的关联关系可能更加有用，特别是在超出成对关联的情况下。作者提出了一种新的超图注意力模型（HyperSA），专门用于骨骼数据的动作识别，以更好地捕获这些高阶关系。这是一种创新性的方法，有望提高动作识别性能。</p><h2 id="Preliminaries">Preliminaries</h2><p>自注意力（Self-Attention）机制是一种用于处理序列数据的方法，通常在Transformer等深度学习模型中使用。在这个机制中，首先，输入序列中的每个标记都会被映射成**Key（键），Query（查询）和Value（值）**三元组。然后，两个标记之间的所谓注意力分数由它们的Query和Key的点积经过softmax函数计算而得。最终，<strong>每个位置的输出是所有值的加权和，权重由这些注意力分数决定</strong>。多头自注意力（Multi-Head Self-Attention，MHSA）是Transformer等模型中常用的扩展版本，<strong>它将通道维度划分为子组，并在每个子组上并行应用自注意力，以学习不同类型的依赖关系</strong>。</p><p>超图表示（Hypergraph representation）与标准图的边（edges）不同，它的超边（hyperedge）可以连接两个或多个顶点。一个==无权超图==（unweighted hypergraph）可以表示为H = (V, E)，其中包括<strong>一个顶点集合V和一个超边集合E</strong>。超图H可以用一个|V|×|E|的关联矩阵H来表示，其元素定义如下：如果顶点v在超边e中，则hv,e = 1，否则为0。节点v的度数（degree）定义为d(v) = ∑e∈E hv,e，超边e的度数定义为d(e) = ∑v∈V hv,e。度数矩阵De和Dv是通过将所有边的度数和所有顶点的度数设置为它们的对角线条目而构建的。</p><p>在这个工作中，我们考虑了一种特殊情况，即对于所有顶点，d(v) = 1，也就是说，身体关节被划分为|E|个不相交的子集，这在实践中是高效的。值得注意的是，在这种情况下，关联矩阵H等效于一个分区矩阵，其中每一行都是一个独热向量，表示每个关节所属的组。</p><p>简单来说，自注意力机制用于处理输入序列中的关系，而超图表示用于捕捉高阶关系，这对于骨骼数据等复杂关系数据的建模非常重要。</p><h3 id="总结-3">总结</h3><p><strong>3.1. Self-Attention</strong></p><p><strong>自注意力（Self-Attention）机制是一种用于处理序列数据的方法，通常在Transformer等深度学习模型中使用。</strong></p><p>Given an input sequence in the form of X = (x1, …, xn), each token xi is first projected into Key ki , Query qi and Value vi triplets.</p><p><strong>对于以X = (x1, …, xn)形式的输入序列，首先将每个标记xi投影为Key ki，Query qi和Value vi三元组。</strong></p><p>Then the so-called attention score Aij between two tokens is obtained by applying a softmax function to the dot product of qi and kj [39]:</p><p><strong>然后，两个标记之间的所谓注意力分数Aij通过将qi和kj [39]的点积应用softmax函数来获得：</strong></p><p>Aij = qi · kj,</p><p>the final output at each position is computed as the weighted sum of all Values:</p><p><strong>每个位置的最终输出是所有Value的加权和：</strong></p><p>yi = ∑j=1 to n Aijvj.</p><p>An extension called Multi-Head Self-Attention (MHSA) is often adopted by Transformers in practice. It divides the channel dimension into subgroups and apply Self-Attention to each subgroup in parallel to learn different kinds of inter-dependencies.</p><p><strong>实际上，Transformer等模型通常采用名为Multi-Head Self-Attention (MHSA)的扩展版本。它将通道维度划分为子组，并在每个子组上并行应用自注意力，以学习不同类型的依赖关系。</strong></p><p>For simplicity, we omit the notation of MHSA in this paper.</p><p><strong>为简单起见，在本文中，我们省略了MHSA的符号表示。</strong></p><p><strong>3.2. Hypergraph representation</strong></p><p><strong>3.2. 超图表示</strong></p><p>Unlike standard graph edges, a hyperedge in a hypergraph connects two or more vertices. An unweighted hypergraph is defined as H = (V, E), which consists of a vertex set V and a hyperedge set E.</p><p><strong>不同于标准图的边，超图中的超边连接两个或多个顶点。无权超图被定义为H = (V, E)，包括一个顶点集合V和一个超边集合E。</strong></p><p>The hypergraph H can be denoted by a |V| × |E| incidence matrix H, with entries defined as follows:</p><p><strong>超图H可以用一个|V|×|E|的关联矩阵H来表示，其元素定义如下：</strong></p><p>hv,e = 1, if v ∈ e 0, if v /∈ e</p><p><strong>如果顶点v在超边e中，则hv,e = 1，否则为0。</strong></p><p>The degree of a node v ∈ V is defined as d(v) = ∑e∈E hv,e, and the degree of a hyperedge e ∈ E is defined as d(e) = ∑v∈V hv,e.</p><p><strong>节点v ∈ V的度数定义为d(v) = ∑e∈E hv,e，超边e ∈ E的度数定义为d(e) = ∑v∈V hv,e。</strong></p><p>The degree matrices De and Dv are constructed by setting all the edge degrees and all the vertex degrees as their diagonal entries, respectively.</p><p><strong>度数矩阵De和Dv是通过将所有边的度数和所有顶点的度数分别设置为它们的对角线条目而构建的。</strong></p><p>In this work, we consider the special case of d(v) = 1 for all vertices, i.e., body joints are divided into |E| disjoint subsets, which is efficient in practice.</p><p><strong>==在这项工作中，我们考虑了一种特殊情况，即对于所有顶点，d(v) = 1，也就是说，身体关节被划分为|E|个不相交的子集，在实践中是高效的==。</strong></p><p>Notably, the incidence matrix H is equivalent to a partition matrix in this case. Each row is a one-hot vector denoting the group to which each joint belongs.</p><p><strong>值得注意的是，在这种情况下，关联矩阵H等效于一个分区矩阵。每一行都是一个独热向量，表示每个关节所属的组。</strong></p><p>简单来说，自注意力机制用于处理输入序列中的关系，而超图表示用于捕捉高阶关系，这对于骨骼数据等复杂关系数据的建模非常重要。</p><h3 id="超图-超边和节点点集合，以及边度和节点度矩阵">超图 超边和节点点集合，以及边度和节点度矩阵</h3><h3 id="Relax-the-binary-partition-matrix">Relax the binary partition matrix</h3><p>“Relax the binary partition matrix” 是一个用于聚类分析的术语，通常用于描述在聚类算法中，将传统的二进制（0或1）的分区矩阵（partition matrix）进行一种松弛（relax）或放宽，以允许每个数据点同时属于多个簇或类别的情况。</p><p>在传统的硬聚类（Hard Clustering）中，每个数据点仅属于一个簇，因此分区矩阵中的每个元素要么是0，表示数据点不属于某个簇，要么是1，表示数据点属于某个簇。这种二进制分区矩阵通常用于K均值聚类等算法。</p><p>然而，在某些情况下，数据点可能不仅仅属于一个簇，而是模糊地属于多个簇。为了处理这种情况，可以使用&quot;relax the binary partition matrix&quot; 的方法，将分区矩阵中的元素变成介于0和1之间的小数值，表示数据点对于不同簇的隶属度（membership degree）。这种方法通常用于软聚类（Soft Clustering）或模糊聚类（Fuzzy Clustering）中。</p><p>举例来说，考虑一个具有三个簇的数据集，一个硬聚类的分区矩阵可能如下所示：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">csharpCopy</span> code[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]<span class="meta"></span></span><br><span class="line"><span class="meta">[0, 1, 0]</span><span class="meta"></span></span><br><span class="line"><span class="meta">[0, 0, 1]</span></span><br></pre></td></tr></table></figure><p>在这个分区矩阵中，每个数据点被分配给一个唯一的簇。</p><p>然而，如果我们采用&quot;relax the binary partition matrix&quot; 的方法，我们可以得到一个软聚类的分区矩阵，如下所示：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">csharpCopy</span> code[<span class="number">0</span>.<span class="number">8</span>, <span class="number">0</span>.<span class="number">2</span>, <span class="number">0</span>.<span class="number">0</span>]<span class="meta"></span></span><br><span class="line"><span class="meta">[0.1, 0.9, 0.0]</span><span class="meta"></span></span><br><span class="line"><span class="meta">[0.0, 0.1, 0.9]</span></span><br></pre></td></tr></table></figure><p>在这个分区矩阵中，每个数据点对于不同簇都有一定的隶属度。例如，第一个数据点在簇1中的隶属度为0.8，在簇2中的隶属度为0.2，而不再是严格的0或1。</p><p>这种松弛的方法允许更灵活地处理数据，特别是在存在数据点模糊隶属度的情况下。它在模糊聚类、概率图模型等领域中有广泛应用。</p><blockquote><p>Speciﬁcally, a novel Hypergraph Self-Attention (HyperSA) layer is in- troduced, which makes Transformers aware of extra higher- order relations shared by a subset of joints connected to each hyperedge.</p></blockquote><p>这句话是在描述一种新的层次结构，称为&quot;Hypergraph Self-Attention (HyperSA)&quot;层。这个层次结构被引入到模型中，以使Transformer模型能够意识到一组关节连接到每个&quot;hyperedge&quot;的额外高阶关系。</p><p>在这里，&quot;hyperedge&quot;表示超图中的一种特殊边，连接着两个或更多的顶点（关节）。所谓的&quot;higher-order relations&quot;是指连接到这个超边的关节之间的更复杂的关系，这些关系不仅仅是直接连接的关系，而是涉及到多个关节的互动。</p><p>因此，这个新的HyperSA层的作用是在处理输入数据时，允许Transformer模型考虑到与超边连接的关节之间的更复杂的关系，这样模型可以更好地捕获数据中的高阶结构和特征。这有助于提高模型对数据的理解和性能。</p><h3 id="4-1-Deriving-the-hyperedge-feature-导出超边缘特征">4.1 Deriving the hyperedge feature 导出超边缘特征</h3><ol><li>“Given an incidence matrix H, we propose an effective approach to obtain the feature representation for each subset of joints connected to a hyperedge.”<ul><li>给定一个关联矩阵 H，我们提出了一种有效的方法，用于获取与超边连接的每个关节子集的特征表示。</li></ul></li><li>“Let C denote the number of feature dimensions, individual joint features X ∈ R|V|×C are first aggregated into subset representations E ∈ R|E|×C by the following rule:”<ul><li>令 C 表示特征维度的数量，首先根据以下规则将各个关节的特征 X ∈ R|V|×C 聚合成子集表示 E ∈ R|E|×C：</li></ul></li><li>“E = D−1 e HXWe,”<ul><li>E = D^(-1)_e H * (X * We)，</li><li>这一步骤是将每个关节特征与相应的超边连接进行汇总。这里使用了度矩阵的逆来进行归一化，以及投影矩阵 We 来最终转换超边的特征。</li></ul></li><li>“Then we construct an augmented hyperedge representation Eaug ∈ R|V|×C by assigning hyperedge representations to the position of each associated joint:”<ul><li>然后，我们通过将超边的表示分配给每个关联关节的位置来构建增强的超边表示 Eaug ∈ R|V|×C：</li></ul></li><li>“Eaug = HD−1 e HXWe.”<ul><li>Eaug = HD^(-1)_e H * (X * We)。</li><li>这一步骤将超边的表示重新分配到与之关联的每个关节的位置上，以构建一个更丰富的超边表示。</li></ul></li></ol><p>在这个章节中，作者讨论了如何根据超边连接的关节子集来获取特征表示。通过对关联矩阵和关节特征的处理，他们构建了超边的表示，并将其分配给每个相关联的关节，以创建一个更丰富的超边表示。这有助于模型更好地理解超边上的高阶关系，从而提高了超图注意力机制的性能。</p><h3 id="4-2-Encoding-human-skeleton-structure">4.2 Encoding human skeleton structure</h3><ol><li>“Human body joints are naturally connected with bones and form, together with the latter, a bio-mechanical model.”<ul><li>人体关节自然与骨骼相连接，并与后者一起形成生物力学模型。</li></ul></li><li>“In such a mechanical system, the movement of each joint in an action is strongly influenced by their connectivities.”<ul><li>在这种机械系统中，动作中每个关节的运动都受到它们的连接性的强烈影响。</li></ul></li><li>“Therefore, it is beneficial to take the structural information of human skeleton into account.”<ul><li>因此，考虑人体骨骼的结构信息是有益的。</li></ul></li><li>“Analogous to the established Relative Positional Embedding (RPE) for image [42] and language [15, 30] Transformer, we propose a powerful k-Hop Relative Positional Embedding Rij ∈ RC, which is indexed from a learnable parameter table by the Shortest Path Distance (SPD) between the ith and jth joints.”<ul><li>类似于已建立的用于图像和语言Transformer的相对位置嵌入（RPE），我们提出了一种强大的k-Hop相对位置嵌入 Rij ∈ RC，它是根据第i个和第j个关节之间的最短路径距离（SPD）从可学习参数表中索引的。</li></ul></li><li>“In comparison to the learnable scalar spatial encoding in [48], it has larger capacity and interacts with the query additionally.”<ul><li>与[48]中可学习的标量空间编码相比，它具有更大的容量，并额外与查询进行交互。</li></ul></li></ol><p>在这个章节中，作者讨论了如何将人体骨骼的结构信息编码到模型中。他们提出了一种相对位置嵌入方法，类似于用于图像和语言Transformer的方法，但针对骨骼数据的特点进行了定制。这种嵌入方法有助于模型更好地理解骨骼结构，并与查询进行交互，从而提高了模型性能。这有助于改进动作识别的准确性和效率。</p><h3 id="4-3-Hypergraph-Self-Attention">4.3 Hypergraph Self-Attention</h3><ol><li>“With the obtained hyperedge representation and skeleton topology encoding, we now define our Hypergraph Self-Attention as follows:”<ul><li>通过获取超边表示和骨骼拓扑编码，我们现在定义我们的Hypergraph Self-Attention如下：</li></ul></li><li>“Aij = qi · kj (a) + qi · Eaug,j (b) + qi · Rφ(i,j) © + u · Eaug,j (d), (6) where u ∈ RC is a learnable static key regardless of the query position.”<ul><li>Aij = qi · kj（a） + qi · Eaug,j（b） + qi · Rφ(i,j)（c） + u · Eaug,j（d），其中u ∈ RC 是一个可学习的静态密钥，不受查询位置影响。</li></ul></li><li>“Term (a) alone is the vanilla SA, which represents joint-to-joint attention.”<ul><li>术语（a）单独是普通的SA，表示关节对关节的注意力。</li></ul></li><li>“Term (b) computes the joint-to-hyperedge attention between the ith query and the corresponding hyperedge of the jth key.”<ul><li>术语（b）计算第i个查询与第j个密钥的相应超边之间的关节对超边的注意力。</li></ul></li><li>“Term © is the term for injecting the structural information of human skeleton with k-Hop Relative Positional Embedding.”<ul><li>术语（c）用于通过k-Hop相对位置嵌入注入人体骨骼的结构信息。</li></ul></li><li>“Term (d) is intended for calculating the attentive bias of different hyperedges independent of the query position. It assigns the same amount of attention to each joint connected to a certain hyperedge.”<ul><li>术语（d）旨在计算不同超边的注意偏差，与查询位置无关。它为连接到某个超边的每个关节分配相同数量的注意力。</li></ul></li><li>“Note that terms (a) and (b) can be combined by distributive law and require merely an extra step of matrix addition.”<ul><li>请注意，术语（a）和（b）可以通过分配法则组合，只需要额外的矩阵加法步骤。</li></ul></li><li>“Moreover, term (d) has O(|V|C2) complexity and thus requires negligible computation in comparison to term (a).”<ul><li>此外，术语（d）具有O（|V|C2）复杂性，因此与术语（a）相比需要极少的计算。</li></ul></li><li>“Relational Bias Transformers assume the input tokens to be homogeneous, whereas human body joints are inherently heterogeneous, e.g. each physical joint plays a unique role and thus has different relations to others.”<ul><li>关系偏置变换器假定输入标记是同质的，而人体关节本质上是异质的，例如，每个物理关节都有独特的角色，因此与其他关节的关系不同。</li></ul></li><li>“Taking the heterogeneity of the skeleton data into account, we propose to represent the inherent relation of each joint pair as a scalar trainable parameter Bij, called Relational Bias (RB).”<ul><li>考虑到骨骼数据的异质性，我们提出将每对关节的固有关系表示为标量可训练参数Bij，称为关系偏置（RB）。</li></ul></li><li>“It is added to the attention scores before aggregating the global information:”<ul><li>它在聚合全局信息之前添加到注意分数中：</li></ul></li><li>“yi = n j=1 (Aij +Bij)vj”<ul><li>yi = n j=1 (Aij +Bij)vj</li></ul></li></ol><p>在这个章节中，作者介绍了Hypergraph Self-Attention的定义和计算过程。他们<strong>将注意力分为不同的项，包括传统的自注意力项以及考虑超边关系和关节之间固有关系的项</strong>。这种设计有助于模型更好地理解骨骼数据的复杂性，并提高了动作识别的性能。此外，作者还提到了<strong>关系偏置（Relational Bias），它有助于处理人体关节之间的异质性。Hypergraph Self-Attention的设计和关系偏置的引入有助于改进模型的表现</strong>。</p><h3 id="4-4-Partition-strategy分区策略">4.4 Partition strategy分区策略</h3><ol><li>“Empirically, human skeletons could be divided into a number of body parts, which have been well studied in previous work [37, 17, 36].”<ul><li>从经验上看，人类骨架可以分为多个身体部分，这在以前的研究中已经得到了很好的研究。</li></ul></li><li>“We experimentally show that our Hyperformer with empirical partitions yields excellent performance.”<ul><li>我们通过实验证明，采用经验性分区的Hyperformer表现出色。</li></ul></li><li>“However, finding an optimal empirical partition strategy is laborious and the optimal partition strategy is restricted to a certain skeleton with a fixed number of recorded joints.”<ul><li>然而，找到最佳的经验分区策略是费时的，最佳的分区策略限制在具有固定数量记录关节的特定骨架上。</li></ul></li><li>“In this work, we also provide an approach to automate the search process for an effective partition strategy.”<ul><li>在这项工作中，我们还提供了一种自动化搜索有效分区策略的方法。</li></ul></li><li>“To make the partition matrix learnable, we parameterize and relax the binary partition matrix to its continuous version by applying a softmax along its column axis:”<ul><li>为了使分区矩阵可学习，我们通过在其列轴上应用softmax将二进制分区矩阵参数化和松弛为其连续版本：</li></ul></li><li>“The problem of finding an optimal discrete partition matrix H is thus reduced to learning an optimal continuous partition matrix H˜, which can be optimized jointly with Transformer parameters.”<ul><li>因此，寻找最佳离散分区矩阵H的问题被简化为学习最佳连续分区矩阵H˜，可以与Transformer参数一起进行优化。</li></ul></li><li>“At the end of the optimization, a discrete partition matrix can be obtained by applying an argmax operation along each row of ˜H:”<ul><li>在优化结束时，可以通过在˜H的每一行上应用argmax操作来获得离散分区矩阵：</li></ul></li><li>“Note that a number of different proposals can be easily acquired by varying the initialization of H˜.”<ul><li>请注意，通过改变H˜的初始化，可以轻松获得许多不同的提议。</li></ul></li><li>“We experimentally show that all the proposals prove to be reasonable.”<ul><li>我们通过实验证明，所有的提议都是合理的。</li></ul></li><li>“Interestingly, all the learned proposals are symmetric as shown in Fig. 3, indicating that symmetry is an important aspect of inherent joint relations.”<ul><li>有趣的是，所有学到的提议都是对称的，如图3所示，这表明对称性是固有关节关系的重要方面。</li></ul></li></ol><p>在这个章节中，作者讨论了关于如何分割人体骨骼数据以提高模型性能的问题。他们提到，虽然经验性的分区方法可以取得良好的效果，但寻找最佳的经验分区策略是困难且具有限制性的。因此，他们提出了一种自动化的方法，通过<strong>将分区矩阵参数化为连续版本</strong>，<strong>并将其与Transformer参数一起进行优化来找到最佳的分区</strong>策略。最后，他们指出所有学到的提议都具有对称性，这表明对<strong>称性在关节关系中是一个重要的方面。</strong></p><h3 id="4-5-Model-architecture">4.5 Model architecture</h3><ol><li>“We first revisit the architectural design of Transformers for skeleton data.”<ul><li>我们首先重新审视了用于骨骼数据的Transformer的架构设计。</li></ul></li><li>“Then we built our Hyperformer based on our analysis.”<ul><li>然后，我们根据我们的分析构建了我们的Hyperformer。</li></ul></li><li>“<strong>HyperSA</strong> is employed for <strong>spatial modeling</strong> of each frame and a <strong>lightweight convolutional module</strong> is adopted for <strong>temporal modeling</strong>, following the design of state-of-the-art models [7, 3] in this field.”<ul><li>我们采用HyperSA来对每个帧进行空间建模，采用轻量级卷积模块来进行时间建模，遵循了这一领域现有先进模型的设计。</li></ul></li><li>“Spatial Modeling We apply Layer Normalization (LN) before the multi-head HyperSA and add a residual connection to the output, following the standard Transformer architecture [39].”<ul><li>空间建模：我们在多头HyperSA之前应用层归一化（LN），并在输出中添加了残差连接，遵循了标准的Transformer架构。</li></ul></li><li>“Based on our analysis in Sec. 1, we further remove the Multi-Layer-Perceptron (MLP) layers. To introduce non-linearity, a ReLU layer is added after each block of spatial and temporal modeling modules instead.”<ul><li>根据我们在第1节中的分析，我们进一步<strong>移除了多层感知机（MLP）层</strong>。为了引入非线性，我们在每个空间和时间建模模块块之后添加了ReLU层。</li></ul></li><li>“Temporal Modeling To model the temporal correlation of the human pose, we adopt the Multi-Scale Temporal Convolution (MS-TC) module [3, 26, 7] for our final model. This module contains three convolution branches with a 1 × 1 convolution to reduce channel dimension, followed by different combinations of kernel sizes and dilations. The outputs of convolution branches are concatenated.”<ul><li>时间建模：为了建模人体姿势的时间相关性，我们采用了<strong>多尺度时间卷积（MS-TC）模块作为我们的最终模型</strong>。该模块包含三个卷积分支，其中包括1×1卷积以减小通道维度，然后是不同组合的核尺寸和膨胀。卷积分支的输出被级联在一起。</li></ul></li><li>“Hyperformer is constructed by stacking HyperSA and Temporal Convolution layers alternately as follows:”<ul><li>==Hyperformer是通过交替堆叠HyperSA和时间卷积层来构建的==，如下所示：</li></ul></li><li>“z(l) = HyperSA(LN(z(l−1))) + z(l−1)”<ul><li>这里定义了Hyperformer的一个层次结构，其中z(l)表示第l层的输出。在每一层，首先应用HyperSA，然后进行层归一化（LN），最后将前一层的输出添加到当前层的输出中。</li></ul></li><li>“z(l) = TemporalConv(LN(z(l))) + z(l−1)”<ul><li>这是Hyperformer中的另一层次结构，其中z(l)表示第l层的输出。在每一层，首先应用TemporalConv，然后进行层归一化（LN），最后将前一层的输出添加到当前层的输出中。</li></ul></li><li>“z(l) = ReLU(z(l))”<ul><li>这一步引入了非线性，通过在每一层的输出上应用ReLU激活函数来增加模型的表达能力。</li></ul></li></ol><p>在&quot;Model architecture&quot;章节中，作者描述了Hyperformer模型的架构设计。他们使用了HyperSA来进行空间建模，并采用轻量级卷积模块进行时间建模。模型的构建基于Transformer架构，但根据他们的分析，他们移除了MLP层并添加了ReLU层来引入非线性。这个章节还详细描述了模型中的空间建模和时间建模层次结构，以及它们之间的堆叠方式。这些设计和结构对于理解Hyperformer的工作原理和性能至关重要。</p><h1>GRAPH CONTRASTIVE LEARNING FOR SKELETON BASED ACTION RECOGNITION</h1><h2 id="abstract-3">abstract</h2><p>这篇文章提出了一种用于骨骼动作识别的图对比学习框架（SkeletonGCL），旨在<strong>探索跨所有序列的全局上下文</strong>。当前表现出色的图卷积网络（GCN）在构建自适应图进行特征聚合时利用了序列内部的上下文信息，但作者认为这种上下文仍然是局部的，因为丰富的跨序列关系尚未明确研究。SkeletonGCL的主要思想是<strong>通过强制要求图具有类别判别性，即类内紧凑而类间分散，来实现跨序列的图学习</strong>。这提高了<strong>GCN在区分不同动作模式方面的能力</strong>。此外，SkeletonGCL还设计了两个内存库，<strong>用于从两个互补的层面（即实例级别和语义级别）丰富跨序列上下文</strong>，从而在多个上下文尺度上进行图对比学习。因此，SkeletonGCL建立了一种新的训练范式，并可以无缝地与当前的GCNs结合使用。在不失一般性的情况下，作者将SkeletonGCL与三个GCNs（2S-ACGN、CTR-GCN和InfoGCN）相结合，取得了NTU60、NTU120和NW-UCLA基准测试中持续的改进。</p><p>总结要点：</p><ul><li>该论文提出了SkeletonGCL框架，用于骨骼动作识别，旨在探索跨多个序列的全局上下文。</li><li>SkeletonGCL通过强化图的类别判别性来改善GCN的性能，使其能够更好地区分不同的动作模式。</li><li>框架包括两个内存库，用于在多个上下文尺度上进行图对比学习。</li><li>SkeletonGCL的方法可以与当前的GCNs无缝结合，且在NTU60、NTU120和NW-UCLA基准测试中都取得了一致的改进。</li></ul><h2 id="introduction-2">introduction</h2><ol><li><strong>Graph convolutional networks (GCNs) have been widely applied in skeleton-based action recognition since they can naturally process non-grid skeleton sequences.</strong><ul><li>GCNs广泛应用于基于骨骼的动作识别领域，因为它们可以自然地处理非网格骨骼序列。</li></ul></li><li><strong>For GCN-based methods, how to effectively learn the graphs remains a core and challenging problem.</strong><ul><li>对于基于GCN的方法，如何有效地学习图仍然是一个核心且具有挑战性的问题。</li></ul></li><li><strong>In particular, ST-GCN (Yan et al., 2018) is a milestone work, using pre-defined graphs to extract the action patterns. However, the pre-defined graphs only enable each joint to access the fixed neighboring joints but are hard to capture long-range dependency adaptively.</strong><ul><li>具体来说，ST-GCN（Yan等人，2018年）是一项具有里程碑意义的工作，使用预定义的图来提取动作模式。然而，预定义的图仅允许每个关节访问固定的相邻关节，但难以自适应地捕捉长距离依赖性。</li></ul></li><li><strong>Therefore, a mainstream of subsequent works (Li et al., 2019; Shi et al., 2019; Zhang et al., 2020b;a; Ye et al., 2020; Chen et al., 2021b; Chi et al., 2022) take efforts to solve this issue by generating adaptive graphs. The adaptive graphs can dynamically aggregate the features within each sequence and thus show significant advantages in performance comparison.</strong><ul><li>因此，随后的大部分研究（Li等人，2019年；Shi等人，2019年；Zhang等人，2020b；Ye等人，2020年；Chen等人，2021b；Chi等人，2022年）努力通过生成自适应图来解决这个问题。自适应图可以动态地聚合每个序列中的特征，因此在性能比较中显示出明显的优势。</li></ul></li><li><strong>Generally, adaptive graphs are constructed by using intra-sequence context. However, such context will still be “local” when viewing the cross-sequence information as an available context. Therefore, we wonder: Is it possible to involve the cross-sequence context in graph learning?</strong><ul><li>通常，自适应图是通过使用序列内部的上下文构建的。然而，当将跨序列信息视为可用的上下文时，这种上下文仍然是“局部”的。因此，我们想知道：是否可能将跨序列的上下文引入到图学习中？</li></ul></li><li><strong>To find out the answer, in Fig. 1, we visualize the adaptive graphs learned from sequences of two easily confused classes (“point to something” and “take a selfie”). The graphs are learned by a strong GCN, i.e., CTR-GCN (Chen et al., 2021b).</strong><ul><li>为了找到答案，在图1中，我们可视化了从两个容易混淆的类别（“指向某物”和“自拍”）的序列中学到的自适应图。这些图是由强大的GCN，即CTR-GCN（Chen等人，2021b）学习的。</li></ul></li><li><strong>These observations remind us that graph learning in current adaptive GCNs can implicitly learn class-specific graph representations to some extent. But without explicit constraints, it leads to class-ambiguous representations in some cases, thereby affecting the GCN capacity to discriminate classes (in Tab. 9 of Sec. 4.4, we provide quantitative results to further support our hypothesis).</strong><ul><li>这些观察结果提醒我们，当前自适应GCNs中的图学习在一定程度上可以隐式地学习特定类别的图表示。但是，在没有明确约束的情况下，这在某些情况下会导致类别不明确的表示，从而影响了GCN区分类别的能力（在第4.4节的表格9中，我们提供了定量结果以进一步支持我们的假设）。</li></ul></li><li><strong>Therefore, we speculate that if the cross-sequence semantic relations are incorporated as guidance in graph learning, the class-ambiguity issue will be alleviated and the graph representations will better express individual characteristics of actions.</strong><ul><li>因此，我们推测，如果将跨序列的语义关系作为图学习的指导因素，将会缓解类别不明确的问题，图表示将更好地表达动作的个体特征。</li></ul></li><li><strong>In recent years, contrastive learning has achieved great success in self-supervised representation learning (He et al., 2020; Chen et al., 2020; 2021a). It studies cross-sample relations in the dataset. The essence of contrastive learning is “comparing”, which pulls together the feature embedding from positive pairs and pushes away the feature embedding from negative pairs.</strong><ul><li>最近几年，对比学习在自监督表示学习（He等人，2020年；Chen等人，2020年；2021a年）中取得了巨大的成功。它研究数据集中的样本间关系。对比学习的本质是“比较”，它将正样本的特征嵌入拉拢在一起，将负样本的特征嵌入推开。</li></ul></li><li><strong>Based on the analysis above and the advances in contrastive learning, we propose a graph contrastive learning framework for skeleton-based action recognition in the fully-supervised setting, dubbed SkeletonGCL.</strong></li></ol><ul><li>基于上述分析和对比学习的进展，我们<strong>提出了一个用于完全监督的基于骨骼的动作识别的图对比学习框架，称为SkeletonGCL</strong>。</li></ul><ol><li><strong>Instead of just using the local information within each sequence, SkeletonGCL explores the cross-sequence global context to improve graph learning.</strong></li></ol><ul><li>SkeletonGCL==不仅仅使用每个序列内的局部信息，还探索跨序列的全局上下文以改进图学习==</li></ul><ol><li><strong>SkeletonGCL achieves consistent improvements combined with three GCNs (2S-AGCN (Shi et al., 2019), CTR-GCN (Chen et al., 2021b), and InfoGCN (Chi et al., 2022)), on three popular benchmarks (NTU60 (Shahroudy et al., 2016), NTU120 (Liu et al., 2019) and NW-UCLA (Wang et al., 2014)).</strong></li></ol><ul><li>SkeletonGCL与三个GCNs（2S-AGCN（Shi等人，2019年），CTR-GCN（Chen等人，2021b年）和InfoGCN（Chi等人，2022年）结合使用，在三个流行的基准测试（NTU60（Shahroudy等人，2016年），NTU120（Liu等人，2019年）和NW-UCLA（Wang等人，2014年）上取得了一致的改进。</li></ul><ol><li><strong>SkeletonGCL only introduces a small amount of training consumption but has no impact at the test stage.</strong></li></ol><ul><li>SkeletonGCL仅引入了少量的训练消耗，但对测试阶段没有影响。</li></ul><ol><li><strong>Though there exist some works that apply contrastive learning in skeleton-based action recognition (Li et al., 2021; Guo et al., 2022; Mao et al., 2022), our method differs from them as follows:</strong></li></ol><ul><li>尽管存在一些将对比学习应用于基于骨骼的动作识别的方法（Li等人，2021年；Guo等人，2022年；Mao等人，2022年），但我们的方法与它们有以下不同：</li></ul><ol><li><strong>Summarily, the contribution of this paper can be concluded as follows:</strong></li></ol><ul><li>总之，本文的贡献可以总结如下：</li></ul><p>以上是引言部分的逐句翻译和内容解释。这个引言部分介绍了论文的研究背景、问题陈述、动机和贡献，以及提出的SkeletonGCL框架的核心思想。它强调了如何利用对比学习来改进图学习，以及如何将跨序列语义关系引入图学习中。最后，它提到了SkeletonGCL在基准测试中的一致改进和其与以往方法的不同之处。</p><blockquote><p>逐字翻译************************************************************************************************************************************************************************************************************************************************************************</p></blockquote><ol><li><strong>The core idea is to pull together the learned graphs from the same class while pushing away the learned graphs from different classes.</strong><ul><li>核心思想是将来自相同类别的学习图聚集在一起，同时将来自不同类别的学习图推开。</li><li>解释：这句话描述了SkeletonGCL的核心概念。它的目标是通过拉近相同类别的图，同时将不同类别的图分开，以帮助模型更好地理解不同的动作模式。</li></ul></li><li><strong>Since graphs can reveal the action patterns of sequences, enforcing graph consistency in the same class and inconsistency among different classes helps the model understand various motion modes.</strong><ul><li>由于图可以揭示序列的动作模式，==<strong>强化相同类别中的图一致性以及不同类别之间的不一致性有助于模型理解各种运动模式</strong>==。</li><li>解释：这句话强调了在相同类别内保持图的一致性，同时在不同类别之间引入不一致性，以帮助模型更好地理解不同的动作模式。</li></ul></li><li><strong>In addition, to enrich the cross-sequence context, we build memory banks to store the graphs from historical sequences.</strong><ul><li>此外，为了丰富跨序列的上下文，我们建立了存储历史序列图的内存库。</li><li>解释：为了增强模型对跨序列上下文的理解，作者引入了内存库，用于存储历史序列的图信息。</li></ul></li><li><strong>In specific, an instance-level memory bank stores the sequence-wise graphs, which hold the individual properties of each sequence. In contrast, a semantic-level memory bank stores the aggregation of graphs from each class, which contains the class-level representation.</strong><ul><li>具体而言，<strong>==一个实例级内存库存储了按序列划分的图，其中包含了每个序列的个体属性。相比之下，一个语义级内存库存储了来自每个类别的图的汇总，其中包含了类别级别的表示==</strong></li><li>解释：作者详细解释了内存库的构建方式。实例级内存库存储了按序列组织的图，这些图包含了每个序列的特定属性。语义级内存库存储了来自每个类别的图的汇总信息，其中包含了类别级别的信息。</li></ul></li><li><strong>The two banks are complementary to each other, enabling us to leverage more samples.</strong><ul><li>这两个内存库相辅相成，使我们能够利用更多的样本。</li><li>解释：这句话强调了两个内存库的互补性，它们一起提供了更多的样本信息，有助于模型的训练。</li></ul></li><li><strong>SkeletonGCL can be seamlessly combined with existing GCNs.</strong><ul><li>SkeletonGCL可以无缝地与现有的GCNs结合使用。</li><li>解释：==SkeletonGCL可以轻松集成到现有的GCN模型中，为这些模型增加了图对比学习的功能==。</li></ul></li><li><strong>Eventually, we combine SkeletonGCL with three GCNs (2S-AGCN (Shi et al., 2019), CTR-GCN (Chen et al., 2021b), and InfoGCN (Chi et al., 2022)), and conduct experiments on three popular datasets (NTU60 (Shahroudy et al., 2016), NTU120 (Liu et al., 2019) and NW-UCLA (Wang et al., 2014)).</strong><ul><li>最后，我们将SkeletonGCL与三个GCNs（2S-AGCN（Shi等人，2019），CTR-GCN（Chen等人，2021b）和InfoGCN（Chi等人，2022））相结合，并在三个流行的数据集上进行实验（NTU60（Shahroudy等人，2016年），NTU120（Liu等人，2019年）和NW-UCLA（Wang等人，2014年））。</li><li>解释：这句话描述了作者的实验设计。他们将SkeletonGCL与三个不同的GCN模型结合，并在三个常用的数据集上进行了实验以评估其性能。</li></ul></li><li><strong>SkeletonGCL achieves consistent improvements with these models using different testing protocols (single-modal or multi-modal) on all three datasets, which widely demonstrates the effectiveness of our design.</strong><ul><li>SkeletonGCL在这些模型上使用不同的测试协议（单模态或多模态）在所有三个数据集上都取得了一致的改进，这充分证明了我们设计的有效性。</li><li>解释：这句话总结了实验结果，指出SkeletonGCL在不同的测试协议和数据集上都取得了一致的性能改进，证明了其有效性。</li></ul></li><li><strong>Notably, SkeletonGCL only introduces a small amount of training consumption but has no impact at the test stage.</strong><ul><li>值得注意的是，SkeletonGCL仅引入了少量的训练开销，但在测试阶段没有影响。</li><li>解释：这句话强调了SkeletonGCL的训练效率以及其在测试阶段的轻量性。</li></ul></li><li><strong>Though there exist some works that apply contrastive learning in skeleton-based action recognition (Li et al., 2021; Guo et al., 2022; Mao et al., 2022), our method differs from them as follows:</strong><ul><li>尽管存在一些将对比学习应用于基于骨骼的动作识别的工作（Li等人，2021年；Guo等人，2022年；Mao等人，2022年），但我们的方法与它们有以下不同：</li><li>解释：这句话强调了SkeletonGCL与之前的工作的不同之处，特别是与应用对比学习的工作的区别。</li></ul></li><li><strong>Summarily, the contribution of this paper can be concluded as follows:</strong><ul><li>总之，本文的贡献可以总结如下：</li><li>解释：这句话总结了论文的主要贡献，包括提出了一种新的图学习方法，结合了对比学习和跨序列图学习的思想，实现了一致的性能改进，并且具有高效的训练性能</li></ul></li></ol><h2 id="method-2">method</h2><h3 id="3-1-preliminary">3.1 preliminary</h3><ol><li><strong>We denote a human skeleton as a vertex set V = {v1, v2, …, vN}, where N denotes the number of vertices. For each vertex vi, the feature dimension is set as C. Hence, a skeleton sequence with T frames can be denoted as X ∈ RT×N×C.</strong><ul><li>我们将人类骨架表示为一个顶点集合 V = {v1, v2, …, vN}，其中 N 表示顶点的数量。对于每个顶点 vi，特征维度设置为 C。因此，具有 T 帧的骨架序列可以表示为 X ∈ RT×N×C。</li><li>解释：这部分介绍了作者如何表示人类骨架数据，其中 V 是顶点集合，vi 表示单个顶点，N 表示顶点数量，C 表示特征维度，X 是表示骨架序列的张量，包括时间帧数 T、顶点数量 N 和特征维度 C。</li></ul></li><li><strong>Graph topology is used to represent the correlations between joints, formulated as g.</strong><ul><li>图拓扑结构用于表示关节之间的相关性，表示为 g。</li><li>解释：这句话说明了作者将图拓扑结构用于表示关节之间的关系，将其表示为符号 g。</li></ul></li></ol><h4 id="GCNs-in-Skeleton-Based-Action-Recognition">GCNs in Skeleton-Based Action Recognition.</h4><ol><li><strong>GCNs in Skeleton-Based Action Recognition. Generally, GCN models alternatively apply graph convolutions and temporal convolutions to extract the spatial configuration and motion pattern of skeletons, respectively.</strong><ul><li>骨骼动作识别中的GCN。通常情况下，<strong>GCN模型交替应用图卷积和时间卷积来分别提取骨架的空间配置和运动模式</strong>。</li><li>解释：这部分介绍了骨骼动作识别中的GCN模型的工作原理，它们交替使用图卷积和时间卷积来提取骨架的空间和时间特征。</li></ul></li><li><strong>The graph g is vital for graph convolutions since it determines the message passing among joints. In current adaptive GCNs, g is learned within each sequence and has different sizes…</strong><ul><li>图 g 在图卷积中至关重要，因为它决定了关节之间的消息传递。<strong>在当前的自适应GCNs中，g 在每个序列中学习，而且具有不同的大小</strong>…</li><li>解释：这句话指出了图 g 在图卷积中的关键作用，它决定了关节之间的信息传递。当前的自适应GCNs中，每个序列都学习自己的图 g，而且这些图的大小可以不同。</li></ul></li><li><strong>The graph convolution is defined as: XS = KS∑k=1 gkXWkS, (1) where XS ∈ RT×N×C denotes the spatial extracted feature with C channels, and WS ∈ RKS×C×C denotes the spatial feature transformation filters.</strong><ul><li>图卷积的定义如下：XS = KS∑k=1 gkXWkS，其中 XS ∈ RT×N×C 表示带有 C 通道的空间提取特征，而 WS ∈ RKS×C×C 表示空间特征变换滤波器。</li></ul></li><li>解释：这句话给出了图卷积的数学定义，其中 XS 表示空间特征，KS 表示子图的数量，gk 表示子图，X 表示输入特征，WkS 表示空间特征变换滤波器。</li></ol><h4 id="Self-Supervised-Contrastive-Learning">Self Supervised Contrastive Learning</h4><ol><li><strong>Self-Supervised Contrastive Learning. In the context of self-supervised contrastive learning, each input sample is processed by data augmentations to produce a positive pair: I and I+. Through a feature extraction network, I and I+ are transformed into feature vectors f ∈ RD and f+ ∈ RD.</strong><ul><li>自监督对比学习。在自监督对比学习的背景下，每个输入样本都经过数据增强处理，以产生正样本对：I 和 I+。通过特征提取网络，I 和 I+ 被转化为特征向量 f ∈ RD 和 f+ ∈ RD。</li><li>解释：这句话描述了自监督对比学习的背景，其中输入样本经过数据增强处理，产生正样本对，然后通过特征提取网络将它们转化为特征向量。</li></ul></li><li><strong>As for the negative samples, they are selected from the dataset excluding I and I+, represented as a negative set N−. Each negative in N− is denoted as f− ∈ RD. The similarity between two feature vectors is calculated as sim(f+, f−) =f+f− .</strong><ul><li>至于负样本，它们是从除了 I 和 I+ 之外的数据集中选择的，表示为负样本集合 N−。N− 中的每个负样本用 f− ∈ RD 表示。两个特征向量之间的相似度计算为 sim(f+, f−) =f+f− 。</li><li>解释：这部分描述了如何选择负样本，并计算特征向量之间的相似度，其中正样本是 I 和 I+，负样本从数据集中排除了这两个样本。</li></ul></li><li><strong>InfoNCE (Gutmann &amp; Hyv¨arinen, 2010; Oordet al., 2018) is widely adopted for contrastive learning, which is formulated as: LNCE = −log sim(f, f+)/τ sim(f, f+)/τ + f−∈N− sim(f, f−)/τ , (3) where temperature τ &gt; 0 is a hyper-parameter.</strong><ul><li>InfoNCE（Gutmann＆Hyv¨arinen，2010; Oord等人，2018）被广泛用于对比学习，其公式为：LNCE = −log sim(f, f+)/τ sim(f, f+)/τ + f−∈N− sim(f, f−)/τ ，其中温度 τ &gt; 0 是一个超参数。</li><li>解释：这句话介绍了InfoNCE对比学习方法的公式，它用于计算对比损失，其中 τ 是一个超参数。对比学习的目标是使正样本之间的相似度高，而正样本与负样本之间的相似度低。</li></ul></li></ol><h3 id="3-2-GRAPH-CONTRASTIVE-LEARNING">3.2 GRAPH CONTRASTIVE LEARNING</h3><ol><li><strong>The proposed SkeletonGCL is illustrated in Fig. 2. The framework consists of two branches, where the classification branch is parallel to the graph contrast branch.</strong><ul><li>所提出的SkeletonGCL在图2中进行了说明。该框架由两个分支组成，其中分类分支与图对比分支平行。</li><li>解释：这句话介绍了SkeletonGCL方法，它包含两个分支，一个用于分类，另一个用于图对比。</li></ul></li><li><strong>Graph Projection Head. In order to contrast the graphs in a common feature space, we embed the graphs into vectors by a graph projection head.</strong><ul><li>图投影头。为了在公共特征空间中对比图，我们通过图投影头将图嵌入为向量。</li><li>解释：这里提到了一个“图投影头”，它用于将图形式的数据嵌入到向量空间中，以便在共同的特征空间中进行对比学习。</li></ul></li><li><strong>Memory Bank. To enrich the cross-sequence context, we build memory banks to store the cross-batch graphs.</strong><ul><li>内存银行。为了丰富跨序列的上下文信息，我们构建了内存银行来存储跨批次的图形数据。</li><li>解释：内存银行用于存储来自不同批次的图形数据，以增强模型对跨序列上下文的理解。</li></ul></li><li><strong>Hard Sampling. As the training continues, most samples become too easy, which contribute less to the training. Therefore, methods in… are proposed to use hard mining strategies to focus on informative samples.</strong><ul><li>硬采样。随着训练的进行，大多数样本变得太容易，对训练的贡献较小。因此，提出了…中的方法，用于使用硬采样策略，以便集中关注信息丰富的样本。</li><li>解释：在训练过程中，一些样本可能会变得太容易，无法提供足够的信息。因此，硬采样策略被用来选择更具挑战性的样本，以改善训练效果。作者提到了一种方法来选择硬样本。</li></ul></li></ol><p>这一章节主要描述了SkeletonGCL方法的关键部分，包括图对比和内存库的构建，以及硬采样策略的应用。该方法旨在通过对比学习来提取骨架数据中的特征，并借助内存库来丰富跨序列的上下文信息，以改善动作识别性能。硬采样策略用于选择更具挑战性的样本，以更有效地进行训练。</p><h3 id="3-2-1Graph-Projection-Head">3.2.1Graph Projection Head</h3><p>为了在一个共同的特征空间中对比这些图形，我们通过一个图形投影头将这些图形嵌入到向量中。不同GCNs的投影头是相似的（详细信息请参见附录6.1）。在图2中，以CTR-GCN（Chen等人，2021b）中学到的图形g∈RKS×C×N×N为例，我们首先通过平均池化层将g在通道维度上挤压成g∈RKS×N×N。然后，我们将图g展平为1D向量，表示为g∈RKSN2，并通过FC层WG∈RKSN2×Cg将g投影到向量v∈RCg中。WG中的不同通道对应于图中的不同顶点，因此图形投影是顶点感知的，因此可以编码骨架的结构。随后，我们使用v更新了两个内存库，这些内存库在图2中有详细说明。</p><p>这段话描述了如何将学习到的图形嵌入到向量中以进行对比学习。首先，图形被平均池化以适应相同维度，然后被展平并通过FC层进行投影。这些投影的目的是将图形编码为向量，以便后续进行对比学习。投影的权重是由不同的顶点和通道组成的，以确保对图形的不同部分进行了考虑。这些向量将用于更新两个内存库，这些库将在后文中进一步详细解释。这个过程有助于提高图形学习的效果，使模型能够更好地理解动作序列的结构和特征。</p><h3 id="3-2-2-Memory-bank">3.2.2 Memory bank</h3><p>为了丰富跨序列的上下文，我们建立了内存库来存储跨批次的图形数据。具体而言，我们构建了两个内存库，即一个是实例级别的内存库MIns，维度为RCk×P×Cg，另一个是语义级别的内存库MSem，维度为RCk×Cg。其中，P表示每个类别在MIns中存储的实例数。具体而言，MIns中的每个元素代表来自一个类别的图形实例。相比之下，MSem中的每个元素代表一个类别的图形汇总。因此，这两个内存库在互补的层次上工作，实例级别的内存库强调每个序列的实例区分，而语义级别的内存库涵盖了跨序列的类别属性。</p><p>我们以先进先出的方式更新MIns，以保持每个类别的实例数量为P。至于MSem，我们使用动量更新策略，该策略整合了来自当前时间戳和所有先前时间戳的相同类别的图形，被视为长期表示。动量更新定义如下：</p><p>mc∗ sem ← αmc∗ sem + (1 − α)v， 其中mc∗ sem是类别c∗的表示，c∗是输入I的类别标签，0&lt;α&lt;1是一个超参数。</p><p>这段文本描述了为了增强跨序列的上下文信息，作者建立了两个内存库：实例级别的内存库（MIns）和语义级别的内存库（MSem）。这些内存库用于存储不同类别的图形数据，从而帮助模型更好地理解不同动作模式。MIns按照先进先出的原则更新，而MSem使用动量更新策略，整合了来自多个时间戳的图形数据，以获得更全面的类别属性信息。这些内存库允许模型在训练过程中利用不同类别和实例的上下文信息，有助于提高动作识别的性能。</p><h3 id="3-2-4Hard-Sampling">3.2.4Hard Sampling.</h3><p>随着训练的进行，大多数样本变得太容易，对训练的贡献减少。因此，提出了一些方法（如Tabassum等人在2022年，Robinson等人在2020年，Kalantidis等人在2020年，Wang等人在2021年）来使用硬采样策略，集中关注信息丰富的样本。在本文中，考虑到MIns中包含大量实例，与所有这些实例进行对比自然会导致冗余并妨碍训练。为了缓解这个问题，我们提出在MIns中进行硬样本挖掘。具体来说，我们将相似度计算sim(v, v)作为评估样本难度的标准。更难的正样本具有较低的相似度，而更难的负样本具有较高的相似度。总而言之，对于MIns，我们选择K+ H个最难的正样本、K− H个最难的负样本和K− R个随机负样本。</p><p>这段话讨论了训练过程中的样本难度问题。随着训练的进行，许多样本变得太容易，它们对进一步的训练没有太大帮助。因此，一些研究提出了使用“硬采样策略”的方法，专注于选择更具挑战性和信息丰富的样本来进行训练。然而，在本文中，由于MIns中包含了大量的实例，与所有这些实例进行对比会导致重复的信息，而且会妨碍训练。因此，作者提出了在MIns中进行“硬样本挖掘”的方法，以选择更具挑战性的正样本和负样本。为了确定难度，他们使用样本之间的相似度计算，较低相似度的样本被认为更难，而较高相似度的样本被认为更容易。最终，他们选择了一定数量的最难的正样本、最难的负样本和随机负样本来进行训练，以更好地应对样本难度的问题。</p><h1>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations用于基于骨架的动作识别和不断增强的分层一致对比学习</h1><h2 id="abstract-4">abstract</h2><p>这段文字描述了一个名为&quot;HiCLR&quot;的骨骼动作识别方法，它基于对比学习技术。<strong>对比学习</strong>已被证明在自监督骨骼动作识别中具有益处。大多数对比学习方法利用<strong>精心设计的增强技术</strong>，以生成相同语义的<strong>骨骼不同运动模式的样本对</strong>。然而，目前<strong>尚未解决的问题是如何应用强增强技术</strong>，因为它们会扭曲图像/骨骼的结构并导致语义丢失，从而使训练不稳定。</p><p>因此，在本文中，研究人员探讨了<strong>采用强增强技术的潜力</strong>，并提出了一种<strong>通用的分层一致对比学习框架（HiCLR）<strong>用于骨骼动作识别。具体而言，首先</strong>设计了一个逐渐增强的增强策略</strong>，以<strong>生成多个有序的正样本对</strong>，这些<strong>对</strong>  <strong>有助于实现从不同视角学到的表示的一致性</strong>。然后，提出了一种<strong>不对称损失</strong>，通过特征空间中的方向性聚类操作来强制执行分层一致性，将来自强增强视角的表示拉近到来自弱增强视角的表示，以获得更好的泛化性能。</p><p>同时，研究人员还提出并评估了三种用于3D骨骼的强增强技术，以证明他们方法的有效性。大量实验证明，HiCLR在三个大规模数据集（NTU60、NTU120和PKUMMD）上明显优于最先进的方法。</p><p><strong>简而言之，这个方法研究了如何应用强增强技术来提高骨骼动作识别的性能，并提出了一种分层一致对比学习框架（HiCLR），该框架通过生成有序的正样本对和引入不对称损失来实现一致性学习。它在多个大规模数据集上表现出色，超越了其他方法。</strong></p><h2 id="introduction-3">introduction</h2><p><strong>1 Introduction</strong></p><p>人体动作识别对于连接人工系统和人类在现实世界中的互动至关重要。它在视频理解、人机交互、娱乐等领域广泛应用（Tang等，2020年；Rodomagoulakis等，2016年；Shotton等，2011年）。由于轻量化、稳健性和隐私保护等优势，基于3D骨架数据的动作识别近来引起了广泛关注。有许多作品针对基于骨架的动作识别（Shi等，2019年；Cheng等，2020年；Liu等，2020年b；Chen等，2021年），但大多数是以全监督方式设计的，需要大量标记的数据。考虑到大规模数据集的注释成本高昂且耗时，最近越来越多的研究人员开始关注从无标签数据中学习表示（Lin等，2020年；Li等，2021年；Kim等，2022年）。</p><p>在各种自监督学习方法中，对比学习是一种有效的方法，并且已经被证明对于基于骨架的动作识别（Rao等，2021年；Thoker，Doughty和Snoek，2021年；Li等，2021年）非常成功。对于对比学习，数据增强被证明非常关键，它为相同语义引入了各种运动模式，直接影响了模型学习的特征表示的质量（Tian等，2020年；Guo等，2022年）。然而，对于骨架数据，仍然没有完全研究到应该使用什么增强以及如何使用它。与人体动作的RGB表示相比，3D骨架数据是一种更高级的模态表示，这加剧了对增强的敏感性，使得在设计更高级的对比学习方法中谨慎选择增强成为瓶颈。一方面，如表3右侧所示，一些增强（例如随机遮挡）会导致基线算法性能下降。根据（Bai等，2022年），这些增强被称为强增强（也称为重增强），它们会扭曲图像/骨架结构并引起语义损失，从而导致不稳定的训练。一些研究揭示了使用强数据增强的潜力（Cubuk等，2020年；Wang和Qi，2021年）。但是，<strong>如何直接准确地测量和约束对比学习的基础——一致性，从强增强的视角，仍然具有困难。</strong></p><p>另一方面，基于对比学习的先前作品通常平等对待所有增强，<strong>忽视了不同增强的重要性差异</strong>。最近的研究（Tian等，2020年；Zhang和Ma，2022年）已经表明，每种增强对下游任务有不同的影响，因此<strong>在不区分应用增强的重要性的情况下学习增强后的不变性，会不可避免地导致下游任务的非最优表示</strong>。</p><p>为了解决上述问题，我们受到启发，探索了一种应用逐渐增强的一般性对比框架。在本文中，我们提出了一种新颖的层次一致对比学习框架（HiCLR），它从<strong>层次逐渐增强的不变性中学习，并以不同的方式处理不同的增强</strong>。与以往的作品（Li等，2021年；Guo等，2022年）不同，我们<strong>关注如何更好地利用和从包括强增强在内的多个增强中受益</strong>。具体地，我们<strong>提出了一个逐渐增强的层次增强策略，用于构建多个相关的正样本对</strong>。这些正样本中的每一个都是通过比前一个更多的增强生成的，并扩展了特征分布。然后，为了更好地利用强增强带来的新模式，我们提出了一种不对称的层次学习策略。与直接同时学习所有增强的方法不同，我们的目标建议了一种不同增强的层次一致学习方式，如图1所示。与此同时，这种不对称设计鼓励强增强视图与弱增强视图相似，有助于模型更好地泛化。基于我们的新框架，我们进一步分析和评估了不同的强增强策略。在图形卷积网络（GCNs）和变换器上进行了大量实验，以验证我们方法的有效性。</p><p>我们的贡献可以总结如下：</p><p>• 我们提出了一个层次一致的对比学习框架HiCLR，成功地将强增强引入传统的骨架对比学习流程中。层次设计集成了不同的增强，减轻了从强增强视图学习一致性的困难，这些视图伴随着严重的语义信息丢失。</p><p>• 我们引入了逐渐增强，以及不对称的层次学习，用于约束构建的正样本对的表示一致性。借助这些，模型通过利用强增强带来的丰富信息进一步提高了表示能力。</p><p>• 基于我们提出的框架，我们进一步设计和分析了三种强增强策略：随机遮罩、删除/添加边缘和SkeleAdaIN。尽管在直接应用它们时观察到了不利影响，但它们在我们的HiCLR下变得明显有效，并超过了最先进的性能。</p><p>以下是对要点的总结：</p><ol><li>引言介绍了人体动作识别的重要性，特别是基于3D骨架数据的动作识别，以及当前存在的问题，包括大规模数据集的标注成本和对数据表示的需求。</li><li>对比学习作为一种自监督学习方法，被广泛用于骨架数据的动作识别，但如何选择和使用增强对骨架数据进行了初步讨论。</li><li>3D骨架数据的高级表示增加了对比学习对增强的敏感性，这导致了对增强选择的谨慎。某些增强被称为强增强，它们可能导致性能下降和训练不稳定。</li><li>先前的对比学习方法通常平等对待所有增强，没有区分应用增强的重要性。</li><li>为了解决这些问题，引入了逐渐增强的概念，提出了HiCLR框架，该框架利用层次结构的增强策略，有针对性地处理不同的增强。</li><li>使用不对称的层次学习策略，鼓励强增强视图与弱增强视图相似，以改善模型的泛化能力。</li><li>实验结果表明，HiCLR在骨架动作识别中的性能显著优于先前方法，并且引入了强增强后表现出色。</li></ol><p>这些要点突出了HiCLR框架的创新性和有效性，以及强增强在骨架动作识别中的潜力。</p><h3 id="Contrastive-Representation-Learning-strong-augmentations">Contrastive Representation Learning  &amp;&amp; strong augmentations</h3><blockquote><p>列举了对比学习在无监督方向的应用以及影响</p><p>提及了一些主要的强增强的来源 以及本文的基础</p></blockquote><blockquote><p>这些作品仍然缺乏使用强增强的有效设计，并且强增强的潜力没有得到充分利用。</p></blockquote><h2 id="3-Proposed-Method-HiCLR">3 Proposed Method: HiCLR</h2><h4 id="Contrastive-Learning-for-Skeleton">Contrastive Learning for Skeleton</h4><p>这里首先我们要给出对于骨架的对比学习（Bai等人，2022年）的统一公式，遵循最近的研究，包括以下要素：</p><ul><li>数据增强模块，包含增强策略集T，用于生成原始数据的不同视图，这些视图被视为正样本对。</li><li>查询/关键编码器f(·)，用于将输入映射到潜在特征空间。</li><li>嵌入式投影仪h(·)，用于将潜在特征映射到嵌入空间，自监督损失将应用于该空间。</li><li>自监督损失，在嵌入空间中执行特征聚类操作。</li></ul><p>总结： 这一章介绍了骨架的对比学习方法的基本构成要素，包括数据增强模块、查询/关键编码器、嵌入式投影仪和自监督损失。<strong>这些要素共同构成了骨架对比学习的框架，用于在嵌入空间中执行特征聚类操作</strong>。</p><p>SkeletonCLR (Li et al. 2021)采用了最近的<strong>对比学习框架MoCov2 (Chen et al. 2020b) 作为我们方法的基准算法</strong>。具体来说，给定一个骨架序列s，通过T构建了正样本对（x，x）。随后，我们可以通过查询/关键编码器f(·)和嵌入式投影仪h(·)分别获得<strong>相应的特征表示（z，z</strong>）。维护了一个<strong>存储许多负样本</strong>用于对比学习的<strong>内存队列M</strong>。整个网络通过<strong>InfoNCE损失</strong>（Oord，Li和Vinyals 2018）进行优化： LInfo = −log exp(z · z/τ) exp(z · z/τ) + M i=1 exp(z · mi/τ) , (1) 其中mi是与第i个负样本对应的M中的特征，M是负特征的数量，τ是超参数。在每个训练步骤之后，批处理中的所有样本将按照先进先出的原则更新为M中的负样本。Key编码器是Query编码器的动量更新版本，通过梯度更新。具体来说，将Query编码器和Key编码器的参数分别表示为θq和θk，关键编码器的更新如下：θk ← mθk + (1 −m)θq，其中m ∈ [0, 1) 是动量系数。</p><p>总结： 这一部分<strong>介绍了SkeletonCLR方法</strong>，它<strong>采用了对比学习框架，并通过构建正样本对和维护内存队列来执行对比学习。方法的优化是通过InfoNCE损失进行的，该损失用于衡量正样本和负样本之间的相似性。关键编码器是通过动量更新来不断更新的，以提高对比学习的性能。</strong></p><h4 id="Hierarchical-Consistent-Contrastive-Learning-分层次的一致性对比学习">Hierarchical Consistent Contrastive Learning[分层次的一致性对比学习]</h4><p>传统的对比学习方法<strong>直接一次性应用增强集合以生成正样本对</strong>。当应用强烈的增强时，这些正样本会严重受到语义信息的丢失影响，共享较少的相关性。然而，从这些退化样本的一致性约束中学习到有用的信息非常困难。为了解决这个问题，我们提出了一个分层一致性对比学习框架。我们通过<strong>逐渐增加的增强逐渐生成一系列高度相关的正样本对</strong>。因此，这些样本对提供了<strong>特征相似性的分层指导，并有助于模型从不同视角的强增强中学习一致性的知识</strong>。</p><p>首先，我们概述一下我们的方法。如图2所示，HiCLR具有多个分支来提取特征，主要由两个组件组成：（1）<strong>逐渐增长的增强策略</strong>，构建与不同增强对应的多个正样本对。（2）来自<strong>强增强视图的表示一致性的不对称分层学习约束</strong>。接下来，我们将详细介绍每个组件。</p><p>总结： 本节介绍了分层一致性对比学习（HiCLR）方法，旨在解决传统对比学习方法在应用强增强时可能出现的语义信息丢失问题。HiCLR通过逐渐增加的增强逐渐生成高度相关的正样本对，从而提供了分层的特征相似性指导，并有助于模型从不同视角的强增强中学习一致性的知识。方法主要包括逐渐增长的增强策略和不对称分层学习约束两个组件。</p><h5 id="1-Gradual-growing-augmentation-逐渐增长增强">1)Gradual growing augmentation.逐渐增长增强</h5><p>为了促进学习过程，以实现更好的增强不变性，<strong>引入了逐渐增长的增强策略</strong>。增强策略包括多个增强集合，每个集合是现有集合的扩展版本。借助这一策略，生成了<strong>多个有序的正样本对</strong>，其扭曲逐渐增加。</p><p>下面我们正式描述一下我们的增长增强流程。所提出的增长分层增强策略构建了以下增强集合：T0，T1，…，Tk-1，其中k是不同增强集合的总数。T0包含基本的增强策略，并且<strong>每个集合比前一个集合采用更多的增强实例</strong>。这些集合可以表示为：</p><p>T0 = {a0,0}, T1 = {a0,1, a1,1}, … Tk-1 = {a0,k-1, a1,k-1, … , ak-1,k-1},</p><p>其中ai,j表示<strong>从属于第i个增强策略的第j个增强集合中采样的实例</strong>。请注意，==我们<strong>在每个增强集合中重新采样每个增强策略的实例</strong>，这意味着ai,j ≠ ai,j’，j ≠ j’。重新采样策略进一步扩展了特征分布，使模型能够为下游任务学习到更可区分的特征空间==。</p><p>借助该模块，我们构建了k-1个有序正样本对(v0, v1)，…，(vk-2, vk-1)，其中vi = Ti (s)。与以前的工作不同，应用于一个正样本对的增强是不同的，这==使得模型在特征聚类方面具有方向性==。同时，我们还可以通过T0（用于查询和关键）获得基本正样本对，如第3.1节所述，(v0, v0)。逐渐增长的增强策略使模型<strong>能够通过调整其应用的分支并解耦不同增强的学习</strong>来以不同方式处理增强。</p><p>总结： 本节介绍了逐渐增长的增强策略，旨在提高模型对增强的不变性。该策略包括多个逐渐扩展的增强集合，每个集合包含多个增强实例，逐渐增加扭曲程度。这种方法允许模型根据应用的增强和不同的学习来处理增强，生成有序的正样本对，从而有助于特征聚类和下游任务的学习。</p><h5 id="2-Asymmetric-hierarchical-learning-非对称分层学习">2) Asymmetric hierarchical learning.非对称分层学习</h5><p>先前的对比学习工作使用了等式（1）中的InfoNCE损失进行表示学习。然而，当应用强增强时，这通常会导致性能下降，这是因为不同的增强视图之间缺乏足够的互信息。因此，提出了一种<strong>分层自监督学习目标，以学习多个增强视图的表示一致性</strong>。</p><p>如图2所示，首先对正样本对进行特征嵌入编码。形式上，对于骨架序列s，我们构建了<strong>正样本对</strong>（v0，v1），…，（vk-2，vk-1）和（v0，v0），如上所述。然后，依次应<strong>用查询编码器fθq和MLP头部hθq来提取特征表示</strong>： zi = hθq (fθq (vi)) ，i = 0, 1, …，k-1。 同样，我们可以通过关键编码器fθk和MLP hθk获得特征表示z0： z0 = hθk (fθk (v0))。</p><p>模型通过不同的增强视图（vi-1，vi）的特征相似性来<strong>优化学习相邻分支之间的表示一致性</strong>。由于==这些相邻视图共享更多的增强策略，它允许目标特征更顺利地收敛到潜在聚类的中心==。然而，根据以前的工作（Bai等，2022），由于严重的扭曲，当强度增强的视图用作模拟目标时，可能会导致性能下降。因此，我们设计了一个不对称损失来单方面地将特征拉近。</p><p>==分层自监督学习目标通过相邻分支的特征相似性计算，可以表述为： Lh = k-1∑i=1 sim (zi, stopgrad (zi-1))。 在这里，我们使用<strong>stop-gradient（stopgrad）操作</strong>来获取<strong>更自信的相似性学习目标</strong>。<strong>强度增强的视图zi受限于减小与弱增强的视图zi-1的特征距离，但反之则不然。sim（·）可以是任何度量两个特征嵌入之间相似性的函数，例如余弦相似性和Kullback-Leibler（KL）散度（Kullback和Leibler，1951）。这可以看作是一种不对称的表示一致性学习设计，类似于Simsiam（Chen和He，2021），BYOL（Grill等，2020）和CO2（Wei等，2020）。通过从多个正样本对中进行不对称分层学习，模型利用了强增强带来的丰富信息，进一步提高了对下游任务的泛化能力。</strong>==</p><p>总结： 这一部分介绍了不对称分层学习的方法，旨在提高对强增强的不变性，并通过从多个正样本对中学习来自丰富信息，进一步提高了对下游任务的泛化能力。</p><h4 id="3-Instantiation">3) Instantiation.</h4><p>我们接下来给出了我们方法的一个具体实例。对于不对称分层学习，我们使用KL散度作为sim（·）函数。**一个问题是很难计算特征zi的理想准确分布。**受到（Wang and Qi 2021）的启发，我们获取了特征zi的条件分布，其中包括由关键编码器输出的正特征和M中维护的大量负特征。具体来说，zi的条件分布如下： p（z | zi）= exp（z · zi / τ） exp（z0 · zi / τ）+ M i = 1 exp（mi · zi / τ） 。==等式（6）描述了由正特征和负特征测量的特征zi的相似性分布。根据Wang和Qi的发现（Wang和Qi 2021），通过随机初始化的网络，p（z | zi）和p（z | zi-1）的分布是相似的==。这启发我们优化p（z | zi）和p（z | zi-1）之间的分布距离，即DKL（stopgrad（p（z | zi-1）），p（z | zi））作为sim（·），以学习不同增强视图之间的一致性。此外，我们将LInfo应用于基本正样本对（z0，z0）并联合优化模型。总体损失如下所示： L = LInfo + λhLh， 其中λh是分层自监督损失的权重。对于增强，该模型被实例化为k=3，包括基本增强集（对于a0，<em>）、正常增强集（对于a1，</em>）和随机遮罩（对于a2，*）。我们将在下一节更多讨论包括随机遮罩在内的强增强。</p><p>总结： 该部分描述了该方法的一个具体实例，详细介绍了不对称分层学习的方法，以及用于计算特征分布的KL散度。还介绍了损失函数的组成部分，包括信息最大化自监督损失（LInfo）和分层自监督损失（Lh），它们的组合构成了总体损失L。</p><blockquote></blockquote><h1><strong>InfoGCN: Representation Learning for Human Skeleton-based Action Recognition—InfoGCN:基于骨骼的动作识别的表征学习</strong></h1><h2 id="abstract-5">abstract</h2><ol><li>“Human skeleton-based action recognition offers a valuable means to understand the <strong>intricacies of human behavior</strong> because it can handle the complex relationships between physical constraints and intention.”<ul><li>翻译：基于人体骨骼的动作识别为理解人类行为的复杂性提供了有价值的手段，因为它能够处<strong>理物理约束和意图之间的复杂关系</strong>。</li><li>解释：这句话介绍了人体骨骼的动作识别的重要性，因为它可以帮助我们理解人类行为中涉及到的物理约束和意图之间的复杂关系。</li></ul></li><li>“Although several studies have focused on encoding a skeleton, less attention has been paid to <strong>embed this information into the latent representations of human action</strong>.”<ul><li>翻译：尽管有几项研究专注于对骨骼进行编码，但较少关注将这些信息嵌入到人类动作的潜在表示中。</li><li>解释：这句话指出，过去的研究主要关注了如何对人体骨骼数据进行编码，但较少关注如何将这些编码后的信息有效地融入到人类动作的潜在表示中，这是本研究的关注点。</li></ul></li><li>“InfoGCN proposes a learning framework for action recognition <strong>combining a novel learning objective and an encoding method</strong>.”<ul><li>翻译：InfoGCN提出了一个用于动作识别的学习框架，结合了一种新颖的学习目标和编码方法。</li><li>解释：这句话提到了一个名为InfoGCN的方法，它提出了一个学习框架，用于动作识别。这个框架结合了一种新颖的学习目标和编码方法，旨在提高动作识别的性能。</li></ul></li><li>“First, we design an <strong>information bottleneck-based learning objective</strong> to guide the model to learn informative but <strong>compact latent representations</strong>.”<ul><li>翻译：首先，我们设计了一个<strong>基于信息瓶颈的学习目标</strong>，以指导模型学习具有信息丰富但紧凑的潜在表示。</li><li>解释：这句话介绍了InfoGCN方法的第一部分。他们设计了<strong>一种基于信息瓶颈的学习目标</strong>，旨在引导模型学习具有信息丰富但紧凑的潜在表示，这有助于提高动作识别的效果。</li></ul></li><li>“To provide discriminative information for classifying action, we introduce attention-based graph convolution that captures the context-dependent intrinsic topology of human action.”<ul><li>翻译：为了提供用于分类动作的区分信息，我们引入了基于注意力的图卷积，用于<strong>捕获与上下文相关的人体动作的内在拓扑结构</strong>。</li><li>解释：这句话说明了InfoGCN方法的另一方面。他们引入了基于注意力的图卷积，以捕获人体动作的内在拓扑结构，这有助于提供区分不同动作的信息。</li></ul></li><li>“In addition, we present a multi-modal representation of the skeleton using the relative position of joints, designed to provide complementary spatial information for joints.”<ul><li>翻译：此外，我们提出了一种<strong>使用关节的相对位置的多模态骨骼表示，旨在为关节提供互补的空间信息。</strong></li><li>解释：这句话介绍了InfoGCN方法的另一个方面，他们提出了一种多模态的骨骼表示方法，利用关节的相对位置信息，以提供对关节的互补空间信息。</li></ul></li><li>“InfoGCN1 surpasses the known state-of-the-art on multiple skeleton-based action recognition benchmarks with the accuracy of 93.0% on NTU RGB+D 60 cross-subject split, 89.8% on NTU RGB+D 120 cross-subject split, and 97.0% on NW-UCLA.”<ul><li>翻译：InfoGCN1在多个基于骨骼的动作识别基准上取得了突破，其准确率分别为NTU RGB+D 60跨主体分割上的93.0％，NTU RGB+D 120跨主体分割上的89.8％，以及NW-UCLA上的97.0％。</li><li>解释：这句话总结了InfoGCN1方法的性能，指出它在多个骨骼动作识别基准测试上取得了比已知最先进方法更高的准确率。</li></ul></li></ol><p>总之，这个摘要介绍了一个名为InfoGCN的方法，它旨在提高基于人体骨骼的动作识别的性能。它通过<strong>引入新颖的学习目标、编码方法和基于注意力的图卷积</strong>，以<strong>及多模态表示来实现这一目标</strong>，并且在多个基准测试中取得了显著的性能提升。</p><h2 id="introduction-4">introduction</h2><p>Introduction: 人体动作识别是计算机视觉中的一个基本问题，具有广泛的应用，包括紧急检测、手语识别和虚拟/增强现实中的手势识别等。特别是基于骨架的人体动作识别因其对杂乱背景的鲁棒性而引起了计算机视觉领域的极大兴趣。骨架表示（skeleton-based）人体动作识别的关键成就之一是基于图卷积网络（GCN）的方法。</p><p>本文介绍了一种<strong>新颖的基于骨架的动作识别预测框架</strong>。我们的方法在三个关键方面推动了技术发展。首先，是<strong>表示学习的算法</strong>。大量研究已经证明，表示学习极大地影响了机器学习任务的性能。我们的方法<strong>受信息瓶颈（IB）理论启发</strong>。我们提出了新的IB目标和相应的损失函数，以便<strong>在条件和边际上压缩输入信息的同时，学习使潜在表示对目标变量最具信息性的表示</strong>，如图1顶部所示。使用提出的目标训练的模型通过<strong>编码隐式和通用的潜在表示来执行识别，将输入级别的物理信息与动作语义相连接</strong>。</p><p>其次，是<strong>骨架的编码方法</strong>。使用<strong>骨连通性（外部拓扑结构）的骨架图表示在固有上有一定局限性</strong>，它可能<strong>忽略了可能的关节关系，称为固有拓扑结构</strong>。例如，当我们自拍时，手拿手机与上半身之间可能存在内在关系，因为我们共同移动它们以将上半身定位在手机屏幕上（如图1中我们模型推断的内在拓扑结构所示）。关节的固有拓扑结构为人体动作识别提供了上下文信息。在这个背景下，我们<strong>开发了一种新的基于自注意力的图卷积（SA-GC）模块</strong>，用于在编码骨架序列时提取内在图结构。如图1底部所示，<strong>对于出现在不同动作中的相似姿势，根据它们的行为背景，推断出的拓扑结构可以是不同的</strong>。</p><p>最后，我们通过利用关节的相对位置提出了多模态骨架表示。<strong>它提供了关节的补充空间信息</strong>。使用表示的模型集合极大地提高了识别性能。</p><p>通过结合上述三个提案，我们引入了一个名为InfoGCN的基于骨架的动作识别新学习框架。为验证我们方法的有效性，我们在骨架基础的动作识别中进行了实证评估，并将我们的结果与三个常用的基准数据集上的竞争性基线进行了比较：NTU RGB+D 60和120以及NW-UCLA。实验结果表明，我们的模型在准确性方面在这三个数据集上均达到了最先进的性能水平。分析表明，所学到的动作的潜在表示遵循了所提出的信息瓶颈约束，并且上下文相关的固有拓扑结构是根据行为背景自适应地推断出来的。</p><p>我们的贡献如下：</p><ul><li>信息瓶颈目标：我们引入了一种基于信息瓶颈的新型学习目标，旨在学习动作的高效压缩潜在表示。</li><li>基于自注意力的图卷积：我们提出了一个SA-GC模块，用于在骨架的空间建模中推断上下文相关的内在拓扑结构。</li><li>多模态表示：我们提出了一种多模态骨架表示，用于模型集合，极大地提高了动作识别性能。</li><li>实验证明：大量实验证明了我们工作的优势。InfoGCN在基于骨架的动作识别中在这三个数据集上均取得了最先进的性能。</li></ul><h2 id="method-3">method</h2><h3 id="3-1Information-Bottleneck-Objectives信息瓶颈目标和损失">3.1Information-Bottleneck Objectives信息瓶颈目标和损失</h3><h4 id="3-1-1Learning-Objective">3.1.1Learning Objective</h4><p>学习目标 我们的目标是设计一个包含与输入变量X（一系列骨架）相关的压缩信息的随机潜在变量Z，同时为了目标变量Y（动作标签）保留最大的信息。这种受限制的优化问题可以通过拉格朗日乘数转化为无约束问题：maxZ I(Z; Y) - β1I(Z; X)，其中I(·; ·)是互信息，β1是拉格朗日乘数。与以前的工作一样，我们假设变量之间的关系遵循图模型Z ← X ↔ Y，<strong>唯一可访问的内容是随机编码器p(z|x)</strong>。在infoGCN中，我们提出了以下与最先进的信息瓶颈(IB)目标等效的目标（请参阅附录）：</p><p>R(Z) = I(Z; Y) - λ1I(Z; X) - λ2I(Z; X|Y)，其中λ1和λ2是控制参数。第一项I(Z; Y)<strong>强制Z对于预测Y具有足够的信息</strong>。第二项确保Z是简洁的。第三项允许在给定类别时，将潜在变量Z相对于输入变量X进行压缩。我们的<strong>目标采用了VIB和CEB中的压缩正则化项的组合</strong>，同时保持了信息瓶颈的哲学。我们得到的目标比[2, 13]的目标更加通用，同时将先前的目标作为特殊情况包括在内（当λ1 = 0时是VIB，当λ2 = 0时是CEB）。</p><h4 id="3-1-2Variational-Bound">3.1.2Variational Bound</h4><p>这里我们推导出了我们IB目标（方程1）的变分界限。R(Z)的每一项的变分界限都是根据最近的研究[2,4,37]进行推导的，这些研究使用可计算的变分界限和深度学习技术来估计互信息。我们使用变分分类器q(y|z)获得了I(Z;Y)的变分下界：</p><p>I(Z;Y) ≥ E_{p(x, y)p(z|x)}[log q(y|z)] + H(Y) (方程2)</p><p>其中RHS的第一项对应于对数似然，RHS的第二项是常数，当底层数据生成分布固定时，它不会影响优化。类似于[13, 17]，我们将r(z)定义为变分边际，r(z|y)定义为变分类条件边际。我们按照[13, 17]的方式获得了I(Z;X)和I(Z;X|Y)的变分上界：</p><p>I(Z;X) ≤ E_{p(x)p(z|x)}[log(p(z|x)/r(z))] (方程3) I(Z;X|Y) ≤ E_{p(x)p(z|x)p(y|x)}[log(p(z|x)/r(z|y))] (方程4)</p><p>将方程（2）至（4）代入方程（1），我们得到了R(Z)的下界：</p><p>R(Z) ≥ E_{p(x, y)p(z|x)}[log q(y|z)] - λ1 E_{p(x)p(z|x)}[log(p(z|x)/r(z))] - λ2 E_{p(x)p(z|x)p(y|x)}[log(p(z|x)/r(z|y))] (方程5)</p><p>方程（2）至（4）的推导在附录中提供。</p><h4 id="3-1-3Training-Loss">3.1.3Training Loss</h4><p><img src="../assets/image-20230925220717504.png" alt="image-20230925220717504"></p><h3 id="3-2-Neural-Architecture">3.2. Neural Architecture</h3><h4 id="3-2-1The-Importance-of-Learning-Intrinsic-Topology">3.2.1The Importance of Learning Intrinsic Topology</h4><p>我们通过展示<strong>仅使用外在拓扑的GC可能导致消息传递方面的严重低效性和信息损失</strong>来==强调内在拓扑的重要性==。假设由于身体结构的双侧对称性，两只手的关节都存在内在关系。这两个叶子节点必须通过物理连接路径传递信息以相互传递信息。当按照GC的机制传递信息时，需要根据消息传递的最短路径长度增加网络的深度，这意味着节点之间的信息交换效率非常低。</p><p>此外，可能会发生信息丢失。GC在对邻居节点的特征进行平均后<strong>执行非线性变换</strong>。如果表示节点信息的特征向量不是线性独立的，那么在平均后很难重构每个邻居的信息。假设α是节点因向量组合而引起的信息稀释的最小部分。如果两个节点之间的距离具有内在关系l，则信息可以以(1 − α)l的最大比率传输。当α &gt; 0时，距离l越长，信息稀释越多。</p><p>一个直接的方法是通过像[19, 26]中那样将邻接矩阵的幂次作为卷积核大小，但这不是理想的，因为它不能动态建模可能的内在拓扑。<strong>更好的解决方案是自适应地推断需要改变信息的关节关系。因此，我们提出了一种利用自注意机制来捕捉内在拓扑的架构。这可以更好地适应复杂的关节关系，从而提高信息传递的效率。</strong></p><h4 id="3-2-2Architecture-Overview">3.2.2Architecture Overview</h4><p>该编码器由一个嵌入块和一堆L = 9个编码块组成，后面跟着一个全局平均池化层。嵌入块将骨架的序列转换为初始关节表示。然后，编码块从初始关节表示中提取时空特征。我们利用了<strong>VAE中的再参数化技巧</strong>[20]。通过一个辅助独立的随机噪声 ϵ ∼ N(0, I)，z 被采样为 z = µ + Σϵ，其中多元高斯分布的均值 µ 和对角协方差矩阵 Σ 是从编码器的输出推断出来的。这个<strong>技巧通过使用基于梯度的优化以端到端的方式估计无偏梯度来使模型可训练</strong>。</p><p>分类器由单一的线性层和 softmax 函数组成，将潜在向量 z 转换为分类分布的模型参数。</p><p>总结：这一部分描述了模型的编码器和分类器。编码器将输入的骨架序列转换成潜在表示，使用了VAE中的再参数化技巧。然后，分类器将这个潜在表示映射到分类分布的参数。这个架构用于将骨架动作映射到动作类别。</p><h4 id="3-2-3Embedding-Block">3.2.3Embedding Block</h4><p>这段文字描述了人体骨架的表示方式以及嵌入块的功能。</p><ol><li>人体骨架可以被表示为一个图 G(V, E)，其中关节作为一组 N 个顶点 V，骨骼作为边 E。边可以表示为一个邻接矩阵 A ∈ RN×N，其中如果关节 i 和 j 在物理上连接，则 Ai,j = 1，否则为 0。一系列<strong>骨架图的序列表示为关节特征张量 X ∈ RT×N×C</strong>，其中 <strong>T 是骨架的总帧数，C 是特征维度。</strong></li><li>嵌入块将关节特征线性转换为具有<strong>可学习参数的 D(0) 维向量</strong>，然后<strong>添加位置嵌入（PE）以注入关节的位置信息</strong>。这里采用了可学习的 PE，它在时间上是共享的。</li></ol><p>总结：嵌入块的作用是<strong>将关节特征转换为低维向量，并通过可学习的参数和位置嵌入来编码关节的位置信息</strong>。这有助于模型理解骨架序列中的时空关系。</p><h4 id="3-2-4Encoding-Block">3.2.4Encoding Block</h4><ol><li>编码块的核心包括两个子模块：基于自注意力的图卷积（SA-GC）模块用于空间建模，以及多尺度时间卷积（MS-TC）模块用于时间建模。关节的输入和隐藏表示通过SA-GC、MS-TC、残差连接和层归一化依次编码。</li><li>空间建模：SA-GC模块用于推断上下文相关的内部拓扑结构。SA-GC<strong>利用关节特征的自注意力来推断内部拓扑</strong>，<strong>并将拓扑作为图卷积的邻域顶点信息</strong>。SA-GC使用自注意力映射关节表示为查询和键，以获得自注意力图。此外，SA-GC还学习了一个共享的拓扑结构，用于不同时间和实例，以及多头自注意力机制来同时关注不同的表示子空间。</li><li>时间建模：MS-TC模块用于建模人体骨架的时间特征。该模块由三个卷积分支组成，具有不同的卷积核尺寸和膨胀率组合。卷积分支的输出被串联在一起，周围有一个带有1×1卷积的残差连接。</li></ol><p>总结：编码块包括SA-GC模块和MS-TC模块，用于分别进行空间建模和时间建模。SA-GC模块利用自注意力来推断内部拓扑结构，而MS-TC模块用于捕捉时间特征。这些模块协同工作，有助于模型理解人体骨架序列的复杂时空关系。</p><h5 id="空间建模">空间建模</h5><p><strong>The core of our encoding block consists of two sub-modules: a Self-Attention based Graph Convolution (SA-GC) module for spatial modeling and a Multi-Scale Temporal Convolution (MS-TC) module for temporal modeling. The input and hidden representation of joints are encoded sequentially with an SA-GC, an MS-TC, a residual connection, and a layer normalization [3] (See Fig. 2).</strong></p><p>编码块的核心包括两个子模块：一个用于空间建模的基于自注意力的图卷积（SA-GC）模块，以及一个用于时间建模的多尺度时间卷积（MS-TC）模块。关节的输入和隐藏表示通过一系列步骤进行编码，包括SA-GC、MS-TC、残差连接以及层归一化。</p><p><strong>Spatial Modeling. We propose a novel module SA-GC to infer context-dependent intrinsic topology. Before describing SA-GC, we revisit vanilla GC [21], which is composed of two processes; 1) average neighborhood vertex features and 2) linearly transform aggregated features. The update rule of hidden representation for GC is as follows</strong></p><p>空间建模：我们提出了一个新的模块SA-GC，用于推断依赖上下文的内部拓扑结构。在描述SA-GC之前，我们回顾了传统的图卷积（GC）[21]，它由两个步骤组成：<strong>1）对邻域顶点特征的平均值和2）线性变换聚合特征</strong>。GC的隐藏表示更新规则如下：</p><p><strong>SA-GC utilizes the self-attention [51] of joint features to infer intrinsic topology and uses the topology as a neighborhood vertex information for the GC. A self-attention is an attention mechanism that relates different joints of the body. Considering all possible joint relations, SA-GC infers positive and bounded weight, called self-attention map, to represent the strength of the relation. We linearly project joint representation Ht to queries and keys of D′ dimensions with learned matrices WQ, WK ∈ RD×D′ to get a self-attention map.</strong></p><p>SA-GC<strong>利用关节特征的自注意力[51]来推断内部拓扑结构，并将该拓扑结构用于GC的邻域顶点信息</strong>。自注意力是一种关联人体不同关节的关注机制。考虑所有可能的关节关系，**SA-GC推断正值且有界的权重，称为自注意力图，以表示关系的强度。**我们将关节表示Ht线性投影到D′维度的查询和键上，使用学习到的矩阵WQ、WK∈RD×D′来获取自注意力图。</p><p><strong>In addition to the self-attention map, we let SA-GC learn a topology ˜A shared over time and instance as in [6, 44]. The shared-topology and self-attention map have M multi-head to make the model jointly attend from different representation subspaces. For a head in 1 ≤ m ≤ M, we combine the shared topology ˜Am ∈ RN×N with the self-attention map SAm(Ht) ∈ RT×N×N to obtain the intrinsic topology.</strong></p><p>除了自注意力图外，我们允许SA-GC学习一个类似于[6, 44]中的<strong>共享拓扑结构˜A</strong>，该结构随时间和实例而共享。<strong>共享的拓扑结构和自注意力图都有M个多头</strong>，以使模型可以从不同的表示子空间共同关注。对于1 ≤ m ≤ M中的一个多头，我们将共享的拓扑结构˜Am ∈ RN×N与自注意力图SAm(Ht) ∈ RT×N×N相结合，以获取内部拓扑结构。</p><p><strong>SA-GC utilizes ˜Am ⊙ SAm(Ht) as neighborhood information for GC. The overall update rule of joint representation is formulated as</strong></p><p>SA-GC利用˜Am ⊙ SAm(Ht)作为GC的邻域信息。关节表示的整体更新规则如下：</p><p><strong>We employ a residual connection [15] with 1 × 1 convolution around the SA-GC module.</strong></p><p>我们在SA-GC模块周围使用1×1卷积进行残差连接。</p><h5 id="时间建模">时间建模</h5><p><strong>Temporal Modeling. To model the temporal feature of the human skeleton, we adopt the MS-TC module [6, 33] as shown in Fig. 2. This module consists of three convolution branches with different combinations of kernel sizes and dilation rates. The outputs of convolution branches are concatenated. A residual connection with 1×1 convolution is around this module.</strong></p><p>时间建模：为了对人体骨架的时间特征进行建模，我们采用如图2所示的MS-TC模块[6, 33]。该模块包括三个卷积分支，具有不同的卷积核尺寸和膨胀率的组合。卷积分支的输出被串联在一起。在该模块周围有一个带有1×1卷积的残差连接。</p><p>总结：编码块包括SA-GC模块和MS-TC模块，用于分别进行空间建模和时间建模。SA-GC模块利用自注意力来推断内部拓扑结构，MS-TC模块用于捕捉时间特征。这些模块协同工作，有助于模型理解人体骨架序列的复杂时空关系。</p><h2 id="3-3Ensemble-with-Multi-Modal-Representation">3.3Ensemble with Multi-Modal Representation</h2><p><strong>In this section, we introduce a generalized form of well-known skeleton representation such as bone and joint, which we call multi-modal representation. We train our model with each modal representation and ensemble upon inference. The representation provides complementary features using the relative position of joints. See Fig. 3 for illustration.</strong></p><p>在这一部分，我们介绍了一种<strong>通用形式的骨骼表示</strong>，如骨骼和关节等，我们称之为多模态表示。我们使用每种模态表示训练我们的模型，并在推理时进行集成。这种<strong>表示利用关节的相对位置提供互补的特征</strong>。请参见图3以进行说明。</p><p><strong>Shi et al. [44] introduce bone information, which is defined as a vector pointing toward its target joint from its source joint that are physically connected, as shown at k = 1 in Fig. 3. Previous works [6, 33, 44] show that the ensemble of models trained with bone and joint information drastically improves action recognition performance, implying that these different representations of skeleton are complementary. We propose multi-modal skeleton representation to define additional representations, based on the fact that bone information is a linear transformation of joint.</strong></p><p>Shi等人[44]介绍了骨骼信息，它被定义为从其物理连接的源关节指向目标关节的向量，如图3中k = 1所示。先前的研究[6, 33, 44]表明**，使用骨骼和关节信息训练的模型集合显著提高了动作识别性能，这意味着骨骼的不同表示与关节的表示相互补充。我们提出了多模态骨骼表示，以定义基于骨骼信息是关节的线性变换的附加表示。**</p><p><strong>In detail, we generalize joint-bone relation at time t as</strong></p><p>具体而言，我们将时间t的关节-骨骼关系进行了泛化：</p><p><strong>\tilde {\textbf {\text {X}}}^{(k)}_t=(\textbf {\text {I}}-\textbf {\text {P}}^k) \textbf {\text {X}}_t, (16)</strong></p><p><strong>where P ∈ RN×N denotes a binary matrix that contains source-target relations of the skeleton graph, Pij = 1 if the i-th joint is the source of the j-th joint, otherwise 0. We set the row corresponding to the center of mass in P as a zero vector so that it does not have a source joint. We refer to ˜X (k) t as the k-th mode representation of a skeleton. The representations with different k values provide distinct spatial features for a joint. We define K = maxv d(v) + 1 for v ∈ V, where d(v) gives the shortest distance in the number of hops between the vertex v and the center of mass. Then, if k = 1, the k-th mode representation ˜X (k) t corresponds to the bone as defined in [44] and if k = K, joint since PK = 0. For instance, at k = 1 in Fig. 3, a joint of the center of mass is represented as a blue dot, so K is equal to 5 in this case.</strong></p><p>其中，P ∈ RN×N表示一个<strong>包含骨骼图源-目标关系的二进制矩阵</strong>，如果第i个关节是第j个关节的源，则Pij = 1，否则为0。我们将与质心对应的行在P中设为零向量，以便它不具有源关节。我们<strong>将˜X (k) t 称为骨骼的第k种模态表示</strong>。具有不同k值的表示为关节提供了不同的空间特征。我们为v ∈ V定义K = maxv d(v) + 1，其中d(v)表示<strong>顶点v与质心之间的最短距离</strong>（通过跳数计算）。然后，如果k = 1，第k种模态表示˜X (k) t 对应于[44]中定义的骨骼，如果k = K，则对应于关节，因为PK = 0。例如，在图3中的k = 1处，质心的关节表示为蓝点，因此在这种情况下K等于5。</p><p><strong>总结：</strong> 本节介绍了一种<strong>多模态表示，它基于骨骼的不同表征，如骨骼和关节。模型使用每种模态</strong></p><h2 id="3-4Learning-Framework">3.4Learning Framework</h2><p><strong>This section describes the overall training regime of InfoGCN. Sequences of the skeletons are batched together after being resized to 64 frames as in [6].</strong></p><p>本节描述了InfoGCN的总体训练过程。骨骼的序列在被调整为64帧后被分批处理，就像[6]中一样。</p><p><strong>The model is updated to minimize the total loss (Eq. (10)) using SGD optimizer with a momentum coefficient 0.9.</strong></p><p>使用动量系数为0.9的SGD优化器更新模型，以最小化总损失（Eq.（10））。</p><p><strong>We set the µr(z) to be 0 so that LmMMD behaves as a regularizer of the norm of µˆp(z). We set the µr(z|y) of each action class as random orthogonal vectors [41] with a scale of 3.</strong></p><p>我们将µr(z)设置为0，以便LmMMD表现为µˆp(z)范数的正则化器。我们将每个动作类别的µr(z|y)设置为随机正交向量[41]，尺度为3。</p><p><strong>During the training, we estimate ˆµp(z) and ˆµp(z|y) by averaging marginal and class conditional marginal latent vectors of a mini-batch, respectively.</strong></p><p>在训练过程中，我们分别通过对小批次的边际和类别条件边际潜在向量进行平均来估计ˆµp(z)和ˆµp(z|y)。</p><p><strong>Also, we employ label smoothing [48] of value 0.1. During inference, we ensemble models that trained with different k-mode representations as the multi-stream ensemble in [6, 33, 44].</strong></p><p>此外，我们采用标签平滑[48]，其值为0.1。在推理过程中，我们将使用不同k模态表示训练的模型进行集成，就像[6, 33, 44]中的多流集成一样。</p><p><strong>总结：</strong> 本节概述了InfoGCN的整体训练框架，包括骨骼序列的处理、损失函数的优化、超参数的设置以及训练和推理过程中的策略。这个框架的目标是训练一个能够准确识别骨骼动作的模型，并且在推理时集成不同的k模态表示以提高性能。</p><h1>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</h1><p><strong>引言</strong></p><p>人类动作识别已成为许多实际应用的重要组成部分，包括但不限于安全领域和人机交互。由于姿势估计[13]和传感器[45]的发展，获取骨架数据并不困难。因此，由于骨架数据对外观和背景变化具有稳健性，以及提供了个体的无偏表示能力，人类骨架数据已经成为传统RGB视频数据的有希望的替代选择。</p><p>许多研究人员探索了这一任务的全面监督方法，这需要大量已标记的训练样本。然而，在现实场景中，由于许多动作的样本收集耗时且昂贵，处理众多的动作类别并不经济。因此，如果没有训练样本，而只有一些语义信息，如新类别的名称、属性或描述可用，那么零样本学习[18, 29]用于识别新类别。由于3D骨架动作数据的注释和标记因包含深度信息和人类动作语义的复杂性而变得更加困难，因此，在实际应用中，零样本基于骨架的动作识别非常有吸引力，因为它可以显著减少收集和标记新动作的需求。</p><p>在图1中，鉴于已提取的视觉和语义特征，零样本学习的核心是建立在已知类别之间的视觉和语义空间之间的连接模型。在测试阶段，学到的模型用于促进从已知类别到未知类别的知识传输。为解决迁移学习问题，零样本动作识别依赖于外部知识库，即来自预训练的大规模语言模型（如Sentence-Bert [28]或CLIP [27]）的每个类别标签的语义嵌入。有效利用语义信息对于弥合两个不同模态之间的差距至关重要。有一些关于零样本基于骨架的动作识别的研究。现有方法[9, 14]将动作序列嵌入到视觉特征中。为了建立视觉和语义空间的连接模型，根据训练阶段中已知类别的数据学习了一个兼容的投影函数[14]或深度度量[6]。然后在测试阶段，测试动作序列的视觉特征与未知类别的句子嵌入[14]或词性标记的单词[41]之间的相似性要么在投影的公共空间中要么通过已学习的度量来衡量。然而，投影操作仅将视觉或语义特征映射到嵌入空间中的公共锚点，忽略了视觉和语义特征的分布之间的全局对齐。此外，已学习的投影或度量未充分利用语义信息来捕获两种模态之间的关联。由于视觉和语义空间之间的显著差异，尝试执行跨模态重建而不对齐分布是具有挑战性的，最终导致难以推广到具有不同分布的新类别。</p><p>其次，在零样本动作识别任务场景中，由于一些语义类别需要动态信息来区分彼此，信息丧失情况变得严重。例如，“行走&quot;和&quot;跳跃&quot;之间的区别仅在局部部分，因为这两个动作序列的初始帧相似。要观察到&quot;跳跃”，必须观察到人体起立过程。因此，对于人类动作来说，利用固有的时间动态信息也在零样本连接模型的泛化能力中发挥作用。</p><p>在本文中，我们提出了一种基于骨架的零样本动作识别的互信息估计和最大化框架（SMIE）。为了更好地捕获视觉和语义空间之间的依赖关系，我们的方法避免了直接映射，而是使用全局对齐模块来对齐这两个空间的分布。该模块利用互信息作为相似性度量，并应用基于Jensen-Shannon散度（JSD）的估计器来最大化成对的视觉和语义特征之间的互信息，同时最小化未成对的视觉和语义特征之间的互信息。在测试阶段，将神经网络作为连接模型来估计JSD估计器中的相似性得分，该得分在未知类别上使用。然后，考虑到动作的固有时间信息，SMIE提出了一个时间约束模块，以鼓励视觉和语义特征之间的互信息在执行动作的更多部分时增加。具体而言，JSD估计器应用对比学习来估计全局互信息。成对的视觉和语义特征形成正样本，而未成对的特征形成负样本。为了识别包含动作序列中更多差异信息的关键帧，时间约束模块计算每个序列的运动关注度，并遮盖具有更高关注度的关键帧，以生成包含部分时间信息丧失的额外正样本。在训练期间，与全局互信息相比，计算了具有相同负样本的时间约束互信息，并且保持较小。</p><p>本文的主要贡献有三个方面： • 我们提出了基于互信息最大化的基于骨架的零样本动作识别的互信息估计和最大化框架（SMIE），这是一种新的零样本方法，用于基于互信息最大化的骨架动作识别，可以捕获视觉空间和文本语义空间分布的复杂统计相关性。 • 提出了一种新颖的时间约束模块，用于计算时间约束互信息，并应用了时间排名损失来帮助连接模型捕获动作的固有时间信息。 • 大量的实验和分析证明了所提方法的有效性，该方法在性能上明显优于基线方法。</p><h1>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</h1><p><strong>1. 引言</strong></p><p>人体动作识别（HAR）是通过接收视频数据作为输入来对动作类别进行分类的任务。HAR用于许多应用，如人机交互和虚拟现实。随着深度学习技术的发展，最近提出了几种基于RGB和骨架的HAR方法。然而，基于RGB的方法[31, 29]不能稳健地识别人的动作，因为它们受到背景颜色、光亮度和服装等环境噪声的强烈影响。因此，使用骨架模态的方法[35, 24, 36, 26, 5, 4, 18, 2, 15]引起了注意，因为它们不受这些噪声的影响。这些方法通过接收主要人体关节的2D或3D坐标作为时间序列输入来识别动作。</p><p>最近的方法[24, 18, 4, 2]采用了图卷积网络（GCNs）将人体骨架图应用于卷积层。然而，现有的基于GCN的方法[35, 24, 25, 4, 2]存在以下局限性。 （1）使用广泛使用的手工制作图形，无法识别远处关节节点之间的关系，因为它们仅使用人体骨架中的PC边缘的关系。虽然具有PC边缘的图形具有语义意义，但仅具有PC边缘的图形由于是经验性固定的，因此存在长程依赖性问题。然而，要使人类识别动作，远处关节之间以及相邻关节之间的关系强相关。尽管一些方法[24, 2]已经尝试通过训练引导注意力的可学习图形来解决此类局限，但它们仍然使用[35]的手工制作图形以及它们的可学习图形。此外，由于[35]图形的元素值比可学习图形的元素值更主导，它们不能充分突出远程节点之间的关系。 （2）最近的一些方法[35, 24, 4, 18]冒着通过简单聚合边缘特征并忽略每个边缘的贡献而陷入亚优性的风险，从而不完全识别哪些边缘对每个骨架样本是重要的。例如，在“蹲下”动作的情况下，腿和手之间的交互应该受到强调。</p><p>受到这些局限性的启发，我们提出了一种分层分解的图卷积网络（HD-GCN），其中包括分层分解的图形（HD-Graph）和引导注意的分层聚合（A-HA）模块。此外，我们提出了一种有效利用我们的HD-Graph的六种合奏方法。我们提出的方法的框架如图1所示，用于“蹲下”动作。HD-GCN将GCN与我们的HD-Graph相结合，该图形在相同的语义空间（例如右手和左手、右脚和左脚）中识别远程关节节点之间的关系。通过从图的质心（CoM）节点逐步向外移动来形成相同的语义空间。例如，如果腹部是CoM节点，则第一个语义空间包括腹部节点，下一个空间包括胸部和臀部节点，随后的空间包括左肩和右肩以及左臀和右臀节点。相同语义空间中的节点被定义为层次节点集。要检测远程关节节点之间的关系，网络应具有较大的感知场。所提出的HD-Graph通过连接相邻层次节点集中的所有节点，并识别用于大感知场的那些节点之间的连接来包含具有意义的相邻和远程关节节点。我们采用类似于根树状结构的结构以有效地表示每个边缘。我们应用空间边缘卷积（S-EdgeConv）层来考虑不能被HD-Graph捕获的在每个样本中不能被捕获的语义接近边缘。要创建S-EdgeConv层，我们借用了广泛用于3D点云的[33]的结构。</p><p>为了考虑每个边缘集的贡献，选择主导层次信息的过程应该依赖于动作数据样本，以适当关注最主导的边缘集。例如，为了识别“拍手”动作，必须强调包括双手的层次边缘集。为了解决这个问题，我们提出了一个引导注意的分层聚合（A-HA）模块，它包括两个子模块：代表性空间平均池化（RSAP）和分层边缘卷积（H-EdgeConv）。如果我们在没有任何节点提取过程的情况下使用空间平均池化模块，将出现缩放偏差问题，因为每个节点具有不同数量的相邻节点。为了防止这种情况，我们应用了RSAP，其中包括一个代表性节点提取过程，触发池层后的特征来表示每个节点。为了有效管理由RSAP获得的分层特征，我们应用了分层边缘卷积（H-EdgeConv）层。H-EdgeConv将每个分层特征视为图节点，并通过特征空间中的欧几里德距离识别应该强调哪些分层特征。通过RSAP和H-EdgeConv，我们的模型成功确定了哪些分层边缘集和关节应该在输入特征中受到强调。</p><p>现有的合奏方法使用由联合、骨骼坐标、关节运动和骨骼运动流数据组成的四个流数据，它们分别是原始骨架坐标、关节坐标之间的空间差异和关节的时间差异以及骨骼的时间差异。大多数现有的合奏方法[25, 2]使用额外的运动数据，但仅利用运动数据的模型表现相对较差。为了解决这个问题，我们提出了一种新的方法，六向合奏。我们通过使用带有关节和骨流数据的三个HD-Graph来应用此合奏方法。每个图都有不同的质心节点，以提取不同语义空间的特征（参见附录）。</p><p>我们在四个基准动作识别数据集上进行了广泛的实验：NTU-RGB+D 60 [22]，NTU-RGB+D 120 [16]，Kinetics-Skeleton [11]和Northwestern-UCLA [30]。</p><p>我们的主要贡献总结如下：</p><p>• 我们提出了一种分层分解的图形（HD- 图），以全面识别相同层次节点集之间重要的远程边缘。 • 我们提出了一个引导注意的分层聚合 (A-HA) 模块，以代表性的空间平均池化 (RSAP) 和分层边缘卷积 (H-EdgeConv) 强调关键边缘集。 • 我们使用一种新的六向合奏方法来进行基于骨架的动作识别，使用具有不同重心（CoM）的HD-Graph，优于没有任何运动数据的常规合奏。 • 我们的HD-GCN在基于骨架的动作识别的四个基准数据集上优于最先进的方法。</p><h1>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</h1><p><strong>引言</strong> 动作识别是应用于各种应用程序的最重要的视频理解任务之一，如虚拟现实和人机交互。最近关于动作识别的研究分为两种方法，基于RGB的方法[33, 31]和基于骨架的方法[36, 26, 27, 2, 4, 16]。使用骨架模态的动作识别接收具有主要人体关节的三维坐标的视频序列作为输入。骨架模态的动作识别具有以下优点，通过紧凑地压缩人体结构可以创建计算复杂性低的轻量级模型。此外，与基于RGB的方法不同，它具有鲁棒性，不受背景噪声、天气和光照条件的影响。</p><p>早期的方法[7, 8, 38, 14, 23]通过独立处理每个关节来提取特征，这意味着它们不考虑结构相关的人体关节之间的信息。然而，人体关节之间的连接被识别为Yan等人提出骨架模态的时空图卷积网络（GCNs）后的图形结构。最近的方法[27, 4, 2, 6]将GCNs作为基线，并尝试在空间领域上扩大感知场。然而，基于Yan等人的GCNs的方法存在一些局限性。 (1) 当一个人执行动作时，他们的身体部分的运动同时发生在空间和时间上，这两个方面的运动本质上是相互关联的。尽管同时包含空间和时间组件可以提供更全面和准确的人体动作表示，但直接利用这种时空互连性是不可行的，因为时空模块是相互独立的。 (2) 由于它们使用的图仅包括物理上相邻的关节的连接，因此具有此类图的网络具有较小的空间感知场。尽管提出了几种自我注意方法[27, 2]来增加空间感知场，但它们仍然依赖于使用物理上相邻的图，这可能导致对那些物理上相邻的图产生偏见，并突出显示了它们有效性的潜在局限性。为了处理这个问题，Liu等人提出了一种多尺度图，用于识别结构上远离的节点之间的关系。然而，正如Yan等人所述，虽然将人体动作区分为同心和离心模式至关重要，但Liu等人的方法并不考虑这些模式。此外，Liu等人的模型存在模型复杂性高的限制，因为在单个层中存在太多并行操作。</p><p>为了解决限制（1），我们提出了一个时空曲线（STC）模块，以反映骨架序列中直接的时空依赖性。除了应用时间卷积来聚合节点级别的时序特征，我们构建曲线，考虑每个节点的时序时空特征，并将它们与输入特征图聚合在一起。为了创建这些曲线，我们在特征空间中选择了在所有相邻帧之间具有高度相关性的节点，并将它们连接起来。因此，通过赋予节点之间的时间连接以自主性，可以自适应生成更具语义效果的图形结构。图1比较了现有方法[36, 27, 2]和我们提出的方法的时空GCN的时间流。图1 (a)显示，该模型仅在每一帧中反映相同节点的特征，而图1 (b)显示，该模型通过生成的曲线考虑了不同帧中不同节点的时空相关性。受[35]启发，我们使用一个聚合模块来有效地组合所有曲线特征并将其应用于输入特征图。</p><p>为了解决限制（2），我们提出了图卷积的扩展内核（DK-GC），以使骨架模态具有大的空间感知场，而无需任何额外参数。用于人体骨架的GCNs聚合朝内（向心）、标识和朝外（离心）特征，与卷积神经网络（CNNs）不同，后者聚合了左侧、标识和右侧像素特征。为了将扩张内核应用于这种GCNs，我们创建相邻矩阵来通过修改向心和离心矩阵来识别结构上远离的关系。为了从低级到高级整合空间感知场，我们将空间模块分成具有不同扩张窗口的几个分支操作。同时，Li等人[17]已经为3D点云分析任务引入了扩张图卷积。然而，Li等人的扩张图卷积与我们提出的方法完全不同，并且不适用于人体骨架模态。首先，这种方法不利用给定的相邻矩阵，而是使用k最近邻（k-NN）算法通过动态图来。未能利用给定的相邻矩阵会降低动作识别模型的鲁棒性，因为Shi等人[27]通过实验证明，不使用这些矩阵会导致性能不佳。此外，单独使用k-NN无法识别所有物理相邻的节点。第二个原因是Li等人的方法需要大量的GPU资源。由于动态图是通过计算每个GCN层的所有节点之间的两两距离来构建的，因此它会导致GPU内存消耗非常高，推理速度较慢。</p><p>为了验证我们的STC-Net的优越性，在四个基于骨架的动作识别基准数据集上进行了广泛的实验：NTU-RGB+D 60 [25]，NTU-RGB+D 120 [18]，Kinetics-Skeleton [12]和Northwestern-UCLA [32]。</p><p>我们的主要贡献总结如下： • 我们提出了时空曲线（STC）模块，以利用不同帧不同节点之间的直接时空相关性。 • 我们提出了图卷积的扩展内核（DK-GC），通过修改给定的骨骼相邻矩阵，使模型具有大的空间感知场，而无需任何额外参数。 • 我们提出的STC-Net在四个基于骨架的动作识别基准数据集上优于现有的最先进方法。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读4</title>
      <link href="/project/posts/cfc9c95f.html"/>
      <url>/project/posts/cfc9c95f.html</url>
      
        <content type="html"><![CDATA[<h1>TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential</h1><p>Abstract—Graph convolution networks (GCNs) have been<br>widely used and achieved fruitful progress in the skeleton-based<br>action recognition task. In GCNs, node interaction modeling<br>dominates the context aggregation and, therefore, is crucial<br>for a graph-based convolution kernel to extract representative<br>features. In this article, we introduce a closer look at a powerful<br>graph convolution formulation to capture rich movement patterns<br>from these skeleton-based graphs. Specifically, we propose a<br>novel heterogeneous graph convolution (HetGCN) that can be<br>considered as the middle ground between the extremes of<br>(2 + 1)-D and 3-D graph convolution. The core observation of<br>HetGCN is that multiple information flows are jointly intertwined<br>in a 3-D convolution kernel, including spatial, temporal, and<br>spatial–temporal cues. Since spatial and temporal information<br>flows characterize different cues for action recognition, HetGCN<br>first dynamically analyzes pairwise interactions between each<br>node and its cross-space–time neighbors and then encourages<br>heterogeneous context aggregation among them. Considering<br>the HetGCN as a generic convolution formulation, we further<br>develop it into two specific instantiations (i.e., intra-scale and<br>inter-scale HetGCN) that significantly facilitate cross-space–time<br>and cross-scale learning on skeleton graphs. By integrating these<br>modules, we propose a strong human action recognition system<br>that outperforms state-of-the-art methods with the accuracy of<br>93.1% on NTU-60 cross-subject (X-Sub) benchmark, 88.9% on<br>NTU-120 X-Sub benchmark, and 38.4% on kinetics skeleton.</p><p>摘要图卷积网络(GCNs)在基于骨骼的动作识别中得到了广泛的应用，并取得了丰硕的成果。在GCNs中，节点交互建模主导了上下文聚合，因此，对于基于图的卷积核提取代表性特征至关重要。在本文中，我们将详细介绍一个功能强大的图卷积公式，它可以从这些基于骨架的图中捕获丰富的运动模式。具体来说，我们提出了一种新的异构图卷积(HetGCN)，它可以被认为是(2 + 1)-D和3d图卷积两个极端之间的中间地带。HetGCN的核心观测是多个信息流共同交织在一个三维卷积核中，包括空间线索、时间线索和时空线索。由于时空信息流具有不同的动作识别线索，HetGCN首先动态分析每个节点与其跨时空邻居之间的交互作用，进而鼓励节点之间的异构上下文聚集。将HetGCN作为一个泛型卷积公式，我们进一步将其发展为两个具体的实例化(即尺度内和尺度间HetGCN)，这两个实例化显著促进了骨架图上的跨时空和跨尺度学习。通过整合这些模块，我们提出了一个强大的人体动作识别系统，该系统在NTU-60交叉学科(X-Sub)基准上的准确率为93.1%，在NTU-120 X-Sub基准上的准确率为88.9%，在动力学框架上的准确率为38.4%，优于最先进的方法。</p><h1>Learning Heterogeneous Spatial–Temporal Context for Skeleton-Based Action Recognition学习异构时空上下文的骨架动作识别</h1><h2 id="abstract">abstract</h2><p>摘要：图卷积网络（GCNs）已经被广泛应用并在基于骨骼的动作识别任务中取得了丰富的进展。在GCNs中，节点间的相互作用建模主导了上下文聚合，因此对于图卷积核来说，提取代表性特征至关重要。在本文中，我们对一个强大的图卷积公式进行了更详细的研究，以从这些基于骨骼的图中捕获丰富的运动模式。具体而言，我们提出了一种新颖的异构图卷积（HetGCN），可以被看作是（2 + 1）-D和3-D图卷积的中间地带。HetGCN的核心观察是，在3-D卷积核中多种信息流联合交织在一起，包括空间、时间和时空线索。由于空间和时间信息流表征了动作识别的不同线索，因此HetGCN首先动态分析每个节点与其跨时空邻居之间的成对交互，并鼓励在它们之间进行异构上下文聚合。将HetGCN视为通用卷积公式，我们进一步将其发展为两个具体实例（即，尺度内和尺度间的HetGCN），这些实例极大地促进了骨骼图上的跨时空和跨尺度学习。通过集成这些模块，我们提出了一个强大的人体动作识别系统，其在NTU-60跨主体（X-Sub）基准测试中达到了93.1％的准确率，在NTU-120 X-Sub基准测试中达到了88.9％的准确率，在运动学骨骼测试中达到了38.4％的准确率，超越了最先进的方法。</p><p>要点总结：</p><ul><li>介绍了在基于骨骼的动作识别任务中广泛使用的图卷积网络（GCNs）。</li><li>提出了一种新颖的异构图卷积（HetGCN）来捕获骨骼图中的丰富运动模式。</li><li>HetGCN考虑了空间、时间和时空线索，动态分析节点之间的交互，鼓励异构上下文聚合。</li><li>将HetGCN发展成两个具体实例，分别是尺度内和尺度间的HetGCN，有助于骨骼图上的跨时空和跨尺度学习。</li><li>提出了一个强大的人体动作识别系统，在多个基准测试中超越了最先进的方法，准确率达到了93.1％、88.9％和38.4％。</li></ul><h2 id="introduction">introduction</h2><p>这篇文章介绍了骨架动作识别领域的研究。随着深度摄像头和姿势估计技术的发展，基于骨架的动作识别因其对外观和视角扰动的稳健性而备受关注。在这一领域，图卷积网络（GCN）是一种常见的方法。GCN将每一帧中的每个身体关节视为一个节点，并通过边连接相邻节点，以从骨架序列中提取代表性的运动特征以进行动作识别。</p><p>最近的空间-时间特征学习方法可以分为两种策略：传统方法交替使用2-D空间（例如GCNs）和1-D时间（例如TCNs）模块来提取运动特征，以处理骨架序列中的运动特征。这种分解的（2 + 1）-D卷积方案首先使用图卷积提取每个帧内的空间关系，然后使用1-D卷积层来建模时间动态。然而，这种分解使得长距离建模成为可能，但阻碍了用于捕捉复杂的区域性空间-时间联合依赖性的直接跨空间-时间信息流。为了克服这一限制，一些方法将跨空间-时间边界定为3-D卷积核，促进了空间-时间域内无阻碍的信息传播。然而，正如之前的实验结果所验证的那样，这种3-D特征提取器表现不如（2 + 1）-D卷积。因此，这种直接的3-D扩展是否能够有效地模拟空间外观和时间动态仍然未被充分探讨。</p><p>因此，文章提出了一种新的方案，介于（2 + 1）-D（或分解）卷积和3-D（或全卷积）之间，用于空间-时间特征学习。核心观点是，传统的3-D卷积核背后的同质化上下文聚合削弱了其在空间-时间特征学习方面的能力。作者认为，从空间和时间维度观察到的身体节点提供了动作识别的不同线索。一种描述了每一帧中的身体姿势，另一种反映了跨多帧的轨迹运动。因此，如果直接扩展2-D GCN以具有跨空间-时间边缘，那么这种直接的3-D卷积会将不同帧中的节点放入相同的上下文中，而不区分空间和时间关系。因此，需要一种更深入的方法来提取空间-时间图上强大的特征提取器。</p><p>文章的关键观点是，在3-D卷积核中交织着三种元路径：1）空间元路径，连接每帧内的空间相邻节点；2）时间元路径，链接相邻帧中的相同语义节点；3）空间-时间元路径，连接来自相邻帧的空间相邻节点。这些元路径背后的节点交互是不同的。空间元路径反映了身体关节之间的姿势相关性。时间元路径描述了时间序列中各个节点的轨迹相关性。空间-时间元路径反映了不同身体关节在空间和时间上的共同运动相关性。因此，3-D卷积核封装了其中的多种节点交互上下文，丰富了动作识别的各种线索。</p><p>为了实现对空间-时间邻居节点之间的交互的动态特征提取，作者提出了一种强大的特征提取器HetGCN。此外，为了探索更全面的关系，作者通过在多个尺度上表示人体，超越了身体关节关系。作者将通用的HetGCN公式发展成两个具体的实例：intra-scale HetGCN和inter-scale HetGCN。通过集成这些模块，文章构建了一个具有金字塔网络结构的动作识别系统，可以在多个图尺度上有效提取分层运动表示。</p><p>文章的贡献总结如下： 1）提出了异质图卷积（HetGCN）来重新制定骨架图序列上的空间-时间特征学习。作为一个强大的特征提取器，HetGCN鼓励空间-时间邻居节点之间的交叉信息传播，同时动态地表征它们之间的异质配对交互。 2）将通用HetGCN公式进一步发展为两个具体的实例（intra-scale HetGCN和inter-scale HetGCN），它们共同促进了跨空间-时间和跨尺度信息的传播，丰富了多个图尺度上的分层运动特征。 3）通过集成intra-scale和inter-scale HetGCN层，构建了一个强大的基于骨架的动作识别系统，在三个大型广泛使用的公共数据集上明显优于最先进的方法。</p><p>文章的要点总结如下：</p><ul><li>介绍了骨架动作识别的重要性和GCNs在其中的应用。</li><li>分析了当前的空间-时间特征学习方法，并提出了中间地带方法，介于（2 + 1）-D和3-D卷积之间。</li><li>讨论了3-D卷积核中的三种元路径，即空间、时间和空间-时间元路径，并提出了HetGCN来动态地处理它们。</li><li>发展了HetGCN的两个具体实例，以在多个尺度上提取运动特征。</li><li>展示了通过集成HetGCN层构建的动作识别系统在多个数据集上的卓越性能</li></ul><h3 id="实例">实例</h3><p>当然，以下是对文章段落的要点总结，包括相关示例：</p><ul><li>介绍了骨架动作识别领域的重要性以及GCNs在其中的应用。</li><li>分析了当前的空间-时间特征学习方法，并提出了一种介于（2 + 1）-D和3-D卷积之间的新方法，以更好地捕捉复杂的空间-时间特征。</li><li>讨论了3-D卷积核中的三种元路径：空间、时间和空间-时间元路径，并解释了它们的不同。</li><li>提出了HetGCN作为一种强大的特征提取器，它动态地处理了这些元路径之间的异质交互。</li><li>发展了HetGCN的两个具体实例（intra-scale HetGCN和inter-scale HetGCN），以在多个尺度上提取运动特征。</li><li>展示了通过集成HetGCN层构建的动作识别系统在三个大型数据集上的卓越性能。</li></ul><p>这些要点总结涵盖了文章中介绍的主要概念和贡献，并强调了3-D卷积、元路径、HetGCN等关键概念。希望这有助于您更好地理解文章。如果文章中有特定的示例，也会在相关要点中提到。</p><h2 id="相关工作">相关工作</h2><p>以下是文章中提到的相关技术和先验知识的总结：</p><p>A. 空间-时间表示学习</p><ol><li>空间-时间图序列的特征学习是计算机视觉中许多任务的基本问题，包括预测和识别。</li><li>最近的特征学习策略主要分为两种策略：(2 + 1)-D GCNs和3-D GCNs。</li><li>(2 + 1)-D方法交替使用分离的2-D空间卷积（GCN）和1-D时间卷积（TCN）模块来提取骨骼序列的运动特征。</li><li>3-D GCNs引入了密集的跨空间-时间边缘以进行直接的空间-时间信息传播，与(2 + 1)-D方法不同，这些3-D卷积核倾向于联合提取空间和时间依赖性。</li><li>目前尚不清楚直接的3-D扩展是否能有效地建模空间外观和时间动态。</li></ol><p>B. 多尺度表示学习</p><ol><li>除了仅构建基于身体关节的空间-时间图，我们还通过多尺度图表征方法来表示人体。</li><li>多尺度图允许我们共同提取身体部件内的更细的依赖关系和更粗的部件间关系。</li><li>这种方法可以更好地捕捉到人体动作的特征。</li></ol><p>C. 基于骨骼的动作识别</p><ol><li>鉴于身体骨架的拓扑结构是一个自然图形，最近在基于骨骼的动作识别任务中引入了GCNs，并取得了丰硕的进展。</li><li>大多数方法依循(2 + 1)-D特征学习策略，主要关注节点的空间相关建模。</li><li>在空间-时间邻居之间进行有效的交互建模仍然是一个未充分探讨的问题</li></ol><h3 id="实例-2">实例</h3><p>以下是文章中提到的相关技术和先验知识，以及相应示例的总结：</p><p>A. 空间-时间表示学习</p><ol><li>空间-时间图序列的特征学习是计算机视觉中许多任务的基本问题，包括预测和识别。这意味着在视频分析、动作识别等领域有广泛应用。</li><li>最近的特征学习策略主要分为两种策略：(2 + 1)-D GCNs和3-D GCNs。例如，传统方法交替使用分离的2-D空间卷积（GCN）和1-D时间卷积（TCN）模块，以提取骨骼序列的运动特征。</li><li>3-D GCNs引入了密集的跨空间-时间边缘，以进行直接的空间-时间信息传播。例如，MS-G3D提出了一个3-D特征提取器，通过在2-D GCN与跨空间-时间边缘的交叉中进行混合来处理3-D信息。</li></ol><p>B. 多尺度表示学习</p><ol><li>多尺度图允许我们共同提取身体部件内的更细的依赖关系和更粗的部件间关系。例如，通过将身体关节分组成多个语义粒度级别的身体部件，可以更好地捕捉人体动作的特征。</li><li>多尺度图的示例包括将身体姿势从细到粗地抽象化，以更好地理解动作。例如，&quot;行走&quot;动作可以基于抽象的手臂和腿部之间的协同运动来理解。</li></ol><p>C. 基于骨骼的动作识别</p><ol><li>最近在基于骨骼的动作识别任务中引入了GCNs，并取得了丰硕的进展。这些方法主要关注节点的空间相关建模，例如，通过建立GCN图来模拟骨骼数据。</li><li>在空间-时间邻居之间进行有效的交互建模仍然是一个未充分探讨的问题。这意味着目前的方法可能未能充分考虑节点之间的空间-时间关系。</li></ol><h2 id="method">method</h2><p>以下是文章中涉及的主要要点和内容总结：</p><p><strong>III. PRELIMINARIES</strong> A. Notation Definition</p><ul><li>人体骨架的拓扑结构自然形成一个图，骨架动作视频可以用空间-时间图来表示。</li><li>空间-时间图的节点集合由V表示，其中T表示骨架序列的帧数，N表示每帧中的身体关节数。</li><li>节点的动态性质是它们的初始特征X ∈ R^3×T×N，表示在世界坐标系中的3D位置。</li></ul><p>B. Multiscale Initialization</p><ul><li>除了给定的初始图尺度外，进一步在多个尺度级别上表示人体。</li><li>通过逐步抽象人体姿势，从精细到粗糙获得一系列的姿势。</li><li>初始尺度s1的骨架图的节点是动作执行的最小单元：身体关节。</li><li>尺度s2和s3上的每个节点分别表示两个不同语义粒度级别的身体部件。</li><li>根据人体结构的先验知识，将身体关节分组为多个语义粒度级别的身体部件，并用单个节点表示每个身体部件。</li><li>除了初始尺度s1上的身体特征X之外，还获得了尺度s2和s3上的其他身体特征Xs2和Xs3。</li><li>Ns2和Ns3表示图尺度s2和s3上的节点数。</li></ul><p>这些要点涵盖了文章的预备知识和背景信息，为进一步理解文章的方法和实验提供了基础。接下来，如果您想了解更多关于文章的内容或其他方面的信息，请告诉我。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode26</title>
      <link href="/project/posts/bbe8217d.html"/>
      <url>/project/posts/bbe8217d.html</url>
      
        <content type="html"><![CDATA[<h1>动态规划</h1><p>学习动态规划的套路以及动态规划优化套路</p><p><a href="https://www.playoffer.cn/691.html">https://www.playoffer.cn/691.html</a></p><p><a href="https://www.playoffer.cn/693.html">https://www.playoffer.cn/693.html</a></p><p>动态规划有点类似于 递归的套路</p><blockquote><ol><li>都是先根据题目 选定对应的含义</li></ol><p>这里动态规划可能使用的是一维数组，也可能使用二维数组，需要根据题目的意思进行选择</p><p>选定了一维数组或者二维数组之后，需要将要求的结果或者目标与对应的数组的arr[m] 或者arr[m] [n]  对应起来，确定好要求的值的意思（用对应的下标表示出来）</p></blockquote><blockquote><ol start="2"><li>找递推式； 找到第n 项与第n-1 项  或者第arr[m] [n] 项 与第 arr[m-1] [n-1] 之间的关系进行表示</li></ol><p>找到递推关系式能够直接将要求的结果逐个递减，用小的表示出大的数据，直至找到最小值</p></blockquote><blockquote><ol start="3"><li><p>在找到递推式之后，将问题化小，会一直递推到初始值，也就是第0 项目或者第1 项，此时要做的就是需要找到对应的初始值，</p><p>根据题目要求找到那个特殊点，用该值表示出来结果，就能逐层从小向大进行递推，直至求出最后的值</p></li></ol></blockquote><p>经典题目:青蛙跳台阶</p><p>经典题目:从左上角走到右下角的路径种数</p><p>经典题目: 从左上角走到右下角的最小路径和</p><p>经典题目: 股票的最大利润</p><blockquote><p>类似于最小路径和，不过是使用min函数罢了</p></blockquote><p>经典题目: 礼物的最大价值（类似于最小路径和）</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读3</title>
      <link href="/project/posts/51ad5cfc.html"/>
      <url>/project/posts/51ad5cfc.html</url>
      
        <content type="html"><![CDATA[<h1>1 IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action RecognitionIIP变压器：基于骨架的动作识别中的内部-部件间Transformer</h1><h2 id="abstract">abstract</h2><p><strong>翻译：</strong> 最近，基于<strong>Transformer的网络</strong>在基于骨骼的动作识别任务上显示出了巨大的潜力。<strong>捕获全局和局部依赖性的能力</strong>是成功的关键，但它也<strong>带来了二次计算和内存成本</strong>。另一个问题是，先前的研究<strong>主要关注个别关节之间的关系，这经常受到由传感器的嘈杂输入或不准确估算引入的嘈杂骨骼关节的影响。<strong>为了解决以上问题，我们提出了一种</strong>新颖的基于Transformer的网络（IIP-Transformer）</strong>。与利用个别关节之间的相互作用不同，我们的==<strong>IIP-Transformer同时包括了身体关节和部位之间的相互作用，<strong>因此可以</strong>高效而有效地捕获关节级别（部内）和部分级别（部间）的依赖性</strong>。==从数据方面来看，我们==<strong>引入了一种部分级别的骨骼数据编码，显著降低了计算复杂性</strong>==，并对关节级别的骨骼噪声更具鲁棒性。此外，我们提出了一种**==新的部分级别数据增强方法==**，以提高模型的性能。在两个大规模数据集NTU-RGB+D 60和NTU RGB+D 120上，所提出的IIP-Transformer实现了超越DSTA-Net（目前最先进的基于Transformer的方法）的性能，计算复杂性降低了8倍以上。</p><p><strong>要点摘要：</strong></p><ul><li>Transformer-based网络在骨骼动作识别方面表现出潜力。</li><li>捕获全局和局部依赖性对成功至关重要，但会增加计算和内存成本。</li><li>先前的研究主要关注个别骨骼关节之间的关系，但受到嘈杂输入和不准确估算的影响。</li><li>提出了一种新的Transformer网络（IIP-Transformer），能同时捕获关节级别和部分级别的依赖性。</li><li>引入了部分级别的骨骼数据编码，降低了计算复杂性，更具鲁棒性。</li><li>提出了部分级别的数据增强方法以提高模型性能。</li><li>在两个大规模数据集上，IIP-Transformer实现了超越目前最先进的Transformer方法（DSTA-Net）的性能，同时计算复杂性降低了8倍以上。</li></ul><h2 id="introduction">introduction</h2><ol><li>引言 在过去的几十年里，人体动作识别已经受到广泛研究，并在许多应用中取得了令人满意的进展，从人机交互到视频检索等 [4, 12, 28, 29]。最近，基于骨骼的表示因其在描述人体运动中动态变化的紧凑性而备受关注 [14]。先进的姿势估计算法 [2] 和诸如Kinect [38] 和RealSense [15] 等体感摄像头的进步使得以低成本准确快速地获得身体关键点成为可能。此外，与RGB表示相比，基于骨骼的表示对照明变化和背景噪声的稳健性更强。这些优点吸引了研究人员开发各种方法来利用骨骼进行动作识别。</li><li>先前的基于骨骼的动作识别方法利用图形拓扑或手动设计的规则将原始骨骼序列转换为伪图像 [10, 17, 20] 或图形等网格状结构，然后将其馈送到RNN、CNN或GCN [36] 中提取特征。然而，并没有保证手工制定的结构是捕捉关节关系的最佳选择，这限制了先前工作的泛化性能。</li><li>最近，提出了基于Transformer的方法 [22, 27]，==依赖于多头自注意机制==，该机制自适应地<strong>探索骨骼关节之间的潜在依赖关系</strong>。具体来说，Shi等人 [27] <strong>将每个单独的关节视为一个令牌，但自注意计算随着令牌数量呈二次增长，因此引入了大量计算。<strong>该工作还</strong>受到传感器收集的嘈杂骨骼关节或不准确估算的影响</strong>。</li><li>为了解决这些问题，我们将==身体部位的概念引入Transformer网络中==。大多数动作，例如站立或跳舞，都是通过身体部位的协同运动来执行的。实际上，<strong>身体部位可以被视为动作执行的最小单元</strong>，这意味着<strong>只能通过身体部位的运动来识别这些动作</strong>。与其他复杂的分割策略不同，我们简单地将身体关节汇总到几个部分中，==根据人体拓扑结构==，并鼓励模型利用复杂的交互作用。具体来说，我们将v个身体关节划分为p个部分，并将每个部分编码为一个令牌，这将空间自注意计算成本降低了v2/p2倍。我们提出的分区编码的另一个优点是，它使模型可以接受更稀疏的帧作为时间输入，从而带来额外的计算减少。为了鼓励模型进行全局推理，而不是依赖于特定部分，我们提出了一种名为Part-Mask的新数据增强方法，在训练过程中随机屏蔽一个部分。这个新策略使模型在挑战性情况下更加稳健。</li><li>由于身体关节被划分为部分，关节级别的信息可能会丢失。对于一些细粒度的动作，例如鼓掌或写字，需要额外捕捉身体关节之间的交互作用。我们提出了新颖的Intra-Inter-Part Transformer网络（IIP-Transformer）来解决这个问题，并与标准Transformer网络进行了三项主要改进对比。首先，提出了Intra-Inter-Part自注意机制，可以同时捕捉部分内特征和部分间特征，而不增加太多计算复杂性。其次，受到BERT [9] 的启发，我们引入了一个可学习的类别令牌，而不是汇总由骨干提取的所有特征。最后，我们提出了一种新的空间-时间Transformer，用于建模空间和时间依赖关系，从而减小了模型的大小，同时提高了模型的泛化性能。</li><li>我们的代码和模型将在https://github.com/qtwang0035/IIP-Transformer 上公开提供。</li><li>总体而言，我们的贡献可以总结如下：<ul><li>我们将身体部位的概念引入基于Transformer的骨骼动作识别中。我们提出的分区编码显著降低了自注意计算复杂性，并相对不太受到关节噪声的影响。</li><li>我们提出了IIP-Transformer，一种新颖的空间-时间Transformer网络，可以同时捕捉部分内部和部分间的关系。</li><li>在两个大规模骨骼动作数据集，如NTU RGB+D 60和120上进行了广泛的实验，结果显示我们提出的IIP-Transformer在计算成本减少2 ∼ 36倍的情况下实现了最先进的性能。</li></ul></li></ol><p><strong>要点摘要：</strong></p><ul><li>人体动作识别在多个应用中取得了进展，包括人机交互和视频检索。</li><li>基于骨骼的表示因其紧凑性而备受关注，现在可以通过先进的姿势估计算法和体感摄像头准确快速地获取身体关键点。</li><li>先前的方法通过手工设计的结构将原始骨骼序列转换为网格结构，然后进行特征提取，但这可能不是最佳选择。</li><li>最近，基于Transformer的方法提出，但计算复杂性较高且受到嘈杂的骨骼关节数据影响。</li><li>为了解决这些问题，提出了==将身体部位的概念引入Transformer网络，并提出了分区编码来降低计算复杂性==。</li><li>引入了新的数据增强方法（Part-Mask）来提高模型的鲁棒性。</li><li>提出了Intra-Inter-Part Transformer网络（IIP-Transformer）来同时捕捉部分内部和部分间的关系。</li><li>实验表明，IIP-Transformer在计算成本显著减少的情况下实现了最先进的性能</li></ul><blockquote><ol><li><strong>引言</strong><ul><li>人体动作识别在多个应用中取得进展，包括人机交互和视频检索。</li></ul></li><li><strong>基于骨骼的表示</strong><ul><li>基于骨骼的表示因其紧凑性而备受关注。</li><li>先进的姿势估计算法和体感摄像头使得获取身体关键点准确快速成为可能。</li><li>基于骨骼的表示对照明变化和背景噪声更加稳健。</li></ul></li><li><strong>问题与挑战</strong><ul><li>先前的方法使用手工设计的结构，将骨骼序列转换为网格结构，但这可能不是最佳选择。</li><li>基于Transformer的方法存在计算复杂性高和嘈杂骨骼关节数据的问题。</li></ul></li><li><strong>解决方法</strong><ul><li>引入了身体部位的概念，将身体关节划分为部分，提出分区编码来降低计算复杂性。</li><li>提出了新的数据增强方法（Part-Mask）以提高模型的鲁棒性。</li><li>引入了Intra-Inter-Part Transformer网络（IIP-Transformer），同时捕捉部分内部和部分间的关系。</li></ul></li><li><strong>实验与结果</strong><ul><li>IIP-Transformer在计算成本显著减少的情况下实现了最先进的性能。</li><li>对两个大规模骨骼动作数据集的广泛实验支持了该方法的有效性。</li></ul></li><li><strong>示例和应用</strong><ul><li>通过引入身体部位的概念，模型能够更好地理解人体动作，例如站立或跳舞，这是通过身体部位的协同运动来执行的。</li><li>提出的分区编码降低了计算复杂性，使模型对关节噪声不太敏感。</li><li>引入的数据增强方法（Part-Mask）有助于模型更好地处理具有挑战性的情况。</li><li>Intra-Inter-Part Transformer网络（IIP-Transformer）通过同时捕捉部分内部和部分间的关系，提高了动作识别性能。</li></ul></li></ol></blockquote><blockquote><ol><li><strong>引言</strong>：<ul><li>人体动作识别是一个重要的领域，它在多个应用中发挥关键作用，包括改善人机交互和视频检索技术。</li><li>人体动作识别的研究在过去几十年取得了显著进展，这得益于先进的技术和算法的不断发展。</li></ul></li><li><strong>基于骨骼的表示</strong>：<ul><li>基于骨骼的表示是一种重要的动作识别方法，它具有紧凑性，能有效地捕捉人体运动的动态变化。</li><li>先进的姿势估计算法以及体感摄像头（如Kinect和RealSense）的发展，使得获取身体关键点成本低且准确。</li><li>与传统的RGB表示相比，基于骨骼的表示更加稳健，能够应对不同光照条件和背景噪声的挑战。</li></ul></li><li><strong>问题与挑战</strong>：<ul><li>先前的骨骼动作识别方法通常使用手工设计的结构，将原始骨骼序列转换为网格状结构，然后通过神经网络提取特征。</li><li>然而，这些手工设计的结构可能不是最佳选择，因此可能会限制模型的性能和泛化能力。</li><li>基于Transformer的方法引入了自注意机制来处理骨骼关节之间的依赖关系，但计算复杂性高，且容易受到嘈杂的骨骼关节数据的干扰。</li></ul></li><li><strong>解决方法</strong>：<ul><li>为了解决问题，作者引入了身体部位的概念，将身体关节划分为部分，提出了分区编码方法。</li><li>分区编码方法显著降低了计算复杂性，同时相对不太受到噪声关节数据的影响。</li><li>引入了新的数据增强方法（Part-Mask），通过在训练过程中随机屏蔽部分来提高模型的鲁棒性。</li><li>提出了Intra-Inter-Part Transformer网络（IIP-Transformer），以同时捕捉部分内部和部分间的关系。</li></ul></li><li><strong>实验与结果</strong>：<ul><li>作者进行了广泛的实验，使用了两个大规模的骨骼动作数据集，包括NTU RGB+D 60和NTU RGB+D 120。</li><li>结果表明，提出的IIP-Transformer方法在显著降低计算成本的同时，实现了最先进的性能，这是一个重大突破。</li></ul></li><li><strong>示例和应用</strong>：<ul><li>引入身体部位的概念使模型能够更好地理解人体动作，例如站立或跳舞，这是通过身体部位的协同运动来执行的。</li><li>分区编码的引入降低了计算复杂性，并使模型对嘈杂的关节数据更具鲁棒性。</li><li>新的数据增强方法（Part-Mask）有助于模型更好地处理具有挑战性的情况。</li><li>Intra-Inter-Part Transformer网络（IIP-Transformer）通过同时捕捉部分内部和部分间的关系，提高了动作识别性能。</li></ul></li></ol></blockquote><h1>2 Spatial Temporal Transformer Network for Skeleton-based Action Recognition基于时空Transformer网络的骨架动作识别</h1><h2 id="abstract-2">abstract</h2><p>摘要：近年来，基于骨骼的人体动作识别引起了广泛关注，因为骨骼数据已被证明对光照变化、身体尺度、动态摄像机视角和复杂背景具有稳健性。然而，对于潜在在3D骨骼之下的信息的有效编码仍然是一个未解之谜。在这项工作中，我们提出了一种新颖的空间-时间Transformer网络（ST-TR），它使用Transformer的自注意机制来建模关节之间的依赖关系。在我们的ST-TR模型中，使用了一个空间自注意模块（SSA）来理解不同身体部位之间的帧内交互，以及一个时间自注意模块（TSA）来建模帧间的相关性。这两者被结合在一个双流网络中，在NTU-RGB+D 60和NTU-RGB+D 120两个数据集上，它在使用相同输入数据的情况下优于同类方法，成为目前最先进的模型之一。</p><p>要点总结：</p><ul><li>基于骨骼的人体动作识别在最近引起了广泛兴趣，因为它对各种因素的稳健性。</li><li>然而，有效地编码3D骨骼下潜在信息仍然是一个未解之谜。</li><li>本研究提出了一种新型的Spatial-Temporal Transformer网络（ST-TR），它使用Transformer自注意机制来建模关节之间的依赖关系。</li><li>ST-TR模型包括一个空间自注意模块（SSA）和一个时间自注意模块（TSA），用于理解帧内和帧间的交互。</li><li>ST-TR模型在NTU-RGB+D 60和NTU-RGB+D 120数据集上表现出色，超越了同类方法，成为当前最先进的模型之一。</li></ul><h2 id="introduction-2">introduction</h2><ol><li><strong>引言</strong>：<ul><li>基于骨骼的活动识别近年来引起了越来越多的兴趣，这要归功于3D骨骼姿态估计设备的进步，无论在准确性还是分辨率方面。</li><li>然而，有关提取上下文感知的细粒度空间-时间特征的算法和神经架构，这些特征能够释放基于骨骼的动作识别的真正潜力，在文献中仍然缺乏。</li><li>目前，执行基于骨骼的动作识别的最常见方法是Spatial-Temporal Graph Convolutional Network（ST-GCN），因为它是一种有效的非欧几里得数据表示，能够有效地捕获空间（帧内）和时间（帧间）信息。</li><li>然而，ST-GCN模型存在一些结构限制，一些已在其他研究中讨论，包括固定的图拓扑结构、2D卷积的局部性和关节之间的相关性低估等问题。</li></ul></li><li><strong>解决方法</strong>：<ul><li>本文通过使用修改后的Transformer自注意机制来应对上述限制。虽然Transformer自注意最初是为自然语言处理（NLP）任务设计的，但其在建模长距离依赖性方面的灵活性使其成为解决ST-GCN弱点的理想选择。</li><li>与以前的工作类似，本研究利用自注意机制，不同的是，它应用于代表人体骨骼的关节，以提取适应性的低级特征，模拟了空间和时间上的人体动作相互作用。</li><li>在空间维度上，引入了Spatial Self-Attention模块（SSA），用于动态构建关节之间的链接，表示人体部位之间的关系，不受动作和自然人体结构的限制。</li><li>在时间维度上，引入了Temporal Self-Attention模块（TSA）来研究关节在时间上的动态性。</li></ul></li><li><strong>贡献</strong>：<ul><li>本研究提出了一种新颖的双流Transformer模型，既在空间维度又在时间维度上使用自注意机制。</li><li>设计了Spatial Self-Attention（SSA）模块，用于动态构建关节之间的链接，表示人体部位之间的关系，不受动作和自然人体结构的限制。</li><li>引入了Temporal Self-Attention（TSA）模块，用于研究关节在时间上的动态性。</li><li>本模型在NTU-RGB+D数据集上胜过了ST-GCN基线，并在相同输入数据上超越了先前的最先进方法。</li></ul></li></ol><blockquote><ol><li><strong>引言</strong>：</li></ol><ul><li>基于骨骼的活动识别在近年来备受关注，得益于3D骨骼姿态估计设备的不断进步，无论在准确性和分辨率方面都有显著提升。</li><li>尽管如此，提取上下文感知的精细空间-时间特征的算法和神经架构仍然在文献中不足。</li><li>目前，最常见的基于骨骼的动作识别方法是Spatial-Temporal Graph Convolutional Network（ST-GCN），它能够有效捕获空间和时间信息。</li><li>然而，ST-GCN模型存在一些结构限制，如固定的图拓扑结构、局部性2D卷积和低估关节之间的相关性。</li></ul><ol start="2"><li><strong>解决方法</strong>：<ul><li>本文通过引入修改后的Transformer自注意机制来解决上述问题。尽管Transformer自注意最初用于NLP任务，但其灵活性适用于建模长距离依赖性，使其成为解决ST-GCN弱点的理想选择。</li><li>本研究将自注意机制应用于代表人体骨骼的关节，以提取适应性的低级特征，模拟了空间和时间上的人体动作相互作用。</li><li>在空间维度上，引入了Spatial Self-Attention模块（SSA），用于动态构建关节之间的链接，表示人体部位之间的关系，不受动作和自然人体结构的限制。</li><li>在时间维度上，引入了Temporal Self-Attention模块（TSA），研究关节在时间上的动态性。</li></ul></li><li><strong>贡献</strong>：<ul><li>本研究提出了一种新颖的双流Transformer模型，同时在空间和时间维度上使用自注意机制，以解决现有方法的限制。</li><li>设计了Spatial Self-Attention（SSA）模块，用于动态构建关节之间的链接，独立于自然人体结构，表示人体部位之间的关系。</li><li>引入了Temporal Self-Attention（TSA）模块，以研究关节在时间上的动态性。</li><li>本模型在NTU-RGB+D数据集上超越了ST-GCN基线，并在相同输入数据上超越了先前的最先进方法。</li></ul></li></ol><p>这些要点总结突出了文章介绍部分的核心内容，包括问题陈述、解决方法和研究的贡献，同时强调了自注意机制的灵活性以及在空间和时间上的应用。如果您需要更多关于特定部分的详细信息或示例，请随时提问。</p></blockquote><blockquote><ol><li><strong>引言</strong>：<ul><li>引言部分介绍了基于骨骼的人体动作识别领域的背景和挑战。</li><li>提到了3D骨骼姿态估计设备的进步，这些设备在准确性和分辨率方面都有显著提升，因此引发了基于骨骼的活动识别的兴趣。</li><li>骨骼数据在应对光照变化、身体尺度、摄像机视角变化和复杂背景噪声等方面表现出了稳健性。</li><li>作者指出，尽管有了这些进展，但目前缺乏能够有效提取空间和时间特征的算法和神经架构。</li></ul></li><li><strong>问题陈述</strong>：<ul><li>引言中提出了当前主流的基于骨骼的动作识别方法，即Spatial-Temporal Graph Convolutional Network（ST-GCN）。</li><li>ST-GCN被认为是一种有效的非欧几里得数据表示方法，能够在空间和时间上捕获信息。</li><li>然而，作者列举了ST-GCN的一些结构限制，如固定的图拓扑结构、局部性2D卷积和关节之间相关性的低估。</li><li>特别地，作者指出ST-GCN无法捕捉不直接连接的关节之间的相关性，这在某些动作（如&quot;拍手&quot;）中很重要。</li></ul></li><li><strong>解决方法</strong>：<ul><li>为了解决上述问题，本文提出了一种新的方法，即Spatial-Temporal Transformer network（ST-TR）。</li><li>ST-TR利用修改后的Transformer自注意机制，这种机制最初用于NLP任务，但对于建模长距离依赖性非常有效。</li><li>作者提到了之前的研究，将自注意机制应用于图像像素以克服卷积运算的局部性。</li><li>本研究的创新之处在于将自注意机制应用于代表人体骨骼的关节，以模拟人体动作的空间和时间相互作用。</li></ul></li><li><strong>研究贡献</strong>：<ul><li>本研究的主要贡献包括：<ul><li>提出了一种新颖的双流Transformer模型，它同时在空间和时间维度上使用自注意机制。</li><li>设计了Spatial Self-Attention（SSA）模块，用于==动态构建关节之间的链接，表示人体部位之间的关系，独立于动作和自然人体结构==。</li><li>引入了Temporal Self-Attention（TSA）模块，以研究关节在时间上的动态性。</li><li>通过在NTU-RGB+D数据集上的实验，证明了ST-TR模型超越了ST-GCN基线和其他最先进的方法。</li></ul></li></ul></li></ol><p>这些详细介绍突出了文章介绍部分的每个方面，包括问题陈述、解决方法和研究贡献，以及如何利用自注意机制来克服当前方法的限制。如果您需要更深入的解释或有任何疑问，请随时提问。</p></blockquote><h1>3 Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization基于互信息估计和最大化的零镜头骨架动作识别</h1><h2 id="abstract-3">abstract</h2><p>这篇文章的摘要介绍了一种零样本基于骨架的动作识别方法。该方法的目标是在接受已知类别数据的训练后，能够识别未知类别的动作。关键在于建立从已知类别到未知类别的视觉和语义空间之间的连接。以往的研究主要集中在将序列编码成一个特征向量，然后将这些特征映射到嵌入空间中的相同锚点上。然而，它们的性能受到以下两个问题的限制：</p><ol><li>忽略了全局视觉/语义分布的对齐，导致难以捕捉这两个空间之间的真正相互关系。</li><li>忽视了时间信息，因为基于帧的特征含有丰富的动作线索，但直接被汇总成一个单一的特征向量。</li></ol><p>为了解决这些问题，文章提出了一种新的零样本基于骨架的动作识别方法，通过互信息（MI）的估计和最大化来实现。具体来说：</p><ol><li>作者最大化了视觉和语义空间之间的互信息，以实现分布对齐。</li><li>作者利用了时间信息，通过鼓励互信息随着观察到的帧数的增加而增加来估计互信息。</li></ol><p>作者通过在三个大规模骨架动作数据集上进行广泛实验，证实了他们方法的有效性。文章的主要要点包括：</p><ul><li>提出了零样本基于骨架的动作识别方法，通过互信息来连接不同类别之间的视觉和语义空间。</li><li>强调了时间信息的重要性，以提高动作识别性能。</li><li>通过大规模实验验证了提出方法的有效性。</li></ul><h3 id="总结">总结</h3><ul><li>文章介绍了一种零样本基于骨架的动作识别方法。</li><li>该方法的目标是在接受已知类别数据的训练后，能够识别未知类别的动作。</li><li>方法的关键在于建立从已知类别到未知类别的视觉和语义空间之间的连接。</li><li>以往的方法主要将动作序列编码成一个特征向量，然后映射到嵌入空间中的相同锚点，但存在问题。</li><li>问题1：以往方法忽略了全局视觉/语义分布的对齐，限制了捕捉这两个空间之间的真正相互关系。</li><li>问题2：以往方法忽略了时间信息，因为它们直接将帧级别的特征汇总成一个单一的特征向量。</li><li>为了解决这些问题，文章提出了一种新方法，使用互信息（MI）的估计和最大化。</li><li>方法的具体步骤包括最大化视觉和语义空间之间的互信息以实现分布对齐。</li><li>方法还利用时间信息，通过鼓励互信息随着观察到的帧数的增加而增加来估计互信息。</li><li>文章通过在三个大规模骨架动作数据集上进行广泛实验，验证了提出方法的有效性。</li></ul><h2 id="introduction-3">introduction</h2><p>这篇文章介绍了零样本骨架动作识别的重要性和挑战，以及提出的解决方法。以下是文章介绍的要点分条总结：</p><ol><li><strong>动机</strong>:<ul><li>人体动作识别在现实世界中的应用非常广泛，包括安全领域和人机交互。</li><li>随着姿势估计和传感器的发展，获取骨架数据变得容易。</li><li>骨架数据相对于传统的RGB视频数据具有鲁棒性，对外观和背景的变化不敏感，能够提供无偏见的个体表示。</li></ul></li><li><strong>问题</strong>:<ul><li>许多研究者探索了全面监督的方法，但这需要大量标记的训练样本，而在现实情况下处理众多动作类别不经济。</li><li>零样本学习用于识别新的动作类别，如果没有训练样本，但有一些语义信息，如名称、属性或描述。</li><li>3D骨架动作数据的标注和标记更加困难，因此零样本骨架动作识别在实际应用中非常有价值，因为它可以显著减少收集和标记新动作的需求。</li></ul></li><li><strong>零样本学习核心</strong>:<ul><li>在零样本动作识别中，核心是建立在已知类别的视觉和语义空间之间的连接模型。</li><li>在测试阶段，学习的模型用于从已知类别向未知类别进行知识转移。</li><li>为了解决转移学习问题，零样本动作识别依赖于外部知识库，即每个类别标签的语义嵌入，来自预训练的大规模语言模型。</li></ul></li><li><strong>现有方法限制</strong>:<ul><li>存在一些零样本骨架动作识别的研究，但它们主要将动作序列嵌入到视觉特征中。</li><li>这些方法通过在训练阶段的已知类别数据上学习兼容的投影函数或深度度量来建立视觉和语义空间之间的连接模型。</li><li>然而，它们的投影操作只是将特征映射到嵌入空间中的公共锚点，忽视了视觉和语义特征分布的全局对齐。</li><li>此外，学习的投影或度量不充分利用语义信息来捕捉两种模态之间的关联，难以推广到具有不同分布的新类别。</li></ul></li><li><strong>时间信息重要性</strong>:<ul><li>在零样本动作识别任务中，信息丢失问题变得严重，因为一些语义类别需要动态信息才能区分。</li><li>例如，“走路”和“跳跃”只有在局部部分不同，因为这两个动作序列的初始帧相似；只有观察到人站起来的过程才能识别“跳跃”。</li><li>因此，对于人体动作，利用内在的时间动态信息也对零样本连接模型的泛化能力起到作用。</li></ul></li><li><strong>提出方法</strong>:<ul><li>本文提出了一种基于骨架的互信息估计与最大化框架（SMIE），用于零样本动作识别。</li><li>该方法通过全局对齐模块避免了直接映射，并利用互信息作为相似性度量，采用基于Jensen-Shannon散度的估计器来最大化配对的视觉和语义特征之间的互信息。</li><li>为了考虑动作的内在时间信息，SMIE提出了一个时间约束模块，鼓励视觉和语义特征之间的互信息随着动作的执行而增加。</li></ul></li><li><strong>贡献</strong>:<ul><li>本文的主要贡献有三个方面：<ul><li>提出了基于骨架的互信息估计与最大化框架（SMIE），基于互信息最大化的新型零样本动作识别方法，可以捕捉视觉空间和文本语义空间分布之间的复杂统计相关性。</li><li>提出了一个新颖的时间约束模块，用于计算时间约束的互信息，应用了时间排序损失，有助于连接模型捕捉动作的内在时间信息。</li><li>大量实验和分析证明了所提方法的有效性，明显优于基线方法。</li></ul></li></ul></li></ol><blockquote><ol><li><strong>动机</strong>:<ul><li>人体动作识别在各种现实应用中至关重要，包括安全和人机交互。</li><li>随着姿势估计和传感器技术的发展，获取骨架数据变得更容易。</li><li>骨架数据相对于传统的RGB视频数据更具鲁棒性，不受外观和背景变化的影响，能够提供无偏见的个体表示。</li></ul></li><li><strong>问题</strong>:<ul><li>大多数研究关注全面监督方法，但这需要大量标记的训练数据。</li><li>在实际应用中，处理许多不同动作类别的样本收集非常耗时和昂贵。</li><li>零样本学习用于识别新动作类别，即使没有训练数据，只有一些语义信息，如名称、属性或描述。</li><li>3D骨架动作数据的标注更加困难，因为它包含深度信息和复杂的人体动作语义。</li></ul></li><li><strong>零样本学习核心</strong>:<ul><li>零样本动作识别的核心在于建立已知类别的视觉和语义空间之间的连接模型。</li><li>在测试阶段，学习的模型用于将知识从已知类别传递到未知类别。</li><li>为了解决这个问题，零样本动作识别依赖于外部知识库，例如每个类别标签的语义嵌入，这些嵌入来自预训练的大规模语言模型。</li></ul></li><li><strong>现有方法限制</strong>:<ul><li>目前有一些零样本骨架动作识别的研究，它们将动作序列嵌入到视觉特征中。</li><li>这些方法在训练阶段学习了用于建立视觉和语义空间连接的投影函数或深度度量。</li><li>然而，它们的投影操作只是将特征映射到嵌入空间中的公共锚点，忽视了视觉和语义特征分布的全局对齐。</li><li>此外，它们学习的投影或度量不充分利用语义信息来捕捉两种模态之间的关联，难以推广到具有不同分布的新类别。</li></ul></li><li><strong>时间信息重要性</strong>:<ul><li>在零样本动作识别任务中，信息丢失问题变得尤为严重，因为某些语义类别需要动态信息才能区分。</li><li>举例来说，&quot;走路&quot;和&quot;跳跃&quot;之间的区别主要在于局部部分，因为这两个动作序列的初始帧相似。</li><li>&quot;跳跃&quot;动作需要观察人体的上升过程，才能与&quot;走路&quot;区分开来。</li><li>因此，对于人体动作，考虑内在的时间动态信息对于零样本连接模型的泛化能力至关重要。</li></ul></li><li><strong>提出方法</strong>:<ul><li>本文提出了Skeleton-based Mutual Information Estimation and maximization framework（SMIE），这是一种基于互信息最大化的零样本骨架动作识别方法。</li><li>SMIE通过全局对齐模块避免了直接映射，并利用互信息来衡量相似性。</li><li>全局对齐模块使用基于Jensen-Shannon散度的估计器，最大化了配对的视觉和语义特征之间的互信息，同时最小化了未配对的特征之间的互信息。</li><li>在测试阶段，SMIE使用神经网络作为连接模型，估计Jensen-Shannon散度估计器中的相似性分数，用于未知类别的动作识别。</li><li>此外，SMIE考虑了动作的内在时间信息，引入了时间约束模块，鼓励视觉和语义特征之间的互信息随着动作的执行部分增加而增加。</li></ul></li><li><strong>贡献</strong>:<ul><li>本文的主要贡献有三个方面：<ul><li>提出了一种新型的零样本骨架动作识别方法（SMIE），基于互信息最大化，可以捕捉视觉空间和文本语义空间分布之间的复杂统计相关性。</li><li>引入了时间约束模块，计算时间约束的互信息，并应用时间排序损失，有助于连接模型捕捉动作的内在时间信息。</li><li>大量实验和分析证明了所提方法的有效性，明显优于基线方法。</li></ul></li></ul></li></ol></blockquote><h1>4 Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient基于骨骼运动梯度的骨骼人体动作识别的硬无盒对抗攻击</h1><h2 id="abstract-4">abstract</h2><p>这篇文章的摘要介绍了骨架基础的人体活动识别方法最近被证明容易受到对抗性攻击的问题。然而，这些攻击方法要求要么完全了解受害者（即白盒攻击），要么访问训练数据（即基于转移的攻击），要么频繁地查询模型（即黑盒攻击）。它们的要求都非常严格，引发了对这种脆弱性的严重性的疑问。因此，本文展示了这种脆弱性确实存在。作者提出了一个新的攻击任务：攻击者无法访问受害者模型、训练数据或标签，这被称为“硬无盒攻击”。</p><p>具体来说，作者首先学习了一个运动流形，定义了一个用于计算攻击的新梯度的对抗损失，称为“骨架运动信息（SMI）梯度”。这个梯度包含了动作动态的信息，与现有的基于梯度的攻击方法不同，后者假设数据中的每个维度都是独立的。SMI梯度可以增强许多基于梯度的攻击方法，从而产生一系列新的无盒攻击方法。大量的评估和比较结果显示，我们的方法对现有的分类器构成了真正的威胁。它们还表明，SMI梯度在无盒和基于转移的黑盒设置中提高了对抗样本的可传递性和不可察觉性。</p><p>要点总结：</p><ul><li>最近，骨架基础的人体活动识别方法已经显示出容易受到对抗性攻击。</li><li>现有的对抗攻击方法要求攻击者要么完全了解受害者，要么访问训练数据，要么频繁地查询模型，这些要求非常严格。</li><li>本文提出了一种新的攻击任务，称为“硬无盒攻击”，其中攻击者无法访问受害者模型、训练数据或标签。</li><li>作者提出了一种新的梯度，称为“骨架运动信息（SMI）梯度”，用于进行无盒攻击。</li><li>SMI梯度可以增强基于梯度的攻击方法，产生一系列新的无盒攻击方法。</li><li>实验证明，SMI梯度对现有的分类器构成了真正的威胁，并提高了对抗样本的可传递性和不可察觉性。</li></ul><p>以下是文章摘要的要点分条总结：</p><ul><li>最近，骨架基础的人体活动识别方法容易受到对抗性攻击。</li><li>攻击方法通常要求攻击者了解受害者、访问训练数据或频繁查询模型，这些要求很严格。</li><li>本文提出了一种新的攻击任务，即“硬无盒攻击”，攻击者无法访问受害者模型、训练数据或标签。</li><li>作者引入了一个新的梯度，称为“骨架运动信息（SMI）梯度”，用于进行无盒攻击。</li><li>SMI梯度可以增强基于梯度的攻击方法，创造一系列新的无盒攻击方法。</li><li>实验证明，作者的方法对现有分类器构成真正威胁，同时提高了对抗样本的可传递性和不可察觉性。</li></ul><h2 id="introduction-4">introduction</h2><ol><li><strong>引言</strong>：<ul><li>深度学习模型容易受到对抗性攻击，攻击者会精心计算数据扰动，以欺骗受过训练的网络。</li><li>近年来，对骨架基础的人体活动识别（S-HAR）的对抗攻击引起了关注，因为骨架数据在安全关键应用中得到广泛应用。</li><li>现有的S-HAR攻击分为白盒和黑盒两种方法，但它们要求攻击者要么了解受害者模型的全部细节，要么需要大量的模型查询或访问训练数据和标签，这在实际情况下往往不可行。</li></ul></li><li><strong>新的威胁模型</strong>：<ul><li>本文引入了一个新的威胁模型，不需要访问受害者模型、训练数据或标签。</li><li>这个新的威胁模型被称为“硬无盒攻击”，与近期对图像的“无盒攻击”（不需要访问受害者模型但仍需要访问标签）不同。</li><li>硬无盒攻击在所有攻击设置中需要的知识最少，因为它只能访问没有标签的测试数据。</li></ul></li><li><strong>挑战和解决方法</strong>：<ul><li>挑战在于，没有受害者模型的访问权限，攻击方法不能依赖于分类损失的梯度、训练过程中的数据操作或分类器的反馈。</li><li>为了应对这些挑战，作者提出了一种基于对比学习（CL）的解决方案，并引入了基于流形的无盒对抗损失。</li><li>CL用于学习一个潜在数据流形，其中相似样本自然聚集在一起，不需要类标签。这有助于生成骨架对抗样本。</li><li>CL适用于硬无盒攻击设置，因为它能够在受限的攻击条件下捕捉具有判别性的高级特征。</li><li>此外，作者设计了一种新的无盒对抗损失，用于最大化每个对手与正样本的不相似性，同时最小化与负样本的相似性。这个损失指导了梯度攻击的搜索过程。</li></ul></li><li><strong>基于骨架运动信息的梯度</strong>：<ul><li>传统的梯度攻击方法如I-FGSM在S-HAR攻击中已被证明是有效的，但梯度是基于受害者模型和标签计算的，因此不适用于硬无盒攻击。</li><li>作者提出了一种新的梯度，称为“骨架运动信息（SMI）梯度”，用于描述给定运动周围的局部流形。</li><li>SMI梯度采用动态模型（例如马尔可夫和自回归模型）来表示运动动态，用于计算损失梯度。</li><li>与传统方法不同，SMI梯度明确考虑了时间中帧之间的依赖关系，从而更适合硬无盒攻击设置。</li></ul></li><li><strong>实验结果</strong>：<ul><li>大量实验表明，作者的方法生成的对抗样本成功攻击了各种数据集上的受害者模型（HDM05、NTU60和NTU120）。</li><li>基于SMI梯度的攻击提高了在无盒和转移黑盒设置中的攻击传递性，并具有更好的不可察觉性。</li></ul></li><li><strong>贡献</strong>：<ul><li>本文引入了一种新的硬无盒攻击威胁模型，并提出了首个在没有访问受害者模型、训练数据或标签的情况下生成对抗样本的方法。</li><li>作者提出了一种新的骨架运动信息梯度，用于在局部流形中引导对抗攻击的搜索，明确考虑了时空特性。</li><li>提出了一系列新的基于梯度的攻击策略，改善了无盒和转移黑盒攻击中对抗样本的传递性和不可察觉性。</li></ul></li></ol><h3 id="翻译">翻译</h3><p><strong>1. 引言</strong>：</p><p>深度学习模型容易受到对抗性攻击，攻击者会精心计算数据扰动，以欺骗受过训练的网络。自从其被发现以来，不同任务中的各种模型都受到了攻击，引起了严重关切，因为这些扰动对人类来说是难以察觉的。最近，在骨架基础的人体活动识别（S-HAR）中的对抗攻击引起了关注，因为骨架数据已广泛用于诸如体育分析、生物力学、监控和人机交互等安全关键应用中。</p><p>现有的S-HAR攻击分为白盒和黑盒两种方法。白盒方法要求攻击者预先了解受害者模型的全部细节，而黑盒方法要求攻击者大量查询受害者模型或访问训练数据和标签。一方面，在现实世界的情况下，攻击者不太可能获得受害者模型的详细信息以及训练数据和标签。另一方面，频繁进行大量查询（例如数万次）以查询受害者模型是耗时的且容易引起怀疑的。换句话说，现有S-HAR攻击的设置过于严格。成功攻击的关键在于降低对受害者模型、训练数据和标签的所需信息。</p><p><strong>2. 新的威胁模型</strong>：</p><p>本文引入了一个新的威胁模型，不需要访问受害者模型、训练数据或标签。我们将这个新的威胁模型称为“硬无盒攻击”，与最近对图像的“无盒攻击”（不需要访问受害者模型但仍需要访问标签）有所不同。表1展示了不同设置下的对抗攻击的比较情况。在所有的攻击设置中，我们的硬无盒攻击需要的知识最少，因为它只能访问没有标签的测试数据。设计这种攻击是非常复杂和具有挑战性的。没有对受害者模型、标签和训练样本的访问权限，也就意味着无法训练替代模型来攻击或估计数据分布。</p><p><strong>3. 挑战和解决方法</strong>：</p><p>挑战在于，没有受害者模型的访问权限，攻击方法不能依赖于分类损失的梯度、训练过程中的数据操作或分类器的反馈。为了应对这些挑战，我们提出了一种基于对比学习（CL）的解决方案，并引入了基于流形的无盒对抗损失。首先，我们介绍了一种新的CL应用，用于学习潜在数据流形，其中相似样本自然聚集在一起，不需要类标签。这有助于生成骨架对抗样本。CL适用于硬无盒攻击设置，因为它能够在受限的攻击条件下捕捉具有判别性的高级特征。其次，我们计算扰动以将数据样本从其潜在空间中的相似邻居中拖离，受到预定义预算的限制。特别地，我们设计了一种新的无盒对抗损失，以最大化每个对手与正样本的不相似性，同时最小化其与负样本的相似性。这个损失为梯度攻击方案中的对手搜索提供了指导。</p><p><strong>4. 基于骨架运动信息的梯度</strong>：</p><p>传统的基于梯度的攻击方法，如I-FGSM，在S-HAR攻击中已被证明是有效的，但梯度是基于受害者模型和标签计算的，因此不适用于硬无盒攻击。由于对抗样本可能位于或接近运动流形，理想情况下，我们希望沿着流形探索。换句话说，对抗损失梯度的计算应考虑到局部运动流形。为此，我们提出了显式建模运动动态来描述给定运动周围局部流形的方法。具体来说，我们引入了“骨架运动信息（SMI）梯度”，它使用动态模型（例如马尔可夫和自回归模型）来表示损失梯度计算中的运动动态。因此，与传统方法不同，SMI梯度明确考虑了时间中帧之间的依赖关系。此外，SMI梯度与现有的基于梯度的方法（包括I-FGSM和MI-FGSM）兼容，使我们能够有效地构建一系列新的无盒攻击方法。</p><h3 id="总结-2">总结</h3><ol><li><strong>引言</strong>：<ul><li>深度学习模型容易受到对抗性攻击，攻击者通过巧妙计算数据扰动来欺骗这些训练过的网络。</li><li>骨架基础的人体活动识别（S-HAR）中的对抗攻击近年来引起了关注，因为骨架数据在安全关键应用中广泛使用。</li></ul></li><li><strong>新的威胁模型</strong>：<ul><li>引入了一个新的威胁模型，称为“硬无盒攻击”，攻击者无需访问受害者模型、训练数据或标签。</li><li>硬无盒攻击相对于传统的攻击方式需要更少的信息。</li></ul></li><li><strong>挑战和解决方法</strong>：<ul><li>没有对受害者模型、训练数据和标签的访问权限，攻击方法不能依赖于梯度、训练过程中的数据操作或分类器的反馈。</li><li>为了应对这些挑战，引入了对比学习（CL）方法和基于流形的无盒对抗损失。</li><li>CL用于学习数据流形，无需类标签，有助于生成骨架对抗样本。</li><li>提出了一种新的无盒对抗损失，用于指导对手搜索。</li></ul></li><li><strong>基于骨架运动信息的梯度</strong>：<ul><li>传统的梯度攻击方法不适用于硬无盒攻击，因为它们需要访问受害者模型和标签。</li><li>引入了“骨架运动信息（SMI）梯度”，考虑了运动动态，以更好地描述局部流形。</li><li>SMI梯度与现有的基于梯度的方法兼容，可以用于构建新的无盒攻击方法。</li></ul></li><li><strong>实验结果</strong>：<ul><li>实验证明，作者的方法可以生成有效的对抗样本，成功攻击不同数据集上的受害者模型。</li><li>基于SMI梯度的攻击提高了攻击的传递性和对抗样本的不可察觉性。</li></ul></li><li><strong>贡献</strong>：<ul><li>引入了一种新的硬无盒攻击威胁模型，并提出了生成对抗样本的新方法，无需访问受害者模型、训练数据或标签。</li><li>提出了一种基于骨架运动信息的梯度，用于明确考虑运动动态，改善了攻击的传递性和不可察觉性。</li></ul></li></ol><h1>5 STST: Spatial-Temporal Specialized Transformer for Skeleton-based Action Recognition 用于基于骨架的动作识别的时空专用Transformer</h1><h2 id="abstract-5">abstract</h2><p>摘要：</p><p>鉴于骨架基础的动作识别在适应动态环境和复杂背景方面具有很强的适应性，因此已经得到广泛研究。为了从骨架序列中识别不同的动作，必须对由骨架表示的人的姿势以及其在时间维度上的变化进行建模，这是至关重要和关键的。然而，大多数现有工作将骨架序列在时间和空间维度上以相同的方式处理，忽视了骨架数据中时间和空间维度之间的差异，这不是一种最佳建模骨架序列的方式。我们提出了在每个帧中分别对骨架表示的姿势进行建模的方法。同时，需要捕捉骨架在时间维度上的整体运动。因此，我们为在空间和时间维度上建模骨架序列设计了“空间变换块”和“方向时间变换块”。由于受到遮挡、传感器问题、原始视频等的影响，提取的骨架数据在时间和空间维度上都存在噪声，降低了模型的识别能力。为了适应这种不完美的信息条件，我们提出了一种多任务自监督学习方法，通过在不同情况下提供混淆样本来提高模型的鲁棒性。结合以上设计，我们提出了我们的“时空专用变换器”（STST），并在SHREC、NTU-RGB+D和Kinetics-Skeleton上对我们的模型进行了实验。广泛的实验结果展示了所提方法的性能改进和分析。</p><p>要点总结：</p><ol><li>骨架基础的动作识别在动态环境和复杂背景下具有广泛的应用。</li><li>为了准确识别骨架序列中的不同动作，需要同时考虑骨架在时间和空间维度上的姿势和运动。</li><li>现有的方法通常未充分考虑骨架数据中时间和空间维度之间的差异。</li><li>本文提出了两种块，即“空间变换块”和“方向时间变换块”，用于分别建模骨架序列的空间和时间特性。</li><li>骨架数据常常包含噪声，作者提出了一种多任务自监督学习方法，以提高模型的鲁棒性。</li><li>所提出的“时空专用变换器”（STST）在多个数据集上进行了实验，结果表明方法性能得到了改进。</li></ol><h3 id="总结-3">总结</h3><ol><li><strong>背景</strong>：<ul><li>骨架基础的动作识别在复杂背景和动态环境下具有广泛应用。</li><li>识别不同动作需要考虑骨架姿势和时间维度上的变化。</li></ul></li><li><strong>问题</strong>：<ul><li>大多数现有方法未充分考虑骨架数据中时间和空间维度之间的差异。</li></ul></li><li><strong>解决方法</strong>：<ul><li>引入了“空间变换块”和“方向时间变换块”，用于分别建模骨架序列的空间和时间特性。</li><li>提出了多任务自监督学习方法，通过混淆样本提高模型的鲁棒性。</li></ul></li><li><strong>模型</strong>：<ul><li>提出的“时空专用变换器”（STST）用于综合以上设计，以提高骨架动作识别的性能。</li></ul></li><li><strong>实验结果</strong>：<ul><li>在多个数据集上进行了实验，结果表明方法性能得到了改进。</li></ul></li></ol><p>这些要点涵盖了文章摘要中的关键信息，有助于理解该研究的主要贡献和方法。</p><h2 id="introduction-5">introduction</h2><p><strong>1. 引言</strong></p><ul><li>动作识别在视频理解中起着重要作用，广泛应用于视频监控、人机交互、虚拟现实等领域。</li><li>骨架基础的动作识别因其对动态环境和复杂背景的强适应性而受到研究者的关注。</li><li>骨架数据采用人体关节坐标来表示视频中的人体动作，它是一种树形结构的表示，能动态描述人体姿势和关节运动，因此在视觉噪声环境或背景复杂情况下比其他方法更稳健。</li></ul><p><strong>2. 挑战</strong></p><ul><li>现有的骨架方法常常将骨架数据转化为点序列、伪图像或图形，但依赖于手动设计的遍历规则或图拓扑结构，不一定是最优的。</li><li>传统模型使用RNN和CNN来捕捉骨架运动，但在处理长时间跨度的视频时效率低下。</li><li>最近，随着Transformer的成功，它也被应用于基于骨架的模型。然而，大多数基于Transformer的方法在时间和空间维度上都采用相同的策略，忽略了骨架序列数据中时间和空间维度之间的差异。</li></ul><p><strong>3. 解决方法</strong></p><ul><li>为了设计更好的骨架基础动作识别模型，需要分别从时空两个视角建模动作，并为每个视角设计特定的模块。</li><li>模型需要能够从空间视图中表达每帧的姿势，同时从时间视图中捕捉姿势的运动模式。</li><li>模型还需要处理异常情况，因为骨架结构并不总是稳定或强壮。</li></ul><p><strong>4. 模型提出</strong></p><ul><li>提出了“时空专用变换器”（STST），能够在时空维度上分别捕捉骨架序列的信息，同时对各种异常场景具有鲁棒性。</li><li>使用Transformer编码器来模拟骨架序列，分别为空间和时间操作设计模块。</li><li>提取每个关节的坐标信息、语义信息和时间信息，以使编码器充分利用骨架数据而不丢失信息。</li></ul><p><strong>5. 增强鲁棒性</strong></p><ul><li>提出了多任务自监督学习方法，以增强模型对三种不完美信息情况的鲁棒性，这些情况可视为三种信息中的噪声。</li></ul><p><strong>6. 实验结果</strong></p><ul><li>在三个广泛使用的数据集上进行实验，STST模型在不使用自监督策略的情况下也超越了先前的方法。</li></ul><p><strong>总结：</strong></p><ul><li>本文的贡献包括引入了STST模型，它通过不同的关节组织策略在时空维度上捕获骨架的运动。</li><li>还提出了一种多任务自监督学习方法，增强了模型对三种不完美信息情况的鲁棒性。</li><li>在多个数据集上进行的实验表明，STST模型在骨架基础的动作识别中表现出色，甚至在不使用自监督策略的情况下也如此。</li></ul><h3 id="实例">实例</h3><p><strong>1. 引言</strong></p><ul><li>动作识别在视频理解中具有广泛应用，特别适用于视频监控、人机交互和虚拟现实等领域。</li><li>骨架基础的动作识别因其对动态环境和复杂背景的强适应性而备受研究者关注。</li><li>骨架数据使用人体关节坐标表示人体动作，具有树状结构，能够动态描述姿势和关节运动。</li></ul><p><strong>2. 挑战</strong></p><ul><li>现有方法对骨架数据的处理方式不一定最优，通常需要手动设计关节遍历规则或图拓扑结构。</li><li>传统模型对长时间跨度的视频处理效率低下。</li><li>大多数基于Transformer的方法未充分考虑骨架序列数据中时间和空间维度的差异。</li></ul><p><strong>3. 解决方法</strong></p><ul><li>需要从时空两个视角分别建模动作，并为每个视角设计特定模块。</li><li>模型应能够从空间视图中表达每帧的姿势，同时从时间视图中捕捉姿势的运动模式。</li><li>模型还需要处理异常情况，如骨架数据中的噪声。</li></ul><p><strong>4. 模型提出</strong></p><ul><li>提出了“时空专用变换器”（STST），能够在时空维度上分别捕捉骨架序列的信息，同时对各种异常场景具有鲁棒性。</li><li>使用Transformer编码器来模拟骨架序列，分别为空间和时间操作设计模块。</li><li>提取每个关节的坐标信息、语义信息和时间信息，以使编码器充分利用骨架数据而不丢失信息。</li></ul><p><strong>5. 增强鲁棒性</strong></p><ul><li>提出了多任务自监督学习方法，以增强模型对三种不完美信息情况的鲁棒性，这些情况可视为三种信息中的噪声。</li></ul><p><strong>6. 实验结果</strong></p><ul><li>在三个广泛使用的数据集上进行实验，STST模型在不使用自监督策略的情况下也超越了先前的方法。</li></ul><p>通过这些要点，文章介绍了STST模型的提出，以及如何处理骨架序列数据中的时空信息和噪声，最终实现了在骨架基础的动作识别中的优越性能</p><p><strong>1. 引言</strong></p><ul><li>引言部分强调了动作识别的重要性，尤其是在视频理解领域的应用。</li><li>骨架基础的动作识别因其对动态环境和复杂背景的强适应性而备受研究者关注。</li><li>这一部分还强调了骨架数据的独特性，它采用人体关节坐标来表示动作，具有树状结构，能够动态描述姿势和关节运动。</li></ul><p><strong>2. 挑战</strong></p><ul><li>介绍了当前骨架基础动作识别面临的挑战：<ul><li>现有方法通常将骨架数据转换为点序列、伪图像或图形，但这些方法依赖于手动设计的遍历规则或图拓扑结构。</li><li>传统模型在处理长时间跨度的视频时效率低下。</li><li>大多数基于Transformer的方法未充分考虑骨架序列数据中时间和空间维度的差异。</li></ul></li></ul><p><strong>3. 解决方法</strong></p><ul><li>详细说明了如何设计更好的骨架基础动作识别模型：<ul><li>强调了需要分别从时空两个视角建模动作，因为这两个视角的处理机制不同。</li><li>模型应具备以下能力：<ul><li>从空间视图中表达每一帧的姿势。</li><li>从时间视图中捕捉姿势的运动模式。</li></ul></li><li>模型还需要处理骨架数据中的异常情况，如噪声和干扰。</li></ul></li></ul><p><strong>4. 模型提出</strong></p><ul><li>详细描述了“时空专用变换器”（STST）模型的提出：<ul><li>使用Transformer编码器来模拟骨架序列，但关键在于，空间和时间操作分别设计为特殊模块。</li><li>骨架序列中的每个关节包含三种信息：坐标信息、语义信息和时间信息。这三种信息被显式地提取为三种令牌。</li><li>描述了Spatial Transformer Block（空间变换块）和Directional Temporal Transformer Block（方向性时间变换块）的设计和功能。</li></ul></li></ul><p><strong>5. 增强鲁棒性</strong></p><ul><li>解释了如何增强模型对不完美信息情况的鲁棒性：<ul><li>引入了多任务自监督学习方法，以增强模型对坐标、语义和时间信息中的噪声和干扰的抵抗能力。</li><li>这些自监督学习任务被设计为高度并行且易于实施，与Transformer的特性非常契合。</li></ul></li></ul><p><strong>6. 实验结果</strong></p><ul><li>提供了在三个广泛使用的数据集上进行的实验结果，包括SHREC、NTU-RGB+D和Kinetics-Skeleton。</li><li>强调STST模型在这些数据集上的性能表现，甚至在没有使用自监督策略的情况下也超越了以前的方法。</li></ul><p>这些详细的介绍有助于理解文章的核心思想和方法，以及作者是如何应对动作识别中的挑战并提出新模型的。</p><p>transformer +GCN</p><p>Unet变形的 超图解码和编码器，实现时空的高阶依赖信息的提取</p><blockquote><p><strong>多尺度</strong> <strong>超图卷积模块为编码器，多尺度超图融合模块为解码器</strong>，更好的构建时间空间依赖关系 <strong>多尺度时间空洞卷积模块</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读2</title>
      <link href="/project/posts/26aa6c6a.html"/>
      <url>/project/posts/26aa6c6a.html</url>
      
        <content type="html"><![CDATA[<h3 id="STAR-Sparse-Transformer-based-Action-Recognition">STAR: Sparse Transformer-based Action Recognition</h3><h1>INFO</h1><h3 id="摘要总结">摘要总结</h3><blockquote><ul><li><strong>要点总结：</strong><ul><li>本文介绍了一种新颖的骨架（skeleton）-based人体动作识别模型。</li><li>该模型在空间维度上使用了稀疏注意力（sparse-attention）和在时间维度上使用了分段线性注意力（segmented linear attention）。</li><li>该模型能够处理不同长度的视频剪辑，并将它们分组为单个批次进行处理。</li><li>实验结果显示，该模型在性能上与基线模型相媲美，但可训练参数较少，并且在训练和推理速度上表现出色。</li><li>与基线模型相比，该模型实现了4∼18倍的速度提升和1/7∼1/15的模型大小减小，同时保持了竞争性的准确性。</li></ul></li></ul></blockquote><h3 id="介绍总结">介绍总结</h3><blockquote><p><strong>引言</strong> 人体动作识别在许多现实世界应用中起着关键作用，如整体场景理解、视频监视和人机交互。尤其是，基于骨架的人体动作识别近年来引起了广泛关注并显示出其有效性。骨架表示包含人体关键关节的2D或3D坐标的时间序列，提供了动态身体运动信息，与原始RGB表示相比，对光照条件和背景噪声的变化具有稳健性。 早期的基于骨架的人体动作识别方法专注于设计从关节坐标中提取的手工制作特征，并使用RNNs和CNNs聚合学习特征。然而，这些方法很少探索身体关节之间的关系，结果表现不佳。最近的方法专注于探索人体关节的自然连接，并成功采用了图卷积网络（GCNs），特别是对于非欧几里德域数据，类似于卷积神经网络（CNNs），但执行卷积操作以聚合连接和相关关节的特征。Yan等人提出了一种ST-GCN模型，用于从身体关节的空间和时间图中提取有区别的特征。在ST-GCN的成功之后，许多工作提出了对ST-GCN的优化，以提高性能和网络容量。 然而，现有的基于GCN的模型在实时应用中通常是不切实际的，因为它们具有庞大的计算复杂性和内存使用量。基线GCN模型，例如ST-GCN，包含超过309万个参数，并且至少需要16.2 GFLOPs才能在单个动作视频样本上运行推理。DGNN，由增量GCN模块组成，甚至包含2600万个模型参数。这种高模型复杂性导致模型训练和推理困难，使模型不适合部署在边缘设备上。此外，这些基于GCN的模型通过填充重复帧和零来处理固定大小的动作视频，以匹配视频中显示的最大帧数和人数。这些额外的填充增加了延迟和内存要求，阻碍了它们在实时和嵌入式应用中的采用。 本文提出了一种基于稀疏变换的动作识别（STAR）模型，作为骨架动作建模的新基线，以解决上述缺点。变换器已经成为自然语言处理中的一种流行选择。最近，它们已被用于计算机视觉中，以获得与卷积网络相比具有竞争力的结果，同时需要更少的计算资源来训练。受这些变换器架构的启发，我们的模型包括空间和时间编码器，它们分别对骨架序列的空间和时间维度应用稀疏注意和分段线性注意。我们的稀疏关注模块沿空间维度执行稀疏矩阵乘法，以提取连接关节的相关性，而以前的方法使用密集矩阵乘法，其中大多数条目都是零，导致额外的计算。沿时间维度的分段线性注意机制进一步减少了计算和内存使用，通过处理可变长度的序列。我们还将分段位置编码应用于数据嵌入，以提供沿时间维度的可变长度骨架数据的时间序列排序概念。此外，分段上下文注意在整个视频帧上执行加权摘要，使我们的模型与GCN基于模型相比具有固定长度的时间维度上的鲁棒性。 与基线GCN模型（ST-GCN）相比，我们的模型（STAR）在NTU RGB+D 60和120这两个数据集上取得了更高的性能，而模型尺寸要小得多。本文的主要贡献如下：</p><ol><li>我们专注于基于自我注意机制的高效模型的设计。我们提出了基于稀疏变换的动作识别（STAR）模型，可以处理可变长度的骨架动作序列，无需额外的预处理和零填充。我们的模型的灵活性有益于具有有限计算资源的实时应用或边缘平台。</li><li>我们提出了一种有效执行稀疏矩阵乘法以捕获人体骨架关节之间空间相关性的稀疏自注意模块。</li><li>我们提出了一种能够有效捕获时间维度上动态关节运动的分段线性自注意模块。</li><li>实验证明，与基线模型相比，我们的模型尺寸更小，提供了4∼18倍的执行速度提升。</li></ol></blockquote><p>总结</p><ul><li>人体动作识别在多个现实世界应用中至关重要。</li><li>基于骨架的人体动作识别因其对光照条件和背景噪声的鲁棒性而备受关注。</li><li>早期的方法主要侧重于手工制作特征，但未探索关节之间的关系，结果表现不佳。</li><li>近年来，图卷积网络（GCNs）在探索人体关节的自然连接方面表现出色。</li><li>现有的GCN模型在实时应用中的计算复杂性和内存使用方面存在问题。</li><li>==本文提出了一种稀疏变换器（STAR）模型，用于骨架动作建模，以解决现有模型的不足==</li><li>STAR模型==包括稀疏关注和分段线性关注模块，用于处理骨架序列的空间和时间维度==。</li><li><strong>与传统方法相比，STAR模型通过使用稀疏注意机制来更有效地捕捉关节之间的空间关系。</strong></li><li><strong>STAR模型还通过分段线性注意机制在时间维度上进一步降低计算和内存使用，处理可变长度的序列。</strong></li><li>STAR模型相对于基线模型（ST-GCN）在性能上更胜一筹，而且模型更小。</li><li>本文的主要贡献包括高效的模型设计、稀疏自我注意模块和分段线性自我注意模块的提出以及性能和模型尺寸的改进。</li></ul><h4 id="增加示例">增加示例</h4><blockquote><ul><li>人体动作识别在现实世界中的多个领域都具有重要作用，如场景理解、视频监控和人机交互。</li><li>基于骨架的人体动作识别因其对光照和噪声的鲁棒性而备受关注，其骨架表示包含了人体关键关节的动态坐标信息。</li><li>早期的方法主要侧重于手工特征设计，但未能充分探索关节之间的关系，导致性能不佳。</li><li>近年来，通过引入图卷积网络（GCNs），特别是在非欧几里得数据领域，成功地探索了人体关节之间的自然连接。</li><li>但是，现有的GCN模型在计算和内存使用方面效率不高，限制了其在实时应用和边缘设备上的应用。</li><li>本文提出了一种新型骨架动作识别模型，名为稀疏变换器（STAR），以解决现有模型的不足。</li><li>STAR模型包括针对骨架序列的空间和时间维度的稀疏关注和分段线性关注模块。</li><li>与传统方法相比，STAR模型通过使用稀疏注意机制来更有效地捕捉关节之间的空间关系。</li><li>STAR模型还通过分段线性注意机制在时间维度上进一步降低计算和内存使用，处理可变长度的序列。</li><li>实验结果表明，STAR模型相对于基线模型（ST-GCN）在性能上更出色，同时模型规模更小。</li><li>本文的主要贡献包括高效的模型设计、稀疏自我注意模块和分段线性自我注意模块的引入以及性能和模型大小的显著改进。</li></ul></blockquote><h3 id="相关工作">相关工作</h3><blockquote><p><strong>关于基于骨骼的动作识别（Skeleton-Based Action Recognition）：</strong></p><ul><li>Skeleton-Based Action Recognition是一项重要的研究领域，特别是近年来骨骼数据的紧凑表示使得模型更高效且不受光照条件和环境噪声变化的影响。</li><li>早期的方法主要集中在设计从关节坐标中提取的手工制作特征和关节之间的关系。</li><li>近年来，图卷积网络（GCN），尤其是ST-GCN，已经在这一领域取得了巨大的成功。ST-GCN模型包括空间和时间卷积模块，类似于用于图像的传统卷积滤波器。</li><li>许多后续的研究工作提出了对ST-GCN的改进，如AS-GCN和DGNN，以提高性能和网络容量。</li><li>一些研究关注GCN-based方法的计算复杂性，提出了减少计算复杂性的方法，如Shift-GCN和multi-branch ResGCN。</li></ul><p><strong>关于Transformers和自注意力机制（Transformers and Self-Attention Mechanism）：</strong></p><ul><li>Transformers最初用于机器翻译，是各种自然语言处理任务中的最先进方法，如GPT和BERT。</li><li>Transformer架构的核心组件是自注意力机制，它学习序列中每个元素之间的关系，能够建模长距离依赖性。</li><li>近年来，Transformer-based模型在计算机视觉领域引起了广泛关注，代替了传统的卷积操作。</li><li>Vision Transformer（ViT）是第一个在大规模图像识别任务中完全取代标准卷积的工作。</li><li>某些研究工作探索了在骨骼动作识别任务中使用Transformer编码器和GCN模块的混合模型。</li></ul><p>这些技术和先验知识对于文章中提出的Sparse Transformer-based Action Recognition（STAR）模型的设计和改进起到了关键作用。</p></blockquote><blockquote><p>增加示例</p></blockquote><blockquote><p><strong>关于基于骨骼的动作识别（Skeleton-Based Action Recognition）：</strong></p><ul><li>早期方法主要集中在设计手工制作特征。示例包括Vemulapalli、Arrate和Chellappa（2014）以及Wang等人（2012）的工作。</li><li>近年来，Graph Convolutional Networks（GCNs）变得流行，特别是ST-GCN模型。该模型使用空间和时间卷积模块来提取骨骼动作序列的高级空间表示。示例为Yan、Xiong和Lin（2018）的ST-GCN模型。</li><li>针对ST-GCN的改进工作包括AS-GCN（Li等人，2019）、DGNNs（Shi等人，2019a）和G3D（Liu等人，2020）等。</li></ul><p><strong>关于Transformers和自注意力机制（Transformers and Self-Attention Mechanism）：</strong></p><ul><li>Transformers最初用于机器翻译，取得了显著成果。示例包括Vaswani等人（2017）的工作以及GPT和BERT等模型。</li><li>Vision Transformer（ViT）是第一个在大规模图像识别任务中完全取代标准卷积的工作，由Dosovitskiy等人（2020）提出。</li><li>在计算机视觉领域，一些研究探索了将Transformer架构应用于图像处理，包括使用稀疏注意力的工作（Huang等人，2019）。</li><li>文章中提到的一个相关工作是将Transformer编码器和GCN模块结合在骨骼动作识别任务中，但没有完全取代GCN（Plizzari、Cannici和Matteucci，2020）。</li></ul></blockquote><h3 id="方法">方法</h3><h4 id="Spatial-domain-Sparse-MHSA机制">Spatial domain: Sparse MHSA机制</h4><blockquote><p>空间域的稀疏多头自注意力机制是我们空间变换编码器的关键组成部分。传统的基于GCN（图卷积网络）的模型以及之前的Transformer模型，如ST-GCN和ST-TR，利用稠密的骨骼表示来聚合相邻节点的特征。这种密集的邻接矩阵表示包含了NTU数据集的625个条目，而实际上表示骨架的关节连接数量仅为24个。这意味着对于零条目来说，96%的矩阵乘法是不必要的计算。因此，我们提出了一种稀疏注意机制，它只在稀疏节点连接上执行矩阵乘法。这使得每个关节只能根据分配给相应连接的注意系数聚合其相邻关节的信息。</p><p>这些关节连接基于骨架的拓扑结构，它是一个树状结构。从这个拓扑结构继承的注意力被看作是物理注意力（或真实注意力），如图1所示。为了增加关注领域，我们还根据身体部位的逻辑关系人工添加了更多的关节之间的链接，我们将这些人工创建的注意力称为人工注意力，如图1所示的虚线黄色箭头。为简单起见，假设骨架邻接矩阵为A，那么用于额外空间注意力的人工链接是通过A2和A3获得的。因此，在我们的模型中，空间注意力图是根据A+A2+A3的拓扑表示来评估的。</p><p>稀疏注意力是根据关节之间的连接计算的。如下所示：</p><p>在等式1中进行嵌入之后，首先通过两个连接关节的特征向量的点积的指数分数来计算一对连接关节之间的关节-关节注意力（等式2），然后按照等式3中描述的所有相邻关节的指数分数的指数分数之和对分数进行归一化。</p><p>其中，Q、K和V分别是Transformer术语中的查询、键和值；qi = Q(i)、kj = K(j)、vj = V(j)，以及 q, k = exp(qTk/√d)。最后，我们得到了多维（多头）稀疏矩阵的注意力图A，这些矩阵共享由单个邻接矩阵描述的相同拓扑结构（包括人工注意力的链接），其中注意力系数为A(i, j) = αi,j。稀疏操作可以通过并行进行的张量聚集和分散操作来完成。</p></blockquote><blockquote><p>这段描述了文章中的空间域中的稀疏多头自注意力机制的关键要点：</p><ol><li><strong>稀疏自注意力机制的背景</strong>：传统的GCN和Transformer模型在处理骨骼数据时通常使用密集的邻接矩阵，这导致大量不必要的计算，特别是对于关节连接的稀疏数据而言。</li><li><strong>稀疏注意力的动机</strong>：为了提高效率，文章提出了一种稀疏注意力机制，该机制只在稀疏的关节连接上执行矩阵乘法，允许每个关节仅根据其动态分配的关注系数聚合相邻关节的信息。</li><li><strong>关节连接和拓扑结构</strong>：文章基于骨骼的拓扑结构，将关节之间的连接看作是物理注意力，同时还引入了人工注意力，这些额外的连接是根据身体部位的逻辑关系创建的。</li><li><strong>稀疏注意力计算</strong>：稀疏注意力的计算基于连接的关节之间。它使用查询（Q）、键（K）和值（V）的张量表示，其中关节-关节的注意力通过特征向量的点积的指数分数计算，然后通过对所有相邻关节的指数分数之和进行归一化。</li><li><strong>多头注意力</strong>：稀疏注意力生成多维（多头）稀疏矩阵，这些矩阵共享相同的拓扑结构，注意力系数由单个邻接矩阵描述。这允许并行处理，提高了效率。</li></ol><p>总的来说，该稀疏自注意力机制旨在减少计算复杂性，同时保持骨骼数据的关键信息，以提高骨骼动作识别模型的效率和性能。</p></blockquote><blockquote></blockquote><h4 id="Temporal-domain-Segmented-Linear-MHSA机制">Temporal domain: Segmented Linear MHSA机制</h4><blockquote><p><strong>时间域：分段线性MHSA机制</strong></p><p>在先前的方法中存在明显的缺点，包括（1）对于每个视频剪辑使用固定数量的帧和（2）对于不存在第二人员的零填充。第一个缺点限制了它们处理长于预定义长度的视频剪辑以及在较短视频剪辑上的灵活性。第二个缺点是由于零的参与而导致计算延迟降低。此外，在计算过程中分配了大量内存空间给这些零值数据。因此，我们提出了一种紧凑的数据格式来避免这些缺点。此外，我们提出了分段线性MHSA来处理我们的紧凑数据格式。</p><p><strong>可变帧长度数据格式</strong>：图2显示了我们的数据格式与先前工作使用的格式之间的比较。在先前工作采用的数据格式中，较长的视频被截断为预定义的长度，较短的视频则用重复的帧进行填充。此外，单人的帧都被填充为零，以匹配固定数量的人员。图2上的数据格式示例说明了先前工作使用的NTU RGB+D数据格式。在每个固定长度的视频V(i)中，P(i)1和P(i)2分别表示两个人员。在NTU RGB+D 120数据集中，只有120个动作中的26个是相互动作，这意味着大多数数据样本中第二人的骨架只是零（P(i)2=0）。与先前的数据格式不同，所提出的格式保持了每个视频剪辑的原始长度。此外，当一个视频剪辑包含两个人时，我们沿着帧维度将它们连接在一起。我们进一步将视频剪辑在帧维度上连接在一起，辅助向量存储了批次索引，指示帧属于哪个视频剪辑，如图2底部的数据格式所示。此外，鉴于新的维度（N，V，C）如图2所示，其中N是沿时间维度连接视频剪辑后的总帧数，V是骨架关节的数量，我们将维度N视为空间注意的逻辑批次大小，维度V视为时间注意的逻辑批次大小。</p><p><strong>分段线性注意力</strong>：在前一部分介绍的新数据格式的基础上，我们提出了一种专为此数据格式定制的新型线性多头注意力，称之为分段线性注意力。正如前面部分所述，Transformer最初是为顺序数据设计的。在人体骨骼序列中，每个关节在帧之间的动态运动可以视为时间序列。因此，每个关节的3D坐标（即(x，y，z)）都可以沿时间维度的轨迹单独处理，并且注意力的应用可以提取由帧表示的时间步之间的交互作用。</p><p><strong>线性注意力</strong>：标准的点积注意力机制（Vaswani等人，2017年）（方程4）在具有N个输入的全局感受野的情况下由于二次时间和内存复杂性O(N2)而变得非常慢。二次复杂性还使得Transformer难以训练并限制了上下文。最近对线性化注意力机制的研究导出了基于核函数的Softmax注意力的近似。其中最有吸引力的是基于核函数的线性Transformer（Katharopoulos等人，2020年；Choromanski等人，2021年；Shen等人，2021年），这些基于核函数的线性Transformer可以提高推理速度，而不会损失太多的预测性能。给定用于查询、键和值的输入张量的投影嵌入Q、K和V，根据观察到的在位置i的查询Qi的积累值Vi ∈ Rd，其中d是通道维度，线性化注意力可以从方程4转化为方程5，当d远小于N时，计算复杂度减小为O(Nd)，计算复杂度接近于线性O(N)：</p><p>其中φ(·)是核函数。在（Katharopoulos等人，2020年）的工作中，核函数简单地模拟为ELU，φ(x) = elu(x) + 1；而在（Choromanski等人，2021年）中，引入了通过正交随机特征（FAVOR）映射作为核函数，φ(x) = √c Mf(Wx + b)T，其中c &gt; 0是一个常数，W ∈ RM×d是高斯随机特征矩阵，M是控制随机特征数量的矩阵的维度。</p><p><strong>分段线性注意力</strong>：由于我们沿时间维度在一个批次中连接了不同长度的视频剪辑，直接应用线性注意力将导致跨剪辑注意力，从一个视频剪辑到另一个视频剪辑，导致不相关信息被考虑，如图4所示。因此，我们将视频剪辑的帧排列为一个段，然后通过重新制定具有分段索引的方程5来设计分段线性注意力。因此，对于段Sm中的每个Vi，我们总结</p><p>其中Sm是第m个段，减少操作  j∈Sm f(x)可以通过对段的索引进行简单的实现；并且借助于采集和散播操作（Fey 2021a），<strong>分段线性注意力保持高度并行计算</strong>。图3说明了不同注意力操作的比较。</p></blockquote><blockquote><p>下面是关于“Temporal domain: Segmented Linear MHSA机制”的要点总结：</p><p><strong>分段线性MHSA（Multi-Head Self-Attention）机制</strong></p><ol><li><strong>先前方法的问题</strong>：<ul><li>固定帧数限制了处理长视频和短视频的灵活性。</li><li>零填充导致计算延迟和内存浪费。</li></ul></li><li><strong>紧凑的数据格式</strong>：<ul><li>提出了一种新的数据格式，保持了视频剪辑的原始长度。</li><li>对于包含两个人的视频剪辑，将它们沿帧维度连接在一起。</li></ul></li><li><strong>线性多头自注意力</strong>：<ul><li>针对新的数据格式，设计了分段线性多头自注意力机制。</li><li>将关节的3D坐标视为时间序列，允许处理每个关节的动态运动。</li><li>引入了线性化注意力机制，通过核函数来近似Softmax注意力，提高计算效率。</li></ul></li><li><strong>分段线性注意力</strong>：<ul><li>为避免跨视频剪辑的注意力，将视频剪辑的帧排列为段。</li><li>设计了分段线性注意力，以在每个段内计算注意力。</li></ul></li></ol><p>该机制克服了以往方法中的限制，提供了更灵活、高效的处理骨骼序列的方式。</p></blockquote><h3 id="实验">实验</h3><h4 id="实验介绍">实验介绍</h4><blockquote><ol><li>数据集：</li></ol></blockquote><blockquote><ol><li>性和鲁棒性。在不同设置下，LST持续地提高性能。</li></ol></blockquote><h4 id="详细介绍">详细介绍</h4><blockquote><ol><li>数据集</li></ol></blockquote><h3 id="文章总结">文章总结</h3><blockquote><p>方法部分：</p><ol><li>LST：本</li></ol></blockquote><h1>INFO</h1><h3 id="摘要总结-2">摘要总结</h3><blockquote><ul><li></li></ul></blockquote><h3 id="介绍总结-2">介绍总结</h3><blockquote></blockquote><h4 id="增加示例-2">增加示例</h4><blockquote><p><strong>HUMAN 动作识别概述：</strong></p></blockquote><h3 id="相关工作-2">相关工作</h3><blockquote><p>**拓扑构建（</p></blockquote><blockquote><p>增加示例</p></blockquote><blockquote><p>**拓扑构建（Topol</p></blockquote><h3 id="方法-2">方法</h3><h4 id="方法介绍">方法介绍</h4><blockquote><h3 id=""></h3></blockquote><blockquote></blockquote><h4 id="详细介绍-2">详细介绍</h4><blockquote><h3 id="-2"></h3></blockquote><h3 id="实验-2">实验</h3><h4 id="实验介绍-2">实验介绍</h4><blockquote><ol><li>数据集：</li></ol></blockquote><blockquote><ol><li>性和鲁棒性。在不同设置下，LST持续地提高性能。</li></ol></blockquote><h4 id="详细介绍-3">详细介绍</h4><blockquote><ol><li>数据集</li></ol></blockquote><h3 id="文章总结-2">文章总结</h3><blockquote><p>方法部分：</p><ol><li>LST：本</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode20</title>
      <link href="/project/posts/528b8448.html"/>
      <url>/project/posts/528b8448.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode19</title>
      <link href="/project/posts/7a6f2f.html"/>
      <url>/project/posts/7a6f2f.html</url>
      
        <content type="html"><![CDATA[<p>今天两道题也还是挺难的，除非做过，，，所以这种题，还是得做接触，才能想到</p><h2 id="暂时放弃">暂时放弃</h2><p><a href="https://leetcode.cn/problems/1nzheng-shu-zhong-1chu-xian-de-ci-shu-lcof/">剑指 Offer 43. 1～n 整数中 1 出现的次数</a></p><p><a href="https://leetcode.cn/problems/shu-zi-xu-lie-zhong-mou-yi-wei-de-shu-zi-lcof/">剑指 Offer 44. 数字序列中某一位的数字</a></p><p>下面是具体说明</p><h4 id="剑指-Offer-43-1～n-整数中-1-出现的次数">剑指 Offer 43. 1～n 整数中 1 出现的次数</h4><ul><li><p>难度：★★★</p></li><li><p>要求掌握程度：★★★</p></li><li><p>说明：找规律题，一般考的不多，不过可以了解下思路</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/588.html"> https://www.playoffer.cn/588.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2914.html">点击直达本题打卡</a></p><h4 id="剑指-Offer-44-数字序列中某一位的数字">剑指 Offer 44. 数字序列中某一位的数字</h4><ul><li><p>难度：★★★</p></li><li><p>要求掌握程度：★★</p></li><li><p>说明：找规律题，一般考的不多，不过可以了解下思路</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/590.html"> https://www.playoffer.cn/590.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2915.html">点击直达本题打卡</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode18</title>
      <link href="/project/posts/777d5fb9.html"/>
      <url>/project/posts/777d5fb9.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode17</title>
      <link href="/project/posts/e7c24228.html"/>
      <url>/project/posts/e7c24228.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://leetcode.cn/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-lcof/">剑指 Offer 56 – I. 数组中数字出现的次数</a></p><p><a href="https://leetcode.cn/problems/shu-zu-zhong-shu-zi-chu-xian-de-ci-shu-ii-lcof/">剑指 Offer 56 – II. 数组中数字出现的次数 II</a></p><p>下面是具体说明</p><h4 id="剑指-Offer-56-–-I-数组中数字出现的次数">剑指 Offer 56 – I. 数组中数字出现的次数</h4><ul><li><p>难度：★★</p></li><li><p>要求掌握程度：★★★</p></li><li><p>说明：这种题其实没啥技巧，得做过才知道可以用位运算</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/618.html"> https://www.playoffer.cn/618.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2906.html">点击直达本题打卡</a></p><h4 id="剑指-Offer-56-–-II-数组中数字出现的次数-II">剑指 Offer 56 – II. 数组中数字出现的次数 II</h4><ul><li><p>难度：★★</p></li><li><p>要求掌握程度：★★★</p></li><li><p>说明：这种题也一样其实没啥技巧，得做过才知道可以用位运算</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/621.html"> https://www.playoffer.cn/621.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2907.html">点击直达本题打卡</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode16</title>
      <link href="/project/posts/90c572be.html"/>
      <url>/project/posts/90c572be.html</url>
      
        <content type="html"><![CDATA[<h1>位运算专题</h1><p><a href="https://www.playoffer.cn/970.html">一些位运算技巧</a>，包含了位运算在常用的情况下比较简单且能提高速度的5个示例</p><p><img src="../assets/image-20230829222432278.png" alt="image-20230829222432278"></p><h4 id="剑指-Offer-15-二进制中1的个数">剑指 Offer 15. 二进制中1的个数</h4><ul><li><p>难度：★</p></li><li><p>要求掌握程度：★★★★★</p></li><li><p>说明：常规题，要求技术 n &amp; (n – 1) 这个公式的特点。</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/524.html"> https://www.playoffer.cn/524.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2903.html">点击直达本题打卡</a></p><h4 id="剑指-Offer-16-数值的整数次方">剑指 Offer 16. 数值的整数次方</h4><ul><li><p>难度：★★</p></li><li><p>要求掌握程度：★★★★</p></li><li><p>说明：常规题</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/526.html"> https://www.playoffer.cn/526.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2904.html">点击直达本题打卡</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode15</title>
      <link href="/project/posts/9cc2304.html"/>
      <url>/project/posts/9cc2304.html</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://leetcode.cn/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof">剑指 Offer 11. 旋转数组的最小数字</a></li></ul><p>下面是详细说明</p><h4 id="剑指-Offer-11-旋转数组的最小数字">剑指 Offer 11. 旋转数组的最小数字</h4><ul><li><p>难度：★★★★</p></li><li><p>要求掌握程度：★★★★</p></li><li><p>说明：这道题剑指offer标记简单题，其实是一道 hard 题，要用二分查找的思维去做哈，记住二分查找就是<strong>不断缩小范围</strong>哦。</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/514.html">https://www.playoffer.cn/514.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2901.html">点击直达本题打卡</a></p><p><img src="../assets/image-20230829213909003.png" alt="image-20230829213909003"></p><blockquote><p>这个题目比较难理解，需要自己仔细审一下题目</p></blockquote><p>数组长度是n&gt;0,同时能进行至少一次一次数组翻转，还能最多进行n次数组翻转</p><p>数组在翻转的时候，会导致数组内数据变换；</p><p>在最小值边界处，最小值<strong>右边的所有数据</strong> 一定是<strong>小于等于</strong> 最小值左边的任意数</p><blockquote><p>自己在一开始分析题目的时候，把题目想成了在分组边界的地方，也就是最小值的情况，一定是小于数组的首位元素</p><p>也就是只考虑了  3 4 5 1 2 3, 这时候找1 的索引是相当于找小于num[0] 的左边界，但是真实情况下不只是有这一种情况</p></blockquote><ol><li>其他情况： 1 2 3 4 5 6  翻转完成之后 还是 1 2 3 4 5 6  这是极端的翻转情况，最终在翻转完成之后，不发生变化</li><li>3 4 5 1 2 3  这种情况是 在翻转完成之后，等于边界值    还有可能是 3 3 3 4   1 2</li></ol><p>帅地给出的思路是 使用最左边的nums[I]作为target值，借用target与num[mid]不断的切割没用的数据，</p><p>切割mid左边的数据【也就是大于最小值的数据】如果mid 指向数小于num[I] <strong>说明mid可能指向了最小值，或者最小值右边的数据，而l一定是指向的最小值或者最小值的左边才有可能让mid指向的值&lt;num[I],而l指向的一定是在mid的左边，r一定在mid 的右边，</strong>，可以让r= mid将右边界向左边移动继续向左去找最小值</p><p>如果mid指向的值 等于nums[I]的话，==l=l+1== 示例：3 3 5 1 2 3     l 指向0  mid 可能指向1 或者5  这样将l向右移动一位仍然能保证是一个旋转数组，这样向右逼近 能慢慢找到最小值；</p><blockquote><p>理解的话，如果mid 指向的值 等于l指向的数据的话，因为数组一开始是有序且非递减的，那么要么l 和 mid 靠着  要么mid 指向的是被旋转到最右边的地方，【旋转数组的最小值在中间】所以这时候移动最左边的l 向右变大一个位置，仍然不影响旋转数组的最小值在中间，并且能达到缩减数组的作用</p></blockquote><p>如果mid指向的值 大于nums[I]的话，说明当前mid指向的值 左边一定是比他小的，所以mid指向的值一定是比较大的，所以可以砍掉mid指向的值以及1其前边的值，最小值绝对在他的后边</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode14</title>
      <link href="/project/posts/7ecb1392.html"/>
      <url>/project/posts/7ecb1392.html</url>
      
        <content type="html"><![CDATA[<h1>二分查找的变体应用</h1><h2 id="剑指-Offer-53-–-I-在排序数组中查找">剑指 Offer 53 – I. 在排序数组中查找</h2><ul><li><p>难度：★★★</p></li><li><p>要求掌握程度：★★★</p></li><li><p>说明：这个其实就是我们上面四个模版的运用，学过模版的会简单的，没学过的还是有一点小难度。</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/608.html">https://www.playoffer.cn/608.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2898.html">点击直达本题打卡</a></p><blockquote><p>思路出现问题，使用前两个模板出错，只能使用帅地提出的向左或者向右的收缩边界的模板</p></blockquote><h2 id="剑指-Offer-53-–-II-0～n-1中缺失的数字">剑指 Offer 53 – II. 0～n-1中缺失的数字</h2><ul><li><p>难度：★★★</p></li><li><p>要求掌握程度：★★★★</p></li><li><p>说明：这个其实也类型，不过这道题相对会重要一点点，主要就是掌握方法吧。</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/610.html">https://www.playoffer.cn/610.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2899.html">点击直达本题打卡</a></p><blockquote><p>还是不太熟练，只是单纯的学会了写模板，不知道怎样套用在题目上</p><p>这个题目在分析的时候，还是不太会用；应该先划清楚对应的边界值</p></blockquote><p>题目上给出了描述，需要自己设定出对应题目的边界值（这个题需要把数组下标当成target，然后用nums[mid]进行比较，只有两种情况，相等或者不等）</p><p>在正常的范围之内，元素的坐标 是等于 数组中对应的值，而在边界值后边，数组中的值不等于元素的下标，这样就可以转换成对应的二分查找的条件更改</p><blockquote><p>可以划分成异常数据中找左边界， nums[mid]!=mid 的时候，right=mid</p></blockquote><p>同时，还有中特殊情况，就是缺失的元素不构成正常和异常数据的边界，也就是数组中全是正常的数据，没有异常数据【数组中缺失的数据是n-1】需要单独拿出来进行处理一下</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode13</title>
      <link href="/project/posts/e0af8631.html"/>
      <url>/project/posts/e0af8631.html</url>
      
        <content type="html"><![CDATA[<h1>二分查找专题</h1><p>二分查找意味着<strong>数组有序</strong>，所以你们在做题的时候，如果看到<strong>有序</strong>，那么脑子里应该要浮现是否可以采取<strong>二分查找</strong>这样一种方法，二分查找的题很多，不过我们这个专题，重点是掌握<strong>二分查找的一些变种</strong>。</p><blockquote><p><strong>二分查找是找对target的值的索引，这里变体指的是找到对应的边界值或者对应的左右一个</strong></p></blockquote><p>我们常规的二分查找，是寻找某个元素 target 的下标（并且该数组没有重复的元素），不过还有四种变形的二分查找（<strong>有重复元素</strong>）</p><h2 id="二分查找">二分查找</h2><blockquote><p>有序的数组，但是不重复数据</p><p>给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target  ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。</p></blockquote><p>常规的二分查找，我们使用左闭右闭区间[left,right],所以在while循环中使用while(left&lt;=right)进行区间遍历，直至left&gt;right 结束区间所有数据遍历</p><p>我们定义 target 是在一个在左闭右闭的区间里，<strong>也就是[left, right] （这个很重要非常重要）</strong>。</p><p>区间的定义这就决定了二分法的代码应该如何写，<strong>因为定义target在[left, right]区间，所以有如下两点：</strong></p><ul><li>while (left &lt;= right) 要使用 &lt;= ，因为left == right是有意义的，所以使用 &lt;=</li><li>if (nums[middle] &gt; target) right 要赋值为 middle - 1，因为当前这个nums[middle]一定不是target，那么接下来要查找的左区间结束下标位置就是 middle - 1</li></ul><p>例如在数组：1,2,3,4,7,9,10中查找元素2，如图所示：</p><p><img src="../assets/20210311153055723.jpg" alt="704.二分查找"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(nums ==  <span class="literal">null</span> || nums.length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 避免当 target 小于nums[0] nums[nums.length - 1]时多次循环运算</span></span><br><span class="line">        <span class="keyword">if</span> (target &lt; nums[<span class="number">0</span>] || target &gt; nums[nums.length - <span class="number">1</span>]) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, right = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> left + ((right - left) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (nums[mid] == target)</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; target)</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &gt; target)</span><br><span class="line">                right = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>这里使用 left + ((right - left) &gt;&gt; 1) 等价于 (left+ right)/2 只是为了防止left+right超出int范围，所以使用了等价的写法，一定要注意&gt;&gt; 要先用()进行运算</p><p>注意，left=0;right = length-1;</p><p>left&lt;= right</p><p>在nums[mid] == target 的时候进行跳出循环【无重复数据】</p></blockquote><h2 id="四种变体（边界）">四种变体（边界）</h2><p>本次打卡的任务很简单，就是给出四种变形二分查找的模版代码，比如对于一个<strong>有序数组</strong> 且数组中 有重复的元素的时候</p><p>arr = [1，2，<strong>3，3，3</strong>，4，5，5，7]，target = 3，那么</p><h4 id="第一个大于target的元素下标-》向左缩小">第一个大于target的元素下标==》向左缩小</h4><blockquote><p>1.寻找第一个大于 target 的元素的下标，本案例中4 就是第一个大于 target 的元素 ，下标 = 5。<br>对应方法名格式为</p><p>int upper(int[] arr, int target){</p><p>//如果都不存在，则返回-1<br>}</p></blockquote><blockquote><p>这里不同于常规的 二分查找，直接使用= 来进行判断，这里需要使用范围来进行控制，也就是需要对界限来进行判定</p><p><strong>先达成几个共识</strong></p><ol><li><strong>使用left &lt; right</strong>  这里没有像 二分查找那样 使用left &lt;= right  而是直接使用的left &lt; right</li><li>使用左闭右闭区间  【left，right】使得在最后区间中剩余一个元素，最后对这个元素进行特殊条件判定，如果符合条件则就是要找的边界值</li><li>int left = 0, right = nums.length - 1;  设定最左值和最右值，从两边往中间找，最右值为length-1</li><li>这里在寻找边界值的时候，根据对应的情况，向左或者向右移动左右指针直至找到对应的情况</li><li>从 &gt; = &lt; 这个顺序进行判定，逐步分析各种情况</li></ol><p>==这里需要深入理解将边界值向左缩小还是向右缩小，在找同一个值的左边界或者找第一个大于target的值的时候是向左缩小边界，在找同一个值的右边界或者找最后一个小于target的值的时候是向右缩小边界==</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(nums ==  <span class="literal">null</span> || nums.length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 这里先进行特殊的情况判断【nums的最大值 如果小于target则数组中没有比target大的元素，则返回null】</span></span><br><span class="line">        <span class="keyword">if</span> (target &gt; nums[right]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> left + ((right - left) &gt;&gt; <span class="number">1</span>);<span class="comment">//[left mid right]</span></span><br><span class="line">            <span class="keyword">if</span> (nums[mid] &gt; target)</span><br><span class="line">                right = mid;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] = target)</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span>    <span class="comment">// (nums[mid] &lt; target)</span></span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="右边界或等于target-的最后一个-》向右缩小">右边界或等于target 的最后一个==》向右缩小</h4><h5 id="方法1-间接寻找法">方法1 间接寻找法</h5><blockquote><p>==这里找等于target的最后一个    与找第一个大于target的索引 正好相反，因为一个是大于target的第一个是向左缩小，而找最后一个target则是向右缩小==</p></blockquote><blockquote><p>2.如果数组中存在元素等于 target，则返回最后一个等于target 的元素下标，如果不存在，则返回第一个大于 target 的元素下标。本案例中最后一个等于target的下标 = 4。【arr = [1，2，<strong>3，3，3</strong>，4，5，5，7]，target = 3】<br>对应方法名<br>int floor_upper(int[] arr, int target){<br>//如果都不存在，则返回-1<br>}</p></blockquote><blockquote><p>==思路1==找最后一个target的索引 有一种思路可以想成先找到第一个大于target的值的索引【套用上边的代码】，然后判断该索引左边第一个值是不是target，如果是的话则返回该值索引，这个索引也就是最后一个target的索引值  ==存在问题: 对于一些特殊情况，也就是如果列表中不存在大于target的值那么就需要特殊判定一下，不存在大于target的值也就是length-1 就是target，直接判断nums[length-1 ] 是不是target 就可以了==</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(nums ==  <span class="literal">null</span> || nums.length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 这里先进行特殊的情况判断【nums的最大值 如果小于target则数组中没有比target大的元素，则返回null】</span></span><br><span class="line">        <span class="keyword">if</span> (target &gt; nums[right]) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>( nums[right] ==  target)&#123;<span class="comment">// 对于没有大于target的情况做特殊处理</span></span><br><span class="line">            <span class="keyword">return</span> right;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> left + ((right - left) &gt;&gt; <span class="number">1</span>);<span class="comment">//[left mid right]</span></span><br><span class="line">            <span class="keyword">if</span> (nums[mid] &gt; target)</span><br><span class="line">                right = mid;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] == target)</span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span>    <span class="comment">// (nums[mid] &lt; target)</span></span><br><span class="line">                left = mid + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(left - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; nums[left - <span class="number">1</span>] == target)&#123;<span class="comment">// 防止一直向左缩小导致数组越界</span></span><br><span class="line">            <span class="keyword">return</span> left -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//return left;// return left 是返回的第一个大于target的值的索引</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;<span class="comment">// -1表示没找到</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="方法2-直接寻找法">方法2  直接寻找法</h5><p>/**</p><p>帅地给出的答案是直接找到了最后一个target的索引，在遍历的过程中，</p><p>只要是&lt;= target 的索引  都要保存 left = mid  如果是nums[mid]&gt;target的话 right 要向左移</p><p>这样在缩小的范围中一定有一个值，但是这个值如果不是target的话，那就是第一个大于target值的索引</p><p>注意这里是移动的左边界left 需要设置成上取整，即 (right - left + 1) / 2 + left</p><p>*/</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//4.寻找最后一个等于 target 的数下标</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">search4</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(nums == <span class="literal">null</span> || nums.length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>, right = nums.length - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(left &lt; right)&#123;</span><br><span class="line">        <span class="comment">//向上取整</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> (right - left + <span class="number">1</span>) / <span class="number">2</span> + left;</span><br><span class="line">        <span class="keyword">if</span>(nums[mid] &lt;= target)&#123;</span><br><span class="line">            left = mid;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断该数是否存在</span></span><br><span class="line">    <span class="keyword">if</span>(nums[left] != target) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>3.寻找最后一个小于 target 的元素的下标，本案例中 2 则是最后一个小于 target 的，下标 = 1.<br>对应方法名格式为<br>int lower(int[] arr, int target){</p><p>//如果都不存在，则返回-1<br>}</p></blockquote><blockquote><p>4.如果数组中存在元素等于 target，则返回第一个等于target 的下标，如果不存在，则返回最后一个小于 target 的元素的下标。本案例中第一个等于target的下标 = 2。<br>对应方法名格式为<br>int floor_lower(int[] arr, int target)</p><p>//如果都不存在，则返回-1<br>}</p></blockquote><h4 id="4种二分查找模版讲解-跟着帅地玩转校招，拿捏大厂offerPlayOffer"><a href="https://www.playoffer.cn/992.html">4种二分查找模版讲解-跟着帅地玩转校招，拿捏大厂offerPlayOffer</a></h4>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode6</title>
      <link href="/project/posts/eb61cabc.html"/>
      <url>/project/posts/eb61cabc.html</url>
      
        <content type="html"><![CDATA[<h1>专题3：二叉树</h1><h4 id="二叉树面试题目分析">二叉树面试题目分析</h4><blockquote><p>链表考察频率第一，二叉树则可以是<strong>排名第二</strong>，并且考察的问题也不会很难，<strong>easy + medium 考察的比较多</strong>，和链表一样，二叉树多描述也是很简单，<strong>描述简单</strong>是考察高频的一个原因。</p><p>另外一个原因则和链表不同，链表思路简单，考察代码严谨性；而<strong>二叉树，则考察你的抽象能力</strong>，基本大多数二叉树的问题，都得<strong>用递归</strong>来处理，不然就是<strong>用队列或者栈来辅助（队列居多）</strong>，而且好多还要你懂<strong>回溯</strong>的思想，这意味着，你懂二叉树，说明你<strong>至少是对一些算法思想了解的</strong>，否则你就是一脸懵逼。</p></blockquote><h4 id="二叉树掌握要点分析">二叉树掌握要点分析</h4><p>1、<strong>前中后三种遍历</strong>，你得会，怎么着得会<strong>递归的解法</strong>吧？当然，最好也要会<strong>非递归</strong>的方法。</p><p>2、能够熟悉使用递归去解决问题，部分题还得用回溯，所以这里我也给大家准备了<strong>递归</strong> + <strong>回溯</strong> 的入门文章。</p><p><a href="https://www.playoffer.cn/688.html">【递归】关于递归，我总结的解题步骤</a><br><a href="https://www.playoffer.cn/712.html">【回溯】一文学会回溯算法解题步骤</a></p><p>3、掌握前中后遍历的一些关系：二叉树的问题，有时候会让你去重建一颗二叉树，这个时候你就得明白他们的一些关系了。</p><p>会了 1 和 2，你其实可以解决挺多二叉树的问题的，然后二叉树有很多打印的问题，如果你要轻松掌握打印这类问题，那你要去研究<strong>二叉树与队列</strong>的一些关系，可以说，遇到<strong>层序遍历</strong>相关，基本就是和队列相关了。</p><h4 id="递归详解">递归详解</h4><blockquote><p>递归思想</p><ol><li>设定递归方法的目的，也就是要解决的问题的目标设定，用来确定好当前这个函数是为了解决什么问题</li><li></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode5</title>
      <link href="/project/posts/72689b06.html"/>
      <url>/project/posts/72689b06.html</url>
      
        <content type="html"><![CDATA[<h1>栈的变形题目</h1><p>今天这两道题，面试考的特别多，特别是最小栈的设计，我当时面试 shopee 时还遇到过变形题，今天的题如下</p><ul><li><p>今天这两道题，面试考的特别多，特别是最小栈的设计，我当时面试 shopee 时还遇到过变形题，今天的题如下</p><ul><li><a href="https://leetcode.cn/problems/bao-han-minhan-shu-de-zhan-lcof/">剑指 Offer 30. 包含min函数的栈</a></li><li><a href="https://leetcode.cn/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/">剑指 Offer 31. 栈的压入、弹出序列</a></li><li>变形题：最小栈的最优解（本问题卡打卡区）</li></ul></li></ul><h3 id="剑指-Offer-30-包含min函数的栈">剑指 Offer 30. 包含min函数的栈</h3><ul><li><p>难度：★★</p></li><li><p>要求掌握程度：★★★★★</p></li><li><p>说明：高频题，两种方法的思路都要掌握，一种是时间高，空间低，一种是空间高，时间低</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/552.html">https://www.playoffer.cn/552.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2815.html">点击直达本题打卡</a></p><blockquote><p><a href="https://leetcode.cn/problems/min-stack/solutions/42521/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-38/">155. 最小栈 - 力扣（LeetCode）</a></p><p>这个题解给出了4中方法，前三种是比较常规的方法，但是也不太好实现；第四种比较巧妙一点，==还没有看==</p><p>前三种方法:</p><ol><li><p>第一种使用两个栈操作，用一个辅助栈存放最小值</p></li><li><p>第二种使用一个栈来操作，并将最小值同时压入到栈中，相对于第一种节省了很多的空间，但是也需要在栈里边存放数据</p></li><li><p>第三种使用一个栈来操作，并借用min变量存储最小值，并在栈中存放当前插入元素与最小值的差值，也即x-min，在操作中不断更新min和栈内的差值</p><p>第一种的空间复杂度较高O(n),第二种稍微降低，但是没有达到O(1),第三种直接将复杂度降低到了O(1)</p></li></ol></blockquote><h3 id="剑指-Offer-31-栈的压入、弹出序列">剑指 Offer 31. 栈的压入、弹出序列</h3><ul><li><p>难度：★★★</p></li><li><p>要求掌握程度：★★★★</p></li><li><p>说明：没啥好说，也没啥算法思想。</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/554.html">https://www.playoffer.cn/554.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2816.html">点击直达本题打卡</a></p><blockquote><p><a href="https://leetcode.cn/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/solutions/215152/mian-shi-ti-31-zhan-de-ya-ru-dan-chu-xu-lie-mo-n-2/">剑指 Offer 31. 栈的压入、弹出序列 - 力扣（LeetCode）</a></p><p>这个是动画讲解，进行思路的理清</p><p><a href="https://leetcode.cn/problems/zhan-de-ya-ru-dan-chu-xu-lie-lcof/solutions/2166821/shuang-zhi-zhen-yuan-di-mo-ni-kong-jian-l2zgv/">剑指 Offer 31. 栈的压入、弹出序列 - 力扣（LeetCode）</a></p><p>比官方的空间复杂度要低，并使用双指针法</p></blockquote><h3 id="Code">Code</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">validateStackSequences</span><span class="params">(<span class="type">int</span>[] pushed, <span class="type">int</span>[] popped)</span> &#123;</span><br><span class="line">        <span class="comment">// 这个题目 使用一个栈来先进行模拟入栈，使用入栈的序列进行入栈操作</span></span><br><span class="line">        <span class="comment">// 同时将入栈的元素与出栈序列的第i个值进行比较，如果相等的话，则模拟出栈操作</span></span><br><span class="line">        <span class="comment">// 直至出栈操作结束或者入栈序列为空时候，进行判定，如果两个栈都为空的话，</span></span><br><span class="line">        <span class="comment">// 说明出栈序列是对的，否则是错误的出栈序列</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(pushed==<span class="literal">null</span>|| popped.length)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Stack&lt;Integer&gt; stack = <span class="keyword">new</span> <span class="title class_">Stack</span>&lt;&gt;();</span><br><span class="line">        <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span><span class="number">0</span>; i&lt;pushed.length; i++)&#123;</span><br><span class="line">            stack.push(pushed[i]);<span class="comment">//将数据压入到栈中</span></span><br><span class="line">            <span class="keyword">while</span>(!stack.isEmpty()&amp;&amp;stack.peek()==popped[j])&#123;</span><br><span class="line">                stack.pop();</span><br><span class="line">                j++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> stack.isEmpty();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>在使用for 循环和 while循环的时候，先逐个遍历入栈序列，在入栈的同时，用while循环比较出栈值与栈顶元素，若相等则模拟出栈操作，直至两个序列遍历完毕，最后验证栈为空</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!stack.isEmpty()&amp;&amp;stack.peek()==popped[j]) </span><br><span class="line">    <span class="comment">// 这个while  序列需要注意，使用的是短路&amp;&amp;，因为要先保证调用栈的peek()函数的时候，栈不能为空，否则会报错，所以要先保证栈内有数据，然后在进行比较</span></span><br><span class="line">    <span class="comment">// 栈为空的时候 可能第一个插入的数据是2 然后在出栈序列里边第一个序列也是2  然后就把2 弹出了，然后在while循环的时候，栈空了，就会报错，所以要判读</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 另外是比较栈顶的元素 而不是只比较 新插入的元素pushed[i] 自己一开始编码的时候就弄错了 如果这样写的话会导致 栈内之前插入的数据被忽略</span></span><br></pre></td></tr></table></figure><h3 id="变形题：最小栈的最优解">变形题：最小栈的最优解</h3><p>这道题和第一道一样，不过是一道变形题，当时面试 shopee 时被问过，还 有点难度，大概描述如下</p><blockquote><p>实现一个这样的栈，这个栈除了可以进行普通的push、pop操作以外，还可以进行getMin的操作，getMin方法被调用后，会返回当前栈的最小值。栈里面存放的都是 int 整数，并且数值的范围是 [-100000, 100000]。要求所有操作的时间复杂度是 O(1)。<br><strong>附加：如果空间复杂度也能O(1)的话可加分</strong></p></blockquote><p>大家可以想一想，如果要求空间和时间都是O(1)，应该怎么做。具体看打卡区。<br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2817.html">点击直达本题打卡</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode4</title>
      <link href="/project/posts/56fab90.html"/>
      <url>/project/posts/56fab90.html</url>
      
        <content type="html"><![CDATA[<h1>栈和队列基本数据结构的实现</h1><blockquote><p>面试的时候，可能会让使用栈去实现队列，或者用队列实现栈，栈是先进后出，而队列是后进先出，即相似又相反的数据结构</p></blockquote><h2 id="剑指-Offer-09-用两个栈实现队列">剑指 Offer 09. 用两个栈实现队列</h2><ul><li><p>难度：★★</p></li><li><p>要求掌握程度：★★★★</p></li><li><p>说明：基本功，没啥好说，</p></li></ul><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/508.html">https://www.playoffer.cn/508.html</a><br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2798.html">点击直达本题打卡</a></p><blockquote><p>解题方法</p></blockquote><blockquote><p>使用两个栈去实现一个队列，一个栈用于存放新插入队列的数据，一个栈用于出队列的数据<br>当插入队列的时候，将数据存放到入栈中，出队列时候，先从出栈中取出元素，如果出栈为空，<br>则将入队列中的数据全部放入到出栈并输出栈顶元素</p></blockquote><h2 id="LeetCode-225-用队列实现栈">LeetCode 225. 用队列实现栈</h2><ul><li>难度：★★</li><li>要求掌握程度：★★★★</li><li>说明：基本功，没啥好说，</li><li>题解和描述：由于本题不是剑指offer里的，所以没有讲解，大家可以Leetcode评论区找答案哦（<a href="https://leetcode.cn/problems/implement-stack-using-queues/">原题直达</a>）</li></ul><p>打卡链接看这里：<a href="https://www.playoffer.cn/question/2799.html">点击直达本题打卡</a></p><blockquote><p>使用队列实现栈的时候，比较复杂的地方在于需要将新插入的数据变成栈的头部，在获取栈top或者出栈的时候，是队列最尾部的数据，所以可以在push 或者pop的时候做特殊处理，将最后一个数据变成队列头部数据方便出栈操作</p></blockquote><blockquote><blockquote><p><a href="https://leetcode.cn/problems/implement-stack-using-queues/solutions/432204/yong-dui-lie-shi-xian-zhan-by-leetcode-solution/">225. 用队列实现栈 - 力扣（LeetCode）</a></p><p>这个题解写了两种方法，第一种是使用两个队列模拟栈的实现函数，在push的时候，直接先用辅助队列进行存储数据并将主队列数据插入到辅助队列中，直接将新插入的数据变成队列头部，然后将主队列和辅助队列功能互换实现新插入数据头部化</p><p>第二种方法是使用一个队列实现栈，本质是一样的，在插入数据后，将前边的数据先出对列，在入队列间接的将新插入的队列变成头部数据</p><p><a href="https://programmercarl.com/0225.%E7%94%A8%E9%98%9F%E5%88%97%E5%AE%9E%E7%8E%B0%E6%A0%88.html#%E5%85%B6%E4%BB%96%E8%AF%AD%E8%A8%80%E7%89%88%E6%9C%AC">代码随想录 (programmercarl.com)</a></p><p>卡尔哥提出的思想是使用辅助队列在出栈的时候，将最后一个入队列的元素前边的元素先放入到辅助队列中暂存，辅助队列只是起一个暂存的作用</p></blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode3</title>
      <link href="/project/posts/9b0b3e33.html"/>
      <url>/project/posts/9b0b3e33.html</url>
      
        <content type="html"><![CDATA[<h1>day3 暂时跳过【待做】</h1><blockquote></blockquote><p>关于剑指offer的题，其实第一天和第二天我们已经把链表相关的题做完了，但是在面试的时候，因为有些题大家都准备过，所以有时候面试官会给出一些变形题，而且这些变形题还考过好几次，所以 Day3 主要给大家看看变形题，让大家以后遇到有思路。</p><p>至于答案，这里没有提供，不过网上很多，大家可以去搜索一下，或者参考已经打卡了的同学。</p><ul><li>字节真题：单链表相加</li><li>百度真题：环形链表分成三等份。</li></ul><p>下面是详细说明：</p><h1>字节真题：单链表相加</h1><ul><li>难度：★★★</li><li>要求掌握程度：★★★★</li></ul><p>这道题还考过挺多次的，你们直接看图片的描述吧</p><p><img src="../assets/image-20221024122407732.png" alt="image-20221024122407732"></p><p>注意，不允许使用栈，队列等数据结构哦。</p><p>==难点就是还得进位，而且链表只能前进不能后退。==</p><p>==思路：可以先他们进行反转，相加之后，得到结果，再把结果进行反转==</p><p><strong>打卡</strong>：请在打卡区给出你的代码，核心模式做就行，打卡区又给方法。<br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2793.html">点击直达本题打卡</a></p><blockquote><p>思路：先反转两个链表，然后遍历两个链表，每次得到进位和一个当前位的值，使用当前位的值构造一个新节点并插入到插入到链表的头节点处，直到两个链表都遍历完并且进位为0<br>时间复杂度：O(N)，空间复杂度：O(N)</p></blockquote><p>有同学希望我找个代码给大家参考，我从打卡区里选了 @猫猫真可爱 同学的代码给大家参考哦，并且做了一些注释。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意，关于链表节点的定义（ListNode），我这里没有给出来哦</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SolutionAdd</span> &#123;</span><br><span class="line">    <span class="comment">// 反转链表</span></span><br><span class="line">    <span class="keyword">public</span> ListNode  <span class="title function_">reverseList</span><span class="params">(ListNode node)</span>&#123;</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">pre</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        ListNode q= <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="literal">null</span>) &#123;</span><br><span class="line">            q= node.next;</span><br><span class="line">            node.next = pre;</span><br><span class="line">            pre = node;</span><br><span class="line">            node = q;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> pre;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 单链表相加</span></span><br><span class="line">    <span class="keyword">public</span> ListNode  <span class="title function_">addTwoNumbers</span><span class="params">(ListNode head1, ListNode head2)</span> &#123;</span><br><span class="line">        <span class="comment">// 虚拟头节点</span></span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">listNode</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">p</span> <span class="operator">=</span> listNode;</span><br><span class="line">        <span class="comment">// sum 用来存放进位</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (head1 != <span class="literal">null</span> || head2 != <span class="literal">null</span> || sum!=<span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (head1 != <span class="literal">null</span>) &#123;</span><br><span class="line">                sum += head1.val;</span><br><span class="line">                head1 = head1.next;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (head2 != <span class="literal">null</span>) &#123;</span><br><span class="line">                sum += head2.val;</span><br><span class="line">                head2 = head2.next;</span><br><span class="line">            &#125;</span><br><span class="line">            p.next = <span class="keyword">new</span> <span class="title class_">ListNode</span>(sum % <span class="number">10</span>);</span><br><span class="line">            sum = sum / <span class="number">10</span>;</span><br><span class="line">            p = p.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> listNode.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>时间复杂度：O(n+m)<br>空间复杂度：O(1)</p></blockquote><p>时间复杂度： O（M+N）， O（1）<br>核心： 维护一个sum 跟 carry 分别代表每次两数相加的和跟要进位的数， 按照正常数学算法的逻辑去写， 就挺容易， 最后答案要反转才会是正确的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt; iostream &gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt; vector &gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ListNode</span> &#123;</span><br><span class="line">     <span class="type">int</span> val;</span><br><span class="line">     ListNode *next;</span><br><span class="line">     <span class="built_in">ListNode</span>(<span class="type">int</span> x) : <span class="built_in">val</span>(x), <span class="built_in">next</span>(<span class="literal">NULL</span>) &#123;&#125;</span><br><span class="line"> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode * <span class="title">reverse</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    ListNode* prev = <span class="literal">NULL</span>;</span><br><span class="line">    ListNode* curr = head;</span><br><span class="line">    <span class="keyword">while</span> (curr != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        ListNode* next = curr-&gt;next;</span><br><span class="line">        curr-&gt;next = prev;</span><br><span class="line">        prev = curr;</span><br><span class="line">        curr = next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> prev;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ListNode* <span class="title">addTwoNumbers</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">    ListNode* head = <span class="literal">NULL</span>;</span><br><span class="line">    ListNode* tail = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">int</span> carry = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (l1 != <span class="literal">NULL</span> || l2 != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="type">int</span> sum = carry; <span class="comment">// this store the carry from the previous iteration</span></span><br><span class="line">        <span class="keyword">if</span> (l1 != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            sum += l1-&gt;val;</span><br><span class="line">            l1 = l1-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (l2 != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            sum += l2-&gt;val;</span><br><span class="line">            l2 = l2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        carry = sum / <span class="number">10</span>;</span><br><span class="line">        ListNode* node = <span class="keyword">new</span> <span class="built_in">ListNode</span>(sum % <span class="number">10</span>);</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            head = node;</span><br><span class="line">            tail = node;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tail-&gt;next = node;</span><br><span class="line">            tail = node;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (carry &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        ListNode* node = <span class="keyword">new</span> <span class="built_in">ListNode</span>(carry);</span><br><span class="line">        tail-&gt;next = node;</span><br><span class="line">    &#125;</span><br><span class="line">    head = <span class="built_in">reverse</span>(head);</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">    ListNode* l1 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">7</span>);</span><br><span class="line">    ListNode* l2 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">8</span>);</span><br><span class="line">    ListNode* l3 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">9</span>);</span><br><span class="line">    l1-&gt;next = l2;</span><br><span class="line">    l2-&gt;next = l3;</span><br><span class="line"></span><br><span class="line">    ListNode* l4 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">3</span>);</span><br><span class="line">    ListNode* l5 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">2</span>);</span><br><span class="line">    ListNode* l6 = <span class="keyword">new</span> <span class="built_in">ListNode</span>(<span class="number">1</span>);</span><br><span class="line">    l4-&gt;next = l5;</span><br><span class="line">    l5-&gt;next = l6;</span><br><span class="line"></span><br><span class="line">    ListNode* result = <span class="built_in">addTwoNumbers</span>(l1, l4);</span><br><span class="line">    <span class="keyword">while</span> (result != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        std::cout &lt;&lt; result-&gt;val &lt;&lt; std::endl;</span><br><span class="line">        result = result-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1>百度真题：环形链表分成三等份。</h1><ul><li>难度：★★★</li><li>要求掌握程度：★★</li></ul><p>题目：给一个环形链表，请你将他三等分。</p><p>面试分析：对于这种题，一定要和面试官讨论一些条件，比如这个链表的节点个数是否为 3 的倍数？如果不是 3 的倍数，是否是让前面一部分的链表的节点个数多一些？</p><p>==反转就是，大家看到一道面试题，切完不能噼里啪啦做，而是要讨论好边界条件，再去做，否则你就要自己判断所有边界，麻烦的很。==</p><p>面试官也是很希望你能和他讨论分析的。</p><p><strong>打卡</strong>：请在打卡区给出你的代码，核心模式做就行，打卡区又给方法。</p><p>最后就是，有些人不大懂的，可以先放一放哦，不一定要全部题会做哦，看我写的掌握程度。<br>打卡链接看这里：<a href="https://www.playoffer.cn/question/2794.html">点击直达本题打卡</a></p><blockquote><p>快中慢3个指针，慢指针一次走一步，中指针一次走2步，快指针，一次走3步<br>时间复杂度O(n)；空间复杂度O(1)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode2</title>
      <link href="/project/posts/ec0c0ea5.html"/>
      <url>/project/posts/ec0c0ea5.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>坚持第二天，还是先从链表开始慢慢来，一点点的追</p></blockquote><p><strong>关于栈与递归的选择</strong>：==单链表只能从前往后遍历，不支持从后往前遍历，所以遇到诸如倒数打印，反转之类的题，我们往往会选择栈来辅助，也可以选择用递归==但是从面试官的解法看，比起栈，递归会更好，因为用栈，那就有点太简单的。</p><blockquote><ol><li>链表不要老是惯性思维，从左到右去遍历，也可以试用一下从右往左遍历，或者二维数组从右下角向左上角遍历，都可以，按照具体情况去实现</li><li>使用栈的实现可能代码要简单，但是面试的时候可能会考察递归【但是递归的话可能会对空间复杂度要求高一点】</li></ol></blockquote><p>不过无论选择递归还是栈，空间复杂度一样，就是选择递归时，还需要考虑一个递归太深的话，出现内存溢出的情况。</p><p>今天的题有三道：</p><ul><li><a href="https://leetcode.cn/problems/cong-wei-dao-tou-da-yin-lian-biao-lcof/">剑指 Offer 06. 从尾到头打印链表</a></li><li><a href="https://leetcode.cn/problems/fan-zhuan-lian-biao-lcof/">剑指 Offer 24. 反转链表</a></li><li><a href="https://leetcode.cn/problems/fu-za-lian-biao-de-fu-zhi-lcof/">剑指 Offer 35. 复杂链表的复制</a></li></ul><p>下面是详细说明：</p><h1>剑指 Offer 06. 从尾到头打印链表</h1><ul><li>难度：★</li><li>要求掌握程度：★★★★</li><li><strong>说明</strong>：基础题，不建议用栈，最好可以会递归，不会的话，勉强用栈也可以了。</li><li>描述+答案+代码：<a href="https://www.playoffer.cn/479.html">https://www.playoffer.cn/479.html</a></li><li>打卡链接看这里：<a href="https://www.playoffer.cn/question/2788.html">点击直达本题打卡</a></li></ul><blockquote><p>这里自己实现了4种方法，并且都用测试用例完成了</p></blockquote><p><img src="../assets/image-20230825171259571.png" alt="image-20230825171259571"></p><h3 id="高兴的地方">高兴的地方</h3><blockquote><p>性能提升 【从递归 改成非递归 【使用原地处理 空间复杂度降低】】，能够看到自己写的代码的提升</p></blockquote><h1>剑指 Offer 24. 反转链表</h1><ul><li>难度：★★</li><li>要求掌握程度：★★★★★</li><li><strong>说明</strong>：基础题，递归反转+原地反转，两种方法都要掌握</li><li>描述+答案+代码：<a href="https://www.playoffer.cn/481.html">https://www.playoffer.cn/481.html</a></li><li>打卡链接看这里：<a href="https://www.playoffer.cn/question/2789.html">点击直达本题打卡</a></li></ul><p><img src="../assets/image-20230824164035737.png" alt="image-20230824164035737"></p><h1>剑指 Offer 35. 复杂链表的复制</h1><ul><li>难度：★★★</li><li>要求掌握程度：★★</li><li><strong>说明</strong>：这道题考的比较少，主要了解下思路，很容易写错，主要考察代码能力。</li><li>描述+答案+代码：<a href="https://www.playoffer.cn/483.html">https://www.playoffer.cn/483.html</a></li><li>打卡链接看这里：<a href="https://www.playoffer.cn/question/2790.html">点击直达本题打卡</a></li></ul><p><img src="../assets/image-20230825103353886.png" alt="image-20230825103353886"></p><blockquote><p>详细思路 见这个链接里边的第二种方法</p><p><a href="https://leetcode.cn/problems/copy-list-with-random-pointer/solutions/2361362/138-fu-zhi-dai-sui-ji-zhi-zhen-de-lian-b-6jeo/">138. 复制带随机指针的链表 - 力扣（LeetCode）</a></p></blockquote><blockquote><p>==还有一种方法使用哈希表进行实现 这里没看==</p></blockquote><h1>真实面试找错误</h1><blockquote><p>链表的操作，需要设定自己的模板</p><p><a href="https://www.playoffer.cn/question/2791.html">【真实面试】找错误：面试某大厂手写链表题，找找他错在哪里了？-跟着帅地玩转校招，拿捏大厂offerPlayOffer</a></p><p>在看第二天打卡的第4个题目的时候，大厂面试官给出的是删除倒数第n个节点，自己按照视频里边他的思路走下来发现就是自己前两天刚做的那个题目，他的思路是对的，只不过是把删除节点和找倒数第n个节点结合在一起，自己就没有把这个综合题目给全面细致的写出来，问题如下</p><ol><li>没有考虑到节点为空的情况，也就是head为空</li><li>没有全面的想到n的范围，n可能大于链表长度 也可能等于链表长度（删除头结点），只考虑了n小于链表长度（删除链表内部的数据，也就是跟着视频走了下来，没自己独立思考）</li><li>看到题目没有想到解题目的方法，没有完成自己对于链表题目的知识体系，例如拿到链表题目之后，先设定好空的头结点，用带头结点的链表进行操作；然后思考怎样去实现对应的算法，暴力法，双指针法，递归解决，栈和队列，基本的链表操作等等；特殊的情况例如范围越界，head为空等特殊情况处理</li></ol><p>==改进如下==</p><ol><li><p>设定自己的模板，看了下代码随想录上的内容，有直接对应删除倒数第n个节点的题目    <strong>代码随想录 (<a href="http://programmercarl.com">programmercarl.com</a>)](<a href="https://programmercarl.com/0019.%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E5%80%92%E6%95%B0%E7%AC%ACN%E4%B8%AA%E8%8A%82%E7%82%B9.html#%E7%AE%97%E6%B3%95%E5%85%AC%E5%BC%80%E8%AF%BE">https://programmercarl.com/0019.删除链表的倒数第N个节点.html#算法公开课</a>)</strong>  忘了之前跟着做过了都，也没有自己手敲，所以基本等于没看</p></li><li><p>整理下自己对于链表题目的套路，设定头结点，以防出现没考虑到的情况，【头指针为空，单独处理第一个头结点 】</p></li><li><p>自己对于题目还是不太熟练，只是单纯自己做了一遍，应该更多的思考总结</p></li><li><p>【视频里边那个哥们 就是拿到题目就先自己做起来了，没有考虑到自己应该怎么去想？这个题目是哪些题的变体？怎样对边界值做处理？面试官要怎样讨论边界值等等问题……然后再自己去做】==反转就是，大家看到一道面试题，切完不能噼里啪啦做，而是要讨论好边界条件，再去做，否则你就要自己判断所有边界，麻烦的很。==</p><p>==面试官也是很希望你能和他讨论分析的==</p></li></ol></blockquote><p>具体内容情况打卡区：打卡链接看这里：<a href="https://www.playoffer.cn/question/2791.html">点击直达本题打卡</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode1</title>
      <link href="/project/posts/75055f1f.html"/>
      <url>/project/posts/75055f1f.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>假期第一天 开始刷题  一定要坚持下去</p><p><strong>既然是假期，就好好休息了，今天是8.22号，回到家里大外甥在家，也真的累了，不想干了。</strong></p><p><strong>先把前边9天的补上，慢慢跟上进度，坚持打卡</strong></p></blockquote><h3 id="链表专题">链表专题</h3><p>我们先来说一说学习链表时，我们最需要掌握的是什么？</p><p>首先我认为大部分的链表题，解法思路都相对简单，<strong>双指针</strong>占一半以上，但是上了战场，写链表题的时候，非常容易出错，常见错误是：</p><p>1、没有考虑节点为 null 导致空指针异常。</p><p>2、容易出现节点位置定位出错，比如往前多走了一步，或者少走了一步。</p><p>所以我觉得，做链表题，最重要的就是要掌握：</p><p>1、规定好自己的模版，比如我习惯自己弄一个辅助头节点，确定下来之后，就一直这么做，不要变来变去。</p><p>2、做的时候，多考虑一步，就是在用 .next 节点的时候，会不会出现异常。</p><p>3、做链表的题还有非常重要的一个点，那就是：<strong>积累解题思路</strong>，多看几道不同类型的题，不然遇到完全陌生的题，那你可能就真的不会的，因为链表的暴力解法都不难</p><blockquote><p>在面试中，题型不多，<strong>出现最多的还是删除倒数第 K 个节点，反转，交点这一些，思路不难</strong>，重在考察<strong>代码能力</strong>。</p><p>所以我觉得，<strong>学习链表最大的技巧就是多做几遍</strong>。</p></blockquote><h4 id="双指针技巧">双指针技巧</h4><blockquote><p>使用双指针技巧可以解决很多经典问题，这里双指针不仅仅是一个方法，而是一个思路</p><ol><li>快慢指针【判断链表中有环】</li><li>先后指针【快速且只用一次遍历找到链表中最中间的节点、找到链表中倒数第N 个节点】</li></ol></blockquote><blockquote><p>掌握了双指针，意味着掌握了大部分常规链表题，当然，双指针不一定就是快慢指针哦，另外就是，双指针另外一个应用的比较多的领域就是：<strong>在排序数组在求和</strong>，关于这个，后续应该也会有题讲到。</p></blockquote><h4 id="题目1">题目1</h4><blockquote><h4 id="剑指-Offer-18-删除链表的节点https-www-playoffer-cn-440-html">剑指 Offer 18. 删除链表的节点https://www.playoffer.cn/440.html</h4><p>难度：★</p><p>要求掌握程度：★★★★★</p><p>说明：基础题，0 技巧，属于基本功了，自己先试着写一写，如果不会的 ，建议多画个图，多写几遍。</p><p>注意点：删除头节点 + 空节点</p></blockquote><blockquote><p>找了两个标准的答案，一个答案的思路是先判定不为</p><p>在改</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode <span class="title function_">deleteNode</span><span class="params">(ListNode head, <span class="type">int</span> val)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(head == <span class="literal">null</span>||head.next == <span class="literal">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;<span class="comment">// 这里对头结点为空 或者链表中只有一个节点删除完之后为空链表做了排除</span></span><br><span class="line">        <span class="keyword">if</span>(head.val == val) <span class="keyword">return</span> head.next;<span class="comment">// 设定头结点为要删除的节点</span></span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">temp</span> <span class="operator">=</span> head;<span class="comment">// 从头结点开始</span></span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">i</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">while</span>(temp.val != val)&#123;<span class="comment">// 从head 开始往后找 找到值为val的节点为止</span></span><br><span class="line">            i = temp;</span><br><span class="line">            temp = temp.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 进行数据删除</span></span><br><span class="line">        i.next = temp.next;</span><br><span class="line">        temp.next = <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>另一个答案是对于链表中有重复节点的情况</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//时间复杂度O(n)</span></span><br><span class="line"><span class="comment">//空间复杂度O(1)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span> ListNode <span class="title function_">deleteNode</span><span class="params">(ListNode head, <span class="type">int</span> val)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(head == <span class="literal">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(head.val == val)&#123;</span><br><span class="line">        <span class="keyword">return</span> head.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 上边排除掉了头结点为空 和要删除元素是头结点的情况</span></span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">temp</span> <span class="operator">=</span> head;<span class="comment">// 暂存头结点 使用head操作</span></span><br><span class="line">    <span class="keyword">while</span>(head.next != <span class="literal">null</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(head.next.val == val)&#123;</span><br><span class="line">            head.next = head.next.next;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        head = head.next;<span class="comment">// 这里对链表中存在重复元素做了处理，如果存在同样的值，则继续往下遍历，直至链表遍历完成</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> temp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>这是第一题打卡<br>总结下踩的坑</p><ol><li>好久没写java，自己写测试函数，都忘记写类的static修饰</li><li>用while循环判定的时候，一开始想把找val值和判定为空写在一块，就判定node节点是否为空一直写不出来，后边换了个环境，脑子清晰了之后就写出来了</li><li>使用java写判定的时候，因为对head节点单独处理，这里直接用了head.equals去判定，而没用val判定，自己测试的时候导致删除头结点失败，后边debug的时候找到了问题所在</li></ol></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Offer18</span>删除链表的节点 &#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> * 下边两段注释的代码因为没用static静态修饰，在静态的main主函数中调用会非法</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Lenovo</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="comment">//public class ListNode &#123;</span></span><br><span class="line">   <span class="comment">//      int val;</span></span><br><span class="line">   <span class="comment">//      ListNode next;</span></span><br><span class="line">   <span class="comment">//      ListNode(int x) &#123; </span></span><br><span class="line">   <span class="comment">//      val = x; </span></span><br><span class="line">   <span class="comment">//  &#125;</span></span><br><span class="line">   <span class="comment">//  &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//public  ListNode deleteNode(ListNode head, int val) &#123;</span></span><br><span class="line"><span class="comment">//        if(head==null)&#123;</span></span><br><span class="line"><span class="comment">//            return head;</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//if(head.val==val) &#123;</span></span><br><span class="line"><span class="comment">//head=head.next;</span></span><br><span class="line"><span class="comment">//            return head;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//        // 下边就处理不是空链表和val不是头结点的情况</span></span><br><span class="line"><span class="comment">//        // 删除指定值的节点 先用first指针找到待删除的前一个节点</span></span><br><span class="line"><span class="comment">//        // 再将前一个指针指向待删除节点的后一个节点即可</span></span><br><span class="line"><span class="comment">//ListNode first=head;</span></span><br><span class="line"><span class="comment">//ListNode node= first.next;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//while (node!=null) &#123;//这里while条件里边只需要判定循环结束，不参与找值的过程</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//if (node.val==val) &#123;//在if中进行找值</span></span><br><span class="line"><span class="comment">//first.next=node.next;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//first=node;</span></span><br><span class="line"><span class="comment">//node=node.next;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//if (node==null) &#123;</span></span><br><span class="line"><span class="comment">//System.out.println(&quot;不存在要删除的节点值val&quot;);</span></span><br><span class="line"><span class="comment">//System.out.println(&quot;下边是未经处理的链表值&quot;);</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//return head;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">ListNode</span> &#123;</span><br><span class="line"><span class="type">int</span> val;<span class="comment">//存储元素</span></span><br><span class="line">ListNode next;</span><br><span class="line"><span class="keyword">public</span> <span class="title function_">ListNode</span><span class="params">(<span class="type">int</span> x)</span>&#123;</span><br><span class="line">val = x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line"><span class="type">StringBuilder</span> <span class="variable">stringBuilder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;value:&quot;</span>+val;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//public static ListNode deleteNode(ListNode head, int val) &#123;</span></span><br><span class="line"><span class="comment">//if(head.equals(val)) &#123;</span></span><br><span class="line"><span class="comment">//head=head.next;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//ListNode preListNode=head;   </span></span><br><span class="line"><span class="comment">//ListNode next= node.next;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//while (!node.equals(null)&amp;&amp;!next.equals(null)) &#123;</span></span><br><span class="line"><span class="comment">//if (next.val==val) &#123;</span></span><br><span class="line"><span class="comment">//node.next=next.next;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//node=next;</span></span><br><span class="line"><span class="comment">//next=next.next;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//return head;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> ListNode <span class="title function_">deleteNode</span><span class="params">(ListNode head, <span class="type">int</span> val)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span>(head==<span class="literal">null</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">if</span>(head.val==val) &#123;</span><br><span class="line">head=head.next;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="comment">// 下边就处理不是空链表和val不是头结点的情况</span></span><br><span class="line">    <span class="comment">// 删除指定值的节点 先用first指针找到待删除的前一个节点</span></span><br><span class="line">    <span class="comment">// 再将前一个指针指向待删除节点的后一个节点即可</span></span><br><span class="line">ListNode first=head;</span><br><span class="line">ListNode node= first.next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (node!=<span class="literal">null</span>) &#123;<span class="comment">//这里while条件里边只需要判定循环结束，不参与找值的过程</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (node.val==val) &#123;<span class="comment">//在if中进行找值</span></span><br><span class="line">first.next=node.next;</span><br><span class="line">&#125;</span><br><span class="line">first=node;</span><br><span class="line">node=node.next;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (node==<span class="literal">null</span>) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;不存在要删除的节点值val&quot;</span>);</span><br><span class="line">System.out.println(<span class="string">&quot;下边是未经处理的链表值&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//这里测试用例 先对list列表赋值一个,再用循环进行插入,不然下边不方便操作</span></span><br><span class="line"><span class="type">ListNode</span> <span class="variable">list</span>  <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">0</span>);</span><br><span class="line"><span class="comment">//ListNode list= null;</span></span><br><span class="line">ListNode temp= list;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">temp.next=<span class="keyword">new</span> <span class="title class_">ListNode</span>(i);</span><br><span class="line">temp=temp.next;</span><br><span class="line">&#125;</span><br><span class="line">ListNode temp1=deleteNode(list,<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (temp1!=<span class="literal">null</span>) &#123;</span><br><span class="line">System.out.println(temp1);</span><br><span class="line">temp1=temp1.next;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="题目2">题目2</h4><blockquote><h4 id="剑指-Offer-22-链表中倒数第k个节点">剑指 Offer 22. 链表中倒数第k个节点</h4><p>难度：★</p><p>要求掌握程度：★★★★★</p><p>说明：快慢双指针的运用。</p><p>注意点：注意空指针异常</p><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/442.html">https://www.playoffer.cn/442.html</a></p></blockquote><blockquote><p>这里自己写的代码 和 标准答案思路有点不一样，自己是按照两个相差 k-1 来计算的，然后在判定的时候，判定的是前指针的next为空 则结束遍历（也就是遍历到了链表的最后一个值）</p></blockquote><blockquote><p>答案上的思路是前指针和后指针相差k个单位，然后当前指针和后指针同时向前走前指针指向空的时候，则为最后一个值，停止遍历</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode <span class="title function_">getKthFromEnd</span><span class="params">(ListNode head, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">        <span class="comment">//一个慢指针在头结点</span></span><br><span class="line">        ListNode slow=head;</span><br><span class="line">        <span class="comment">//快指针在头结点后k-1个</span></span><br><span class="line">        ListNode fast=head;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;k-<span class="number">1</span>;i++)&#123;</span><br><span class="line">            fast=fast.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//到快指针到尾节点，慢指针在尾节点前k-1个，也就是倒数第k个</span></span><br><span class="line">        <span class="keyword">while</span>(fast.next!=<span class="literal">null</span>)&#123;</span><br><span class="line">            slow=slow.next;</span><br><span class="line">            fast=fast.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> slow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="题目3">题目3</h4><blockquote><h4 id="剑指-Offer-25-合并两个排序的链表">剑指 Offer 25. 合并两个排序的链表</h4><p>难度：★★</p><p>要求掌握程度：★★★★★</p><p>说明：本题类似于递归排序算法的 merge 操作，也算是双指针。</p><p>注意点：无</p><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/444.html">https://www.playoffer.cn/444.html</a></p></blockquote><blockquote><p>8.22 状态很好，一口气写出了一个题目 wonderful</p></blockquote><p><img src="../assets/image-20230822210327100.png" alt="image-20230822210327100"></p><blockquote><p>帅地的简洁代码</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode <span class="title function_">mergeTwoLists</span><span class="params">(ListNode l1, ListNode l2)</span> &#123;</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">merge</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">temp</span> <span class="operator">=</span> merge;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(l1 != <span class="literal">null</span> &amp;&amp; l2 != <span class="literal">null</span>)&#123;</span><br><span class="line">            <span class="keyword">if</span>(l1.val &lt; l2.val)&#123;</span><br><span class="line"></span><br><span class="line">                temp.next = l1;</span><br><span class="line">                l1 = l1.next;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                temp.next = l2;</span><br><span class="line">                l2 = l2.next;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            temp = temp.next;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        temp.next = l1 == <span class="literal">null</span> ? l2 : l1;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> merge.next;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="题目4">题目4</h4><blockquote><h4 id="剑指-Offer-52-两个链表的第一个公共节点">剑指 Offer 52. 两个链表的第一个公共节点</h4><p>难度：★★</p><p>要求掌握程度：★★★★★</p><p>说明：由于本题是不存在环的，所以相对容易，可以直接遍历计算节点数量之差，也可以采用双指针，不过建议理解双指针的方法，代码优雅。</p><p>注意点：无</p><p>问题的描述及其答案+代码实现看这里：<a href="https://www.playoffer.cn/446.html">https://www.playoffer.cn/446.html</a></p></blockquote><h5 id="思路">思路</h5><blockquote><p>使用两个链表 M  N 表示链表的长度，当两个链表有公共交点的时候，M  和 N 分别由各自的单独长度 P Q  和公共长度 W组成，帅地提供的思路是使用两个循环。在循环完成M 之后，在循环一次N,而另一个链表在循环完成N 之后，再循环一次M,两个链表走过的总长度是一样的，都是M+N,并且走的步数也是一样的（也即速度），都是先走一次独有的长度P或Q，然后走一次公共长度W，再走对方的独有长度 Q P  最后在公共长度W处相遇，也即第一个公共节点处</p></blockquote><p><img src="../assets/image-20230823081001189.png" alt="image-20230823081001189"></p><blockquote><p>上边来自labuladong 解题思路</p></blockquote><blockquote><p><a href="https://leetcode.cn/profile/articles/">我的题解 - 力扣 (LeetCode)</a></p><p>具体代码见我的题解</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> day1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class 剑指Offer52两个链表的第一个公共节点 &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ListNode  <span class="title function_">getIntersectionNode</span><span class="params">(ListNode headA, ListNode headB)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (headA == <span class="literal">null</span>||headB == <span class="literal">null</span> ) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">temp1</span> <span class="operator">=</span> headA;</span><br><span class="line">    <span class="type">ListNode</span> <span class="variable">temp2</span> <span class="operator">=</span> headB;</span><br><span class="line">    <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (temp1 != <span class="literal">null</span>) &#123;</span><br><span class="line">i++;</span><br><span class="line">temp1=temp1.next;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (temp2 != <span class="literal">null</span>) &#123;</span><br><span class="line">    j++;</span><br><span class="line">    temp2=temp2.next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i&gt;=j) &#123;</span><br><span class="line">        temp1=headA;</span><br><span class="line">        temp2=headB;</span><br><span class="line">    i=i-j;<span class="comment">// 设定为两个链表的差，并且先移动headA链表i-j个长度 </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i2</span> <span class="operator">=</span> <span class="number">0</span>; i2 &lt; i; i2++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (temp1!=<span class="literal">null</span>) &#123;</span><br><span class="line">        temp1=temp1.next;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">        temp1=headA;</span><br><span class="line">        temp2=headB;</span><br><span class="line">    j=j-i;<span class="comment">// 设定为两个链表的差，并且先移动headA链表i-j个长度 </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j2</span> <span class="operator">=</span> <span class="number">0</span>; j2 &lt; j; j2++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (temp2!=<span class="literal">null</span>) &#123;</span><br><span class="line">        temp2=temp2.next;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">while</span> (temp1!=temp2&amp;&amp;temp1!=<span class="literal">null</span>&amp;&amp;temp2!=<span class="literal">null</span>) &#123;</span><br><span class="line">temp1=temp1.next;</span><br><span class="line">temp2=temp2.next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (temp1==temp2) &#123;</span><br><span class="line"><span class="keyword">return</span> temp1;</span><br><span class="line">&#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"><span class="comment">//这里测试用例 先对list列表赋值一个,再用循环进行插入,不然下边不方便操作</span></span><br><span class="line"><span class="type">ListNode</span> <span class="variable">list1</span>  <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">1</span>);</span><br><span class="line"><span class="type">ListNode</span> <span class="variable">list2</span>  <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">2</span>);</span><br><span class="line">ListNode temp1= list1;</span><br><span class="line">ListNode temp2= list2;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 公共链表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">ListNode</span> <span class="variable">list</span>  <span class="operator">=</span><span class="keyword">new</span> <span class="title class_">ListNode</span>(<span class="number">100</span>);</span><br><span class="line">ListNode temp= list;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= <span class="number">5</span>; i=i+<span class="number">1</span>) &#123;</span><br><span class="line">temp.next=<span class="keyword">new</span> <span class="title class_">ListNode</span>(i);</span><br><span class="line">temp=temp.next;</span><br><span class="line">&#125;<span class="comment">//0 1 2 3 4 5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 下边俩是单独列表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">3</span>; i &lt;= <span class="number">15</span>; i=i+<span class="number">2</span>) &#123;</span><br><span class="line">temp1.next=<span class="keyword">new</span> <span class="title class_">ListNode</span>(i);</span><br><span class="line">temp1=temp1.next;</span><br><span class="line">&#125;<span class="comment">// 1 3 5 7 9</span></span><br><span class="line">temp1.next = list;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">4</span>; i &lt;= <span class="number">10</span>; i=i+<span class="number">2</span>) &#123;</span><br><span class="line">temp2.next=<span class="keyword">new</span> <span class="title class_">ListNode</span>(i);</span><br><span class="line">temp2=temp2.next;</span><br><span class="line">&#125;<span class="comment">// 2 4 6 8 10</span></span><br><span class="line"></span><br><span class="line">temp2.next = list;</span><br><span class="line">temp= list;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (temp!=<span class="literal">null</span>) &#123;</span><br><span class="line">System.out.println(temp);</span><br><span class="line">temp=temp.next;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">&quot;*************&quot;</span>);</span><br><span class="line"></span><br><span class="line">temp1=list1;</span><br><span class="line"><span class="keyword">while</span> (temp1!=<span class="literal">null</span>) &#123;</span><br><span class="line">System.out.println(temp1);</span><br><span class="line">temp1=temp1.next;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">&quot;*************&quot;</span>);</span><br><span class="line">temp2=list2;</span><br><span class="line"><span class="keyword">while</span> (temp2!=<span class="literal">null</span>) &#123;</span><br><span class="line">System.out.println(temp2);</span><br><span class="line">temp2=temp2.next;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">&quot;*************&quot;</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里修改</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">temp1=getIntersectionNode(list1,list2);</span><br><span class="line">System.out.println(temp1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="思路2">思路2</h5><blockquote><p>因为是链表指针相交，所以应该是指针指向相同，而不仅仅是指针中存储的数据值val相同，所以使用哈希表存储法也可以进行判定</p></blockquote><blockquote><p>哈希集合<br>思路和算法</p></blockquote><p>判断两个链表是否相交，可以使用哈希集合存储链表节点。</p><p>首先遍历链表 headA 并将链表 headA 中的每个节点加入哈希集合中。然后遍历链表 headB，对于遍历到的每个节点，判断该节点是否在哈希集合中：</p><p>如果当前节点不在哈希集合中，则继续遍历下一个节点；</p><p>如果当前节点在哈希集合中，则后面的节点都在哈希集合中，即从当前节点开始的所有节点都在两个链表的相交部分，因此在链表 headB中遍历到的第一个在哈希集合中的节点就是两个链表相交的节点，返回该节点。</p><p>如果链表 headB中的所有节点都不在哈希集合中，则两个链表不相交，返回 null</p><p>代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> ListNode <span class="title function_">getIntersectionNode</span><span class="params">(ListNode headA, ListNode headB)</span> &#123;</span><br><span class="line">        Set&lt;ListNode&gt; visited = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;ListNode&gt;();</span><br><span class="line">        <span class="type">ListNode</span> <span class="variable">temp</span> <span class="operator">=</span> headA;</span><br><span class="line">        <span class="keyword">while</span> (temp != <span class="literal">null</span>) &#123;</span><br><span class="line">            visited.add(temp);</span><br><span class="line">            temp = temp.next;</span><br><span class="line">        &#125;</span><br><span class="line">        temp = headB;</span><br><span class="line">        <span class="keyword">while</span> (temp != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (visited.contains(temp)) &#123;</span><br><span class="line">                <span class="keyword">return</span> temp;</span><br><span class="line">            &#125;</span><br><span class="line">            temp = temp.next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>复杂度分析</p><p>时间复杂度：O(m+n)，其中 m和 n 是分别是链表 headA 和 headB 的长度。需要遍历两个链表各一次。</p><p>空间复杂度：O(m)O(m)O(m)，其中 mmm 是链表 headA 的长度。需要使用哈希集合存储链表 headA 中的全部节点。</p><p>作者：力扣官方题解<br>链接：<a href="https://leetcode.cn/problems/intersection-of-two-linked-lists/solutions/1/xiang-jiao-lian-biao-by-leetcode-solutio-a8jn/">https://leetcode.cn/problems/intersection-of-two-linked-lists/solutions/1/xiang-jiao-lian-biao-by-leetcode-solutio-a8jn/</a><br>来源：力扣（LeetCode）<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h5 id="思路3">思路3</h5><p>方法二：双指针<br>思路和算法</p><p>使用双指针的方法，可以将空间复杂度降至 O(1)O(1)O(1)。</p><p>只有当链表 headA 和 headB 都不为空时，两个链表才可能相交。因此首先判断链表 headA 和 headB 是否为空，如果其中至少有一个链表为空，则两个链表一定不相交，返回 null。</p><p>当链表 headA 和 headB 都不为空时，创建两个指针pA 和pB，初始时分别指向两个链表的头节点 headA 和 headB，然后将两个指针依次遍历两个链表的每个节点。具体做法如下：</p><p>每步操作需要同时更新指针pA 和pB。</p><p>如果指针pA 不为空，则将指针pA 移到下一个节点；如果指针pB 不为空，则将指针pB 移到下一个节点。</p><p>如果指针pA 为空，则将指针pA 移到链表 headB 的头节点；如果指针pB 为空，则将指针pB 移到链表 headA 的头节点。</p><p>当指针pA 和pB 指向同一个节点或者都为空时，返回它们指向的节点或者 null。</p><p>证明</p><p>下面提供双指针方法的正确性证明。考虑两种情况，第一种情况是两个链表相交，第二种情况是两个链表不相交。</p><p><img src="../assets/image-20230823093505001.png" alt="image-20230823093505001"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ListNode  <span class="title function_">getIntersectionNode1</span><span class="params">(ListNode headA, ListNode headB)</span> &#123;</span><br><span class="line"><span class="keyword">if</span> (headA==<span class="literal">null</span>||headB==<span class="literal">null</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">ListNode</span> <span class="variable">temp1ListNode</span> <span class="operator">=</span>headA;</span><br><span class="line"><span class="type">ListNode</span> <span class="variable">temp2ListNode</span> <span class="operator">=</span>headB;</span><br><span class="line"><span class="keyword">while</span> (temp1ListNode != temp2ListNode) &#123;<span class="comment">// 两个指针不相等 则继续循环</span></span><br><span class="line"><span class="comment">//这下边两句代码 判定了节点是否为空 如果走到了链表最后 则从另一个链表那里重新开始遍历，走对方的路</span></span><br><span class="line">temp1ListNode = temp1ListNode == <span class="literal">null</span> ? headB : temp1ListNode.next;</span><br><span class="line">temp2ListNode = temp2ListNode == <span class="literal">null</span> ? headA : temp2ListNode.next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//跳出循环的时候，要么 两个指针都指向了NULL 要么两个指针的结果值为同一节点，也就是最早的公共节点</span></span><br><span class="line"><span class="keyword">return</span> temp1ListNode;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>复杂度分析</p><p>时间复杂度：O(m+n)，其中 m 和 n 是分别是链表 headA 和 headB 的长度。两个指针同时遍历两个链表，每个指针遍历两个链表各一次。</p><p>空间复杂度：O(1)。</p><p>作者：力扣官方题解<br>链接：<a href="https://leetcode.cn/problems/intersection-of-two-linked-lists/solutions/1/xiang-jiao-lian-biao-by-leetcode-solutio-a8jn/">https://leetcode.cn/problems/intersection-of-two-linked-lists/solutions/1/xiang-jiao-lian-biao-by-leetcode-solutio-a8jn/</a><br>来源：力扣（LeetCode）<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速秒杀系统</title>
      <link href="/project/posts/5dd274f5.html"/>
      <url>/project/posts/5dd274f5.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>every_bug1</title>
      <link href="/project/posts/9a181eba.html"/>
      <url>/project/posts/9a181eba.html</url>
      
        <content type="html"><![CDATA[<h4 id="1-三目运算符新用法">1. 三目运算符新用法</h4><p>判断两个指针 是否为空</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    <span class="regexp">//</span>上边代码执行完 p1  和 p2 一定有一个是空的</span><br><span class="line">    tempListNode.<span class="keyword">next</span> = p1== null ? p2 : p1;<span class="regexp">//</span> 三目运算符 直接获得值</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="regexp">//</span>    <span class="keyword">if</span> (p1==null) &#123;<span class="regexp">//</span>说明p1 执行遍历完，需要将p2剩余的数据插入到新俩表后边</span><br><span class="line"><span class="regexp">//</span>    tempListNode.<span class="keyword">next</span>=p2;</span><br><span class="line"><span class="regexp">//</span>&#125;<span class="keyword">else</span> &#123;</span><br><span class="line"><span class="regexp">//</span>tempListNode.<span class="keyword">next</span>=p1;</span><br><span class="line"><span class="regexp">//</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="2-torch-Tensor-和-torch-tensor-两种函数，它们的区别是什么">2.torch.Tensor() 和 torch.tensor() 两种函数，它们的区别是什么</h4><p><a href="https://zhuanlan.zhihu.com/p/345648168">pytorch中的.Tensor、.tensor、.from_numpy、.as_tensor区别 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/tfcy694/article/details/85338745">【PyTorch】Tensor和tensor的区别_pytorch tensor tensor_玄云飘风的博客-CSDN博客</a></p><p><img src="../assets/image-20230830084204462.png" alt="image-20230830084204462"></p><p><a href="https://blog.51cto.com/u_14691718/5260031">【PyTorch】tensor和Tensor有什么区别？_51CTO博客_pytorch和tensorflow的区别</a></p><p><a href="https://api.cloud.yisu.com/zixun/724395.html">PyTorch中Tensor和tensor的区别是什么 - 开发技术 - 亿速云 (yisu.com)</a></p><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/">torch.Tensor - PyTorch中文文档 (pytorch-cn.readthedocs.io)</a></p><h4 id="3-怎样看别人的github的提交记录">3.怎样看别人的github的提交记录</h4><p><a href="https://blog.csdn.net/weixin_42139800/article/details/122100628">怎么查看github项目的历史提交版本_github页面如何查看历史版本_道上简哥的博客-CSDN博客</a></p><p><img src="../assets/image-20230912224424132.png" alt="image-20230912224424132"></p><p><img src="../assets/image-20230912224429704.png" alt="image-20230912224429704"></p><p><img src="../assets/image-20230912224441906.png" alt="image-20230912224441906"></p><p><a href="https://blog.csdn.net/mfysss/article/details/130715363">新版Github查看历史提交commit记录_github查看提交记录_没反应说说说的博客-CSDN博客</a></p><p><img src="../assets/image-20230912224623104.png" alt="image-20230912224623104"></p><h4 id="4-怎样查看java二维数组的行数-和-列数">4. 怎样查看java二维数组的行数 和 列数</h4><p>a[m] [n]  行为m 列为n</p><p>a.length  表示的是二维数组中有多少的一维数组 所以表示的是行数</p><p>a[0].length  表示的是二维数组第0 行的长度 所以是列的数量</p><blockquote><p>2023.9.13</p></blockquote><h4 id="5-怎样将pycharm设置成护眼色">5.怎样将pycharm设置成护眼色</h4><p>设置背景  将pycharm设置好背景 绿色即可</p><blockquote><p>详细步骤 <a href="https://blog.csdn.net/sht527/article/details/129621483">设置Pycharm/IDEA背景颜色为护眼色_idea 护眼主题_南华-SuiH<s>T</s>~的博客-CSDN博客</a></p></blockquote><blockquote><p>设置对应行的颜色 (例如选中的行 注释的行颜色等)</p><p><a href="https://blog.csdn.net/Acegem/article/details/102508507">PyCharm基本配置：设置绿色护眼绿豆沙背景等_ptcharm行变绿_Acegem的博客-CSDN博客</a></p></blockquote><h4 id="6-idea-出现激活问题">6.idea 出现激活问题</h4><p>Your idea evaluation has expired. Your session will be limited to 30 minutes. For information on how to upgrade your evaluation software please go to <a href="https://www.jetbrains.com/idea/buy/?fromIDE">https://www.jetbrains.com/idea/buy/?fromIDE</a></p><blockquote><p>[idea提示Your idea evaluation has expired. Your session will be limited to 30 minutes<a href="https://blog.csdn.net/RzhenDwo/article/details/125834111">亲测解决]_csde.L的博客-CSDN博客</a></p><p>自己的idea eval Reset 插件过期了，可以登录进去之后更新一下就可以了</p></blockquote><h4 id="7-vue3-点击取消后出现error错误">7.vue3 点击取消后出现error错误</h4><blockquote><p>界面错误展示</p><p>ERROR cancel at handleError (webpack-internal:///./node_modules/webpack-dev-server/client/overlay.js:299:58) at eval (webpack-internal:///./node_modules/webpack-dev-server/client/overlay.js:322:7)</p><p>在bing上搜索不到结果</p></blockquote><blockquote><p>重新跑起来项目，使用f12 去重新看控制台报错，</p><p>找到报错信息   <strong>Uncaught (in promise) cancel</strong></p><p>一开始只是在前端界面上展示出了错误，搜了下搜不到</p></blockquote><blockquote><p>==但是使用楠哥的项目进行的时候不会报错，自己猜测是因为他项目中进行了手动抛出异常处理throw，但是自己在copy代码的时候没有复制过来吧==</p></blockquote><h5 id="搜索到了解决方案-因为vue的取消按钮按下去之后会抛出异常，需要进行异常处理">搜索到了解决方案 因为vue的取消按钮按下去之后会抛出异常，需要进行异常处理</h5><blockquote><p><a href="https://blog.csdn.net/weixin_43267006/article/details/105440882">Vue.js前台报Uncaught (in promise) cancel错误解决办法_Shuai-Wang的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/Dark_programmer/article/details/116796082">Uncaught (in promise) cancel 报错 及 解决方法-CSDN博客</a></p></blockquote><p>2023.12.25日更新</p><h4 id="8-VS运行时无法定位程序输入点xxx于动态链接库xxx-exe上">8.VS运行时无法定位程序输入点xxx于动态链接库xxx.exe上</h4><blockquote><p>安装一个笔记软件 看尚硅谷2019年的java笔记用的</p></blockquote><p><a href="https://blog.csdn.net/aaalifu/article/details/114436662">VS运行时无法定位程序输入点xxx于动态链接库xxx.exe上_vs无法定位程序输入点于动态链接库-CSDN博客</a></p><p><a href="https://www.cnblogs.com/zlslch/p/8960775.html">notecase的下载与安装（全网最详细）（图文详解） - 大数据和AI躺过的坑 - 博客园 (cnblogs.com)</a></p><p><img src="../assets/image-20231225213103839.png" alt="image-20231225213103839"></p><blockquote><p>最后这个版本下载下来 之后能够打开对应的文件【安装1,.9 版本 需要按照上边的步骤进行添加文件啥的 报错没弄好】</p><p>安装其他的版本 安装后不能打开文件 最后尝试这个版本可以了</p><p><a href="http://www.3h3.com/soft/236632.html">NoteCase破解版|NoteCase免费版 v4.5.7 下载_当游网 (3h3.com)</a></p></blockquote><p>在安装完成之后 老师缺少.dll文件</p><blockquote><p>下边两个比较古老的dll文件库</p></blockquote><p><strong><a href="http://www.dllku.com/">http://www.dllku.com/</a></strong></p><p><strong>或者</strong></p><p><strong><a href="http://www.zhaodll.com/">http://www.zhaodll.com/</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> 成长 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bug总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读1</title>
      <link href="/project/posts/bfa33dd0.html"/>
      <url>/project/posts/bfa33dd0.html</url>
      
        <content type="html"><![CDATA[<h1>Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition</h1><h1>INFO</h1><h3 id="摘要总结">摘要总结</h3><blockquote><p>文章的摘要内容如下：</p><p>摘要—人类如何理解和识别他人的动作是一个复杂的神经科学问题，涉及认知机制和神经网络的组合。研究表明，人类大脑中存在用于识别处理自上而下关注信息的动作的区域，例如颞顶联合区。此外，人类还有专门用于理解他人思想并分析其意图的大脑区域，如颞叶的前额中央皮质。基于骨架的动作识别通过创建人类骨骼运动模式和行为之间复杂连接的映射。尽管现有研究对有意义的节点关系进行编码，并通过良好的分类合成行为表示，但其中很少有研究考虑将先验知识纳入其中，以帮助潜在的表示学习以实现更好的性能。LA-GCN 提出了一种使用大规模语言模型（LLM）知识辅助的图卷积网络。首先，LLM 知识被映射到先验全局关系（GPR）拓扑和节点之间的先验类别关系（CPR）拓扑。GPR 指导新“骨骼”表示的生成，旨在从数据级别强调关键节点信息。CPR 映射模拟人脑区域的类别先验知识，由 PC-AC 模块编码，并用于添加额外的监督，迫使模型学习可区分类别的特征。此外，为了提高拓扑模型中的信息传递效率，我们提出了多跳注意力图卷积。它同时聚合每个节点的 k 阶邻居，以加快模型的收敛速度。LA-GCN 在 NTU RGB+D、NTU RGB+D 120 和 NW-UCLA 数据集上达到了最先进的水平。</p></blockquote><blockquote><ul><li>人类动作理解与识别是复杂的神经科学问题，涉及认知机制和神经网络。</li><li>大脑中存在用于处理自上而下关注信息的动作的区域，例如颞顶联合区。</li><li>大脑还有专门用于理解他人思想和意图的区域，如颞叶的前额中央皮质。</li><li>骨架动作识别通过映射骨骼运动模式与行为之间的复杂连接来实现。</li><li>现有研究中很少考虑将先验知识纳入表示学习，以提高性能。</li><li>LA-GCN使用大规模语言模型（LLM）知识辅助，提出图卷积网络。</li><li>LLM知识被映射到先验全局关系（GPR）和先验类别关系（CPR）拓扑。</li><li>GPR指导新“骨骼”表示的生成，强调数据级别的关键节点信息。</li><li>CPR映射模拟人脑中的类别先验知识，添加额外的监督。</li><li>多跳注意力图卷积提高了拓扑建模的信息传递效率。</li><li>LA-GCN在NTU RGB+D、NTU RGB+D 120和NW-UCLA数据集上取得最先进的性能。</li></ul></blockquote><h3 id="介绍总结">介绍总结</h3><blockquote><p><strong>HUMAN 动作识别概述：</strong> 人类动作识别作为视频理解的核心任务之一，是一种从第三人称的视角观察和推断主体行为的分类任务。该任务在人机交互、视频监控、医疗保健和短视频娱乐等领域具有广泛应用。其中，基于骨骼的动作识别因其对视频中各种环境噪声的稳健性以及高度紧凑的特点而广泛应用。基于图卷积网络（GCN）的方法在这一领域取得了革命性的进展。</p><p><strong>人类肌肉骨骼系统：</strong> 人类肌肉骨骼系统允许身体部位移动，从而执行不同的动作。基于骨骼的数据模态符合人体解剖结构，使得GCN的学习更具可解释性。它仅包含人体主要关节的二维或三维坐标，使模型能够通过推理骨骼序列来识别人类运动。此外，骨骼模态比其他模态更加注重隐私保护。</p><p><strong>LA-GCN 概述：</strong> 本文引入了一种语言模型知识辅助的图卷积网络（LA-GCN）来增强基于骨骼的动作识别。受当前认知神经科学研究的启发，并受益于大规模语言模型（LLM）的发展，LA-GCN利用大规模文本知识库来模拟人脑区域，帮助GCN网络进行行为预测。</p><p><strong>LA-GCN 方法概述：</strong> LA-GCN由两部分组成，分别是节点之间的全局先验关系拓扑（GPR图）和节点之间的类别先验关系拓扑（CPR图）。GPR图用于引导生成新的骨骼数据，以建模来自数据层的关键信息，并将其作为输入传递给“神经元簇”GCN进行特征聚合。CPR图用于构建一个模拟人类思维和推理的先验一致性辅助分类（PC-AC）模块，以帮助模型基于增强语义关系的特征进行学习。该方法的框架在“打电话”动作的底部显示。</p><p><strong>GPR-Graph 概述：</strong> LA-GCN中的GPR-Graph包含了相对完整的节点之间的有意义的关系，即使两个节点在空间上相距较远。我们使用LLM的文本编码器提取每个关节的节点特征，并通过找到所有关节的类中心来建立关系。我们采用BERT作为LLM模型，用于在语言文本中提取特征。GPR-Graph通过保留关键“骨骼”根据规则生成新的骨骼数据，用于GCN网络。新骨骼表示的全局先验连接可以降低拓扑建模的难度，获取不同的特征表示。</p><p><strong>CPR-Graph 和 PC-AC 模块：</strong> CPR-Graph是LLM先验知识对于动作类别的映射。我们的PC-AC模块旨在模拟人类如何使用先验知识进行思考和推理。因此，PC-AC模块将CPR-Graph编码为类别模板拓扑，为GCN添加附加的监督。它适用于解决一些具有挑战性的类似过程的动作分类问题，如“阅读”和“写作”。</p><p><strong>MHA-GC 方法概述：</strong> 此外，我们提出了一种新的特征聚合方法，即多跳注意力图卷积（MHA-GC）块，用于改善节点之间的信息传递效率。MHA-GC可以加强具有连接性的节点之间的语义关系，并加速模型的收敛。</p><p><strong>实验和贡献：</strong> 我们在三个基于骨骼的动作识别数据集NTU RGB+D 60、NTU RGB+D 120和NW-UCLA上进行了评估。在前两个基准数据集的交叉主体分割上，性能分别为93.5%和90.7%。在NW-UCLA上，我们获得了97.6%的准确率。实验结果显示，所提出的LA-GCN优于现有技术。我们的主要贡献总结如下：</p><ul><li>提出了一种LA-GCN，利用LLM的先验知识来辅助基于骨骼的动作识别。</li><li>提出了一种新的骨骼表示方法，用于GCN模型的集成。GPR图通过全局信息降低了拓扑建模的难度。</li><li>提出了带有类别信息编码的辅助监督模块PC-AC，用于提高类似动作的识别率。</li><li>提出了一种新的多跳注意力特征聚合方法MHA-GC，用于改善模型的信息传递效率和加速模型的收敛。</li></ul></blockquote><h4 id="示例">示例</h4><blockquote><p><strong>HUMAN 动作识别概述：</strong></p><ul><li>人类动作识别是视频理解中的核心任务，通过第三人称视角进行分类任务，广泛应用于人机交互、视频监控、医疗保健等领域。</li><li>基于骨骼的动作识别因其稳健性和紧凑性而受欢迎，能有效处理视频中的环境噪声。</li></ul><p><strong>人类肌肉骨骼系统：</strong></p><ul><li>人体肌肉骨骼系统允许身体部位移动，基于骨骼的数据模态符合人体解剖结构，提供了解释性。</li><li>骨骼数据包含人体主要关节的二维或三维坐标，使模型能够通过骨骼序列进行人类运动识别。</li></ul><p><strong>LA-GCN 概述：</strong></p><ul><li>通过引入大规模语言模型（LLM）的知识，提出了语言模型知识辅助的图卷积网络（LA-GCN）来增强基于骨骼的动作识别。</li><li>LA-GCN利用文本知识库模拟人脑区域，辅助GCN网络进行行为预测。</li></ul><p><strong>LA-GCN 方法概述：</strong></p><ul><li>LA-GCN包含全局先验关系拓扑（GPR图）和类别先验关系拓扑（CPR图）两部分，分别用于生成新的骨骼数据和辅助模型学习。</li><li>GPR图用于生成新的骨骼数据，以降低拓扑建模难度，其中的全局先验连接有助于获取不同特征表示。</li><li>CPR图和PC-AC模块模拟人类思维和推理，用于加强类别信息编码的附加监督。</li></ul><p><strong>MHA-GC 方法概述：</strong></p><ul><li>提出了多跳注意力图卷积（MHA-GC）块，用于改善节点之间的信息传递效率，加速模型收敛。</li><li>MHA-GC通过多跳注意力在单个GC层中建立远程节点关系，强化了具有连接性的节点之间的语义关系。</li></ul><p><strong>实验和贡献：</strong></p><ul><li>在NTU RGB+D 60、NTU RGB+D 120和NW-UCLA等数据集上进行了实验评估，取得了优越性能。</li><li>主要贡献包括：使用LLM先验知识辅助GCN进行动作识别、提出新的骨骼表示方法降低拓扑建模难度、引入PC-AC模块加强类别信息编码的监督、提出MHA-GC方法加速信息传递和模型收敛。</li></ul></blockquote><h3 id="相关工作">相关工作</h3><blockquote><p><strong>拓扑构建（Topology Construction）：</strong></p><ul><li>早期基于骨骼的动作识别方法包括基于CNN的和基于RNN的方法。</li><li>GCN方法的设计重点在于拓扑建模，先驱性工作如ST-GCN通过人体结构预定义拓扑作为GCN的输入。</li><li>多尺度图构建引入GCN中进行多级关节关系建模。</li><li>静态方法在推断性能上有限，无法进行网络协同优化，动态方法具有更好的推广性能。</li><li>LA-GCN拓扑建模采用动态模式。</li></ul><p><strong>骨骼-基动作识别中的语言模型（Language Model in Skeleton-Based Action Recognition）：</strong></p><ul><li>自然语言处理（NLP）任务的发展催生了预训练表示模型BERT，能够通过微调解决NLP任务。</li><li>提出了Prompt Learning（PL）技术，将模型适应于不同任务，显著提高LLM的知识利用效率。</li><li>CLIP等方法成功将PL应用于计算机视觉（CV）下游任务，展示了强大的图形和文本表示能力。</li><li>ActionCLIP和LST等方法在动作识别领域中使用LLM进行多模态表示学习，LST的PL技术用于骨骼文本配对建模。</li></ul><p><strong>使用LLM知识的人类认知推理过程模型：</strong></p><ul><li>受到认知神经科学的启发，使用LLM知识模拟人脑区域的知识，用于行为推理。</li><li>PL用于构建辅助学习的拓扑图，包含人类动作和关节之间的精细语义关系。</li></ul></blockquote><blockquote><p>增加示例</p></blockquote><blockquote><p><strong>拓扑构建（Topology Construction）：</strong></p><ul><li>早期基于骨骼的动作识别方法包括基于CNN的（例如[34]，[35]，[36]）和基于RNN的方法（例如[25]，[37]，[38]，[39]）。</li><li>GCN方法的设计重点在于拓扑建模，先驱性工作如ST-GCN通过人体结构预定义拓扑作为GCN的输入。例如，ST-GCN将人体骨骼结构映射到图拓扑，以便于动作识别。</li><li>多尺度图构建引入GCN中进行多级关节关系建模。这有助于捕获动作中的不同层次的关系，如基础关节运动和整体动作模式。</li><li>静态方法在推断性能上有限，无法进行网络协同优化，动态方法具有更好的推广性能。例如，Shift-GCN使用移位操作符进行关节特征融合，隐式学习节点关系，适应实时变化的动作。</li><li>LA-GCN拓扑建模采用动态模式。在LA-GCN中，动态的拓扑建模使模型能够更好地适应不同动作的特点，提高了模型的泛化能力。</li></ul><p><strong>骨骼-基动作识别中的语言模型（Language Model in Skeleton-Based Action Recognition）：</strong></p><ul><li>自然语言处理（NLP）任务的发展催生了预训练表示模型BERT，能够通过微调解决NLP任务。例如，BERT通过预训练学习语言表达，然后通过微调适应特定任务，如情感分析。</li><li>提出了Prompt Learning（PL）技术，将模型适应于不同任务，显著提高LLM的知识利用效率。例如，PL技术允许通过添加特定的文本参数来定制LLM模型，以适应不同的下游任务。</li><li>CLIP等方法成功将PL应用于计算机视觉（CV）下游任务，展示了强大的图形和文本表示能力。例如，CLIP使用共同的嵌入空间对图像和文本进行编码，使它们可以在同一空间中进行比较和匹配。</li><li>ActionCLIP和LST等方法在动作识别领域中使用LLM进行多模态表示学习。例如，ActionCLIP使用CLIP框架进行视频动作识别，LST则将LLM用于骨骼文本多模态表示学习。</li></ul><p><strong>使用LLM知识的人类认知推理过程模型：</strong></p><ul><li>受到认知神经科学的启发，使用LLM知识模拟人脑区域的知识，用于行为推理。例如，人类在观察他人行为时，大脑的特定区域会被激活，从而促使相应的行为推理。</li><li>PL用于构建辅助学习的拓扑图，包含人类动作和关节之间的精细语义关系。例如，PL技术可以将动作类别信息嵌入拓扑图中，帮助模型更好地学习动作的语义关系</li></ul></blockquote><h3 id="方法">方法</h3><h4 id="方法介绍">方法介绍</h4><blockquote><h3 id="基于LLM引导的拓扑辅助图卷积网络（LA-GCN）">基于LLM引导的拓扑辅助图卷积网络（LA-GCN）</h3><ol><li><strong>引言</strong><ul><li>LA-GCN：一种新颖的学习框架，利用LLM模型知识进行骨骼序列的动作推理。</li><li>将先验知识整合到多模态骨骼表示中。</li><li>神经架构和基于类别的先验知识损失，用于多任务学习。</li></ul></li><li><strong>LLM生成的全局先验关系图</strong><ul><li>使用大规模预训练BERT模型提取文本特征。</li><li>利用类别标签和关节名称的文本特征用于节点间关系。</li><li>计算类别维度文本特征的中心，生成全局先验关系（GPR）图。</li><li>GPR图编码语义知识，指导骨骼表示生成。</li></ul></li><li><strong>先验骨骼模态表示</strong><ul><li>利用GPR图生成先验模态表示。</li><li>表示包括关节-骨骼关系，使用关节特征的转换。</li><li>骨骼矩阵转换和骨骼选择过程，用于差异化特征。</li><li>优势：捕获远程交互，包含类别属性的上下文依赖。</li></ul></li><li><strong>特征聚合</strong><ul><li>引入多跳注意机制以捕获关节的上下文依赖。</li><li>编码器结构包括MHA-GC和MS-TC子模块，用于空间和时间建模。</li><li>利用多跳注意机制处理互补的邻域节点信息。</li><li>使用注意力扩散模块进行加权聚合。</li></ul></li><li><strong>先验一致性辅助分类模块</strong><ul><li>PC-AC模块用于先验类别关系拓扑图。</li><li>使用文本特征生成类别拓扑图（T-C）。</li><li>主分支和辅助分支的多任务学习。</li><li>损失函数涉及主分支和辅助分支的预测。</li></ul></li><li><strong>实验评估</strong><ul><li>在NTU RGB+D 60、NTU RGB+D 120和NW-UCLA数据集上进行评估。</li><li>性能超过现有技术。</li></ul></li><li><strong>结论</strong><ul><li>LA-GCN框架利用LLM增强骨骼动作识别。</li><li>全局先验关系图（GPR图）指导骨骼表示。</li><li>先验模态表示捕获关节-骨骼关系。</li><li>多跳注意机制改进特征聚合。</li><li>先验一致性辅助分类（PC-AC）模块提升类似动作的识别。</li></ul></li></ol></blockquote><h4 id="详细介绍">详细介绍</h4><blockquote><ol><li><ul><li><p>当然，我会为您逐个部分地详细介绍文章内容：</p><h3 id="1-引言">1. 引言</h3><p>在这篇文章中，介绍了一种名为&quot;LLM GUIDED TOPOLOGY ASSISTANT GRAPH CONVOLUTION NETWORK&quot;（LA-GCN）的新颖学习框架，该框架利用LLM（Language Model）模型知识来进行给定骨骼序列的动作推理。文章的核心目标是在动作识别任务中引入先验知识，并将其融合到骨骼表示的学习过程中，以提高动作识别的性能。</p><h3 id="2-LLM生成的全局先验关系图">2. LLM生成的全局先验关系图</h3><p>这一部分介绍了如何利用大规模预训练BERT模型提取文本特征，并将这些特征用于生成全局先验关系图（GPR图）。GPR图是一个重要的先验知识，在骨骼表示生成过程中发挥了关键作用。文章详细解释了GPR图的构建过程，包括计算类别维度的文本特征中心、节点之间的相关性计算以及GPR图的生成，从而指导骨骼表示的生成和推理。</p><h3 id="3-先验骨骼模态表示">3. 先验骨骼模态表示</h3><p>本节介绍了一种先验骨骼模态表示的方法，即利用GPR图生成的骨骼表示。这种表示不仅包括了关节之间的关系，还引入了骨骼（bone）特征，通过将关节特征转换为骨骼特征来增强表示能力。文章详细描述了骨骼矩阵转换的过程，以及如何从骨骼矩阵中选择关键骨骼特征。这种先验骨骼模态表示的优势在于捕获了远程节点之间的交互以及类别属性的上下文依赖。</p><h3 id="4-特征聚合">4. 特征聚合</h3><p>这一部分引入了多跳注意机制，以增强关节特征的聚合过程，从而更好地捕获关节的上下文依赖。文章介绍了编码器结构，包括两个子模块：MHA-GC和MS-TC，分别用于空间和时间建模。MHA-GC是一种新颖的多跳注意图卷积模块，用于建模节点间的关系。文章详细解释了多跳注意机制的计算过程，并说明了如何利用注意扩散模块进行特征的加权聚合。</p><h3 id="5-先验一致性辅助分类模块">5. 先验一致性辅助分类模块</h3><p>本节介绍了先验一致性辅助分类（PC-AC）模块，该模块设计了一个类别关系拓扑图，用于辅助GCN进行特征聚合。文章说明了如何生成类别拓扑图，以及如何通过多任务学习来强制模型在训练时关注类别先验信息。PC-AC模块的引入有助于提高类似动作的识别性能，并在模型训练过程中充当正则化项。</p><h3 id="6-实验评估">6. 实验评估</h3><p>在这一部分，文章对LA-GCN模型进行了详细的实验评估，包括在NTU RGB+D 60、NTU RGB+D 120和NW-UCLA等数据集上的性能测试。结果表明，该方法在这些数据集上均取得了优越的性能，超越了现有技术。</p><h3 id="7-结论">7. 结论</h3><p>文章总结了提出的LA-GCN框架的关键优势和贡献。通过引入LLM模型的先验知识，将全局先验关系图引入骨骼表示生成过程，以及利用先验一致性辅助分类模块进行类别关系引导，该框架在动作识别任务中取得了显著的性能提升。</p></li></ul></li></ol></blockquote><blockquote><p>详细介绍</p><blockquote><h3 id="1-引言-2">1. 引言</h3><p>文章介绍了一种名为&quot;LLM GUIDED TOPOLOGY ASSISTANT GRAPH CONVOLUTION NETWORK&quot;（LA-GCN）的新颖学习框架，旨在利用LLM模型知识提高骨骼动作识别性能。这一方法将先验知识嵌入到骨骼表示生成过程中，并引入了全局先验关系图（GPR图）以及先验一致性辅助分类模块。以下是更详细的介绍。</p><h3 id="2-LLM生成的全局先验关系图-2">2. LLM生成的全局先验关系图</h3><p>文章首先利用大规模预训练BERT模型提取文本特征，用于生成全局先验关系图（GPR图）。该图是骨骼表示生成的关键指导，具有语义信息。生成GPR图的过程如下：</p><p><strong>步骤：</strong></p><ol><li>通过BERT模型提取每个动作类别和关节点的文本特征。</li><li>计算每个关节点的类别特征中心（JCoCLS）作为GPR图的中心。</li><li>计算节点之间的相似性，以欧几里德距离为相似度度量，得到GPR图，表示节点之间的全局先验信息。</li></ol><p><strong>示例：</strong> 假设我们有动作类别&quot;A&quot;、“B&quot;和关节点&quot;Joint1”、“Joint2”。通过BERT模型，我们得到了类别特征向量[0.2, 0.5]和[0.3, 0.7]，以及关节点特征向量[0.1, 0.4]和[0.6, 0.9]。然后，我们计算了类别特征的中心向量[0.25, 0.6]和关节点特征的中心向量[0.35, 0.65]。接下来，通过计算相似性，我们可以得到GPR图。</p><h3 id="3-先验骨骼模态表示-2">3. 先验骨骼模态表示</h3><p>这部分介绍了一种先验骨骼模态表示，将GPR图引入骨骼表示中，以更好地捕获关节之间的关系。</p><p><strong>步骤：</strong></p><ol><li>将关节点特征表示为骨骼矩阵，其中骨骼特征是关节特征的转换。</li><li>利用GPR图的距离信息，加权骨骼特征，得到先验骨骼模态表示。</li></ol><p><strong>示例：</strong> 考虑两个关节点&quot;Hand&quot;和&quot;Elbow&quot;，它们的特征分别是[0.6, 0.8]和[0.4, 0.7]。通过骨骼矩阵计算，我们可以得到骨骼特征[0.2, 0.1]。然后，利用GPR图的距离信息，加权骨骼特征，得到先验骨骼模态表示。</p><h3 id="4-特征聚合-2">4. 特征聚合</h3><p>这一部分介绍了多跳注意机制，用于增强关节特征的聚合过程，以更好地捕获关节的上下文依赖。</p><p><strong>步骤：</strong></p><ol><li>引入多跳注意机制，对每个节点的邻居节点进行权重聚合。</li><li>利用多跳注意机制进行特征的加权聚合，得到更丰富的特征表示。</li></ol><p><strong>示例：</strong> 考虑节点&quot;A&quot;的邻居节点&quot;B&quot;和&quot;C&quot;，它们的特征分别是[0.2, 0.5]和[0.3, 0.4]。通过多跳注意机制，我们对这些邻居节点进行权重聚合，得到节点&quot;A&quot;的特征表示。</p><h3 id="5-先验一致性辅助分类模块-2">5. 先验一致性辅助分类模块</h3><p>这一部分介绍了先验一致性辅助分类（PC-AC）模块，用于引导模型根据类别关系进行特征聚合。</p><p><strong>步骤：</strong></p><ol><li>生成类别拓扑图，用于描述类别之间的关系。</li><li>利用PC-AC模块，在训练中强制模型根据类别先验信息进行特征聚合。</li><li>通过多任务学习，同时优化主任务和辅助任务，提高模型性能。</li></ol><p><strong>示例：</strong> 考虑两个类别&quot;A&quot;和&quot;B&quot;，以及它们之间的拓扑关系图。在PC-AC模块中，我们强制模型根据这些类别关系进行特征聚合，从而提高了模型对不同类别的区分能力。</p></blockquote></blockquote><h4 id="分节介绍">分节介绍</h4><h5 id="2-LLM生成的全局先验关系图-3">2. LLM生成的全局先验关系图</h5><blockquote><p>“LLM生成的全局先验关系图” 是文章中的一个关键概念，它描述了如何利用预训练的大规模语言模型（LLM）来生成包含语义知识的关系图，以辅助骨骼动作识别任务。以下是对这个概念的详细解释：</p><p><strong>LLM（Language Model）：</strong> LLM是一种基于大规模文本数据进行预训练的深度学习模型，如BERT。它通过学习文本数据中的语义关系和结构，可以获得丰富的语言表示能力。</p><p><strong>全局先验关系图的生成：</strong> 文章中提出了一种方法，通过将骨骼动作类别和关节名称编码为文本输入，利用预训练的LLM模型生成全局先验关系图。这个关系图包含了关节之间的语义关系和动作类别的语义信息，可以指导骨骼动作的表示和识别。</p><p>具体步骤如下：</p><ol><li>对于每个骨骼动作类别，将该类别的名称（如&quot;跳跃&quot;、“举手&quot;等）和关节名称（如&quot;头部”、&quot;手臂&quot;等）作为文本输入，输入到预训练的LLM模型中。</li><li>LLM模型会为每个输入生成对应的文本特征，这些特征捕捉了动作类别和关节之间的语义信息。这些文本特征可以看作是表示该动作类别和关节的语义向量。</li></ol><p><strong>示例：</strong> 考虑一个具体的示例，如&quot;跳跃&quot;这个骨骼动作类别。首先，将&quot;跳跃&quot;和相关的关节名称（如&quot;脚&quot;、&quot;膝盖&quot;等）编码为文本输入，输入到LLM模型中。LLM模型会为这些输入生成对应的文本特征，这些特征包含了&quot;跳跃&quot;这个动作类别和关节之间的语义关系。</p><p><strong>关系图的应用：</strong> 生成的全局先验关系图可以应用于骨骼动作的表示和识别中。具体来说，可以将全局关系图中的每个节点视为一个关节，节点之间的边表示关节之间的语义关系。这样，骨骼动作的表示可以通过关节之间的语义关系来丰富和引导。</p><p><strong>意义和用途：</strong> 生成全局先验关系图的目的是为了将文本知识（动作类别和关节语义）与骨骼动作的表示相结合，从而提高骨骼动作识别的性能。关系图中的语义信息可以指导模型更好地捕捉关节之间的关系，从而更准确地表示和识别不同的骨骼动作。</p><p>总之，“LLM生成的全局先验关系图” 是一种利用预训练的语言模型生成包含语义知识的关系图的方法，用于辅助骨骼动作识别任务，帮助模型更好地理解和表示不同动作类别和关节之间的关系。这有助于提高骨骼动作识别的准确性和性能。</p></blockquote><h5 id="3-先验骨骼模态表示-3">3. 先验骨骼模态表示</h5><blockquote><p>当讨论到&quot;先验骨骼模态表示&quot;时，文章探讨了一种利用先验知识来构建骨骼序列表示的方法，旨在改善骨骼动作识别的性能。以下是对该部分的详细解释：</p><p>先验骨骼模态表示是指在骨骼动作识别任务中，使用先前获得的知识来构建骨骼动作的表示。这种先验知识可以是来自大规模数据集的统计信息、关节之间的关系、以及人体动作的常识等。文章中提出的方法旨在通过有效地整合这些先验信息，以改进骨骼动作识别的性能。</p><p><strong>关节和骨骼表示：</strong> 在骨骼动作中，关节是构成人体姿势的重要组成部分，而骨骼是关节之间的连接。文章指出，骨骼的表示可以通过关节之间的相对位置来实现。具体来说，对于每一对关节，可以通过计算它们之间的相对位置（即关节之间的距离和方向）来获得骨骼特征。</p><p><strong>骨骼表示的构建：</strong> 先验骨骼模态表示方法的核心思想是，将先验信息纳入骨骼表示的构建过程中。为了实现这一点，文章提出了一种基于先验全局关系图的方法。这里的全局关系图是基于自然语言处理（NLP）中的BERT模型生成的文本特征，通过对动作类别和关节名称进行编码。</p><p>具体步骤如下：</p><ol><li>对于每个动作类别，将所有关节的名称作为文本输入到预训练的BERT模型中，获得与该动作类别相关的文本特征。这些文本特征捕捉了关节之间的语义关系和动作类别的语义信息。</li><li>将每个动作类别的关节文本特征进行平均，得到一个表示该动作类别的全局关系特征。这个全局关系特征反映了该动作类别下关节之间的关系。</li></ol><p><strong>骨骼表示的应用：</strong> 文章进一步介绍了如何将这些全局关系特征应用于骨骼表示的构建。具体来说，通过将每个关节的骨骼特征与其所属动作类别的全局关系特征进行加权相加，得到了一个综合的先验骨骼模态表示。这种表示融合了关节之间的相对位置信息和动作类别的语义信息，从而更全面地描述了骨骼动作。</p><p><strong>示例：</strong> 为了更好地理解，考虑一个动作类别为&quot;跳跃&quot;，包含上肢和下肢的关节。通过预训练的BERT模型，可以得到&quot;跳跃&quot;动作类别的全局关系特征，它反映了上肢和下肢之间的关系。然后，对于一个特定的骨骼序列，比如一个人跳跃的动作，可以通过将每个关节的骨骼特征与&quot;跳跃&quot;动作类别的全局关系特征进行加权相加，得到该骨骼序列的先验骨骼模态表示。这个表示不仅考虑了关节之间的相对位置，还包含了&quot;跳跃&quot;这一动作类别的语义信息，从而更准确地表示了跳跃动作。</p><p>总之，先验骨骼模态表示方法通过利用先验信息来构建骨骼动作的表示，可以更全面地描述关节之间的关系和动作类别的语义信息，从而提高骨骼动作识别的性能。</p></blockquote><h3 id="实验">实验</h3><h4 id="实验介绍">实验介绍</h4><blockquote><ol><li>数据集：在实验中使用了三个大规模的骨架动作识别数据集，分别是NTU RGB+D、NTU RGB+D 120和NW-UCLA。</li><li>实现细节：对于NTU RGB+D和NTU RGB+D 120数据集，将每个样本调整为64帧，并使用预处理代码进行数据预处理。对于NW-UCLA数据集，采用相应的数据预处理方法。实验中采用CTR-GCN作为骨架编码器，在比较其他方法时采用CTR-GCN与多尺度时间卷积。文本编码器使用CLIP或BERT的预训练模型，并在训练过程中微调其参数。对比损失的温度参数设置为0.1。</li><li>分析研究：通过消融实验，评估了LST中不同组件的影响。分别对比了不同的体分割策略、文本提示、文本编码器、骨架编码器、文本描述方法和λ参数选择的影响。</li><li>分割策略：不同的体分割策略对LST模型的性能有不同的影响。使用更多的部位和多部位对比损失能稳定提升性能，在使用4个部位时达到85.4%的骨架动作识别准确率。</li><li>文本提示：文本提示设计对模型性能有很大影响。直接使用标签名称作为文本提示仅略微提高性能。使用同义词列表或全局描述段落可显著提升性能。使用部位描述提示可获得更好的性能，结合同义词和部位描述提示可获得最佳性能。</li><li>文本编码器：使用不同的文本编码器对骨架编码器产生积极影响。CLIP-L/32和BERT分别获得良好的性能。</li><li>LST对不同骨架编码器的影响：LST不依赖于网络结构，可用于改进不同的骨架编码器。LST对ST-GCN、CTR-baseline和CTR-GCN等骨架编码器带来一致的提升，且在推理时不增加额外的计算成本。</li><li>文本描述方法：通过不同的方法获得文本提示，包括手动描述、HAKE部位状态和GPT-3生成的描述。其中，GPT-3生成的部位文本描述表现最佳。</li><li>λ参数选择：通过实验选取了最优的λ参数为0.8，并在所有实验中使用该参数。</li><li>与现有方法比较：LST在NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集上均优于现有的状态-of-the-art方法，验证了其有效性和鲁棒性。在不同设置下，LST持续地提高性能。</li></ol></blockquote><h4 id="详细介绍-2">详细介绍</h4><blockquote><ol><li>数据集：实验使用了三个大规模的骨架动作识别数据集，分别是NTU RGB+D、NTU RGB+D 120和NW-UCLA。NTU RGB+D数据集包含56,880个骨架动作序列，其中有交叉主体（Cross-Subject, X-Sub）和交叉视角（Cross-View, X-View）两个设置进行评估。NTU RGB+D 120是NTU RGB+D数据集的扩展，增加了57,367个骨架序列和60个额外的动作类别。NW-UCLA数据集包含1,494个由三个Kinect V1传感器从不同视角录制的视频序列，涵盖10个动作类别。</li><li>实现细节：对于NTU RGB+D和NTU RGB+D 120数据集，每个样本被调整为64帧，并使用(Zhang et al., 2020; Chi et al., 2022)的代码进行数据预处理。对于NW-UCLA数据集，按照(Cheng et al., 2020; Chen et al., 2021; Chi et al., 2022)的数据预处理方法进行处理。在实验中，采用CTR-GCN作为骨架编码器，并在与其他方法进行比较时，使用CTR-GCN与多尺度时间卷积。文本编码器使用CLIP或BERT的预训练模型，并在训练过程中微调其参数。对比损失的温度参数设置为0.1。</li><li>分析研究：通过消融实验，评估了LST中不同组件的影响。</li></ol><ul><li>分割策略：实验中尝试了不同的体分割策略，通过将骨架序列划分为不同的部位进行训练，以探索不同分割策略对LST模型性能的影响。例如，全局描述将整个骨架序列视为一个整体进行对比学习，而使用上下半身分割或四个部位分割等不同策略进行对比学习。实验结果表明，使用更多的部位和多部位对比损失能稳定提升性能，在使用4个部位时达到85.4%的骨架动作识别准确率。</li><li>文本提示：为了探索不同文本提示对LST模型性能的影响，实验中尝试了多种文本提示类型，包括直接使用动作类别的标签名称、使用标签名称的同义词列表、全局描述段落以及部位描述提示。实验结果显示，直接使用标签名称作为文本提示只略微提高性能。而使用同义词列表或全局描述段落能够显著提升性能，因为它们丰富了每个动作类别的语义含义。而使用部位描述提示则可以获得更好的性能，最佳性能实现于结合同义词和部位描述提示的情况下，达到85.5%的准确率。</li><li>文本编码器：实验中尝试了不同的文本编码器，包括CLIP-L/32、CLIP-L/16、CLIP-L/14和BERT。结果显示，CLIP-L/32和BERT都能带来较好的性能，这表明骨架编码器可以从不同的预训练源（图像-语言或纯语言）中受益。最终选择CLIP-L/32作为默认文本编码器，因为它在性能和效率之间取得了良好的平衡。</li><li>LST对不同骨架编码器的影响：为了验证LST的有效性和泛化能力，实验中将LST应用于不同的骨架编码器，包括ST-GCN、CTR-baseline和CTR-GCN等。实验结果显示LST对这些编码器均带来了稳定的性能提升（0.6-1.2%），而在推理时并不增加额外的计算成本。</li><li>文本描述方法：在实验中，对比了不同的文本描述方法，包括手动描述、HAKE部位状态和GPT-3生成的描述。实验中使用这些描述作为全局描述用于LST。实验结果显示，由GPT-3生成的部位文本描述表现最佳，与手动描述和HAKE部位状态相比有显著的性能提升。</li><li>λ参数选择：通过调节λ参数，实验中探索了不同的权衡参数对LST模型性能的影响。结果表明λ=0.8可以取得最佳性能。</li></ul><ol><li>与现有方法比较：将LST与现有的最先进方法进行比较，包括NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集的状态-of-the-art方法。实验结果显示，LST持续地提高了性能，优于CTR-GCN和Info-GCN等先进方法。例如在NW-UCLA数据集上，LST相较于CTR-GCN提高了0.7%的准确率，并超过了采用自注意层和信息瓶颈的Info-GCN（Chi et al., 2022）0.6%的准确率。在NTU RGB+D数据集和NTU RGB+D 120数据集上，LST也分别优于CTR-GCN和Info-GCN。实验结果验证了LST的有效性和鲁棒性，使其成为当前骨架动作识别领域的最佳选择。</li></ol><p>总之，实验结果表明LST方法在多个大规模骨架动作识别数据集上都取得了显著的性能提升，并且在与现有方法进行比较时表现出色，验证了其有效性和泛化能力。通过对比实验的细节和结果，LST在骨架动作识别任务中展现出了出色的潜力和优越性能。</p></blockquote><h3 id="文章总结">文章总结</h3><blockquote><p>方法部分：</p><ol><li>LST：本文提出了一种新的骨架动作识别方法——语义文本增强对比学习（LST）。该方法结合了文本信息和骨架动作序列，通过对比学习来增强动作表示的语义含义，从而提高动作识别性能。</li><li>文本编码器：LST利用预训练的文本编码器，如CLIP或BERT，将动作类别的文本描述嵌入到特征空间中。通过文本编码器，模型能够利用更丰富的语义信息，提升动作特征的判别性。</li><li>对比学习：LST采用对比损失函数来学习骨架动作的语义表示。模型将同一类别的样本拉近，不同类别的样本推远，从而实现更好的特征聚类和分类性能。</li></ol><p>实验部分：</p><ol><li>数据集：作者在三个大规模骨架动作识别数据集上进行了实验，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA。这些数据集分别涵盖了不同的动作类别和动作视角，用于评估模型的性能。</li><li>消融实验：作者进行了多个消融实验，评估LST中各个组件的影响。实验结果显示，在分割策略、文本提示类型、文本编码器、LST对不同骨架编码器的影响以及λ参数选择方面，LST都能显著提升模型性能。</li><li>与现有方法比较：作者将LST与现有的最先进方法进行了比较。实验结果表明，LST在NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集上均取得了优于其他方法的结果，验证了LST的有效性和鲁棒性。</li></ol><p>综上所述，通过引入语义文本增强对比学习方法，本文在骨架动作识别任务中取得了显著的性能提升。该方法有效利用文本信息增强动作表示的语义含义，提高了动作识别的精度。实验结果表明，LST在大规模数据集上表现出色，成为当前骨架动作识别领域的最佳选择。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文献阅读</title>
      <link href="/project/posts/7c36b624.html"/>
      <url>/project/posts/7c36b624.html</url>
      
        <content type="html"><![CDATA[<h1>Language Supervised Training for Skeleton-based Action Recognition</h1><h1>INFO</h1><h3 id="摘要总结">摘要总结</h3><blockquote><p>本文介绍了一种基于骨架的动作识别方法，其具有计算效率高和对光照条件鲁棒性强的特点。现有的基于骨架的动作识别方法通常被视为一个单热编码的分类任务，没有充分利用动作之间的语义关系。例如，“比胜利手势”和“竖起大拇指”是两种手势动作，它们的主要区别在于手的运动。这些信息在动作类别的单热编码中是无法体现的，但可以在动作的语言描述中揭示出来。因此，在训练中利用动作的语言描述可能有助于表示学习。本文提出了一种语言监督训练（LST）方法用于基于骨架的动作识别。具体而言，我们使用大规模语言模型作为知识引擎，为动作的身体部位运动提供文本描述，并提出了一种多模态训练方案，通过利用文本编码器为不同身体部位生成特征向量，并监督骨架编码器进行动作表示学习。实验证明，我们提出的LST方法在不增加推理的额外计算成本的情况下，相比各种基线模型取得了显著的改进。LST在流行的基于骨架的动作识别基准数据集，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA上，取得了新的最优结果。代码可在https://github.com/MartinXM/LST找到。</p></blockquote><blockquote><ul><li>骨架动作识别因计算效率高和对光照条件鲁棒性强而受到关注。</li><li>现有方法通常将骨架动作识别视为一个单热编码的分类任务，未充分利用动作之间的语义关系。</li><li>动作的语言描述中包含了动作间细微差异的信息，但这在单热编码中无法体现。</li><li>本文提出了一种语言监督训练（LST）方法，通过使用大规模语言模型作为知识引擎，为动作的身体部位运动提供文本描述。</li><li>文中采用多模态训练方案，利用文本编码器为不同身体部位生成特征向量，同时监督骨架编码器进行动作表示学习。</li><li>LST方法在不增加推理的额外计算成本的情况下，取得了显著的改进。</li><li>在流行的基于骨架的动作识别基准数据集上，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA，LST取得了新的最优结果。</li></ul></blockquote><h3 id="介绍总结">介绍总结</h3><blockquote><ul><li>动作识别在人机交互、体育和健康分析、娱乐等领域有广泛应用。</li><li>随着深度传感器（如Kinect和RealSense）的出现，可以轻松获取人体关节信息。</li><li>基于骨架的动作识别因计算效率高、对光照条件、视角变化和背景噪声鲁棒性强而受到关注。</li><li>大多数现有方法将骨架动作识别视为单模态任务，输入为一系列骨架坐标。</li><li>本文探讨了语言监督训练是否能够揭示细粒度的语义关系，并在骨架动作识别中受益。</li><li>由于缺乏大规模骨架-文本数据集，无法直接将现有的多模态训练方案应用于骨架动作识别。</li><li>本文提出了一种新的训练范式，利用动作的类别级语言描述，提供丰富的先验知识。</li><li>提出的训练范式包含骨架编码器和文本编码器，并使用大规模语言模型生成有意义的动作文本描述。</li><li>利用精心设计的文本提示，可以生成整个动作和各个身体部位的详细文本描述。</li><li>使用多部分对比损失对骨架和文本特征进行对齐，全局特征采用交叉熵损失。</li><li>新的训练范式可以提高各种骨架动作识别方法的性能，且不增加推理阶段的额外成本。</li><li>文章的贡献总结为：首次将语言模型作为知识引擎，自动生成整个动作和身体部位运动的详细文本描述；提出了利用语言监督训练的多模态范式，通过使用动作和人体部位的知识来增强表示学习；在多个流行的骨架动作识别基准数据集上实现了最先进的性能，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA。</li></ul></blockquote><h4 id="示例">示例</h4><blockquote><p>添加了示例</p></blockquote><blockquote><ul><li>动作识别在人机交互、体育和健康分析、娱乐等领域有广泛应用。</li><li>随着深度传感器（如Kinect和RealSense）的出现，可以轻松获取人体关节信息。</li><li>基于骨架的动作识别因计算效率高、对光照条件、视角变化和背景噪声鲁棒性强而受到关注。</li><li>大多数现有方法将骨架动作识别视为单模态任务，输入为一系列骨架坐标。</li><li>本文探讨了语言监督训练是否能够揭示细粒度的语义关系，并在骨架动作识别中受益。</li><li>举例说明：不同动作侧重于不同身体部位的运动，如“make victory sign”和“thumb up”描述手的手势动作，“arm circles”和“tennis bat swing”描述手臂的运动，“nod head”和“shake head”描述头部的动作，“jump up”和“side kick”依赖于脚和腿的运动。</li><li>由于缺乏大规模骨架-文本数据集，无法直接将现有的多模态训练方案应用于骨架动作识别。</li><li>本文提出了一种新的训练范式，利用动作的类别级语言描述，提供丰富的先验知识。</li><li>提出的训练范式包含骨架编码器和文本编码器，并使用大规模语言模型生成有意义的动作文本描述。</li><li>利用精心设计的文本提示，可以生成整个动作和各个身体部位的详细文本描述。</li><li>使用多部分对比损失对骨架和文本特征进行对齐，全局特征采用交叉熵损失。</li><li>新的训练范式可以提高各种骨架动作识别方法的性能，且不增加推理阶段的额外成本。</li><li>文章的贡献总结为：首次将语言模型作为知识引擎，自动生成整个动作和身体部位运动的详细文本描述；提出了利用语言监督训练的多模态范式，通过使用动作和人体部位的知识来增强表示学习；在多个流行的骨架动作识别基准数据集上实现了最先进的性能，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA。</li></ul></blockquote><h3 id="方法">方法</h3><h4 id="方法介绍">方法介绍</h4><blockquote><p>方法部分总结：</p><ol><li>语言监督训练（LST）框架：<ul><li>LST旨在通过语言监督增强骨骼表示学习，与主干网络相互独立，可以与各种骨骼和语言编码器结合。</li><li>LST框架利用部分文本特征通过多部分对比损失来指导骨骼表示学习。</li><li>通过对不同节点组的特征进行平均汇聚，生成部分特征。</li><li>文本编码器将全局动作描述或身体部位描述转换为整个动作或每个身体部位的文本特征。</li><li>总体训练损失结合了交叉熵分类损失和多部分对比损失。</li></ul></li><li>骨骼编码器：<ul><li>骨骼编码器基于图卷积网络（GCN），用于骨骼动作识别。</li><li>骨骼编码器包含多个GC-MTC（图卷积-多尺度时间卷积）块。</li><li>GC-MTC块包括图卷积（GC）层和多尺度时间卷积（MTC）模块。</li><li>骨骼编码器使用交叉熵损失将人体骨骼数据映射到动作标签的单热编码。</li></ul></li><li>语言监督：<ul><li>文本编码器基于NLP中的Transformer模型，包括自注意力，Layer-Norm和FFN层。</li><li>骨骼-语言对比学习利用自然语言的监督，采用骨骼编码器和文本编码器作为双编码器。</li><li>多部分对比损失用于根据骨骼-文本对之间的相似性得分对齐部分特征。</li></ul></li><li>多部分对比学习：<ul><li>根据不同身体部位（例如，头部，手臂，手，臀部，腿，脚），将骨骼划分为多个组。</li><li>在不同的部分特征和全局特征上应用对比损失，形成多部分对比损失。</li><li>部分特征的汇聚和多部分对比损失仅在训练阶段应用，不会增加推断时的额外内存或计算成本。</li></ul></li><li>动作描述生成：<ul><li>探索不同的文本描述生成方法，包括标签名称，HAKE部分状态，手动描述和大型语言模型（例如GPT-3）。</li><li>设计文本提示，以生成关于整个动作和身体部位运动的详细文本描述。</li></ul></li></ol><p>总体而言，本文介绍了语言监督训练（LST）框架，旨在通过语言监督增强基于骨骼的动作识别。骨骼编码器基于图卷积网络（GCN），文本编码器采用Transformer模型。训练范式包括骨骼-语言对比学习和多部分对比学习。探索了多种动作描述生成方法，并且整体LST框架在几个流行的基于骨骼的动作识别基准上取得了最先进的性能。</p></blockquote><h4 id="详细介绍">详细介绍</h4><blockquote><ol><li>语言监督训练（LST）框架：<ul><li>LST是一种用于增强骨骼表示学习的框架，它通过语言监督的方式引入自然语言描述来指导骨骼动作的学习。</li><li>LST框架与主干网络相互独立，因此可以与各种骨骼和语言编码器结合使用，提高了方法的灵活性和适用性。</li><li>LST框架使用部分文本特征来指导骨骼表示学习，并通过多部分对比损失进一步提升性能。</li><li>在训练阶段，LST框架使用部分文本特征和多部分对比损失，而在测试阶段，直接使用全局特征进行动作概率预测，不会增加推断时的额外内存或计算成本。</li></ul></li><li>骨骼编码器：<ul><li>骨骼编码器在LST框架中被用作主干网络，用于将输入的人体骨骼数据映射到动作标签的单热编码表示。</li><li>骨骼编码器采用了图卷积网络（GCN）作为主要结构，因其高效性和良好的性能而被广泛使用。</li><li>骨骼编码器由多个GC-MTC（图卷积-多尺度时间卷积）块组成，每个块包含一个图卷积（GC）层和一个多尺度时间卷积（MTC）模块。</li><li>GC-MTC块用于对人体骨骼动作进行时空建模，以适应不同的时间尺度和速度。</li></ul></li><li>语言监督：<ul><li>在LST中，使用文本编码器进行语言监督，将动作描述信息引入到骨骼动作识别中。</li><li>文本编码器采用了Transformer模型，在NLP领域取得了显著的成功。</li><li>通过文本编码器，可以将全局动作描述或身体部位描述转换为文本特征，用于后续的骨骼-语言对比学习。</li></ul></li><li>多部分对比学习：<ul><li>多部分对比学习是LST框架中的一项重要技术，旨在充分利用人体骨骼动作中不同身体部位之间的语义关系。</li><li>骨骼被划分为多个组，例如头部、手臂、手、臀部、腿、脚等，每个组称为一个部分。</li><li>在训练阶段，采用多部分对比损失，在不同的部分特征和全局特征之间进行对比，以进一步提高骨骼动作的表示学习性能。</li><li>部分特征的汇聚和多部分对比损失只在训练阶段应用，在测试阶段仍然使用全局特征进行动作概率预测。</li></ul></li><li>动作描述生成：<ul><li>LST框架中需要动作描述用于文本编码器的语言监督，提供对动作的语义信息。</li><li>探索了多种动作描述生成方法，包括直接使用动作标签名称、使用HAKE（人体动作与对象交互数据集）部分状态、手动描述和使用大型语言模型（例如GPT-3）生成文本描述。</li><li>不同的动作描述生成方法可以提供不同程度的语义信息，对骨骼-语言对比学习和整体LST框架的性能有影响。</li></ul></li></ol></blockquote><h3 id="实验">实验</h3><h4 id="实验介绍">实验介绍</h4><blockquote><p>实验部分总结如下：</p><ol><li>数据集：在实验中使用了三个大规模的骨架动作识别数据集，分别是NTU RGB+D、NTU RGB+D 120和NW-UCLA。</li><li>实现细节：对于NTU RGB+D和NTU RGB+D 120数据集，将每个样本调整为64帧，并使用预处理代码进行数据预处理。对于NW-UCLA数据集，采用相应的数据预处理方法。实验中采用CTR-GCN作为骨架编码器，在比较其他方法时采用CTR-GCN与多尺度时间卷积。文本编码器使用CLIP或BERT的预训练模型，并在训练过程中微调其参数。对比损失的温度参数设置为0.1。</li><li>分析研究：通过消融实验，评估了LST中不同组件的影响。分别对比了不同的体分割策略、文本提示、文本编码器、骨架编码器、文本描述方法和λ参数选择的影响。</li><li>分割策略：不同的体分割策略对LST模型的性能有不同的影响。使用更多的部位和多部位对比损失能稳定提升性能，在使用4个部位时达到85.4%的骨架动作识别准确率。</li><li>文本提示：文本提示设计对模型性能有很大影响。直接使用标签名称作为文本提示仅略微提高性能。使用同义词列表或全局描述段落可显著提升性能。使用部位描述提示可获得更好的性能，结合同义词和部位描述提示可获得最佳性能。</li><li>文本编码器：使用不同的文本编码器对骨架编码器产生积极影响。CLIP-L/32和BERT分别获得良好的性能。</li><li>LST对不同骨架编码器的影响：LST不依赖于网络结构，可用于改进不同的骨架编码器。LST对ST-GCN、CTR-baseline和CTR-GCN等骨架编码器带来一致的提升，且在推理时不增加额外的计算成本。</li><li>文本描述方法：通过不同的方法获得文本提示，包括手动描述、HAKE部位状态和GPT-3生成的描述。其中，GPT-3生成的部位文本描述表现最佳。</li><li>λ参数选择：通过实验选取了最优的λ参数为0.8，并在所有实验中使用该参数。</li><li>与现有方法比较：LST在NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集上均优于现有的状态-of-the-art方法，验证了其有效性和鲁棒性。在不同设置下，LST持续地提高性能。</li></ol></blockquote><h4 id="详细介绍-2">详细介绍</h4><blockquote><ol><li>数据集：实验使用了三个大规模的骨架动作识别数据集，分别是NTU RGB+D、NTU RGB+D 120和NW-UCLA。NTU RGB+D数据集包含56,880个骨架动作序列，其中有交叉主体（Cross-Subject, X-Sub）和交叉视角（Cross-View, X-View）两个设置进行评估。NTU RGB+D 120是NTU RGB+D数据集的扩展，增加了57,367个骨架序列和60个额外的动作类别。NW-UCLA数据集包含1,494个由三个Kinect V1传感器从不同视角录制的视频序列，涵盖10个动作类别。</li><li>实现细节：对于NTU RGB+D和NTU RGB+D 120数据集，每个样本被调整为64帧，并使用(Zhang et al., 2020; Chi et al., 2022)的代码进行数据预处理。对于NW-UCLA数据集，按照(Cheng et al., 2020; Chen et al., 2021; Chi et al., 2022)的数据预处理方法进行处理。在实验中，采用CTR-GCN作为骨架编码器，并在与其他方法进行比较时，使用CTR-GCN与多尺度时间卷积。文本编码器使用CLIP或BERT的预训练模型，并在训练过程中微调其参数。对比损失的温度参数设置为0.1。</li><li>分析研究：通过消融实验，评估了LST中不同组件的影响。</li></ol><ul><li>分割策略：实验中尝试了不同的体分割策略，通过将骨架序列划分为不同的部位进行训练，以探索不同分割策略对LST模型性能的影响。例如，全局描述将整个骨架序列视为一个整体进行对比学习，而使用上下半身分割或四个部位分割等不同策略进行对比学习。实验结果表明，使用更多的部位和多部位对比损失能稳定提升性能，在使用4个部位时达到85.4%的骨架动作识别准确率。</li><li>文本提示：为了探索不同文本提示对LST模型性能的影响，实验中尝试了多种文本提示类型，包括直接使用动作类别的标签名称、使用标签名称的同义词列表、全局描述段落以及部位描述提示。实验结果显示，直接使用标签名称作为文本提示只略微提高性能。而使用同义词列表或全局描述段落能够显著提升性能，因为它们丰富了每个动作类别的语义含义。而使用部位描述提示则可以获得更好的性能，最佳性能实现于结合同义词和部位描述提示的情况下，达到85.5%的准确率。</li><li>文本编码器：实验中尝试了不同的文本编码器，包括CLIP-L/32、CLIP-L/16、CLIP-L/14和BERT。结果显示，CLIP-L/32和BERT都能带来较好的性能，这表明骨架编码器可以从不同的预训练源（图像-语言或纯语言）中受益。最终选择CLIP-L/32作为默认文本编码器，因为它在性能和效率之间取得了良好的平衡。</li><li>LST对不同骨架编码器的影响：为了验证LST的有效性和泛化能力，实验中将LST应用于不同的骨架编码器，包括ST-GCN、CTR-baseline和CTR-GCN等。实验结果显示LST对这些编码器均带来了稳定的性能提升（0.6-1.2%），而在推理时并不增加额外的计算成本。</li><li>文本描述方法：在实验中，对比了不同的文本描述方法，包括手动描述、HAKE部位状态和GPT-3生成的描述。实验中使用这些描述作为全局描述用于LST。实验结果显示，由GPT-3生成的部位文本描述表现最佳，与手动描述和HAKE部位状态相比有显著的性能提升。</li><li>λ参数选择：通过调节λ参数，实验中探索了不同的权衡参数对LST模型性能的影响。结果表明λ=0.8可以取得最佳性能。</li></ul><ol><li>与现有方法比较：将LST与现有的最先进方法进行比较，包括NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集的状态-of-the-art方法。实验结果显示，LST持续地提高了性能，优于CTR-GCN和Info-GCN等先进方法。例如在NW-UCLA数据集上，LST相较于CTR-GCN提高了0.7%的准确率，并超过了采用自注意层和信息瓶颈的Info-GCN（Chi et al., 2022）0.6%的准确率。在NTU RGB+D数据集和NTU RGB+D 120数据集上，LST也分别优于CTR-GCN和Info-GCN。实验结果验证了LST的有效性和鲁棒性，使其成为当前骨架动作识别领域的最佳选择。</li></ol><p>总之，实验结果表明LST方法在多个大规模骨架动作识别数据集上都取得了显著的性能提升，并且在与现有方法进行比较时表现出色，验证了其有效性和泛化能力。通过对比实验的细节和结果，LST在骨架动作识别任务中展现出了出色的潜力和优越性能。</p></blockquote><h3 id="文章总结">文章总结</h3><blockquote><p>方法部分：</p><ol><li>LST：本文提出了一种新的骨架动作识别方法——语义文本增强对比学习（LST）。该方法结合了文本信息和骨架动作序列，通过对比学习来增强动作表示的语义含义，从而提高动作识别性能。</li><li>文本编码器：LST利用预训练的文本编码器，如CLIP或BERT，将动作类别的文本描述嵌入到特征空间中。通过文本编码器，模型能够利用更丰富的语义信息，提升动作特征的判别性。</li><li>对比学习：LST采用对比损失函数来学习骨架动作的语义表示。模型将同一类别的样本拉近，不同类别的样本推远，从而实现更好的特征聚类和分类性能。</li></ol><p>实验部分：</p><ol><li>数据集：作者在三个大规模骨架动作识别数据集上进行了实验，包括NTU RGB+D、NTU RGB+D 120和NW-UCLA。这些数据集分别涵盖了不同的动作类别和动作视角，用于评估模型的性能。</li><li>消融实验：作者进行了多个消融实验，评估LST中各个组件的影响。实验结果显示，在分割策略、文本提示类型、文本编码器、LST对不同骨架编码器的影响以及λ参数选择方面，LST都能显著提升模型性能。</li><li>与现有方法比较：作者将LST与现有的最先进方法进行了比较。实验结果表明，LST在NW-UCLA、NTU RGB+D和NTU RGB+D 120数据集上均取得了优于其他方法的结果，验证了LST的有效性和鲁棒性。</li></ol><p>综上所述，通过引入语义文本增强对比学习方法，本文在骨架动作识别任务中取得了显著的性能提升。该方法有效利用文本信息增强动作表示的语义含义，提高了动作识别的精度。实验结果表明，LST在大规模数据集上表现出色，成为当前骨架动作识别领域的最佳选择。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Literature Reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>志愿报考的林林总总</title>
      <link href="/project/posts/a7fe5a1d.html"/>
      <url>/project/posts/a7fe5a1d.html</url>
      
        <content type="html"><![CDATA[<h2 id="志愿报考章">志愿报考章</h2><p><img src="../assets/image-20230802133022707.png" alt="image-20230802133022707"></p><p>转眼间 那个小女孩变成了一个大姑凉，能够自己来回穿梭在济南，感觉比自己要强，自己高中毕业的时候还没有单独跑出过济南，也是因为自己在家里人的庇护下，没有太多的机会吧（这里回想起那段时光，非常的感谢自己的姐姐，能够帮助我很多，我一生的贵人。）</p><blockquote><p>7.24 周一 ，新新来到长清找我，我大概在不到10点钟，从学校出发，去地铁站接了她，然后直接带她回了学校，带她见识一下学校里边有多大，结果没逛多远太热了（手动狗头）</p><p>带她去了图书馆去看了看，然后一起开始了志愿填报之旅</p></blockquote><p><img src="../assets/image-20230802134118191.png" alt="image-20230802134118191"></p><blockquote><p>21号开始聊天，24-26号下午填报完成</p></blockquote><blockquote><p>新新确实长大了，在选择省内省外的时候，她自己自信的说：“ 我都已经18岁了，现在都已经能自己来回从家里到济南了，而且永杰哥哥也都能出去往省外跑了，我也能，我自己选择。哥哥你帮我填就行”</p></blockquote><blockquote><p>按照相关的专升本的政策，从想要的省份中选择学校和专业</p><p>经过对比选择出来的城市有 上海 江苏 湖北 山东</p><p>所有的专业 大部分集中在 学前教育，小学教育，护理医学，法律</p><p>专升本的条件 中最好不要有数学科目，因为新新数学不好，同时大部分省份中都有计算机，可以让新新好好学学计算机，然后新新的语文和英语还是挺好的，让他在这些科目中争取专升本发挥自己的优势</p></blockquote><p><img src="../assets/image-20230802150113143.png" alt="image-20230802150113143"></p><blockquote><p>所有的学校是根据找的报志愿的辅导老师给挑选出的学习进行的筛选，选择自己意向的专业和城市</p></blockquote><blockquote><p>上海先挑选的学校</p><p>先从名单中挑学校，然后看地理位置，最后看网上的学生评价</p><p>宿舍 风气 生活 转身本的选择以及能去的学校</p></blockquote><p><img src="../assets/image-20230802150613151.png" alt="image-20230802150613151"></p><blockquote><p>东北+浙江</p></blockquote><p><img src="../assets/image-20230802150807230.png" alt="image-20230802150807230"></p><blockquote><p>江苏</p></blockquote><blockquote><p>江苏比较喜欢的学校是盐城那个 虽然稍微远一点，但是其他都比较喜欢，还有一个是江苏食品药品</p></blockquote><p><img src="../assets/image-20230802150857601.png" alt="image-20230802150857601"></p><blockquote><p>虽然不太喜欢数学，想去省外，但是架不住还是想要留在靠近家近一点的地方，再加上一开始青岛那个学校本来就想去，所以也重点看了很多山东的学校  尤其是青岛</p></blockquote><p><img src="../assets/image-20230802151120701.png" alt="image-20230802151120701"></p><blockquote><p>湖北，小叔上学的省份，虽然比较偏内陆也距离海比较远，但是他的专升本的目标学校以及省内的政策和学校查完之后，都非常不错，也有喜欢的专业，所以也很想去这个省份</p></blockquote><p><img src="../assets/image-20230802151706031.png" alt="image-20230802151706031"></p><blockquote><p>给新新定了酒店，跟他在酒店里边一找学校就找一天，从24号下午吃完午饭2点多到了酒店，3点睡醒开始找学校，一口气找到7点多，然后出去买了点吃的，随便吃了点就回来继续去找学习</p></blockquote><blockquote><p>中间有个小插曲，把房卡给掉了，最后扣了押金30大洋， 还挺难受，可能是因为买了小酥肉去拿的时候给掉了</p><p>但是不得不说，那个小酥肉确实是好吃，还有那一家的肉夹馍也非常不错</p><p>晚上10点多，从酒店离开，回到宿舍都11点了，25号早上八点就又去了酒店跟他一起，到的时候他还在看电视，害 小姑娘确实是挺爱玩的，她说早上七点不到就醒了，还在看电视</p><p>我买了早饭9点又开始找，一口气找到12点，中间吃了面，我吃的牛肉拌面不好吃，但是新新吃的砂锅两掺米线很好吃</p><p>下午到了图书馆就开始填报，一直弄到下午6点左右，才把前32个志愿确定，中间修修改改一直到26号下午4点，完成了所有的志愿填报</p></blockquote><p><img src="../assets/image-20230802152325590.png" alt="image-20230802152325590"></p><blockquote><p>最后录取到了青岛的那所学校，还是自己想去的地方，而且也不是省外，还好吧，总算也没有报志愿太差，心有所属。</p></blockquote><blockquote><p>自己上大学绝不仅仅是为了一个文凭或者是一个毕业证，而是用自己的知识和能力去改变身边的，尤其是家里的人以幸福，用自己的身份去尽力的带动后边的人，这是自己的一个职责，也是一个非常有挑战性的事情，也能给自己带来非常大的成就感，这不也是自己上这个研究生的意义嘛！！</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 个人遐想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人遐想 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人遐想的巴拉巴拉</title>
      <link href="/project/posts/f22dcf59.html"/>
      <url>/project/posts/f22dcf59.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>windows访问Redis的磕磕绊绊的一路</title>
      <link href="/project/posts/e1ba1f.html"/>
      <url>/project/posts/e1ba1f.html</url>
      
        <content type="html"><![CDATA[<h1>windows访问Redis的磕磕绊绊的一路</h1><blockquote><p>22 号开始学习redis  数据库 (周六)</p><p>23号 初次尝试远程链接redis 结果失败（周天）</p><p>24（上午写周报 + 报志愿） 25 报志愿+ （晚上redis 再次尝试）  26 号 +志愿结束+修改+锻炼+彻底放弃</p></blockquote><p>中间尝试过:</p><h3 id="重装redis-4次以上">重装redis 4次以上</h3><blockquote><p>重新把redis进行安装 并更换启动方式</p></blockquote><h3 id="使用313服务器-重装后-在使用-redis-7-更换版本">使用313服务器 重装后 在使用 redis 7 更换版本</h3><h3 id="使用315-服务器-重装-换版本">使用315 服务器 重装 + 换版本</h3><p>315 服务器的gcc 是9.2 版本  Ubuntu是 18.04版本  不知道为啥老是自己在gcc编译的时候出现问题</p><blockquote><p>redis 安装的坑</p><blockquote><p><a href="https://www.finclip.com/news/f/32103.html">https://www.finclip.com/news/f/32103.html</a></p><p><a href="https://blog.csdn.net/She_lock/article/details/80583234">https://blog.csdn.net/She_lock/article/details/80583234</a></p><p><a href="https://blog.csdn.net/PPlluuttoo/article/details/124533541">https://blog.csdn.net/PPlluuttoo/article/details/124533541</a></p><p><a href="https://www.cnblogs.com/feinian/p/9825232.html">https://www.cnblogs.com/feinian/p/9825232.html</a></p></blockquote><blockquote><p>connect error</p><p><a href="https://developer.aliyun.com/article/1230691">https://developer.aliyun.com/article/1230691</a></p><p><a href="https://blog.csdn.net/qq_36934544/article/details/85118866">https://blog.csdn.net/qq_36934544/article/details/85118866</a></p></blockquote></blockquote><blockquote><p>防火墙的问题</p><p>Failed to stop firewalld.service: Unit firewalld.service not loaded</p><p>防火墙启动失败“Failed to start firewalld - dynamic firewall daemon”</p><p><a href="https://cloud.tencent.com/developer/article/2020770">https://cloud.tencent.com/developer/article/2020770</a></p><p><a href="https://blog.51cto.com/u_12208527/3653812">https://blog.51cto.com/u_12208527/3653812</a></p><p><a href="https://blog.csdn.net/qq_59311764/article/details/121968376">https://blog.csdn.net/qq_59311764/article/details/121968376</a></p><p><a href="https://blog.csdn.net/qq_59311764/article/details/121968376">https://blog.csdn.net/qq_59311764/article/details/121968376</a></p><blockquote><p>网络错误</p><p>getaddrinfo ENOTFOUND是什么意思</p><p><a href="https://cloud.tencent.com/developer/ask/sof/425410">https://cloud.tencent.com/developer/ask/sof/425410</a></p><p><a href="https://blog.csdn.net/cnds123321/article/details/122957218">https://blog.csdn.net/cnds123321/article/details/122957218</a></p></blockquote></blockquote><h5 id="linux常用操作">linux常用操作</h5><blockquote><p>以下关于Linux服务器重启命令写的比较详细，如果你着急重启服务器，可以直接使用重启命令：<strong>shutdown -r now或者reboot</strong>。</p><p>重启: <a href="https://cloud.tencent.com/developer/article/1869022">https://cloud.tencent.com/developer/article/1869022</a></p></blockquote><h4 id="Ubuntu-18-04-独有的问题">Ubuntu 18.04 独有的问题</h4><blockquote><p>在使用Ubuntu18.04  使用的是gcc9.2  想把它升级成gcc9.4  因为这样的话 是可以正常运行的，结果使用yam进行安装的时候，一直不行报各种错误</p></blockquote><blockquote><p>错误记录</p><blockquote><p>Ubuntu系统下：libnvidia-container. Please verify its path and try again</p></blockquote><blockquote><p>Ubuntu系统下：Cannot retrieve repository metadata (repomd.xml) for repository: libnvidia-container. Please verify its path and try again</p><p><a href="https://blog.csdn.net/Young_Black/article/details/108997400">https://blog.csdn.net/Young_Black/article/details/108997400</a></p><p><a href="https://www.jianshu.com/p/1671396e9b02">https://www.jianshu.com/p/1671396e9b02</a></p><p><a href="https://blog.csdn.net/Young_Black/article/details/108997400">https://blog.csdn.net/Young_Black/article/details/108997400</a></p><p><a href="https://blog.csdn.net/Young_Black/article/details/108997400">https://blog.csdn.net/Young_Black/article/details/108997400</a></p></blockquote><blockquote><p>Ubuntu系统yum 更新源</p><p><a href="https://www.jianshu.com/p/1671396e9b02">https://www.jianshu.com/p/1671396e9b02</a></p><h5 id="Linux-Ubuntu系统-安装yum及源的更新-详细操作-文字描述！！！">Linux(Ubuntu系统)安装yum及源的更新(详细操作+文字描述！！！)</h5><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/%E3%80%90%E6%B8%85%E5%8D%8E%E6%BA%90%E3%80%91">https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/【清华源】</a></p><p>更新yamU源头https://blog.csdn.net/wade3015/article/details/94494929</p><p><a href="https://blog.csdn.net/cuicui_ruirui/article/details/107150946">https://blog.csdn.net/cuicui_ruirui/article/details/107150946</a></p></blockquote><blockquote><p>正在读取状态信息… 完成<br>E: 无法定位软件包 epel-release</p><p><a href="https://blog.csdn.net/qq_42178122/article/details/120429661">https://blog.csdn.net/qq_42178122/article/details/120429661</a></p><p><a href="https://blog.csdn.net/Abvcw_13579/article/details/101733515">https://blog.csdn.net/Abvcw_13579/article/details/101733515</a></p></blockquote><blockquote><p>Ubuntu系统下使用yum install 报错</p></blockquote><blockquote><p><a href="https://nvidia.github.io/libnvidia-container/centos7/x86_64/repodata/repomd.xml:">https://nvidia.github.io/libnvidia-container/centos7/x86_64/repodata/repomd.xml:</a> [Errno 14] curl#77 - “Problem with the SSL CA cert (path? access rights?)”<br>尝试其他镜像</p></blockquote><blockquote><p>编译redis文件时候LINK redis-server报错</p><p><a href="https://www.dbs724.com/310901.html">https://www.dbs724.com/310901.html</a></p></blockquote><p><a href="https://blog.csdn.net/huoxingrenhdh/article/details/105186054">https://blog.csdn.net/huoxingrenhdh/article/details/105186054</a></p><p><a href="https://cloud.tencent.com/developer/article/2018839">https://cloud.tencent.com/developer/article/2018839</a></p><p><a href="http://blog.itpub.net/70003733/viewspace-2898091/">http://blog.itpub.net/70003733/viewspace-2898091/</a></p><p><a href="https://blog.csdn.net/Young_Black/article/details/108997400">https://blog.csdn.net/Young_Black/article/details/108997400</a></p><blockquote><p>18.04 nvidia driver / couldn’t find <a href="http://libnvidia-ml.so">libnvidia-ml.so</a> library in your system.</p></blockquote></blockquote><p><img src="../assets/image-20230802181836749.png" alt="image-20230802181836749"></p><blockquote><p>/usr/bin/ld: 最后的链结失败: 输出不可表示的节==超出认知==</p><p><a href="https://forum.ubuntu.com.cn/viewtopic.php?t=488500">ubuntu18.04 最后的链结失败: 输出不可表示的节 - Ubuntu中文论坛</a></p><p><img src="../assets/image-20230726213220477.png" alt="image-20230726213220477"></p><p><img src="../assets/image-20230726213243509.png" alt="image-20230726213243509"></p><p><img src="../assets/image-20230726213502306.png" alt="image-20230726213502306"></p><p>==Ubuntu16.10 最后的链结失败: 输出不可表示的节 #90== ==超出认知==</p><p><a href="https://github.com/apache/brpc/issues/90">Ubuntu16.10 最后的链结失败: 输出不可表示的节 · Issue #90 · apache/brpc · GitHub</a></p><p><img src="../assets/image-20230726213324430.png" alt="image-20230726213324430"></p></blockquote><h4 id="尝试过的方案">尝试过的方案</h4><blockquote><p>重装yum  或者更换yum的源 尝试</p><p>重装gcc版本 因为在313 服务器上 使用gcc 9.4  可以直接make 编译通过</p></blockquote><h3 id="使用非root-账号尝试">使用非root 账号尝试</h3><h3 id="重启电脑使用两个版本的redis-manage-客户端重连尝试（使用自立的电脑重新尝试-）">重启电脑使用两个版本的redis manage 客户端重连尝试（使用自立的电脑重新尝试  ）</h3><h3 id="自己推测问题">自己推测问题</h3><blockquote><p>碰到的问题应该是超出了自己的认知范围，盲猜一波的话 要么是服务器的配置问题</p><p>要么是redis 的网络配置问题 自己连接错误</p><p>自己电脑代理问题</p><p>服务器的redis 就不能远端连接</p></blockquote><h2 id="conclusion">conclusion</h2><p>曾经做过的努力</p><ol><li>用现有的 jedispool 连接池的方法  自己尝试加上ssh  但是一直报错  显示bean对象错误【项目工程   】<img src="../assets/image-20230731101823003.png" alt="image-20230731101823003"><img src="../assets/image-20230731101757534.png" alt="image-20230731101757534"></li><li></li></ol><p>现有的知识 和认知 【还不足以帮助自己实现对redis实现ssh链接的操作  】</p><blockquote><p>三个方向</p></blockquote><ol><li><p>忘记了@bean 以及其他的配置的过程  一些注解</p></li><li><p>不知道springboot的底层原理 怎么可能直接给springboot实现底层配置redis</p></li><li><p>redis 底层怎样写入的springboot 也不知道 以及不知道redis 更深入的应用</p></li></ol><h2 id="柳暗花明又一村">柳暗花明又一村</h2><blockquote><p>在经过群友的帮助和指导，学习了很多的知识，也发现了自己的很多的盲区</p></blockquote><p><img src="../assets/image-20230805141755800.png" alt="image-20230805141755800"></p><p><img src="../assets/image-20230805141813226.png" alt="image-20230805141813226"></p><blockquote><p>最终，找到了一个新的redis链接工具，使用ssh链接，成功的链接上</p></blockquote><blockquote><p>工具介绍</p><h1>Another Redis DeskTop Manage一款免费的Redis可视化工具</h1><p><a href="https://blog.csdn.net/biexiansheng/article/details/116308285">https://blog.csdn.net/biexiansheng/article/details/116308285</a></p><p><a href="https://zhuanlan.zhihu.com/p/473249370">https://zhuanlan.zhihu.com/p/473249370</a></p></blockquote><p>redis 学习【菜鸟教程】 <a href="https://www.runoob.com/redis/redis-strings.html">https://www.runoob.com/redis/redis-strings.html</a></p><blockquote><p>Maven链接数据库失败【因为数据库在pom.xml 文件中配置错误】</p><blockquote><p>Could not find artifact com.mysql:mysql-connector-java:pom:8.0.33 in alimaven</p><p><a href="https://blog.csdn.net/kkk990101/article/details/131138845">https://blog.csdn.net/kkk990101/article/details/131138845</a></p><p><a href="https://stackoverflow.com/questions/51937048/maven-not-able-to-download-mysql-connector-8-0-12-from-central">https://stackoverflow.com/questions/51937048/maven-not-able-to-download-mysql-connector-8-0-12-from-central</a></p><p><a href="https://blog.csdn.net/m0_37149062/article/details/129788103">https://blog.csdn.net/m0_37149062/article/details/129788103</a></p><p><img src="../assets/image-20230805145207725.png" alt="image-20230805145207725"></p></blockquote><p>Could not find artifact org.springframework.boot:spring-boot-starter-parent:pom:3.1.2.RELEASE in alimaven</p><blockquote><p><a href="https://blog.csdn.net/weixin_50843918/article/details/129774024">https://blog.csdn.net/weixin_50843918/article/details/129774024</a></p></blockquote><p>SpringBoot配置mysql时候设定useUnicod=true</p><blockquote><p><a href="https://blog.csdn.net/qq_40321119/article/details/102701935">https://blog.csdn.net/qq_40321119/article/details/102701935</a></p><p><a href="https://blog.csdn.net/weixin_42513632/article/details/113164019">https://blog.csdn.net/weixin_42513632/article/details/113164019</a></p></blockquote><p>异步跳转ajax【学习异步请求】</p><blockquote><p><a href="https://www.runoob.com/ajax/ajax-tutorial.html">https://www.runoob.com/ajax/ajax-tutorial.html</a></p></blockquote><h1>Invalid bound statement (not found)出现原因和解决方法</h1><p><a href="https://blog.csdn.net/weixin_44695793/article/details/107752054">https://blog.csdn.net/weixin_44695793/article/details/107752054</a></p><h1>解决org.apache.ibatis.binding.BindingException: Invalid bound statement (not found)问题</h1><blockquote><p><a href="https://blog.csdn.net/sundacheng1989/article/details/81630370">https://blog.csdn.net/sundacheng1989/article/details/81630370</a></p></blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis</title>
      <link href="/project/posts/7b25d017.html"/>
      <url>/project/posts/7b25d017.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程内容来自网盘</p><p><img src="../assets/image-20230722085444655.png" alt="image-20230722085444655"></p></blockquote><h1>Redis 学习</h1><h2 id="第二章-快速入门">第二章 快速入门</h2><h3 id="1-章节介绍">1.章节介绍</h3><p><img src="../assets/image-20230722103924314.png" alt="image-20230722103924314"></p><h4 id="学习目标">学习目标</h4><blockquote><p>学习Redis的各种数据类型</p><p>学习Redis的存储机制【存储在内存中  减少传统关系数据库的磁盘IO 单线程为基础 提供了事务 锁 等机制】</p><p>安装Redis（Linux环境）并使用</p><p>Redis提供的各种模式：</p><p>​并发10W一下  单机单节点模式【读10w写8w 】</p><p>​并发10-20W    主从模式 使用哨兵保证主节点高可用</p><p>​    最终模式  集群模式</p><p>学习使用项目驱动 构建微服务模式 使用微服务模式+SpringBoot开发</p></blockquote><h4 id="理论知识">理论知识</h4><blockquote><p>对比各种缓存中间件</p><p>了解redis的基本概念 是什么 干嘛用的 解决了什么问题</p><p>微服务相关概念</p></blockquote><h4 id="实操">实操</h4><blockquote><p>Redis 安装+启动以及使用</p><p>redis基本命令+数据结构</p><p>Redis以缓存的方式缓存关系型数据库中的数据【设计表结构】</p><p>SpringBoot +SpringCloud工程搭建</p></blockquote><h3 id="2-Redis-介绍">2. Redis 介绍</h3><p><img src="../assets/image-20230722104905038.png" alt="image-20230722104905038"></p><h4 id="Redis特点与优势">Redis特点与优势</h4><blockquote><p>redis 提供事务支持 以及可以实现分布式操作，并提供发布/订阅  通知 key过期等特性</p></blockquote><blockquote><p>Redis 持久化 RDB  AOF两种机制</p><p>单节点下读写速度较快</p><p>各种场景的数据类型</p><p>高可用 帮助数据备份 高可用主从分离</p><ol><li>高可用的主从模式<ol><li>单节点模式<img src="../assets/image-20230722105336221.png" alt="image-20230722105336221"></li><li>主从模式<img src="../assets/image-20230722105315009.png" alt="image-20230722105315009"></li></ol></li><li>哨兵模式帮助选取主节点防止主节点失效导致不能进行写操作<img src="../assets/image-20230722105253075.png" alt="image-20230722105253075"></li></ol></blockquote><blockquote><p>单节点模式下 使用一个节点进行数据读写，当数据量变大的时候，会导致节点速度变慢，所以引进主从模式，使用主节点来进行写操作，使用一个命令控制从节点只进行写操作，这样就可以实现读写分离。同时主节点也可以进行写操作，只不过大部分写操作由从节点完成</p></blockquote><blockquote><p>同时防止主节点在出现问题的时候不能进行写操作，所以就是用了哨兵模式，借助多个哨兵来进行监控主节点。</p><p>这里为了防止网络波动导致哨兵误判，将正常运行的主节点误判为问题节点，使用多个哨兵（奇数个，当认为是问题节点的哨兵数量大于1/2 的时候，会从从节点中重新选择主节点来替代当前主节点）</p></blockquote><blockquote><p>每次在主节点中和从节点中都存储着相同的数据，使用redis提供的==集群==来进行解决</p></blockquote><h4 id="Ehcache-Memcached与-redis三种缓存的比较">Ehcache   Memcached与 redis三种缓存的比较</h4><p><img src="../assets/image-20230722110056723.png" alt="image-20230722110056723"></p><blockquote><p>Ehcache 不能提供分布式操作  不能实现 分布式项目下的缓存同步共享问题</p><p>Memcached 支持的客户端语言和 支持的数据类型较少</p></blockquote><h3 id="3-Redis-高并发">3.Redis 高并发</h3><h4 id="原理">原理</h4><p><img src="../assets/image-20230722112009667.png" alt="image-20230722112009667"></p><blockquote><p>非阻塞IO  使用的IO多路复用 进行线程保证</p><p>原子性 单线程 是优点 但也是缺点 因为现在的cpu 都是多核的</p></blockquote><h4 id="单线程">单线程</h4><p><img src="../assets/image-20230722114517359.png" alt="image-20230722114517359"></p><h4 id="IO多路复用">IO多路复用</h4><p><img src="../assets/image-20230722114550108.png" alt="image-20230722114550108"></p><blockquote><p>多路复用的目的 就是为了实现高吞吐量，快速的写入和读取</p></blockquote><p>传统情况下 需要耗资建立很多管道</p><p><img src="../assets/image-20230722114647285.png" alt="image-20230722114647285"></p><p>多路复用情况下，只需要建立一个水管，实现各个通路共用即可</p><p><img src="../assets/image-20230722114729086.png" alt="image-20230722114729086"></p><h3 id="4-Redis的安装（6-0-9）">4.Redis的安装（6.0.9）</h3><blockquote><p>使用313 的服务器进行安装，保证服务器能够使用</p><p>313的服务器上有gcc依赖，使用的是9.4.0</p><p><img src="../assets/image-20230722115034588.png" alt="image-20230722115034588"></p></blockquote><p><img src="../assets/image-20230722115045665.png" alt="image-20230722115045665"></p><blockquote><p>下载安装包  链接https://download.redis.io/releases/redis-6.0.9.tar.gz</p><p>先下载到本地 在使用xftp传输上去</p></blockquote><h4 id="Linux文件夹介绍">Linux文件夹介绍</h4><p><img src="../assets/image-20230722131432819.png" alt="image-20230722131432819"></p><p>dev  表示当前服务器设备的存放目录</p><p>etc 安装的软件的一些配置存放目录</p><p>home 是主用户或者自己创建的用户的存放目录</p><p>opt 一些有版权的软件的存放目录</p><p>usr 是unix下的软件的存放目录</p><p>var 是软件运行时候临时数据的存放目录</p><h4 id="Linux下tar文件解压">Linux下tar文件解压</h4><blockquote><p>tar --help 命令进行解压</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">tar</span> -xzvf 文件名</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230722151046008.png" alt="image-20230722151046008"></p><p><img src="../assets/image-20230722151031849.png" alt="image-20230722151031849"></p><p><img src="../assets/image-20230722151008703.png" alt="image-20230722151008703"></p><p><img src="../assets/image-20230722151101512.png" alt="image-20230722151101512"></p></blockquote><h4 id="安装">安装</h4><p>Linux 下文件有两种文件安装方式</p><ol><li>使用绿色文件，下载后解压 可以直接使用（Tomcat）</li><li>下载安装包后，设定好依赖文件经过编译安装后才可以使用（Redis）</li></ol><p><img src="../assets/image-20230722131330569.png" alt="image-20230722131330569"></p><blockquote><p>Centos7 中的gcc版本比较低，或者其他的环境中gcc版本过低的话，会导致安装失败，所以需要在安装之前保证依赖环境，升级gcc版本到版本9</p></blockquote><blockquote><p>将redis 安装在/usr/local/redis目录中，需要的安装包以及资源存放在 /usr/local/src/redis6 中</p></blockquote><p>经过上边的解压之后，需要自己进入解压文件夹中进行make 编译一下【编译之前需要先确定好gcc的版本 是高版本（9.0） 如果比较低 会编译失败】  可能需要先对gcc 进行升级处理</p><p><img src="../assets/image-20230722151308007.png" alt="image-20230722151308007"></p><blockquote><p>预编译的过程 是对改安装文件需要的环境进行测试， 如果环境出现问题 会给出对应的缺失信息 和提醒</p></blockquote><p><img src="../assets/image-20230722151352559.png" alt="image-20230722151352559"></p><blockquote><p>编译之后，可以使用make  install 进行安装</p><p>但是直接用make install 可能会安装到默认路径中，在这一步 需要自己提供安装环境，对安装目录进行指定，来保证能够后期进行特定的配置</p></blockquote><p><img src="../assets/image-20230722151653797.png" alt="image-20230722151653797"></p><blockquote><p>make 后边用 PREFIX 进行制定安装目录 并进行安装</p></blockquote><p>安装之后 可以直接进入看生成一个bin目录并进行解析，里边有很多的可执行文件</p><h4 id="可执行文件介绍">可执行文件介绍</h4><p><img src="../assets/image-20230722152009735.png" alt="image-20230722152009735"></p><h4 id="版本发布说明">版本发布说明</h4><p><img src="../assets/image-20230722132006581.png" alt="image-20230722132006581"></p><blockquote><p>2.6 版本 可以实现键的过期时间确定，同时可以保证从节点实现只读功能不用承担写入数据功能</p></blockquote><p><img src="../assets/image-20230722132703113.png" alt="image-20230722132703113"></p><blockquote><p>2.8 版本可以实现bind白名单同时多个ip地址进行访问</p><p>完善哨兵机制  以及实现发布订阅功能</p></blockquote><p><img src="../assets/image-20230722132819632.png" alt="image-20230722132819632"></p><blockquote><p>3.0 实现 分布式功能，增加分布式技术（集群模式）</p><p>3.2  增加GEO(可以实现附近的人 摇一摇 等功能)List数据类型</p></blockquote><p><img src="../assets/image-20230722133029847.png" alt="image-20230722133029847"></p><blockquote><p>4.0 新增LFU算法功能，同时提供了RDB - AOF</p><p>5.0新增stream数据类型</p><p>6.0 实现多线程（最终根本操作还是由单线程实现，还是原则操作）</p></blockquote><h4 id="启动redis">启动redis</h4><h5 id="前台启动">前台启动</h5><p>直接使用./redis-server 在命令行中启动即可</p><blockquote><p>使用命令行进行启动的时候，可能命令行被redis服务完全占用，不能执行其他命令</p></blockquote><p><img src="../assets/image-20230722152239016.png" alt="image-20230722152239016"></p><h5 id="后台启动">后台启动</h5><blockquote><p>从配置出发，redis以守护进程的形式进行运行</p></blockquote><p>redis 在启动的时候 默认加载的是 在解压文件路径下的redis.conf 文件</p><p><img src="../assets/image-20230722153259648.png" alt="image-20230722153259648"></p><p>这里将redis.conf 进行复制 复制到/usr/local/redis/bin 目录下 每次在后台启动 都是用该目录下修改的redis.conf 进行启动</p><blockquote><p>更改文件的时候 都要先复制一份再进行修改，防止修改失败导致软件不能使用</p></blockquote><blockquote><p>vim 命令行 使用vim 后 用:set nu 可以看到所有的文件的行数，同时按shift+g 可以跳转到vim编辑的文件的最后一行</p></blockquote><p>修改命令行 将守护进程启动的方式打开 并以守护进程的方式启动</p><p><img src="../assets/image-20230722153727109.png" alt="image-20230722153727109"></p><p>将 daemonize 改为yes</p><p>在启动的时候使用修改好的配置文件进行启动就可以了</p><p>在bin目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-server ./redis.conf</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230722154514674.png" alt="image-20230722154514674"></p><blockquote><p>上边命令行在修改完成redis.conf 之后 直接启动了redis服务 并查看redis 的服务进程 可以看到对应的进程启动</p></blockquote><p><img src="../assets/image-20230722154605820.png" alt="image-20230722154605820"></p><p>使用kill -9 +端口号 皆可以进行关闭</p><h4 id="添加为系统启动">添加为系统启动</h4><p><img src="../assets/image-20230722154655748.png" alt="image-20230722154655748"></p><p><img src="../assets/image-20230722154859699.png" alt="image-20230722154859699"></p><p>修改redis.server 后 重启系统服务</p><p><img src="../assets/image-20230722154948403.png" alt="image-20230722154948403"></p><blockquote><p>在linux 下复制是ctrl+insert  粘贴 是shift+insert</p></blockquote><p><img src="../assets/image-20230722154811854.png" alt="image-20230722154811854"></p><p>ExecStart 是指的是在启动的时候 启动哪个服务 用什么配置进行启动 都是要以绝对路径进行配置</p><h3 id="5-Redis的配置">5.Redis的配置</h3><h4 id="redis-配置项">redis 配置项</h4><p><img src="../assets/image-20230723150111028.png" alt="image-20230723150111028"></p><blockquote><p>后台启动 daemonize  是否以守护进程的形式启动redis</p><p>bind 白名单   所有节点都可以访问 配置成 0.0.0.0（企业的时候会限定）</p><p>redis 默认是16个数据库连接</p><p>save 持久化配置项</p><p>dir 配置持久化文件存储路径</p></blockquote><h4 id="windows-连接linux-上的redis">windows 连接linux 上的redis</h4><blockquote><ol><li><p>需要先开启linux 上的redis server服务</p><ol><li><img src="../assets/image-20230723151354480.png" alt="image-20230723151354480"></li></ol></li><li><p>关闭掉linux 防火墙</p><ol><li><img src="../assets/image-20230723150528275.png" alt="image-20230723150528275"></li><li><img src="../assets/image-20230723151202346.png" alt="image-20230723151202346"></li></ol></li><li><p>修改配置文件，开放对应的链接权限<img src="../assets/image-20230723152556106.png" alt="image-20230723152556106"></p><ol><li><blockquote><p>修改 bind 权限对外全部开放</p><p>在企业中是不会这样设定的 会只开放部分对外开放【防止被黑客给攻破随意对redis服务器所有的数据进行修改或者攻破后获取管理员用户权限 来占用整个服务器】</p></blockquote></li></ol></li><li><p>重启redis服务<img src="../assets/image-20230723152943734.png" alt="image-20230723152943734"></p></li></ol></blockquote><blockquote><h3 id=""></h3></blockquote><p>==redis==</p><p>==Mysql==</p><p>==rocketMQ-&gt;bilibili==</p><p>==dormitory==</p><p>分布式 集群设计 了解完了</p><blockquote><p>歪打正着 正好尝试出了链接的过程</p></blockquote><p><img src="../assets/image-20230727192657201.png" alt="image-20230727192657201"></p><p>仿哔站  加密算法</p><p>对称加密</p><p>生产者 消费者 +rocketMQ +redis</p><h3 id="7-Springboot-导入Reids-环境初始化">7. Springboot 导入Reids +环境初始化</h3><h4 id="初始化springboot项目">初始化springboot项目</h4><p><img src="../assets/image-20230727194059881.png" alt="image-20230727194059881"></p><p>设定项目名 001</p><p><img src="../assets/image-20230727194036885.png" alt="image-20230727194036885"></p><blockquote><p>redis 依赖选择</p><p><img src="../assets/image-20230727194149158.png" alt="image-20230727194149158"></p></blockquote><p>springweb</p><p><img src="../assets/image-20230727194233034.png" alt="image-20230727194233034"></p><blockquote><p>最后选择的东西<img src="../assets/image-20230727194550805.png" alt="image-20230727194550805"></p></blockquote><p>项目信息<img src="../assets/image-20230727194617237.png" alt="image-20230727194617237"></p><h3 id="8-需求分析与数据库设计">8.需求分析与数据库设计</h3><h4 id="8-1-需求分析">8.1 需求分析</h4><blockquote><p>只挑选能够展现redis功能的需求进行完成</p></blockquote><p><img src="../assets/image-20230731150038730.png" alt="image-20230731150038730"></p><p><img src="../assets/image-20230731150338646.png" alt="image-20230731150338646"></p><blockquote><p>用户单点登录  每一个用户都要检验是否登录 这里使用单点登录，只需要在一个服务器上登陆后，携带这校验信息，在其他服务器上再次登录就可以直接校验实现免登录功能</p></blockquote><blockquote><p>社交好友功能，好友之间相互交流 关注与取关，并借助feed流来实现交流</p></blockquote><blockquote><p>在订单和优惠券上实现秒杀功能，使用分布式锁 来进行，保证用户并发</p></blockquote><blockquote><p>使用缓存将经常访问的热点数据进行保存，提高系统性能</p></blockquote><blockquote><p>在用户登录的ip地址周围设定热点排行榜，并保证用户可以和附近的人进行沟通交流，同时保证热点餐厅的访问量，增加美食销售量</p></blockquote><h4 id="8-2-数据库设计">8.2 数据库设计</h4><p><img src="../assets/image-20230731151116674.png" alt="image-20230731151116674"></p><p><img src="../assets/image-20230731151323561.png" alt="image-20230731151323561"></p><blockquote><p>食客表（账号是否注销可用）</p><p>feed流</p><p>关注表</p><p>语言表【中英双语】</p><p>代金券表</p><p>交易表【代金券使用情况】</p><p>餐厅表</p></blockquote><h3 id="9-项目架构与微服务搭建">9.项目架构与微服务搭建</h3><blockquote><p>一个大项目被拆分成若干个子模块，每一个模块就是一个服务</p><p>这里单体项目因为耦合度太高，导致开发功能需要对整个项目要进行把握，所以使用微服务反而会帮助深入理解各个功能，从而简化开发</p></blockquote><p><img src="../assets/image-20230731151538289.png" alt="image-20230731151538289"></p><h4 id="9-1整体架构">9.1整体架构</h4><p><img src="../assets/image-20230731151818369.png" alt="image-20230731151818369"></p><blockquote><p>整个服务真的复杂，并且学到的东西很多</p></blockquote><h4 id="9-2-模块联系以及整体把握">9.2 模块联系以及整体把握</h4><blockquote><p>在一开始 使用不同的端口（app 网页 或者平板）登录发出请求后，使用nginx使用反向代理进行转发，</p><p>网关管理所有的服务的对外接口，来进行处理</p><p>使用注册中心管理所有的微服务地址，同时使用配置中心在更改一个小配置的时候不需要重启所有项目</p><p>使用mysql 来进行数据存储并使用主从数据库，同时使用redis实现缓存，并实现一些小功能</p><p>最后使用服务监控模块来监管所有的服务运行情况</p></blockquote><h4 id="9-3-基础微服务项目搭建">9.3 基础微服务项目搭建</h4><blockquote><p>搭建一个最基础的 用户服务 网关 以及注册服务</p></blockquote><h5 id="1-父模块搭建">1.父模块搭建</h5><ol><li>使用maven进行构建</li></ol><p><img src="../assets/image-20230731152656218.png" alt="image-20230731152656218"></p><p>pom.xml 文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 可以集中定义依赖资源的版本信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spring-boot-version</span>&gt;</span>2.3.5.RELEASE<span class="tag">&lt;/<span class="name">spring-boot-version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spring-cloud-version</span>&gt;</span>Hoxton.SR8<span class="tag">&lt;/<span class="name">spring-cloud-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--简化实体类开发 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lombok-version</span>&gt;</span>1.18.16<span class="tag">&lt;/<span class="name">lombok-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--commons-lang工具包 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">commons-lang-version</span>&gt;</span>3.11<span class="tag">&lt;/<span class="name">commons-lang-version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mybatis-starter-version</span>&gt;</span>2.1.3<span class="tag">&lt;/<span class="name">mybatis-starter-version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">mysql-version</span>&gt;</span>8.0.22<span class="tag">&lt;/<span class="name">mysql-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--UI界面的查看--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">swagger-starter-version</span>&gt;</span>2.1.5-RELEASE<span class="tag">&lt;/<span class="name">swagger-starter-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--工具集包--&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">hutool-version</span>&gt;</span>5.4.7<span class="tag">&lt;/<span class="name">hutool-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Google的guava--&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">guava-version</span>&gt;</span>20.0<span class="tag">&lt;/<span class="name">guava-version</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--maven打包编译--&gt;</span>       </span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>pom.xml 全部文件</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>food-social-contact-parent<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myfood<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>ms-registry<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 可以集中定义依赖资源的版本信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spring-boot-version</span>&gt;</span>2.3.5.RELEASE<span class="tag">&lt;/<span class="name">spring-boot-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spring-cloud-version</span>&gt;</span>Hoxton.SR8<span class="tag">&lt;/<span class="name">spring-cloud-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lombok-version</span>&gt;</span>1.18.16<span class="tag">&lt;/<span class="name">lombok-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">commons-lang-version</span>&gt;</span>3.11<span class="tag">&lt;/<span class="name">commons-lang-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mybatis-starter-version</span>&gt;</span>2.1.3<span class="tag">&lt;/<span class="name">mybatis-starter-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mysql-version</span>&gt;</span>8.0.22<span class="tag">&lt;/<span class="name">mysql-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">swagger-starter-version</span>&gt;</span>2.1.5-RELEASE<span class="tag">&lt;/<span class="name">swagger-starter-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">hutool-version</span>&gt;</span>5.4.7<span class="tag">&lt;/<span class="name">hutool-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">guava-version</span>&gt;</span>20.0<span class="tag">&lt;/<span class="name">guava-version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 集中定义依赖，不引入 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- spring boot 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-dependencies<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spring-boot-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">type</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>import<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- spring cloud 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-dependencies<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spring-cloud-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">type</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">scope</span>&gt;</span>import<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- lombok 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;lombok-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- common-lang3 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang3<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;commons-lang-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- mybatis 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mybatis-starter-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- swagger 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.battcn<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>swagger-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;swagger-starter-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- mysql 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;mysql-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- hutool 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.hutool<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hutool-all<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hutool-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- guava 依赖 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;guava-version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 集中定义项目所需插件 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- spring boot maven 项目打包插件 --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>删除src 文件夹，在父工程中创建不同的子项目</li></ol><h5 id="2-register模块搭建">2.register模块搭建</h5><p><img src="../assets/image-20230731154640351.png" alt="image-20230731154640351"></p><p><img src="../assets/image-20230731154817960.png" alt="image-20230731154817960"></p><p><img src="../assets/image-20230731154828023.png" alt="image-20230731154828023"></p><h6 id="1-pom-xml文件">1.pom.xml文件</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myfood<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>food-social-contact-parent<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ms-registry<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="2-register的yml文件">2.register的yml文件</h6><blockquote><p>编写register的yml文件</p></blockquote><p><img src="../assets/image-20230731155003954.png" alt="image-20230731155003954"></p><blockquote><p>设置子项目的yml配置文件</p></blockquote><p><img src="../assets/image-20230731155630560.png" alt="image-20230731155630560"></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line"><span class="comment">#  项目开放端口为8080</span></span><br><span class="line"><span class="comment">#  子项目名称为 ms-register</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">ms-register</span></span><br><span class="line"><span class="comment">#配置 Eureka Server 注册中心</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line">    <span class="attr">register-with-eureka:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">fetch-registry:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#   上边两个配置的意思[是否开启自我注册 自我发现]</span></span><br><span class="line"><span class="comment">#   单节点的时候 设置为false否则会报错 多节点的时候 要删除上边配置 使用默认配置true</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="attr">defaultZone:</span> <span class="string">http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      子项目对外开放的地址是http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      项目注册使用上边地址进行注册</span></span><br></pre></td></tr></table></figure><h6 id="3-配置启动类">3.配置启动类</h6><p><img src="../assets/image-20230731155819227.png" alt="image-20230731155819227"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.registry;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;</span><br><span class="line"></span><br><span class="line"><span class="comment">//激活注册中心的组件</span></span><br><span class="line"><span class="meta">@EnableEurekaServer</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RegistryApplication</span> &#123;</span><br><span class="line"><span class="comment">//    psvm</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">            SpringApplication.run(RegistryApplication.class,args);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731161220223.png" alt="image-20230731161220223"></p><blockquote><p>使用本地配置的http://localhost:8080/  可以看到可视化的服务界面</p><p>可以访问Eureka 提供的注册中心的界面，在这个界面中可以直观的看到我们注册的服务 后边编写的服务就可以直接注册到这里边去</p></blockquote><p><img src="../assets/image-20230731161302843.png" alt="image-20230731161302843"></p><h5 id="3-网关gateway模块搭建">3.网关gateway模块搭建</h5><blockquote><p>所有的请求都会经过 网关，由网关来做路由转发的功能</p></blockquote><blockquote><p>使用网关来实现路由转发的功能</p></blockquote><h6 id="1-新建一个ms-gateway-网关模块">1.新建一个ms-gateway 网关模块</h6><p><img src="../assets/image-20230731161628153.png" alt="image-20230731161628153"></p><h6 id="2-pom-xml">2.pom.xml</h6><p><img src="../assets/image-20230731162106821.png" alt="image-20230731162106821"></p><blockquote><p>这里引入了两个依赖，一个是网关的配置依赖，另一个是eureka的==客户端依赖== 保证网关作为服务的客户端注入到注册中心去</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myfood<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>food-social-contact-parent<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ms-gateway<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-gateway<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--        网关需要作为服务的客户端注册到我们注册中心去，所以需要添加eureka-server 去--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="3-网关的yml文件">3.网关的yml文件</h6><blockquote><p>网关的yml文件 可以直接从register的配置文件复制即可</p></blockquote><blockquote><p>这里对网关的yml文件做了更改 设定端口为80  并且去掉了其他的亮相配置，只需要设定注册中心地址即可</p></blockquote><p><img src="../assets/image-20230731162844051.png" alt="image-20230731162844051"></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line"><span class="comment">#  项目开放端口为80</span></span><br><span class="line"><span class="comment">#  子项目名称为 ms-register</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">ms-gateway</span></span><br><span class="line"><span class="comment">#配置 Eureka Server 注册中心</span></span><br><span class="line"><span class="comment">############ 这里将register的其他配置项去掉</span></span><br><span class="line"><span class="comment">############ 因为这里的网关只是一个客户端，只需要知道注册中心对外开放的地址即可，不需要设定其他项</span></span><br><span class="line"></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line"><span class="comment">#    register-with-eureka: false</span></span><br><span class="line"><span class="comment">#    fetch-registry: false</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    #   上边两个配置的意思[是否开启自我注册 自我发现]</span></span><br><span class="line"><span class="comment">#    #   单节点的时候 设置为false否则会报错 多节点的时候 要删除上边配置 使用默认配置true</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="attr">defaultZone:</span> <span class="string">http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      子项目对外开放的地址是http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      项目注册使用上边地址进行注册</span></span><br></pre></td></tr></table></figure><h6 id="4-配置启动类">4.配置启动类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.gateway;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.cloud.netflix.eureka.EnableEurekaClient;</span><br><span class="line"></span><br><span class="line"><span class="comment">//@EnableEurekaClient</span></span><br><span class="line"><span class="comment">//在现在版本之后 约定大于继承 ，已经不需要上边的注解来配置设定为客户端了   @EnableEurekaClient</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">GatewayApplication</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(GatewayApplication.class,args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>这里有一个网关的配置项 需要设定配置选项 但是已经设定为约定大于配置 不需要自己手写</p></blockquote><p><img src="../assets/image-20230731163352918.png" alt="image-20230731163352918"></p><blockquote><p>启动网关</p><p><img src="../assets/image-20230731163702975.png" alt="image-20230731163702975"></p><p>注意这里要先启动注册中心的工程，再启动这个网关工程，来保证项目可以注册到注册中心</p></blockquote><blockquote><p>启动网关之前</p></blockquote><p><img src="../assets/image-20230731163750909.png" alt="image-20230731163750909"></p><blockquote><p>启动网关之后</p></blockquote><p><img src="../assets/image-20230731163820836.png" alt="image-20230731163820836"></p><h6 id="5-设定网关ip">5.设定网关ip</h6><blockquote><p>可以方便在注册中心可以直接看到网关的ip地址</p></blockquote><blockquote><p>重新修改一下配置的yml文件 就可以使得在注册中心可以看到网关的ip地址</p></blockquote><p><img src="../assets/image-20230731164427428.png" alt="image-20230731164427428"></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line"><span class="comment">#  项目开放端口为80</span></span><br><span class="line"><span class="comment">#  子项目名称为 ms-register</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">ms-gateway</span></span><br><span class="line"><span class="comment">#配置 Eureka Server 注册中心</span></span><br><span class="line"><span class="comment">############ 这里将register的其他配置项去掉</span></span><br><span class="line"><span class="comment">############ 因为这里的网关只是一个客户端，只需要知道注册中心对外开放的地址即可，不需要设定其他项</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line"><span class="comment">#    将实例注册端口ip的开关开启</span></span><br><span class="line"><span class="comment">#    在配置一下实例的端口id</span></span><br><span class="line">    <span class="attr">prefer-ip-address:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">instance-id:</span> <span class="string">$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125;</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line"><span class="comment">#    register-with-eureka: false</span></span><br><span class="line"><span class="comment">#    fetch-registry: false</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    #   上边两个配置的意思[是否开启自我注册 自我发现]</span></span><br><span class="line"><span class="comment">#    #   单节点的时候 设置为false否则会报错 多节点的时候 要删除上边配置 使用默认配置true</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="attr">defaultZone:</span> <span class="string">http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      子项目对外开放的地址是http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      项目注册使用上边地址进行注册</span></span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731164549340.png" alt="image-20230731164549340"></p><p><img src="../assets/image-20230731164630620.png" alt="image-20230731164630620"></p><h5 id="4-食客-diners-模块搭建">4.食客 diners 模块搭建</h5><h6 id="1-新建模块-ms-diners">1.新建模块 ms-diners</h6><p><img src="../assets/image-20230731191759618.png" alt="image-20230731191759618"></p><h6 id="2-pom-xml-配置">2.pom.xml 配置</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>myfood<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>food-social-contact-parent<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ms-diners<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.cloud<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-cloud-starter-netflix-eureka-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731192150163.png" alt="image-20230731192150163"></p><h6 id="3-配置yml文件">3.配置yml文件</h6><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8081</span></span><br><span class="line"><span class="comment">#  项目开放端口为80</span></span><br><span class="line"><span class="comment">#  子项目名称为 ms-diners</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">ms-diners</span></span><br><span class="line"><span class="comment">#配置 Eureka Server 注册中心</span></span><br><span class="line"><span class="comment">############ 这里将register的其他配置项去掉</span></span><br><span class="line"><span class="comment">############ 因为这里的网关只是一个客户端，只需要知道注册中心对外开放的地址即可，不需要设定其他项</span></span><br><span class="line"><span class="attr">eureka:</span></span><br><span class="line">  <span class="attr">instance:</span></span><br><span class="line"><span class="comment">#    将实例注册端口ip的开关开启</span></span><br><span class="line"><span class="comment">#    在配置一下实例的端口id</span></span><br><span class="line">    <span class="attr">prefer-ip-address:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">instance-id:</span> <span class="string">$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125;</span></span><br><span class="line">  <span class="attr">client:</span></span><br><span class="line"><span class="comment">#    register-with-eureka: false</span></span><br><span class="line"><span class="comment">#    fetch-registry: false</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#    #   上边两个配置的意思[是否开启自我注册 自我发现]</span></span><br><span class="line"><span class="comment">#    #   单节点的时候 设置为false否则会报错 多节点的时候 要删除上边配置 使用默认配置true</span></span><br><span class="line">    <span class="attr">service-url:</span></span><br><span class="line">      <span class="attr">defaultZone:</span> <span class="string">http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      子项目对外开放的地址是http://localhost:8080/eureka/</span></span><br><span class="line"><span class="comment">#      项目注册使用上边地址进行注册</span></span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731192331697.png" alt="image-20230731192331697"></p><h6 id="4-1配置启动类">4.1配置启动类</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.diners;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.cloud.netflix.eureka.EnableEurekaClient;</span><br><span class="line"></span><br><span class="line"><span class="comment">//@EnableEurekaClient</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DinersApplication</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(DinersApplication.class,args);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731192532975.png" alt="image-20230731192532975"></p><h6 id="4-2完善controller层">4.2完善controller层</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.imooc.diners.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Controller;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.ResponseBody;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Controller</span></span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloController</span> &#123;</span><br><span class="line">    <span class="meta">@GetMapping(&quot;/hello&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">hello</span><span class="params">(String name)</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;hello &quot;</span>+name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230731202224109.png" alt="image-20230731202224109"></p><h6 id="5-修改网关yml配置路由">5.修改网关yml配置路由</h6><blockquote><p>先运行注册窗口，在运行网关窗口，最后开启diners工程</p></blockquote><blockquote><p>想要通过网关来进行服务访问的时候，这里需要先对网关的配置文件进行修改</p></blockquote><p><img src="../assets/image-20230731193949669.png" alt="image-20230731193949669"></p><p><img src="../assets/image-20230731193932509.png" alt="image-20230731193932509"></p><blockquote><p>两行的解释:  如下边： 已经将两个服务注册到了注册中心，这里需要通过服务的名称去找到对应服务的ip地址，需要将找到的名称转换成小写，然后通过注册中心的路由找到对应的服务端口</p></blockquote><blockquote><p>==配置路由规则==</p></blockquote><blockquote><p>id 就是对应的服务的名字 见名知意 就叫ms-diners</p><p>uri: lb://ms-diners  // ==负载均衡==  根据设定的名字进行查找</p><p>predicates : -Path=/hello/**   //==这里是配置了一个断言规则 叫hello 当后边diners进行改变的时候也就是服务内容改变的时候 就需要改变对应的断言规则==</p></blockquote><p><img src="../assets/image-20230731194639655.png" alt="image-20230731194639655"></p><h6 id="6-测试结果">6.测试结果</h6><p><img src="../assets/image-20230731195805031.png" alt="image-20230731195805031"></p><blockquote><p>==网关的自我保护机制 触发自我保护算法 要将不可访问的ip地址进行剔除出去==</p></blockquote><blockquote><p>使用8081端口访问食客服务</p><p><img src="../assets/image-20230731202128003.png" alt="image-20230731202128003"></p></blockquote><p><img src="../assets/image-20230731202142593.png" alt="image-20230731202142593"></p><blockquote><p>在修改完成配置文件之后，可以直接通过网关的80端口，进行访问食客服务，直接获取结果</p></blockquote><p><img src="../assets/image-20230731202055771.png" alt="image-20230731202055771"></p><h6 id="7-网关配置项">7.网关配置项</h6><blockquote><p>在使用网关的时候，有一个默认的配置项 就是在网关上向服务器去拉取数据的时候是默认30s进行拉取一次，所以说 如果是快速刷新的时候，需要在刷新之后，等待30秒再进行页面刷新，</p></blockquote><p><img src="../assets/image-20230731200645252.png" alt="image-20230731200645252"></p><h2 id="第三章-redis-的基本数据类型与使用">第三章 redis 的基本数据类型与使用</h2><h3 id="1-Redis基本数据类型">1.Redis基本数据类型</h3><h4 id="1-字符串类型">1.字符串类型</h4><blockquote><p>字符串类型的扩容机制，先申请大内存，并且如果进行修改或者覆盖的时候，大于现有的内存长度 1M,则每次按照1M 大小进行扩容，直至达到所需要的长度位置【扩容最大长度能扩容到512M】</p></blockquote><p><img src="../assets/image-20230731202446854.png" alt="image-20230731202446854"></p><blockquote><p>基本的应用场景和操作指令</p><p>可以使用String存储数字和字符，也可以将数字进行自增或者自减操作等</p></blockquote><p><img src="../assets/image-20230731203018662.png" alt="image-20230731203018662"></p><p><img src="../assets/image-20230804194815797.png" alt="image-20230804194815797"></p><blockquote><p>字符串的扩容机制</p><p>​申请略大空间+ 达到1M扩容+每次扩容的时候1M为单位进行扩容达到的最大容量为512M</p></blockquote><h4 id="2-散列（hashes）">2.散列（hashes）</h4><p><img src="../assets/image-20230804202354461.png" alt="image-20230804202354461"></p><blockquote><p>redis 的ReHash算法不同于java的ReHash过程</p><p>java的ReHash 过程会在扩容的时候，全部将数据进行扩充，然后将hash位置打乱全部重新计算hash值，在用户获取数据的时候，可能会需要等待ReHash完成</p><p>Redis的ReHash 过程会先保留着旧的数据位置，然后一点点渐进式的ReHash ,逐步的在另一个扩容的哈希表中存储旧的数据并保证用户再实时访问的时候能够直接获取数据保证效率</p></blockquote><p><img src="../assets/image-20230804203333926.png" alt="image-20230804203333926"></p><h4 id="3-List">3.List</h4><blockquote><p>List 是一个双向链表，能够从左右两边同时插入数据【左边是lift 操作，右边是right操作】</p></blockquote><p><img src="../assets/image-20230804203401113.png" alt="image-20230804203401113"></p><p><img src="../assets/image-20230804203428557.png" alt="image-20230804203428557"></p><p>插入数据的时候，会把之前插入的数据往中间挤，新插入的数据放到最左边或者最右边</p><p><img src="../assets/image-20230804203639306.png" alt="image-20230804203639306"></p><blockquote><p>命令</p></blockquote><p><img src="../assets/image-20230804203701911.png" alt="image-20230804203701911"></p><blockquote><p>结果</p></blockquote><p><img src="../assets/image-20230804203652542.png" alt="image-20230804203652542"></p><p><img src="../assets/image-20230804203808655.png" alt="image-20230804203808655"></p><p><img src="../assets/image-20230804203818545.png" alt="image-20230804203818545"></p><h4 id="4-集合">4.集合</h4><p><img src="../assets/image-20230804203842570.png" alt="image-20230804203842570"></p><p><img src="../assets/image-20230804203915971.png" alt="image-20230804203915971"></p><blockquote><p>集合去重</p></blockquote><p><img src="../assets/image-20230804203954857.png" alt="image-20230804203954857"></p><blockquote><p>移除数据2  返回&quot;1&quot; 返回表示移除了1条数据</p></blockquote><p><img src="../assets/image-20230804204047505.png" alt="image-20230804204047505"></p><blockquote><p>随机移除数据的时候，返回的数据是数据的值</p></blockquote><p><img src="../assets/image-20230804204150098.png" alt="image-20230804204150098"></p><blockquote><p>set 的 并集 交集 和差集</p></blockquote><p><img src="../assets/image-20230804204338392.png" alt="image-20230804204338392"></p><h4 id="5-有序集合">5.有序集合</h4><p><img src="../assets/image-20230804204455274.png" alt="image-20230804204455274"></p><p><img src="../assets/image-20230804204512831.png" alt="image-20230804204512831"></p><blockquote><p>sorted set  是set 和 hash 的结合，既能去重，又能排序，增加权重值来保证数据有效</p></blockquote><p><img src="../assets/image-20230804204648305.png" alt="image-20230804204648305"></p><blockquote><p>实现数据的排序，并返回有序数据</p></blockquote><p><img src="../assets/image-20230804204716519.png" alt="image-20230804204716519"></p><blockquote><p>可以根据score 分数进行排序 也可以根据范围进行筛选，同时获取数据总量</p></blockquote><p><img src="../assets/image-20230804204840033.png" alt="image-20230804204840033"></p><blockquote><p>zrevrank rank zhangsan 使用zrank 加上 rev 进行翻转，实现数据倒序</p></blockquote><p><img src="../assets/image-20230804205133510.png" alt="image-20230804205133510"></p><p><img src="../assets/image-20230804205102484.png" alt="image-20230804205102484"></p><h3 id="2-Sorted-set-底层算法">2.Sorted set 底层算法</h3><p><img src="../assets/image-20230804210144404.png" alt="image-20230804210144404"></p><p>原始链表经过一个==随机概率行算法==，升级成上边的三级索引的链表，然后在查找的时候从最顶层列表逐步向下查找，最终确定要查找的数据位置或者列表中不存在数据</p><blockquote><p>极少的情况下会出现随机算法没处理好，导致性能为O(N)\</p></blockquote><p><img src="../assets/image-20230804210336757.png" alt="image-20230804210336757"></p><p><img src="../assets/image-20230804210413824.png" alt="image-20230804210413824"></p><p><img src="../assets/image-20230804210434594.png" alt="image-20230804210434594"></p><p><img src="../assets/image-20230804210450026.png" alt="image-20230804210450026"></p><p><img src="../assets/image-20230804210506039.png" alt="image-20230804210506039"></p><p><img src="../assets/image-20230804210517121.png" alt="image-20230804210517121"></p><blockquote><p>完整过程</p></blockquote><p><img src="../assets/image-20230804205837500.png" alt="image-20230804205837500"></p><blockquote><p>每一个节点由两部分组成，一个部分是权重，另一个部分是存储的value值</p><p>跳跃列表实际是一个压缩列表</p><p>跳跃列表的性能与红黑树 平衡树  的性能都是O(logn),但是在实现上跳跃列表要简单很多</p></blockquote><h3 id="4-公共环境搭建">4.公共环境搭建</h3><h4 id="pom-xml">pom.xml</h4><p><img src="../assets/image-20230804212507761.png" alt="image-20230804212507761"></p><h4 id="常量类">常量类</h4><p><img src="../assets/image-20230804212553538.png" alt="image-20230804212553538"></p><h4 id="异常类">异常类</h4><p><img src="../assets/image-20230804213903246.png" alt="image-20230804213903246"></p><p><img src="../assets/image-20230804212651416.png" alt="image-20230804212651416"></p><p><img src="../assets/image-20230804213111338.png" alt="image-20230804213111338"></p><h4 id="断言工具类">断言工具类</h4><p><img src="../assets/image-20230804213216784.png" alt="image-20230804213216784"></p><h4 id="公共返回对象">公共返回对象</h4><p><img src="../assets/image-20230804213832417.png" alt="image-20230804213832417"></p><p><img src="../assets/image-20230804213248938.png" alt="image-20230804213248938"></p><h4 id="公共返回工具类">公共返回工具类</h4><p><img src="../assets/image-20230804213944392.png" alt="image-20230804213944392"></p><blockquote><p>请求成功/出错返回 以及返回对应的错误信息和成功信息</p></blockquote><h2 id="第四章">第四章</h2><h3 id="3">3.</h3><h4 id="4">4.</h4><h5 id="5">5.</h5><h6 id="6">6</h6><h4 id="4-2">4.</h4><h5 id="5-2">5</h5><h6 id="6-2">6</h6><h3 id="3-2">3</h3><h1>redis详解</h1><p><img src="../assets/image-20230801150523214.png" alt="image-20230801150523214"></p><blockquote><p>8.4 日完结redis基础</p></blockquote><h2 id="第一章redis基本数据结构（笔记-H-就业突破02-redis-资料-Redis-【三通it学院-www-santongit-com】-docx）">第一章redis基本数据结构（笔记 H:\就业突破02_redis\资料\Redis-【三通it学院 <a href="http://www.santongit.com">www.santongit.com</a>】.docx）</h2><h3 id="1-String">1.String</h3><blockquote><p>字符串操作  getrange（1，-1）</p></blockquote><p><img src="../assets/image-20230728155756177.png" alt="image-20230728155756177"></p><blockquote><p>setrange</p></blockquote><p><img src="../assets/image-20230728160305838.png" alt="image-20230728160305838"></p><blockquote><p>mset  (同时设置多个元素值) mget(同时获得多个值)</p></blockquote><p><img src="../assets/image-20230728160340547.png" alt="image-20230728160340547"></p><p><img src="../assets/image-20230728160414994.png" alt="image-20230728160414994"></p><h3 id="2-基本命令【匹配所有数据类型】">2.基本命令【匹配所有数据类型】</h3><blockquote><p>ping ——pong</p></blockquote><p>redis &gt;ping 返回 PONG</p><p>解释：输入 ping，redis 给我们返回 PONG，表示 redis 服务运行正常</p><p>​               <img src="../assets/image-20230801150936523.png" alt="image-20230801150936523"></p><blockquote><p>select</p></blockquote><blockquote><p>keys *   ?</p></blockquote><blockquote><p>exists key</p></blockquote><blockquote><p>type</p><p>none (key 不存在)</p><p>string (字符串)</p><p>list (列表)</p><p>set (集合)</p><p>zset (有序集)</p><p>hash (哈希表)</p></blockquote><blockquote><p>expire</p></blockquote><blockquote><p>ttl</p></blockquote><blockquote><p>del</p></blockquote><blockquote><p>端正想法  先完善一个宿舍管理系统 好好的做一个项目 吃透他</p><p>慢慢完善  在做一个springboot项目 ==峰迷商城== 或者==知识库====公司线上会议==</p><p>逐步实现分布式  微服务 + 八股</p><p>慢慢在学会c++  c++ 和java 通吃</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>总有些值得纪念的东西</title>
      <link href="/project/posts/bda7ca7f.html"/>
      <url>/project/posts/bda7ca7f.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试!" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="a88bd5fcf28550dfcba220f0c3303e5d0d52787f21fbea27ed003ba89d1f2413">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-xray">      <input class="hbe hbe-input-field hbe-input-field-xray" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-xray" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-xray">这里记录我自己成长的历程，需要密码才能进入!</span>      </label>      <svg class="hbe hbe-graphic hbe-graphic-xray" width="300%" height="100%" viewBox="0 0 1200 60" preserveAspectRatio="none">        <path d="M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0"></path>        <path d="M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0"></path>      </svg>    </div>  </div></div><script data-pjax src="/project/lib/hbe.js"></script><link href="/project/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> myself </category>
          
      </categories>
      
      
        <tags>
            
            <tag> myself </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-study1</title>
      <link href="/project/posts/a6370bed.html"/>
      <url>/project/posts/a6370bed.html</url>
      
        <content type="html"><![CDATA[<h1>Mysql性能优化</h1><h2 id="导入数据">导入数据</h2><p>将本地的mysql数据库 通过命令行将本地的数据库文件导入</p><blockquote><p>教程 :</p><p><a href="https://blog.csdn.net/weixin_30900589/article/details/101676080">(24条消息) Windows下命令行连接mysql及导入sql文件_weixin_30900589的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/Tomonkey/article/details/120867919">(24条消息) Windows使用MySQL命令行导入大批量sql文件_Tomonkey的博客-CSDN博客</a></p></blockquote><blockquote><p>中间报了一个错误 英文执行命令的路径错误，没有将路径进行切换</p><p>错误信息 ： ERROR at line 113: Failed to open file ‘load_departments.dump’, error: 2</p></blockquote><p>解决方法:  切换到数据库路径下 执行命令 就可以了</p><p><a href="https://github.com/datacharmer/test_db/issues/9">第 113 行的错误：无法打开文件“load_departments.dump”，错误：2 ·问题 #9 ·Datacharmer/test_db (github.com)</a></p><p><a href="https://bbs.csdn.net/topics/391040884?page=1">导入SQL数据库提示Failed to open file ‘load_departments.dump’-CSDN社区</a></p><p>数据库文件路径: D:\2023.1.29java\test_db-master</p><p><img src="../assets/image-20230705202154795.png" alt="image-20230705202154795"></p><h2 id="分析数据">分析数据</h2><h3 id="数据模型">数据模型</h3><p><img src="../assets/image-20230710145205172.png" alt="image-20230710145205172"></p><ul><li>employees: 员⼯信息表,  主键是 员⼯编号 emp_no</li><li>departments: 部⻔表, 主键是 部⻔编号 dept_no</li><li>titles: 员⼯职称表, 其中员⼯编号emp_no, 职称名称title, 开始⽇期from_date是⼀起作为联合主键</li><li>salaries: 薪资表, 主键是 员⼯编号 emp_no,开始⽇期from_date作为联合主键<br>导⼊示例数据</li><li>dept_manager: 部⻔管理者  其中员⼯编号emp_no, 部⻔编号dept_no 作为联合主键</li><li>dept_emp: 员⼯和部⻔关联表,其中员⼯编号emp_no, 部⻔编号dept_no 作为联合主键 其中⼀个员⼯<br>有多个部⻔, ⼀个部⻔有多个员⼯ySQL数据库系统架构</li></ul><h3 id="执行计划">执行计划</h3><p><strong>很多时候我们需要分析我们的⼀个查询的SQL语句执⾏的性能,有没有⾛索引, 是否有⼀些性能⽐较差的操作, ⽐如使⽤了临时表空间, 使⽤了⽂件排序, SQL语句没有按照我们的要求进⾏执⾏.这个时候我们可以使⽤Explain⼯具进⾏SQL语句的分析, 在我们查询的SQL语句前⾯ 添加⼀个关键字 explain 。==explain 分析的是SQL语句的执⾏计划, 并不会根据sql语句去查询数据, 如果在SQL语句中有⼦查询在from ⼦句, 则会执⾏==</strong></p><blockquote><p>执行使用explain 的语句结果实例</p></blockquote><p><img src="../assets/image-20230713142934040.png" alt="image-20230713142934040"></p><p>下边会介绍每一个列的作用 以及展示对应列的相应值的含义 帮助提高对sql优化更好的理解以及学会怎样进行sql优化</p><p><img src="../assets/image-20230713151539833.png" alt="image-20230713151539833"></p><p>普通索引 会是ref  唯一索引或者主键索引 是展示 eq_ref</p><p>索引下推 Using index condition</p><p><img src="../assets/image-20230713151240722.png" alt="image-20230713151240722"></p><blockquote><p>这里的key_len  虽然是int类型 但是他可以为空 所以这里的索引长度要+1  变成5</p></blockquote><h3 id="trace-工具（索引分析工具）">trace 工具（索引分析工具）</h3><blockquote><p>trace  工具要开启会话级别，而不是全局级别的工具，只有开启会话级别的工具的时候 才会在对应位置产生分析结果</p><p>开启全局 会影响sql 执行性能</p></blockquote><p>sql在选择是否要用索引的时候 要看时间成本是否要比全表查询要低</p><blockquote><p>会话级别应用完成之后 要关闭</p></blockquote><h2 id="索引高级概念">索引高级概念</h2><h3 id="索引选择性">索引选择性</h3><blockquote><p>在我们选择⼀个索引的时候, ⼀定需要选择⼀个区分度⽐较⼤的,最好能保证区分度在90%以上,对于区分度的计算公式</p></blockquote><p><img src="../assets/image-20230713153032683.png" alt="image-20230713153032683"></p><p>要尽量选择 区别度较大 且占全部数据表条目数90%以上的类来创建索引——为的就是增强索引的选择性</p><h4 id="为什么要增强索引的选择性">为什么要增强索引的选择性</h4><p>因为索引在创建之后，会有主键索引和辅助索引两种概念，当辅助索引在使用的时候，可能在执行某些sql指令的时候会不包含全部的数据，导致一直进行回表查询 或者过多的IO操作 进而导致数据量增多反而降低性能</p><p><img src="../assets/image-20230713153346191.png" alt="image-20230713153346191"></p><blockquote><p>上图是我们创建了⼆级索引, 那么假如我们需要根据⼆级索引来找⼀下gender=1的索引信息, 那么我们可以<br>使⽤下⾯的⽅式来进⾏操作    select * from t7 where gender=0</p><ul><li>如果上⾯的语句, 我们直接⾛索引的情况,<ul><li>扫描⼆级索引树, 找到gender=0的节点 对应的主键索引ID(1,4,5), ⽽且数据也是⽐较多50%</li></ul></li><li>然后在根据主键id(1,4,5)去扫描对应的主键索引</li><li>获取到对应的数据,进⾏返回</li><li>那么在这个过程, ⾸先需要扫描⼤量的⼆级索引树, 有⽐较多的IO操作, 其次需要回表操作, 根据主键再去查<br>询数据, ⼜需要进⾏对应的IO操作, 这个时候索引的意义不是很多, 有⽐较多的IO操作, 总体来说, 花费的时<br>间成本可能还没有直接扫描主键索引树更⼩.</li></ul></blockquote><h3 id="回表">回表</h3><blockquote><p>从辅助索引搜索数据之后，仍然需要回到主键索引上去搜索其他所需要的数据  称之为回表</p></blockquote><blockquote><p>索引覆盖</p><ul><li>能够直接从普通索引或者联合索引中直接查到所需要的数据，并不需要再从其他地方进行数据查询的过程 叫做索引覆盖</li></ul></blockquote><p><img src="../assets/image-20230713161033856.png" alt=""></p><p><img src="../assets/image-20230713161013933.png" alt="image-20230713161013933"></p><h3 id="索引覆盖">索引覆盖</h3><blockquote><p>下边是表中建立好的索引  可以看到有两个普通索引 和一个基本索引</p></blockquote><p><img src="../assets/image-20230713161207099.png" alt="image-20230713161207099"></p><blockquote><p>在使用普通索引直接进行数据查出的叫做索引覆盖</p></blockquote><p><img src="../assets/image-20230713161158095.png" alt="image-20230713161158095"></p><blockquote><p>索引覆盖 就是所需要的数据 在我建立的索引树上都有，直接从索引上进行查询即可</p></blockquote><p><img src="../assets/image-20230713161436972.png" alt="image-20230713161436972"></p><h3 id="索引下推">索引下推</h3><p>ICP  在索引树下直接进行判断 而不是先查询完索引后直接回表进行条件判断</p><p><img src="../assets/image-20230713162252583.png" alt="image-20230713162252583"></p><p>索引下推能够减少多次回表查询</p><p><img src="../assets/image-20230713162151698.png" alt="image-20230713162151698"></p><blockquote><p>关闭索引下推操作</p></blockquote><p><img src="../assets/image-20230713162328767.png" alt="image-20230713162328767"></p><h3 id="复合索引">复合索引</h3><p>复合索引 会相当于创建了多个索引，索引前缀匹配原则，先按照第一索引进行查询，如果相同，一次递推到第二 第三索引</p><p><img src="../assets/image-20230716145838622.png" alt="image-20230716145838622"></p><p>复合索引 会因为其他的查询条件的不同，导致是否会使用索引进行查询出现差距</p><p><img src="../assets/image-20230716145853864.png" alt="image-20230716145853864"></p><p>Or 会影响有索引的列 导致使用全表查询</p><h2 id="索引优化">索引优化</h2><h3 id="前缀索引优化">前缀索引优化</h3><blockquote><p>查询条件顺序改变不影响索引的使用和选择</p></blockquote><p>复合索引 三个条件都走索引</p><p><img src="../assets/image-20230716150122347.png" alt="image-20230716150122347"></p><h3 id="最左前缀匹配">最左前缀匹配</h3><blockquote><p>一个复合索引 相当于多个索引</p></blockquote><h3 id="复合索引优化">复合索引优化</h3><blockquote><p>如果省去一列的话 可能会导致后边的列不会走索引查询</p></blockquote><p><img src="../assets/image-20230716150508397.png" alt="image-20230716150508397"></p><p>这里因为第二个查询条件缺少 导致第一个sql语句不走索引，但是因为第二个sql语句补全了中间的条件（条件包含全部选项内容）但是第二个sql走了索引查询</p><blockquote><p>如果某一个条件选项比较少 可以进行补充上 利用索引增加效率</p></blockquote><h3 id="最左前缀字符串匹配">最左前缀字符串匹配</h3><blockquote><p>字符串模糊匹配 也是可以走索引的    最左匹配原则</p><p>后置% 可以走索引 但是前置的% 不走索引</p><p>like service% 会走索引</p><p>like %service 不走索引</p></blockquote><p><img src="../assets/image-20230716151004169.png" alt="image-20230716151004169"></p><h3 id="范围查询">范围查询</h3><blockquote><p>第一索引 走了范围查询的话  后边的不会再走索引进行查询 就是不在利用索引进行查询 直接走全表（下例子中使用索引长度为4 ）</p></blockquote><p><img src="../assets/image-20230716151403409.png" alt="image-20230716151403409"></p><blockquote><p>第一索引没有走范围查询 但是如果后边使用的话 只能是相当于索引下推 不会继续走索引</p></blockquote><blockquote><p>between and  可能会相当于多值精确匹配，可能走索引</p></blockquote><p><img src="../assets/image-20230716151551357.png" alt="image-20230716151551357"></p><p><img src="../assets/image-20230716151651065.png" alt="image-20230716151651065"></p><h2 id="join-优化">join 优化</h2><p><img src="../assets/image-20230716151930280.png" alt="image-20230716151930280"></p><h3 id="Index-Nested-Loop-Join-INLJ索引嵌套循环连接算法">Index Nested-Loop Join(INLJ索引嵌套循环连接算法)</h3><p><a href="https://www.processon.com/view/link/604199596376893122dc9207">https://www.processon.com/view/link/604199596376893122dc9207</a></p><p><img src="../assets/image-20230716154010984.png" alt="image-20230716154010984"></p><p><img src="../assets/image-20230716154142966.png" alt="image-20230716154142966"></p><p>说⼀下这种连接⽅式的特点:</p><ol><li>对于title岗位表, 我们需要全部扫描, 这个主动扫描的表, 我们称之为驱动表</li><li>对于employee员⼯表, 我们需要根据驱动表的记录数据进⾏扫描, 那么我们称之为被驱动表</li><li>对于上⾯的这种, 被驱动表示有索引进⾏关联查询的, 性能⽐较好, 这种算法我们称之为Index Nested-<br>Loop Join, 简称为NLJ算法, 是MySQL中⽐较常⽤的⼀种算法</li><li>尽量使用小表格去驱动大表格，使得N尽量的小，使得M尽量大，这样的话 时间成本是最短的</li></ol><blockquote><p>在sql语句中驱动表与被驱动表的区分</p><p>上边是驱动表 下边是被驱动表</p></blockquote><p><img src="../assets/image-20230716154647519.png" alt="image-20230716154647519"></p><blockquote><p>​系统会自动优化，选择性能最好的情况设定驱动表和被驱动表，（人工置顶被驱动表 和 驱动表）</p></blockquote><p><img src="../assets/image-20230716154853633.png" alt="image-20230716154853633"></p><blockquote><ol><li>最先解决的就是这个住宿，我们表都交上去了，财务那也走完了，没办法让我们下学期交住宿费了，我们8.15搬去哪？</li><li>如果在校内的话，条件是啥？ 啥叫择优给与？能给与多少？做啥项目？</li><li>其他人都选择自己决定，我们就这样被迫留在校内有点不太公平，政策在改变了，我们怎么办？ 只能留在校内了</li></ol></blockquote><h3 id="Simple-Nested-Loop-Join（简单嵌套循环连接算法）">Simple Nested-Loop Join（简单嵌套循环连接算法）</h3><p>被驱动表中没有索引，直接使用驱动表中的一条数据对被驱动表的所有数据进行全表查询，实现数据慢速查询</p><p><img src="../assets/image-20230717163008502.png" alt="image-20230717163008502"></p><p><img src="../assets/image-20230717163043368.png" alt="image-20230717163043368"></p><h3 id="Block-Nested-Loop-Join-BNLJ块存储嵌套循环连接算法">Block Nested-Loop Join(BNLJ块存储嵌套循环连接算法)</h3><blockquote><p>使用块存储，将驱动表的数据分批次进行导入到块中，然后使用被驱动表的数据导入后进行遍历查询，减少查找驱动表的次数进而减少查询遍历被驱动表的遍历循环次数</p></blockquote><p><img src="../assets/image-20230717194545640.png" alt="image-20230717194545640"></p><blockquote><p>使用块存储的时候，方便减少被驱动表的遍历次数，进而减少时间消耗</p></blockquote><p><img src="../assets/image-20230717194738533.png" alt="image-20230717194738533"></p><h2 id="排序优化">排序优化</h2><blockquote><ol><li>通过有序索引⽽直接取得有序的数据</li><li>通过MySQL 的排序算法将存储引擎中返回的数据进⾏排序然后再将排序后的数据返回</li></ol></blockquote><p><img src="../assets/image-20230717195814123.png" alt="image-20230717195814123"></p><p>数据索引展示:</p><blockquote><p>全部数据进行回表查询 因为数据过多导致性能降低</p><p>另外全部数据回表查询时候随机读取IO 性能会非常差，所以综合考虑使用全表查询</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="comment">-- 排序字段id, 本来就有序, 直接使⽤id字段排序</span></span><br><span class="line"><span class="number">2</span> EXPLAIN <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_emp <span class="keyword">order</span> <span class="keyword">by</span> id limit <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- 上边的语句直接使用id索引排序  </span></span><br><span class="line"><span class="number">3</span> <span class="comment">-- name 是普通字段, 没有索引,直接全表扫描, 使⽤Using filesort</span></span><br><span class="line"><span class="number">4</span> EXPLAIN <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_emp <span class="keyword">order</span> <span class="keyword">by</span> name limit <span class="number">10</span>;</span><br><span class="line"><span class="number">5</span> <span class="comment">-- title_id 是索引字段, 在特定的某些情况下会⾛索引 </span></span><br><span class="line"><span class="number">6</span> EXPLAIN <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_emp <span class="keyword">order</span> <span class="keyword">by</span> title_id limit <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- 这里的特殊情况 是指的有限制的回表查询 要比使用全表查询要快，所以使用全表查询</span></span><br><span class="line"><span class="number">7</span> <span class="comment">-- title_id 是索引字段, 在某些情况下不会⾛索引</span></span><br><span class="line"><span class="number">8</span> EXPLAIN <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_emp <span class="keyword">order</span> <span class="keyword">by</span> title_id;</span><br><span class="line"><span class="comment">-- 这里的特定情况 是指的在使用索引的时候性能更差，比不使用索引的全表查询更慢，那么就不会使用查询</span></span><br></pre></td></tr></table></figure><blockquote><p>对于索引列的排序, 看情况确定是否⾛索引, 如果MySQL优化器觉得⾛索引性能会好, 那么就会选择索引<br>如果MySQL优化器觉得⾛索引性能不好, 那么就不会⾛索引.<br>对于全表的数据的排序, MySQL优化器会认为查找数据的时候<br>回表太多<br>在从主键索引找数据的时候, 是随机读数据, 性能⽐较差<br>综合上⾯的考虑, 认为直接全表扫描性能更快<br>我们可以看⼀下使⽤索引不适⽤索引的时间对⽐</p></blockquote><p><img src="../assets/image-20230717204550066.png" alt="image-20230717204550066"></p><h3 id="双路排序原理">双路排序原理</h3><blockquote><p>双路这里指的是使用内存内部和文件暂存以及使用内存外部排序并行操作的过程</p></blockquote><p><img src="../assets/image-20230721100458550.png" alt="image-20230721100458550"></p><blockquote><p>当我们在执⾏ select * from employee order by title_id 的时候, 我们需要回表查询, 对于<br>主键索引表,需要查询两次, 我们称之为&quot;双路排序&quot;,主要处理流程</p><ol><li>加载数据的排序字段title_id 和业务的row_id(业务主键)到sort_buffer的内存区域</li><li>对内存中的数据进⾏局部排序<br>1 – MySQL优化器⾃动优化, 不会选择索引<br>2 select * from t_emp order by title_id;<br>3 – 强制指定索引<br>4 select * from t_emp force index(title_id) order by title_id ;<br>5 – 性能对⽐<br>6 show profiles;</li><li>把数据写⼊到临时⽂件中</li><li>清空sort_buffer中的数据</li><li>重新循环执⾏1-4的操作, 知道所有复合条件的记录都扫描完成, 会⽣成多个临时⽂件</li><li>对临时⽂件进⾏归并排序, 排序后只有 title_id, row_id</li><li>根据row_id从主键索引进⾏回表查询所有的记录</li><li>返回结果集 排序后的数据</li></ol></blockquote><h3 id="单路排序原理">单路排序原理</h3><blockquote><p>因为双路排序 最后所需要的字段没有全部加载到内存中，导致在最后输出结果集的时候需要进行回表查询操作，所以这里单路排序直接将所要查询的全部数据进行加载到内存中，直接不需要进行回表查询，也就称为单路排序</p></blockquote><p><img src="../assets/image-20230721101735691.png" alt="image-20230721101735691"></p><blockquote><p>直接使用所要查询的全部字段在外部的进行数据排序并输出，而不需要使用回表操作</p></blockquote><blockquote><p>当我们在执⾏ select * from employee order by title_id 的时候, 我们不需要进⾏回表查询,<br>对于主键索引表,只需要查询1次, 我们称之为&quot;单路排序&quot;,主要处理流程</p><ol><li>加载数据的排序字段title_id 和业务的row_id(业务主键)和其他所有需要的字段(根据select确定)到<br>sort_buffer的内存区域</li><li>对内存中的数据进⾏局部排序(快速排序)</li><li>把数据写⼊到临时⽂件中</li><li>清空sort_buffer中的数据</li><li>重新循环执⾏1-4的操作, 知道所有复合条件的记录都扫描完成, 会⽣成多个临时⽂件</li><li>对临时⽂件进⾏归并排序, 排序后有所有需要的字段</li><li>读取归并⽂件, 返回结果集</li></ol></blockquote><h4 id="排序的几点思考">排序的几点思考</h4><ol><li>对于排序内存区域的⼤⼩参数: sort_buffer_size  默认认是256k(排序内存大小)</li><li>单路排序对于需要查询的数据字段少的查询条件比较快（不进行回表操作），但是对于查询字段长的查询语句会消耗内存空间</li><li>排序究竟是⾛单路排序还是双路排序:<br>a. 如果查询的字段, 只有主键和排序字段, 直接使⽤单路排序即可<br>b. 如果查询的字段有其他字段, 则根据参数 max_length_for_sort_data 决定, 如果查询的字段<br>⻓度不超过改参数值, 就使⽤单路排序,否则, 采⽤双路排序</li></ol><blockquote><p>要排序字段的长度不超过4096 也就是4k 就会走单路排序</p></blockquote><p><img src="../assets/image-20230721101939254.png" alt="image-20230721101939254"></p><h4 id="使用trace工具-查看是否使用单路排序">使用trace工具 查看是否使用单路排序</h4><blockquote><p>trace 工具默认关闭</p><p>使用trace工具  先开启工具</p></blockquote><p><img src="../assets/image-20230721102412643.png" alt="image-20230721102412643"></p><blockquote><p>查询之后 查看排序模式</p><p><img src="../assets/image-20230721102601192.png" alt="image-20230721102601192"></p><p>packed_additional_fields  使用全部字段加载进内存 使用的是单路排序</p><p><img src="../assets/image-20230721102653853.png" alt="image-20230721102653853"></p><p>双路排序 使用的是 两个字段 &lt; fixed_sort_key,row_id &gt;</p></blockquote><h3 id="排序的优化">排序的优化</h3><ol><li>尽量保证使⽤单路排序<br>a. 查询列, 排序列都是索引的索引列<br>b. 适当调整 max_length_for_sort_data 保证单路排序</li><li>去掉不必要的返回字段</li><li>适当增加 sort_buffer 的⼤⼩【临时内存大小，每次能多存入几条数据】</li></ol><h3 id="索引使用的常用原则">索引使用的常用原则</h3><blockquote><ol><li>创建的索引的选择性需要⽐较⾼, 有⽐较的区分度, 对于性别等字段不适合创建索引【性别不适合做索引列】</li><li>强烈建议不要使⽤select * , 如果确实需要所有的字段, 建议全部写出来</li><li>对于常⽤的查询尽可能多⽤复合索引, 避免创建多个单个索引</li><li>推荐使⽤⾃增的和业务⽆关的主键<ol><li>使用自增是为了减少页的分裂，避免使用b+树导致存储 页分裂 性能下降</li></ol></li><li>查询尽量使⽤索引, 包括索引覆盖, 索引下推等特性, 减少回表的次数</li><li>对于where 判断条件, order 排序字段,  join 关联字段推荐创建索引</li><li>索引的个数不⽤太多【存在维护成本】</li><li>对于频繁更新的字段, 不适合创建索</li><li>对于Join连接尽量使⽤Index Nested-Loop Join⽅式, 尽量避免使⽤Block Nested-Loop Join</li><li>对于Join连接,使⽤⼩结果集驱动⼤结果集</li><li>对于Join连接可以适当的调整 join_buffer_size</li><li>对于Join连接减少不必要的查询字段<ol><li>有些字段在索引树上，所以可以直接获取，不需要回表查询</li></ol></li><li>对于order by 排序的时候, 尽量使⽤索引列, 并且使⽤limit限制查询数量</li><li>对于order by 查询的时候, 尽量使⽤单路排序</li><li>适当的设置InnoDB的buffer⼤⼩ innodb_buffer_pool_size , ⼀般设置为内存的60%-80%<ol><li>默认是128MB  可以自己调整</li><li><img src="../assets/image-20230721204952761.png" alt="image-20230721204952761"></li></ol></li></ol></blockquote><h2 id="案例分析">案例分析</h2><h3 id="案例分析1">案例分析1</h3><blockquote><p>通过改变sql语句优化，使得sql语句的索引起到作用，不进行全表扫描，而使用索引进行扫描</p></blockquote><p><img src="../assets/image-20230728081934910.png" alt="image-20230728081934910"></p><p><img src="../assets/image-20230728081952761.png" alt="image-20230728081952761"></p><h3 id="案例分析2">案例分析2</h3><blockquote><p>在连接查询的时候，不一定是 数据量小的就是驱动表，数据量大的就是被驱动表</p><p>也有可能是大表 中参与运算的数据量比较少，然后大表是驱动表，小表是被驱动表</p></blockquote><p><img src="../assets/image-20230728145951412.png" alt="image-20230728145951412"></p><h3 id="案例分析3">案例分析3</h3><p><img src="../assets/image-20230728150139309.png" alt="image-20230728150139309"></p><p>在对慢的sql语句创建索引的时候，如果能用复合索引直接替代两个所需要的数据列上的索引的话 尽量直接创建复合索引</p><h3 id="案例分析4">案例分析4</h3><p>已经有的index 索引: (nane ,age ,face_value )</p><p><img src="../assets/image-20230728150756876.png" alt="image-20230728150756876"></p><blockquote><p>上边一条语句 会直接使用文件排序进行查找，并没有用上索引</p><p>因为 name 有序的情况下，先使用的是face_value 进行的排序，不满足复合索引的顺序和情况</p></blockquote><p><img src="../assets/image-20230728150815459.png" alt="image-20230728150815459"></p><blockquote><p>在执行下边的一条语句的时候，将face_value  和 age进行顺序调换 第一个index有序的情况下，会使用age进行排序，然后会使用索引进行排序 age 然后再试face_value</p></blockquote><blockquote><p>当然 在业务上 上边的语句 和下边的语句 执行结果是不一样的，前提条件是业务允许的情况下才能将顺序进行调换</p></blockquote><h2 id="事务与隔离级别">事务与隔离级别</h2><h3 id="事务">事务</h3><h3 id="隔离级别">隔离级别</h3><p><img src="../assets/image-20230729091845872.png" alt="image-20230729091845872"></p><h2 id="MySQL的锁机制">MySQL的锁机制</h2><h3 id="锁分类">锁分类</h3><p><img src="../assets/image-20230729092122310.png" alt="image-20230729092122310"></p><blockquote><p>只有Innodb存储殷勤支持 行锁，其他的都不支持行锁</p></blockquote><blockquote><p>原数据锁 当当前事务在使用DML和DQL语句的时候，会为表添加元数据读锁，但是当前事务并没有提交的时候，如果直接使用DDL语言，会将当前元数据的读锁升级成元数据的写锁，在写锁执行完之后，会直接变成元数据的读锁来继续进行当前事务</p></blockquote><p>注意:对于MDL写锁,在执⾏完成之后,会⾃动释放,和当前的事务没有关系，而MDL读锁与当前的事务有关，只有当前事务进行rollback/commit的时候，才会消失</p><h3 id="行锁的算法">行锁的算法</h3><p>临键锁= 记录锁 +  间隙锁</p><blockquote><p><strong>当我们在进⾏加⾏锁的时候, 主要的⾏锁有记录锁, 间隙锁, 临键锁</strong><br><strong>记录锁: 对于表中存在的记录,⽐如主键上的数据进⾏加锁</strong><br><strong>间隙锁: 表中的记录可能是不连续的, ⽐如id为1,4 两个数据相邻,但是中间还可以插⼊id为2,3的记录的</strong><br><strong>数据, 那么对于中间的缝隙加锁,我们称之为间隙锁,其中间隙锁只是在可重复读(RR)模式下才会存在, 其</strong><br><strong>他隔离级别没有间隙锁, 只有记录锁</strong><br><strong>临键锁: 加锁的最基本的单位, 临键锁=记录锁+间隙锁,  其实 记录锁,间隙锁都是临键锁的⼀个特例</strong><br><strong>通过间隙锁可以解决在RR隔离级别下的幻读的问题</strong></p></blockquote><p><img src="../assets/image-20230729110317601.png" alt="image-20230729110317601"></p><blockquote><p>那么当我们在执⾏我们的SQL语句的, 什么时候使⽤记录锁, 什么时候使⽤间隙锁, 什么时候使⽤临键锁呢?<br>在执⾏的过程中,主要遵循的规则有：</p><ol><li>对于加锁的基本单位就是临键锁next key lock, 其中临键锁的规则是前开后闭</li><li>对于在执⾏过程中扫描到的记录都会加锁</li><li>对于索引上等值查询,如果查询条件是唯⼀索引(包括主键索引), 那么临键锁会退化为记录锁(Record<br>Lock)</li><li>对于索引上的等值查询, 会继续扫描到第⼀个不符合条件的记录值, 转换为间隙锁(GAP Lock)</li><li>⾮唯⼀索引上的范围查询,会访问到不满⾜条件的第⼀个值为⽌<br>准备数据</li></ol></blockquote><p><img src="../assets/image-20230730100726961.png" alt="image-20230730100726961"></p><p><img src="../assets/image-20230730100734115.png" alt="image-20230730100734115"></p><p><img src="../assets/image-20230730100738408.png" alt="image-20230730100738408"></p><p><img src="../assets/image-20230730100744412.png" alt="image-20230730100744412"></p><p>你心里要有个概念，数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据<br>库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景。</p><p>B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，==而B树只能中序遍历所有节点，效率太低==</p><p>数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，而==红黑树这种结构，高度明显要深的多==，并且由于==逻辑上很近的节点(父子)物理上可能很远，无法利用局部性。==</p><p>==B+tree的内部结点并没有指向关键字具体信息的指针(红色部分)，因此其内部结点相对B 树更小。==如果把所有同一内部结点的关键字存放在同一块盘中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了；</p>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>idea1</title>
      <link href="/project/posts/8560d162.html"/>
      <url>/project/posts/8560d162.html</url>
      
        <content type="html"><![CDATA[<h2 id="算法性能提升">算法性能提升</h2><h2 id="采样算法">采样算法</h2><blockquote><p>随机采样算法性能提升</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//目前，大多数图神经网络模型可以是扩展到大比例图是基于随机抽样方法的。然而，这些模型中的采样过程是脱离神经网络的前向传播。此外，相当多的作品设计了基于统计估计方法的图卷积网络采样，并且GCNs节点中消息传递的权重是固定的，使得这些采样方法无法扩展到具有可变权重的消息传递网络，例如图注意力网络。注意到神经网络的端到端学习能力，我们提出了一种可学习的采样方法。</span></span><br><span class="line"></span><br><span class="line">    </span><br></pre></td></tr></table></figure><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608023001326#preview-section-introduction">A learnable sampling method for scalable graph neural networks - ScienceDirect</a></p><h2 id="GCN图分类算法">GCN图分类算法</h2><blockquote><p>GCN图分类算法</p></blockquote><blockquote><p>图卷积网络 （GCN） 是用于非欧几里得结构数据的强大深度学习方法，在许多领域都取得了令人印象深刻的性能。但大多数最先进的GCN模型都是深度不超过3到4层的浅层结构，这极大地限制了GCN模型提取节点高级特征的能力。这主要有两个原因：1）叠加过多的图卷积层会导致过度平滑的问题。2）图卷积是一种局部过滤器，容易受到局部属性的影响。为了解决上述问题，我们首先提出了一种新的图神经网络通用框架，称为非本地消息传递（NLMP）。在此框架下，可以灵活地设计非常深的图卷积网络，并且可以非常有效地抑制过平滑现象。其次，我们提出了一种新的空间图卷积层来提取节点多尺度高级节点特征。最后，我们设计了一个端到端的深度图卷积神经网络II（DGCNNII）模型，用于图分类任务，深度可达32层。通过量化各层的图平滑度和消融研究，证明了所提方法的有效性。在基准图分类数据集上的实验表明，DGCNNII优于大量浅图神经网络基线方法。</p></blockquote><p><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0279604">A deep graph convolutional neural network architecture for graph classification | PLOS ONE</a></p><p><a href="https://pubmed.ncbi.nlm.nih.gov/36897837/">用于图分类的深度图卷积神经网络架构 - PubMed (nih.gov)</a></p><p>[journal.pone.0279604.pdf](file:///D:/2022.11.2谷歌下载/journal.pone.0279604.pdf)</p><p><a href="https://github.com/muhanzhang/pytorch_DGCNN/tree/master">muhanzhang/pytorch_DGCNN: PyTorch implementation of DGCNN (github.com)</a></p><h2 id="记录一些机器学习常识【以防忘记，时常拿来复习用】">记录一些机器学习常识【以防忘记，时常拿来复习用】</h2><blockquote><p>FC 层——全连接层，</p><blockquote><p>在整个卷积神经网络中起到“分类器”的作用</p><p>使用全连接层将前边的卷积层和池化层的数据进行分类，将所谓的<code>好</code> 的特征进行整合，然后通过反向传播进而训练参数，提高模型性能</p><p><strong>[卷积神经网络(CNN)中全连接层(FC layer)的作用_joint fc layer_BlueBlueSkyZ的博客-CSDN博客</strong></p></blockquote><p>Layer Normalization和Batch Normalization</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/492803886">Transformer中的归一化(五)：Layer Norm的原理和实现 &amp; 为什么Transformer要用LayerNorm - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/608655896">BatchNorm与LayerNorm的理解 - 知乎 (zhihu.com)</a></p><ul><li>相同</li></ul><ol><li>都是常见的归一化技术，可以提升神经网络的训练速度，提高模型性能</li><li>都可以通过 和γ和β两个可学习参数来缩放和平移，可学习参数量是 2d ， d为隐藏层维度</li></ol><ul><li>差异</li></ul><ol><li>BN在batch的维度上进行归一化，LN在layer的维度上进行归一化，从代码也可以看出，求均值和方差的维度不同，BN是在batch维，LN是在feature维</li><li>从代码可以看出，BN需要保留训练阶段的均值和方差，供预测时使用；LN则不需要，因为他是在样本内部</li><li>BN一般用于卷积神经网络，适用于图像、语音等数据；LN一般用于RNN，对序列数据效果较好</li></ol><p><img src="../assets/image-20230804220009487.png" alt="image-20230804220009487"></p></blockquote><p>ffn层是什么</p><blockquote><p>FFN（前馈神经网络）</p></blockquote><p>Lcls是什么损失</p><blockquote><ol><li><code>loss_cls</code>:衡量每个预测边界框的<em>分类</em>正确性的损失:每个框可能包含一个对象类或“背景”。这种损失通常称为交叉熵损失。<a href="https://www.coder.work/article/4756317">image-processing - 什么是loss_cls和loss_bbox以及为什么它们在训练中总是为零 - IT工具网 (coder.work)</a></li></ol></blockquote><p><img src="../assets/image-20230804220540686.png" alt="image-20230804220540686"></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 科研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> idea </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computer_Science1</title>
      <link href="/project/posts/1d665bdd.html"/>
      <url>/project/posts/1d665bdd.html</url>
      
        <content type="html"><![CDATA[<h1>第3章网络协议面试核心之IP、TCP、UDP协议</h1><h2 id="传输层协议详解">传输层协议详解</h2><h3 id="面试题">面试题</h3><p><img src="../assets/image-20230705144051485.png" alt="image-20230705144051485"></p><blockquote><p>TCP与UDP协议的区别 以及具体的原理是什么</p></blockquote><h3 id="学习目标">学习目标</h3><p><img src="../assets/image-20230705144124539.png" alt="image-20230705144124539"></p><blockquote><p>了解原理 以及具体差异 和为什么有这样的差异</p></blockquote><h3 id="传输层">传输层</h3><p><img src="../assets/image-20230705144505972.png" alt="image-20230705144505972"></p><blockquote><p>传输层详解</p></blockquote><p><img src="../assets/image-20230705144639693.png" alt="image-20230705144639693"></p><p>传输层协议报文结构包含两个部分 ，一个是传输层协议首部，另一个是传输层协议数据，数据是有上层 ——应用层的头部和数据组成。</p><p><img src="../assets/image-20230705144824442.png" alt="image-20230705144824442"></p><h3 id="端口">端口</h3><p>通过网络层的ip地址 知道数据发送给网络上那个主机   传输层是保证不同进程之间的通信</p><blockquote><p>端口是用来保证进程间通信的根本，用来区分不同的端口</p><p>端口+ip地址 形成套接字 socket   【本意是插座】</p></blockquote><p><img src="../assets/image-20230705145033450.png" alt="image-20230705145033450"></p><h3 id="UDP协议">UDP协议</h3><p><img src="../assets/image-20230705145335196.png" alt="image-20230705145335196"></p><p>源端口号 目的端口号 UDP长度 和 UDp校验和</p><p>源端口号 说明数据的来源端口，目的端口号是接收方进程的判定号</p><p>UDP长度保证的是数据所占的长度  UDP校验和保证的是数据在传输过程中是正确的</p><h3 id="TCP协议">TCP协议</h3><p><img src="../assets/image-20230705145623883.png" alt="image-20230705145623883"></p><p>TCP协议 占用最少20个字节，每一行是4个字节，所以占用最少20个字节，同时最后有一些可选字段，另外填充是为了将数据填充成等长度的4字节</p><h4 id="序号">序号</h4><blockquote><p>序号表示的是第一个字节的序号，是每一个字节流都有自己的序号</p></blockquote><p><img src="../assets/image-20230705145919230.png" alt="image-20230705145919230"></p><h4 id="确认号">确认号</h4><blockquote><p>确认号与序号有一定的关系，用来保证前边N-1个数据已经全部接收到</p></blockquote><p><img src="../assets/image-20230705150046766.png" alt="image-20230705150046766"></p><h4 id="控制位">控制位</h4><p><img src="../assets/image-20230705150221209.png" alt="image-20230705150221209"></p><h4 id="窗口">窗口</h4><blockquote><p>使用滑动窗口的概念，来进行使用窗口进行控制传输数据的大小，同时保证数据全部能够被接受到</p></blockquote><p><img src="../assets/image-20230705150332876.png" alt="image-20230705150332876"></p><h3 id="对比">对比</h3><blockquote><p>复杂度不同  tcp复杂 udp比较简单</p></blockquote><p><img src="../assets/image-20230705150548677.png" alt="image-20230705150548677"></p><p><img src="../assets/image-20230705150643085.png" alt=""></p><p><img src="../assets/image-20230705150716222.png" alt="image-20230705150716222"></p><h2 id="TCP建立连接过程">TCP建立连接过程</h2><blockquote><p>三握四挥</p></blockquote><h3 id="面试题-2">面试题</h3><p><img src="../assets/image-20230707213124912.png" alt="image-20230707213124912"></p><h3 id="课程目标">课程目标</h3><p><img src="../assets/image-20230707213152282.png" alt="image-20230707213152282"></p><h3 id="复习">复习</h3><p><img src="../assets/image-20230707213353080.png" alt="image-20230707213353080"></p><h3 id="三次连接">三次连接</h3><p><img src="../assets/image-20230707213851564.png" alt="image-20230707213851564"></p><p><img src="../assets/image-20230707213858332.png" alt="image-20230707213858332"></p><p>SYN   同步位  表示连接请求报文</p><p>ACK   确认位  ACK为1  表明确认号才会生效</p><h3 id="异常情况">异常情况</h3><blockquote><p>第一次请求握手报文丢失</p><p>第二次握手请求 优先到达接收服务器端，然后服务器端没有收到请求连接报文</p></blockquote><p><img src="../assets/image-20230707214207805.png" alt="image-20230707214207805"></p><p><img src="../assets/image-20230707214256559.png" alt="image-20230707214256559"></p><blockquote><p>如果是两次握手就要建立连接的话，会导致两次先后顺序不一致的连接请求分别建立两次链接，而导致资源的浪费</p><p>同时会导致数据发送异常和程序错误的情况出现</p></blockquote><p>这里如果有第三次握手 可以选择丢弃一次连接请求，然后只建立一次连接，忽略掉另一个链接请求，这时候就可以避免这种情况的发生。</p><p><img src="../assets/image-20230707214452477.png" alt="image-20230707214452477"></p><blockquote><p>面试题 为什么要三次握手而不是两次握手</p></blockquote><h2 id="TCP连接释放">TCP连接释放</h2><h3 id="面试题-3">面试题</h3><p><img src="../assets/image-20230707214744366.png" alt="image-20230707214744366"></p><h3 id="复习-2">复习</h3><p><img src="../assets/image-20230707214807546.png" alt="image-20230707214807546"></p><h3 id="四次挥手">四次挥手</h3><p><img src="../assets/image-20230707215039472.png" alt="image-20230707215039472"></p><blockquote><p>第二次和第三次都是被动方发送的报文，第二次可能是普通报文   是为了保证没有发送的数据全部发送出去</p><p>第三次是特殊的断开链接报文，由被动方主动断开连接，将断开位设定为1</p></blockquote><p><img src="../assets/image-20230707215221308.png" alt="image-20230707215221308"></p><h3 id="TimeWait状态">TimeWait状态</h3><p><img src="../assets/image-20230707215303976.png" alt="image-20230707215303976"></p><blockquote><p>为什么要有timeWait状态 以及TimeWait状态为什么要设定为2MSL</p></blockquote><p><img src="../assets/image-20230707215412626.png" alt="image-20230707215412626"></p><blockquote><p>防止网络中因为之前存留的断开请求 以及之前的建立连接请求再次到达被动方造成其他的影响</p></blockquote><h2 id="TCP可靠传输——滑动窗口工作过程">TCP可靠传输——滑动窗口工作过程</h2><h3 id="面试题-4">面试题</h3><p><img src="../assets/image-20230711164541005.png" alt="image-20230711164541005"></p><p><img src="../assets/image-20230711165125868.png" alt="image-20230711165125868"></p><h3 id="停止等待协议-超时重传">停止等待协议+超时重传</h3><p><img src="../assets/image-20230711165213319.png" alt="image-20230711165213319"></p><blockquote><p>报文没有到达接收方 没有收到来自接收方的确认 那么就会超时后重传数据</p><p>即使有异常情况，也能保证数据能够到达接收方</p></blockquote><p><img src="../assets/image-20230711165302720.png" alt="image-20230711165302720"></p><h3 id="优化版的停止等待协议">优化版的停止等待协议</h3><p><img src="../assets/image-20230711165417390.png" alt="image-20230711165417390"></p><blockquote><p>使用窗口  来实现批量数据传输 增强对数据信道的利用率</p></blockquote><p><img src="../assets/image-20230711165521104.png" alt="image-20230711165521104"></p><h4 id="滑动窗口">滑动窗口</h4><p><img src="../assets/image-20230711165650657.png" alt="image-20230711165650657"></p><blockquote><p>借助窗口这个概念 如果窗口数据传输过来 就可以直接将数据进行确认 可以实现批量数据进行传输</p></blockquote><h4 id="累积确认">累积确认</h4><p><img src="../assets/image-20230711165742889.png" alt="image-20230711165742889"></p><blockquote><p>不在对每一个数据都进行接收确认，而是对接收到的数据的最后一个序号进行确认，如果收到某个数据的确认，那么表明这个序号之前的所有的数据都被确认 来增加数据的确认效率</p></blockquote><h4 id="实例">实例</h4><p><img src="../assets/image-20230711165935043.png" alt="image-20230711165935043"></p><h4 id="特殊情况">特殊情况</h4><p><img src="../assets/image-20230711170036691.png" alt="image-20230711170036691"></p><blockquote><p>已发送未确认</p><p>窗口被迫减小</p></blockquote><p><img src="../assets/image-20230711170123245.png" alt="image-20230711170123245"></p><p>数据全部发送 但是未被确认 并且窗口也不允许被移动，可用窗口为0</p><p><img src="../assets/image-20230711170258797.png" alt="image-20230711170258797"></p><h2 id="TCP拥塞避免算法">TCP拥塞避免算法</h2><h3 id="面试题-5">面试题</h3><p><img src="../assets/image-20230711170450526.png" alt="image-20230711170450526"></p><blockquote><p>掌握什么是拥塞</p><p>常见的拥塞避免算法有什么</p></blockquote><h3 id="拥塞">拥塞</h3><p><img src="../assets/image-20230711170533323.png" alt="image-20230711170533323"></p><blockquote><p>资源可以指的是很多东西  而不只只是网络信道   可以是缓存 路由 带宽 处理机 等等</p></blockquote><blockquote><p>网络可能会因为资源的不足而导致网络的拥塞</p></blockquote><p><img src="../assets/image-20230711170643123.png" alt="image-20230711170643123"></p><h3 id="避免网络拥塞">避免网络拥塞</h3><p><img src="../assets/image-20230711170738325.png" alt="image-20230711170738325"></p><blockquote><p>如果没有拥塞控制的话 网络中很多设备 在拥塞量达到一定程度后 就会无法进行运转 会导致整个网络崩溃</p><p><img src="../assets/image-20230711170913667.png" alt="image-20230711170913667"></p></blockquote><h3 id="拥塞避免算法">拥塞避免算法</h3><p><img src="../assets/image-20230711171026426.png" alt="image-20230711171026426"></p><blockquote><p>慢开始 与拥塞避免</p></blockquote><blockquote><p>门限值 是拥塞避免和慢开始的一个转折点</p></blockquote><p><img src="../assets/image-20230711171232399.png" alt="image-20230711171232399"></p><blockquote><p>在每次的超时之后 拥塞避免的门限值大小都等于最后超时时候窗口的一半并进入慢开始阶段，从1开始重新以指数形式增大直到重新进入拥塞避免阶段</p></blockquote><blockquote><p>每次慢开始都是以指数形式进行增长 而拥塞避免是以线性数据进行增长</p></blockquote><h3 id="快重传和快恢复">快重传和快恢复</h3><p><img src="../assets/image-20230711171749987.png" alt="image-20230711171749987"></p><blockquote><p>在接收方没有收到发送方发过来的数据后，立即确认数据进行三次重复确认 发送给发送方，【这里的确认数据中不捎带其他的确认数据 以及传送的数据等】这样接收方接收到三次确认后 立即将缺失的数据进行呢重传，来实现数据快速传输，这个过程就叫做快重传</p></blockquote><p><img src="../assets/image-20230711171732111.png" alt="image-20230711171732111"></p><blockquote><p>在图中④的地方 拥塞避免达到窗口的时候是16   接收到三次连续的ACK确认 表明这里要进行数据重传 ，并进行快重传过程</p><p>然后门限值变为当前拥塞窗口值的一半并将重新传输的启动窗口变为当前的门限值 直接进入到拥塞避免阶段，不在进行慢启动阶段 来快速提高网络的传输速率 这个过程就叫做 快恢复过程。</p></blockquote><h4 id="总结">总结</h4><p><img src="../assets/image-20230711172515501.png" alt="image-20230711172515501"></p><blockquote><p>慢开始 拥塞避免 快恢复 快重传</p></blockquote><h2 id="TCP协议粘包原理">TCP协议粘包原理</h2><p><img src="../assets/image-20230711172644820.png" alt="image-20230711172644820"></p><h3 id="TCP协议复习">TCP协议复习</h3><p><img src="../assets/image-20230711174540959.png" alt="image-20230711174540959"></p><blockquote><p>TCP协议 是面向字节流进行传输的  没有办法识别数据的边界 也就不能对数据进行拆分  ，因此需要应用层对数据进行拆分</p></blockquote><blockquote><p>为了方便对数据进行划分 使用content-length 对数据进行划分</p></blockquote><p><img src="../assets/image-20230711175646929.png" alt="image-20230711175646929"></p><p>content-length如果错误  HTTP协议获取的报文数据是错误的</p><h3 id="粘包">粘包</h3><p><img src="../assets/image-20230711175845870.png" alt="image-20230711175845870"></p><blockquote><p>粘包 本质上就是两个报文数据 因为数据长度问题错误的被解析成两个错误的报文数据</p><p>解决粘包的两种方法</p></blockquote><p><img src="../assets/image-20230711175924417.png" alt="image-20230711175924417"></p><h3 id="Nagle算法">Nagle算法</h3><p><img src="../assets/image-20230711180036538.png" alt="image-20230711180036538"></p><p>减少数据包 将多个小的数据包合并成一个大的数据包 来共同发送 增加宽带利用率的方式</p><blockquote><p>算法将多个数据报放到一个数据报文中进行传输    所以接收方的应用层需要将数据进行合理的拆分 否则会导致一些数据报解析的问题</p></blockquote><h3 id="粘包的实例">粘包的实例</h3><p><img src="../assets/image-20230711180312409.png" alt="image-20230711180312409"></p><p>TCP编程  会将数据 进行丢失接收 或者多个数据粘在一起进行接收</p><p>如上边左 接收到8个字符 会丢失掉一些数据  上边右会将多次发送的数据合并在一起进行接收</p><blockquote><p>应用层错误的解析数据 或者拆分数据导致的情况</p></blockquote><h3 id="总结-2">总结</h3><p><img src="../assets/image-20230711180506581.png" alt="image-20230711180506581"></p><p>cookie，localstorage和sessionStorage Session  的区别</p><p><img src="../assets/image-20230705213157923.png" alt="image-20230705213157923"></p><p>sessionStorage  随着浏览器的关闭 就不会存在  一般用于存放比较敏感的信息</p>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>峰迷商城1</title>
      <link href="/project/posts/4e47d272.html"/>
      <url>/project/posts/4e47d272.html</url>
      
        <content type="html"><![CDATA[<h1>一、《峰迷商城》项目介绍</h1><h4 id="1-1-项目背景">1.1 项目背景</h4><h5 id="1-1-1B2C">1.1.1B2C</h5><h5 id="1-1-2C2B2C">1.1.2C2B2C</h5><h5 id="1-1-3Java">1.1.3Java</h5><h4 id="1-2项目功能">1.2项目功能</h4><p><a href="http://www.processon.com/view/link/606bde8b1e08534321fd2103">http://www.processon.com/view/link/606bde8b1e08534321fd2103</a></p><h4 id="1-3-技术选型">1.3 技术选型</h4><blockquote><p>SSM企业开发框架 比较基础的开发技术</p></blockquote><h5 id="1-3-1-单体项目">1.3.1 单体项目</h5><blockquote><p>项目的页面和代码（交互和数据存取） 都在同一个项目中，项目开发完成之后都会部署在同一个服务器中</p></blockquote><p><img src="../assets/image-20230130115923610.png" alt="image-20230130115923610"></p><p><strong>单体项目的问题：</strong> 用户对页面的静态资源以及java代码（用户登录需要使用控制器 控制器使用java写的，所以需要用到tomcat上的jvm运行环境进行运行，所以需要请求tomcat）的请求压力都会落在Tomcat服务器上</p><blockquote><p>单体项目 不能实现高并发等</p></blockquote><h5 id="1-3-2技术清单">1.3.2技术清单</h5><ul><li><p>代码块  Ctrl + Shift + K（或者 ``` (三个esc下边的按键（英文输入法）)</p></li><li></li><li><p>快捷键 ctrl + shift + ] ，生成列表(使用星号 *、或加号+ 或 减号 - 作为列表标记，后添加一个空格，生成列表‍)(-[空格]空格 文字)</p></li><li><p>项目架构：前后端分离（前端和后端会在两个服务器中，然后实现服务器之间通讯交互）</p></li><li><p>前端技术:（JQuery不再使用，有渲染和交互功能，但是过于笨重）使用轻巧的技术</p><ul><li>==vue （渲染） 、axios(通讯)==、妹子uI 、 layui、bootstrap</li><li>后端技术：SpringBoot（整合SSM）+MyBatis+ RESTful(开发风格 技术规范)+Swagger（接口呈现）</li></ul></li><li><p>服务器搭建部署：Linux  Nginx（前端）</p></li></ul><h1>二、项目的架构演进</h1><h4 id="互联网项目的三高">互联网项目的三高</h4><p><strong>高并发高可用高性能</strong></p><h4 id="2-1单体架构">2.1单体架构</h4><p><img src="../assets/image-20230130165114012.png" alt="image-20230130165114012"></p><ul><li>特点：前后端都部署在同一台服务器上（前后端代码都在同一个应用中）</li><li>缺点：对静态资源的访问压力也会落在Tomcat上</li></ul><h4 id="2-2前后端分离">2.2前后端分离</h4><p><img src="../assets/image-20230723120038726.png" alt="image-20230723120038726"></p><ul><li>并发性比较高</li><li>前后端分离：前端和后端分离进行开发和部署（前后端部署在不同的服务器上）</li><li>tomcat服务器只负责数据校验，所有的页面跳转等 都是使用Nginx进行（两台服务器同时提供服务，对于页面的请求不会占用tomcat服务器）</li><li>优点：（只需要进行接口调用）将对静态资源的访问和对接口的访问进行分离，Tomcat服务器只负责数据服务的访问（可以实现更多人的访问 即使300的并发tomcat 但也能让500人同时访问）</li></ul><h4 id="2-3-集群搭建（分布式）">2.3 集群搭建（分布式）</h4><blockquote><p>集群在理论上可以解决并发问题，还可以解决可用性问题（防止有的服务器出现问题，其他节点可以帮忙）</p></blockquote><p><img src="../assets/image-20230130171103363.png" alt="image-20230130171103363"></p><blockquote><p>负载均衡： 按需分配</p><p>轮询: 一次一人一个，轮流交替</p><p>权重分配:你占比大，你一次2个 我占比小，我一次一个</p></blockquote><blockquote><p>集群可以解决 高并发问题，还解决了可用性问题</p><p>可以有替代服务器进行可用性保护</p></blockquote><h4 id="2-4分布式（高可用性）">2.4分布式（高可用性）</h4><p><img src="../assets/image-20230130172844686.png" alt="image-20230130172844686"></p><h6 id="2-4-1分布式锁">2.4.1分布式锁</h6><p><strong>redis实现分布锁</strong>，临时存储要操作的数据，其他服务器若并发操作，则等待锁释放后在进行操作</p><ul><li>出现商品超卖问题<ul><li>使用redis 谁操作库存中某一个数据 谁把改数据的id 写入到redis中 （相当于加锁操作）</li><li>三个服务器同时查询一个数据库中的商品，但因为是并发，没有写回数据库导致库存不足</li></ul></li><li>一个服务器的话，不会出现问题，使用sql-net锁</li><li>使用分布式技术控制服务器之间的并发问题</li></ul><h6 id="2-4-2分布式数据库">2.4.2分布式数据库</h6><p>使用mycat数据库负载均衡</p><p>数据库默认连接最大是151个</p><p><strong>多个数据库并发存储</strong>（分布式数据库）</p><p><img src="../assets/image-20230130172432580.png" alt="image-20230130172432580"></p><h6 id="2-4-3-ridis集群">2.4.3 ridis集群</h6><p><img src="../assets/image-20230130172949431.png" alt="image-20230130172949431"></p><blockquote><p>redis 也是用集群 保证高可用性，使用负载均衡 cUster 来保证</p></blockquote><h6 id="2-4-4ElasticSearch数据库临时查询">2.4.4ElasticSearch数据库临时查询</h6><h6 id="（like模糊查询效率低）">（like模糊查询效率低）</h6><p>模糊查询 在直接访问数据库的时候会比较慢（数据量大）</p><ul><li>使用ElastSearch 来专门实现数据库检索 增加效率</li></ul><p><img src="../assets/image-20230130173211074.png" alt="image-20230130173211074"></p><ul><li><p>数据库中间件</p></li><li><p>消息中间件（消息队列）</p></li></ul><h4 id="2-5微服务架构（高性能）">2.5微服务架构（高性能）</h4><p>微服务器主要是提高可用性的目标（聊天系统崩了不会影响看视频模块）</p><p>可以针对具体的模块去解决高并发问题，从更小的层面提高性能（保证了各个模块的可用性）</p><blockquote><p>将原本一个应用中开发的多个模块进行拆分，单开进行开发和部署（鸡蛋不放在一个篮子里边）</p><p>保证可用性和性能</p></blockquote><p><img src="../assets/image-20230130230020671.png" alt="image-20230130230020671"></p><blockquote><p>一个模块故障将导致整个节点不可用</p><p>为了保证系统的扩展和功能堆叠，会不断增加新的功能（但是整体的坏的可能性变高 一个坏了都坏了）</p><p>现在为了避免一个影响整体，将模块进行拆解，各个模块拆解成各个项目，一个模块实现成一个项目</p><p>每个模块都有自己的服务器 并且不同请求 对应不同的服务器</p><p>对于某个模块需要高并发的话，可以转向某一个模块实现集群部署，来提高该模块的性能，从而减少资源浪费</p></blockquote><blockquote><p>微服务项目对技术实现变得困难和复杂，对模块之间的数据通信等要求变高，更加专注于系统==可用性==问题，不在过度关注==并发问题==，也提高了系统的==高性能需求==</p></blockquote><h1>三、峰迷商城项目进行搭建</h1><h4 id="3-1技术储备">3.1技术储备</h4><p><img src="../assets/image-20230730102325426.png" alt="image-20230730102325426"></p><blockquote><p>分为两个部分组成 前端 和后端</p><p>==基于maven的聚合工程完成项目搭建，前端采用vue+axios  后端采用SpringBoot整合SSM==</p></blockquote><h4 id="3-2项目搭建">3.2项目搭建</h4><p><img src="../assets/image-20230130230721762.png" alt="image-20230130230721762"></p><blockquote><p>先去完成SpringBoot 整合ssm 学习完成Maven聚合工程，再去学习项目搭建</p></blockquote><p><img src="../assets/image-20230730102652343.png" alt="image-20230730102652343"></p><h6 id="3-2-1在服务器主机上运行起来一个项目需要的几个步骤">3.2.1在服务器主机上运行起来一个项目需要的几个步骤</h6><blockquote><p>SSM 手动整合工程存在的问题——配置复杂且繁琐</p><p><img src="../assets/image-20230730103434128.png" alt="image-20230730103434128"></p></blockquote><ol><li>安装JDK</li><li>安装Tomcat</li><li>安装mysql</li><li>打包ssm项目</li><li>部署项目: 将war包放到Tomcat中</li><li>启动tomcat项目</li></ol><blockquote><p>在之前的ssm项目中 都需要完成固定步骤且繁琐的一些配置工作，没有含金量但是却比较固定且浪费时间</p><p>所以需要SpringBoot进行简化开发和部署，方便把精力都放到项目开发上去</p></blockquote><blockquote><p>springboot这种技术 就是在要满足这种需要的时候演进出来的，所以需要不断发展</p></blockquote><p><img src="../assets/image-20230730103817109.png" alt="image-20230730103817109"></p><blockquote><p>SpringBoot就是在两个方面进行简化开发过程</p><ol><li>简化配置步骤，甚至可以直接实现0配置就可以来进行项目开发</li><li>项目部署简单，可以实现项目快速部署，甚至在不需要安装tomcat的时候就可以直接部署，因为SpringBoot内置了tomcat</li></ol></blockquote><h4 id="3-3-Springboot简介">3.3 Springboot简介</h4><p><img src="../assets/image-20230730105429461.png" alt="image-20230730105429461"></p><blockquote><p>springboot 面临着动态语言 node.js 等挑战<code> 这些语言都是在java的思想基础上做出的改进，所以比较先进</code> ，开发出了java的春天springboot 框架，内置了很多需要配置项，并且都是主流的配置文件，不需要开发者手动去整合配置，需要的只是在用到的时候直接引用一下即可，这样用<code>习惯大于配置</code>的思想 为开发者简化开发流程</p></blockquote><p><img src="../assets/image-20230730105726041.png" alt="image-20230730105726041"></p><p>==上边是简略图==</p><blockquote><p>注意点:</p><ol><li>springboot 不是没有配置，只是内置了配置</li><li>springboot 是云原生的基础，为后边springcloud 做了准备</li><li>面对缺点的时候，可以用开发者的能力去弥补springboot的高内置，用经验去排查错误</li></ol></blockquote><h4 id="3-4SpringBoot配置项目">3.4SpringBoot配置项目</h4><blockquote><p>springboot 配置mybatis</p></blockquote><h5 id="3-4-1-SpringBoot-项目结构">3.4.1 SpringBoot 项目结构</h5><p><img src="../assets/image-20230730113546187.png" alt="image-20230730113546187"></p><blockquote><p>图中的5个文件和文件夹 可以先删掉 没用</p></blockquote><p><img src="../assets/image-20230730114357866.png" alt="image-20230730114357866"></p><blockquote><p>static中的文件 不会在用户请求的时候，被总控制器给拦截。 总控制器MVC会将用户的请求给进行拦截，然后进行处理，但是放在static中的文件在用户请求的时候，不会被拦截，可以直接获取</p></blockquote><blockquote><p>springboot 不在使用jsp作为默认的动态模板，而是使用Thymeleaf 作为新的模板，优于jsp</p></blockquote><blockquote><p>application.properties 是开发者可以自定义进行修改的配置文件 利用数据库密码等 在这里修改配置</p><p>.properties 是使用键值对的数据配置</p><p>yml是使用层级目录进行配置</p></blockquote><h5 id="3-4-2-⾃定义配置-整合MyBatis">3.4.2 ⾃定义配置(整合MyBatis)</h5><blockquote><p>SpringBoot帮助我们完成通⽤性配置，但是像数据库连接地址、账号、密码等还是需要⼿动完成配置</p></blockquote><blockquote><p>修改mysql驱动的版本（选择性）</p><p>根据自己本地的项目数据库 情况 自己配置对应的数据库版本</p></blockquote><p><img src="../assets/image-20230730131336912.png" alt="image-20230730131336912"></p><p>在SpringBoot主配置⽂件 application.properties ⽂件中配置数据源及扫描包路径</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 配置数据源（key 必须按照SpringBoot的奥球来进行）</span><br><span class="line">spring.datasource.url= jdbc:mysql:<span class="comment">//localhost:3306/fmmall2?characterEncoding=UTF-8</span></span><br><span class="line"></span><br><span class="line">spring.datasource.username= root</span><br><span class="line">spring.datasource.password=<span class="number">123456789</span></span><br><span class="line"></span><br><span class="line">#    下边是链接的驱动器</span><br><span class="line">spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver</span><br><span class="line"></span><br><span class="line"># 配置映射文件路径以及实体类的包名</span><br><span class="line">#在当前文件所在的文件夹下边创建mapper文件夹 存放mybatis的xml映射文件</span><br><span class="line">mybatis.mapper-locations=classpath:mapper<span class="comment">/*Mapper.xml</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置实体类存放的包名路径</span></span><br><span class="line"><span class="comment">mybatis.type-aliases-package=com.qfedu.demo.entity</span></span><br></pre></td></tr></table></figure><p>在SpringBoot启动类通过 @MapperScan 注解指定DAO接⼝的包名</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.qfedu.demo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.mybatis.spring.<span class="keyword">annotation</span>.MapperScan;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.SpringApplication;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.autoconfigure.SpringBootApplication;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="comment">//下边的一行是 利用@MapperScan 注解配置Dao层 的接口包名</span></span><br><span class="line"><span class="meta">@MapperScan(<span class="string">&quot;com.qfedu.demo.dao&quot;</span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SpringbootFmscDemo1Application</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> static void main(String[] args) &#123;</span><br><span class="line">        SpringApplication.run(SpringbootFmscDemo1Application.<span class="keyword">class</span>, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-5启动SpringBoot项目">3.5启动SpringBoot项目</h4><blockquote><p>直接启动springboot 的启动类就可以实现项目启动【SpringBoot应⽤⾃带Servlet容器—Tomcat,因此⽆需进⾏额外的服务器配置，运⾏启动类即可启动⼀个 SpringBoot应⽤】</p></blockquote><h5 id="3-5-1-测试项目——insertUser">3.5.1 测试项目——insertUser</h5><blockquote><p>对数据表Users 增加一条数据 来测试整个SpringBoot项目</p></blockquote><p><strong>流程</strong> 先从数据库表开始，创建实体类，然后实现Dao层【创建UserMapper.xml文件】</p><h5 id="遇到的bug">遇到的bug</h5><blockquote><p>在编写SpringBoot测试的时候，配置项目中 路径写错了，在mapper后边多写了一个s 导致找不到mapper.xml 文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mybatis.mapper-locations=classpath:mappers/*Mapper.xml</span><br><span class="line">// 测试用例如下：</span><br><span class="line">//http://localhost:8080/user/regist/username=lisi&amp;password=12345&amp;nickname=qwe&amp;realname=qwe&amp;userImg=qw// e&amp;userMobile=qwe&amp;userEmail=qwe&amp;userSex=1&amp;userBirth=2023-07-21&amp;userRegtime=2021-04-</span><br><span class="line">// 5%2016:10:53&amp;userModtime=2021-04-15%2016:10:53</span><br></pre></td></tr></table></figure><p><img src="../assets/image-20230730152735441.png" alt="image-20230730152735441"></p><p><img src="../assets/image-20230730152751185.png" alt="image-20230730152751185"></p></blockquote><p>**异步请求 **</p><p><img src="../assets/image-20230730153043412.png" alt="image-20230730153043412"></p><p>==使用ajax 需要返回json对象的时候  需要增加@ResponseBody注解来实现数据 返回，不然会导致数据库中插入了数据，但是这里在前端进行页面跳转的时候没有页面返回，会报错404 错误==</p><p><img src="../assets/image-20230730152751185.png" alt="image-20230730152751185"></p><p>==使用ajax请求 请求对象的时候 要加上@ResponseBody注解，如果使用url请求跳转 但是后端没有相应页面 也就是没有返回ModelAndView或者一个String数据 能够进行页面跳转 否则就要加上@ResponseBody注解==</p><blockquote><p>如果整合第三方的工具  则需要稍加的配置数据</p></blockquote><h4 id="3-6-了解SpringBoot">3.6 了解SpringBoot</h4><h5 id="3-6-1-SpringBoot-的Start">3.6.1 SpringBoot 的Start</h5><blockquote><p>springboot  在一些组件配置上 提供了默认配置 但是在一些组件没有提供默认配置，需要自己手动提供——使用全局配置进行书写</p><p>像redis 是提供默认配置的，配置的是本地localhost，但是mybatis没有配置数据源 和 url，需要自己手动给填写数据源</p></blockquote><blockquote><p>server:<br>port: 9999<br>servlet:<br>context-path: /demo1</p><p>// 这里进行更改端口号 和项目名称 改成demo1  使用demo1进行访问</p></blockquote><p>访问链接: <a href="http://localhost:9999/demo1/user/%E2%80%A6%E2%80%A6">http://localhost:9999/demo1/user/……</a></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p>修改mybati驱动版本</p><p><img src="../assets/image-20230801210000037.png" alt="image-20230801210000037"></p><p>小于8.0 版本  驱动是 com.mysql.jdbc.Driver</p><p>8.0以上的时候 需要加一个cj  com.mysql.cj.jdbc.Driver</p><blockquote><p>8.1-8.10 总结</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> project </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computer_Science</title>
      <link href="/project/posts/de21c907.html"/>
      <url>/project/posts/de21c907.html</url>
      
        <content type="html"><![CDATA[<h2 id="1-课程介绍">1. 课程介绍</h2><p><img src="../assets/image-20230627101912556.png" alt="image-20230627101912556"></p><p><img src="../assets/image-20230627101923934.png" alt="image-20230627101923934"></p><h2 id="2-OSI七层模型详解">2.OSI七层模型详解</h2><h3 id="面试题">面试题</h3><p><img src="../assets/image-20230627105806500.png" alt="image-20230627105806500"></p><p><img src="../assets/image-20230627102059414.png" alt="image-20230627102059414"></p><blockquote><p>计算机网络为什么分层？</p></blockquote><p><img src="../assets/image-20230627102149911.png" alt="image-20230627102149911"></p><p><img src="../assets/image-20230627102203070.png" alt="image-20230627102203070"></p><p>层与层之间是透明的 并且不同的层实现不同的功能</p><p>上层不需要关注下层的实现以及解决的问题，只需要关注本层所完成的功能即可[函数进行拆分  保证耦合性]—— 又叫做解耦</p><p>​</p><h3 id="网络分层具体结构">网络分层具体结构</h3><p><img src="../assets/image-20230627103207295.png" alt="image-20230627103207295"></p><p>每一层单独解决了不同的问题，实现了相应的功能</p><p><img src="../assets/image-20230627103301646.png" alt="image-20230627103301646"></p><p>​</p><p>简化成四层模型</p><p><img src="../assets/image-20230627103756090.png" alt="image-20230627103756090"></p><blockquote><p>各种协议 就是一些规定  【<strong>约定大于配置</strong>】  这些约定是帮助自己在成为一个极客的路上更好的垫脚石</p></blockquote><p>层次与嵌套的结构图示:</p><p><img src="../assets/image-20230627105203019.png" alt="image-20230627105203019"></p><p>从下往上 是各层数据报的结构  而从上往下则是每层数据将自己数据封装后传输给下一层的数据</p><h3 id="网络层">网络层</h3><p><img src="../assets/image-20230627105346912.png" alt="image-20230627105346912"></p><p>网络层 实现最大努力交付  但不保证数据完整，无连接且不保证顺序</p><p><img src="../assets/image-20230627105354172.png" alt="image-20230627105354172"></p><h3 id="传输层">传输层</h3><p><img src="../assets/image-20230627105612572.png" alt="image-20230627105612572"></p><p>传输层在使用TCP的时候能够保证数据的可靠性 但是UDP协议的时候 是不保证数据的可靠性的</p><h3 id="应用层">应用层</h3><p><img src="../assets/image-20230627105725722.png" alt="image-20230627105725722"></p><h2 id="3HTTP协议演进版本-0-9-1-0-1-1-1-2">3HTTP协议演进版本 0.9-1.0-1.1-1.2</h2><h3 id="面试题-2">面试题</h3><p><img src="../assets/image-20230627110001204.png" alt="image-20230627110001204"></p><h3 id="学习目标">学习目标</h3><blockquote><p>理解HTTP版本的演进历程</p><p>掌握不同的版本的改进点以及解决的是什么问题</p><p>掌握最新的http2.0 的新特性</p></blockquote><h3 id="演进历程">演进历程</h3><p><img src="../assets/image-20230627110509233.png" alt="image-20230627110509233"></p><p>从一开始只是一个网络文字的表述 方式 到慢慢的逐渐变成展示更多形象能容的主流方式，备受到时代的欢迎</p><p><img src="../assets/image-20230627110532812.png" alt="image-20230627110532812"></p><p>不同的版本对应的目的或者说功能是不一样的</p><blockquote><p>最早时代是作为网络内容的传输和展示工具   合并网络数据，方便交流沟通</p><p>中间时代 是开始发展成商业化  巨头公司如腾讯网 搜狐网等作为企业中心化</p><p>至今变成人人可以发布网络内容，去中心化 并且内容更加丰富，且展现形式多样化 如视频 文字 图表等</p></blockquote><p><img src="../assets/image-20230627110445939.png" alt="image-20230627110445939"></p><p>不同的特性对应不同的功能</p><p>从最开始只是为了展示文字  最后变成既要考虑表达文字内容的多样性，也要保证数据展示和传输的性能以及稳定性</p><h3 id="长连接-TCP协议">长连接 [TCP协议]</h3><p><img src="../assets/image-20230627111657210.png" alt="image-20230627111657210"></p><p>减少请求和相应的通讯时间，减少请求连接建立和释放的时间成本，保证更快的相应</p><h3 id="HTTP2-0新特性">HTTP2.0新特性</h3><p>优势是时间成本的降低  提高速度</p><p><img src="../assets/image-20230627111918535.png" alt="image-20230627111918535"></p><h4 id="多路复用">多路复用</h4><p><img src="../assets/image-20230627112418445.png" alt="image-20230627112418445"></p><p>多路复用技术通常表示在一个信道上传输多路信号或者数据流的过程和技术。通过使用多路复用，通信运营商可以避免维护多条路线，从而有效的节约运营成本</p><p>多路复用 保证在连接时候 请求与相应 可以实现并行数据传输，例如打电话 和 传统的信件传输 的区别</p><blockquote><p>信件的传输方式 会受限于信内容长度和主题 同时因为传输速度的问题 而受到限制【传统的一问一答】</p><p>电话交流 可以保证传输的快速 从而可以在交流着一个主题的时候 时刻切换到另一个主题上去，保证多主题并行</p></blockquote><blockquote><p>多路复用 可以在一次请求两个文件的时候，同时将两个不同类型的文件同时传输过去【图示例】</p><p>但是要保证的是底层链路的帧间数据传输 能够实现二进制数据的分类确定以及数据构成文件的保证 等复杂过程</p></blockquote><h4 id="头部压缩">头部压缩</h4><p>HTTP头部是一个铭文数据  在传输的过程中 比较占用带宽  所以在2.0中提出了头部压缩的特性</p><p><img src="../assets/image-20230627112934682.png" alt="image-20230627112934682"></p><ul><li><ol><li>使用在客户端和服务端 同时维护一个静态字典 借助字典中简单的key值 来代表复杂的头部数据  例如使用2表示传输方式等</li></ol></li><li><ol start="2"><li>使用哈夫曼编码 进行英文字母的压缩 从而保证数据的长度不会太长<ol><li>​<a href="https://mdnice.com/writing/2021deb7e9374f9883ca534aed8ae89e">https://mdnice.com/writing/2021deb7e9374f9883ca534aed8ae89e</a> 哈夫曼编码解释</li></ol></li></ol></li></ul><h4 id="服务端推送">服务端推送</h4><p><img src="../assets/image-20230627113756439.png" alt="image-20230627113756439"></p><p>客户端 可以自己解析客户端传入的请求 并根据请求内容自动推定所需要的内容进行同时的多数据传输  来减少请求时间</p><h2 id="4HTTP报文结构、请求方式以及状态码详解">4HTTP报文结构、请求方式以及状态码详解</h2><h3 id="面试题-3">面试题</h3><p><img src="../assets/image-20230627145542792.png" alt="image-20230627145542792"></p><blockquote><p>常见的Http请求方式以及使用的场景</p><p>常见的状态码404 503  504 等分别表示什么意思 怎样定位？</p></blockquote><h3 id="学习目标-2">学习目标</h3><blockquote><p>HTTP请求的状态码</p><p>HTTP请求的方法</p><p>HTTP请求的报文结构</p></blockquote><h3 id="HTTP报文结构">HTTP报文结构</h3><p><img src="../assets/image-20230627145846895.png" alt="image-20230627145846895"></p><blockquote><p>请求方法</p><p>请求的状态码</p></blockquote><p><img src="../assets/image-20230627150231593.png" alt="image-20230627150231593"></p><ul><li><p>请求方法 请求地址 Http版本  可以直接获得根据值获得目标地址</p></li><li><p>请求头是以key -value 的方式进行的</p></li><li><p>请求内容 示例是用json结构的数据展示的</p></li><li></li></ul><p>应答的报文结构</p><p><img src="../assets/image-20230627150635565.png" alt="image-20230627150635565"></p><p><img src="../assets/4.jpg" alt="HTTP 的消息格式"></p><p>==图片来自小林coding==</p><h3 id="请求方法">请求方法</h3><p>请求方法: Http请求的本质是对服务器的一些资源进行操作的过程（增删改查 + 系统功能）</p><p>通过定义不同的方法 实现不同操作是清晰并且必要的</p><ul><li>get 方法  最常用的方法 常用语请求服务器的<strong>某个资源</strong></li><li>Head 方法 和get方法类似   但是只是获得一些服务器中资源的请求头部 【获得数据首部】<ul><li>服务器只是返回数据头部</li><li>对服务器中的某些资源进行检查</li><li>不获取资源的情况下 去查看某个资源的具体情况</li><li>查看响应的状态码是否是正常的</li><li>查看首部来确定某些资源或者首部是否是发生改变</li></ul></li><li>post 向服务器写入数据 进行修改<ul><li>账号密码校验 使用post方法 向服务器进行写入并进行查看是否正确</li></ul></li><li>TRACE 观察请求报文到达服务器的最终样式<ul><li>经过防火墙  代理 转发后 到达服务器可能数据已经发生改变</li></ul></li><li>put方法 向服务器<strong>写入资源</strong> 与get方法相反<ul><li><strong>资源包括数据 也包括一些其他形式的数据</strong></li></ul></li><li>delete方法 请求服务器删除url 所制定的资源</li><li>OPTION  需要服务器返回服务器中资源所支持的方法【支持get put post等方法】<img src="../assets/image-20230627152111024.png" alt="image-20230627152111024"></li></ul><h3 id="幂等函数与幂等操作">幂等函数与幂等操作</h3><p><img src="../assets/image-20230627152216861.png" alt="image-20230627152216861"></p><blockquote><p>接口 每次访问都会对某个数据+1  【非幂等操作】</p><p>每次访问 都将某个属性值设置为请求中携带的数据值 例如携带的是5  则该请求 执行1次 与执行10次最后该属性值都是10【幂等操作】</p></blockquote><p><img src="../assets/image-20230627152522358.png" alt="image-20230627152522358"></p><h3 id="Http状态码">Http状态码</h3><p><img src="../assets/image-20230627152603673.png" alt="image-20230627152603673"></p><h4 id="成功状态码">成功状态码</h4><p><img src="../assets/image-20230627152628507.png" alt="image-20230627152628507"></p><h4 id="失败以及其他状态码">失败以及其他状态码</h4><p>304 可以在下边的这种情况时候使用:</p><blockquote><p>本地已经存储了一份服务器的资源 再次请求服务器中对应的资源的时候，可以申请一下判定本地资源是否被修改 如果是304  则表示可以直接使用本地【类似于资源转发】最终使用的资源被重定向，不再向服务器获取 而是从本地获取</p></blockquote><p><img src="../assets/image-20230627153014146.png" alt="image-20230627153014146"></p><p>404  表示资源未被找到</p><p><img src="../assets/image-20230627153053175.png" alt="image-20230627153053175"></p><p>503  通常在服务器宕机 或者不能运转的时候 进行返回数据</p><p>504  表示网关或者代理不能响应</p><p><img src="../assets/image-20230627153143436.png" alt="image-20230627153143436"></p><blockquote><p>Proxy 中间的请求代理，可以进行请求的转发 以及相应的转发处理</p></blockquote><p><img src="../assets/image-20230627153258407.png" alt="image-20230627153258407"></p><h2 id="5-安全传输基础-对称加密以及非对称加密">5 安全传输基础 [对称加密以及非对称加密]</h2><h3 id="面试题-4">面试题</h3><p><img src="../assets/image-20230702155710747.png" alt="image-20230702155710747"></p><blockquote><p>描述 什么是对称加密 什么是非对称加密</p><p>MD5加密算法是什么？ 为什么？ 什么是加盐处理</p></blockquote><h3 id="学习目标-3">学习目标</h3><p><img src="../assets/image-20230702160307831.png" alt="image-20230702160307831"></p><h3 id="安全传输模型">安全传输模型</h3><p><img src="../assets/image-20230702160421167.png" alt="image-20230702160421167"></p><blockquote><p>引入加密和解密的过程</p><p>在不安全的信道中传输的是加密后的数据  用户接收到数据之后 需要进行解密</p></blockquote><p><img src="../assets/image-20230702160534066.png" alt="image-20230702160534066"></p><h4 id="古典密码学常见密码表">古典密码学常见密码表</h4><p><img src="../assets/image-20230702161008210.png" alt="image-20230702161008210"></p><p>使用某组协议 将原文中的字母 智焕成对应的字母设定，导致原文可读性很差，只有知道解密密码表才能将数据进行获取</p><h4 id="古典密码学局限性">古典密码学局限性</h4><p><img src="../assets/image-20230702161153791.png" alt="image-20230702161153791"></p><p>按照全部字符中数量的多少进行解密，将数目进行排序后，[通过字频]一一对应成相应的个数，即可进行解密</p><h3 id="对称加密">对称加密</h3><blockquote><p>通过同一个秘钥 既可以解密 也可以加密</p></blockquote><p><img src="../assets/image-20230702161325031.png" alt="image-20230702161325031"></p><p><img src="../assets/image-20230702161510968.png" alt="image-20230702161510968"></p><h3 id="非对称加密">非对称加密</h3><blockquote><p>秘钥a和秘钥b 不相等 但是存在一定的数学关系；使用秘钥a进行加密 使用a加密 使用b进行解密</p></blockquote><p><img src="../assets/image-20230702161544417.png" alt="image-20230702161544417"></p><blockquote><p>RSA 算法是最常用的 影响力最大的算法</p><p>算法理论: 基于数论作为基础，两个素数相乘得到的积[过程简单] 在进行因式分解 得到两个素数的过程是复杂的，所以要将数据进行分解需要耗费极大精力；以此作为理论基础来进行加密和解密</p></blockquote><h3 id="非对称加密算法解析">非对称加密算法解析</h3><p><img src="../assets/image-20230702161939227.png" alt="image-20230702161939227"></p><h3 id="方法对比">方法对比</h3><p><img src="../assets/image-20230702162001918.png" alt="image-20230702162001918"></p><h3 id="哈希算法">哈希算法</h3><blockquote><p>哈希算法 哈希函数，使用映射方法进行，能够将数据变小或者进行统一，并且从哈希值中根本看不到原文的任何有关数据</p></blockquote><p><img src="../assets/image-20230702162058089.png" alt="image-20230702162058089"></p><blockquote><p>上图例子能够将不同编码集的数据 经过哈希函数后将数据解析成统一大小的编码，方便保存以及传输</p></blockquote><h4 id="注意点">注意点</h4><blockquote><p>如果两个原文得到的哈希值不同，则表明两个原文数据一定是不同的</p><p>但如果两个原文数据得到的哈希值是相同的，两个原文可能不相同【哈希碰撞】</p></blockquote><h4 id="应用">应用</h4><blockquote><p>将用户密码 在数据库中使用哈希算法 进行转换后再保存</p></blockquote><p><img src="../assets/image-20230702163925419.png" alt="image-20230702163925419"></p><blockquote><p>保存的都是用户密码加密后的密文状态</p></blockquote><h4 id="弊端">弊端</h4><blockquote><p>黑客知道用户使用的是MD5哈希算法进行转化 并且本身拥有极大的测试数据，也即用户明文与密文的一一对应用例，便可以挨个遍历进行转换获取用户的明文即可</p><p>黑客只需要将用户的密文与自己数据库中的key-value进行匹配即可获取用户的明文</p></blockquote><p><img src="../assets/image-20230702164304645.png" alt="image-20230702164304645"></p><h3 id="加盐操作">加盐操作</h3><blockquote><p>将用户存储的明文和自定义的字符进行拼接后在进行Hash算法转换获得密文进行数据库存储，来保证用户密码的安全性</p></blockquote><p><img src="../assets/image-20230702164417083.png" alt="image-20230702164417083"></p><p>==hash算法不是加密算法==</p><p>hash是单向的，并不具备逆向解密算法，所以并不是加密算法</p><p><img src="../assets/image-20230702164519780.png" alt="image-20230702164519780"></p><h2 id="6-Https协议安全基础-TLS技术">6 Https协议安全基础:TLS技术</h2><h3 id="面试题-5">面试题</h3><p><img src="../assets/image-20230703113439144.png" alt="image-20230703113439144"></p><h3 id="学习目标-4">学习目标</h3><blockquote><p>掌握HTTP HTTPs 的异同点</p><p>掌握HTTPs的底层原理—— TLS技术</p><p>SSL安全参数握手过程</p></blockquote><p><img src="../assets/image-20230703113448146.png" alt="image-20230703113448146"></p><h3 id="HTTP-vs-HTTPs-协议">HTTP vs HTTPs 协议</h3><p><img src="../assets/image-20230703113550724.png" alt="image-20230703113550724"></p><p>Https 比http协议 多了一层安全性保障，保证通信内容的安全性</p><blockquote><p>Http 通常使用80 端口，而Https 使用的是443 端口</p><p>但是https相对比较复杂一点</p></blockquote><h3 id="TLS技术">TLS技术</h3><p><img src="../assets/image-20230703114007731.png" alt="image-20230703114007731"></p><blockquote><p>TLS 技术应用在 应用层 和 传输层之间 来对应用端的数据进行加密后 进行传输，保证了数据的完整性 和 安全性。</p><p>应用了对称加密 以及 非对称加密的设计算法，来实现数据加密</p></blockquote><h3 id="数字证书">数字证书</h3><blockquote><p>数字证书类似于数字身份证，可以用于网络上身份信息证明，是在网络通信中标志通讯双方身份信息的数字认证。</p><p>数字证书是可信任的组织颁发给特定对象的认证</p></blockquote><p>一些通信是匿名的，但是可以用数字证书来标识通信双方信息</p><p><img src="../assets/image-20230703115207990.png" alt="image-20230703115207990"></p><blockquote><p>数字证书的公开秘钥 是==使用的非对称加密算法来生成对称秘钥的过程==</p><p>Https协议是使用非对称加密算法来生成公开密钥的过程</p></blockquote><h3 id="实例过程">实例过程</h3><p><img src="../assets/image-20230703115859276.png" alt="image-20230703115859276"></p><blockquote><p>双方经过ssl安全握手之后 双方得到对称秘钥 秘钥是经过非对称加密算法得到的</p><p>双方使用对称加密的秘钥 进行数据的加密和解密 以及传输</p></blockquote><blockquote><p>ssl 安全参数握手 综合使用了数字证书  对称加密 和非对称加密算法</p></blockquote><p><img src="../assets/image-20230703115905585.png" alt="image-20230703115905585"></p><blockquote><p>先使用非对称加密算法 来生成通信双方都知道的随机数</p><p>第二步后 双方都拥有 随机数 1 2  以及确定好加密算法</p><p>在使用双方确定==好随机数以及对称加密算法==来生成对称秘钥</p><p>使用对称秘钥算法 来进行数据通讯</p></blockquote><p><img src="../assets/image-20230703115927278.png" alt="image-20230703115927278"></p><blockquote><p>通信双方分别生成自己的对称秘钥 没有传输 所以不会泄露</p><p>因为有随机数3  以及非对称加密算法 的保证 ，降低了对称秘钥被破解的可能</p></blockquote><p>为什么不直接使用非对称加密算法来实现通信？</p><ol><li>成本高 双方都要存储对方的公钥和自己的私钥【非对称加密算法 成本高】</li><li>解密加密过程复杂 比较慢【解密过程运算量大】</li></ol><h2 id="7-DNS服务详解">7.DNS服务详解</h2><h3 id="面试题-6">面试题</h3><p><img src="../assets/image-20230704163027061.png" alt="image-20230704163027061"></p><p>DNS是什么 解决了什么问题</p><p>监视DNS查询服务器的基本流程</p><h3 id="学习目标-5">学习目标</h3><p><img src="../assets/image-20230704163156644.png" alt="image-20230704163156644"></p><h3 id="DNS是什么">DNS是什么</h3><p>IP 指定的是网络中某一台主机</p><p>端口 指定的是主机中某一个进程</p><p>为了帮助记忆 将域名来映射对应的IP 和端口号</p><p><img src="../assets/image-20230704163343175.png" alt="image-20230704163343175"></p><h3 id="DNS工作原理">DNS工作原理</h3><p>域名介绍 分为 顶级 二级 一级 域名</p><p><img src="../assets/image-20230704163605109.png" alt="image-20230704163605109"></p><blockquote><blockquote><p>域名树</p></blockquote></blockquote><h4 id="域名分级">域名分级</h4><p><img src="../assets/image-20230704163656125.png" alt="image-20230704163656125"></p><blockquote><p>从三级域名往上回溯 可以得到很多可以访问的地址 域名组合后 能够有很多的结合</p></blockquote><h4 id="域名服务分级">域名服务分级</h4><p>DNS域名服务也要进行分级</p><p><img src="../assets/image-20230704163922022.png" alt="image-20230704163922022"></p><blockquote><p>根域名服务器  顶级域名服务器  权威域名服务器  解析到达不同的域名服务器</p></blockquote><h3 id="DNS过程">DNS过程</h3><blockquote><p>DNS 发生在客户端的，查询域名所对应的ip地址   分为两种 递归查询  迭代查询</p></blockquote><p><img src="../assets/image-20230704164320646.png" alt="image-20230704164320646"></p><blockquote><p>两种解析方式 虽有不同，但是最终的结果是一致的，都是解析出对应的域名ip值</p></blockquote><h2 id="8-DNS安全">8.DNS安全</h2><h3 id="面试题-7">面试题</h3><p><img src="../assets/image-20230704164738490.png" alt="image-20230704164738490"></p><h3 id="学习目标-6">学习目标</h3><blockquote><p>怎样防范DNS攻击</p></blockquote><h3 id="DNS被攻击现象">DNS被攻击现象</h3><p><img src="../assets/image-20230704164910908.png" alt="image-20230704164910908"></p><blockquote><p>​为了广告费或者推广费  将DNS域名解析 变成特定页面，增加访问量</p></blockquote><h3 id="两种方式">两种方式</h3><h4 id="DNS劫持">DNS劫持</h4><blockquote><p>黑客控制域名管理运营密码 和域名运营运营邮箱，来实现本地DNS管理记录解析到黑客可以控制的域名服务器上，当客户端进行DNS过程的时候，就会到达黑客设定的DNS服务器上</p><p>黑客可以通过服务器返回给用户特定的DNS域名来让用户进行访问</p><p>==本地运营商的问题==</p></blockquote><h5 id="防范方法">防范方法</h5><p><img src="../assets/image-20230704165412227.png" alt="image-20230704165412227"></p><blockquote><p>不适用自动获取运营商的服务器地址，而是使用特定的DNS服务器来实现DNS地址固定，而减少因为运营商被攻击劫持所带来的的困扰</p></blockquote><h4 id="DNS欺骗">DNS欺骗</h4><blockquote><p>没有黑掉某一个具体的服务器，而是在权威服务器那里冒名顶替权威域名服务器，从而使得本地域名服务器获得错误的ip地址来实现访问某些数据的方式</p></blockquote><p><img src="../assets/image-20230704165639713.png" alt="image-20230704165639713"></p><blockquote><p>欺骗  和 劫持  都是类似的，都是返回一个错误的ip地址给客户端，让客户端去访问错误的主页， 但使用的手段不同罢了。</p><p>劫持控制某个服务器，实现错误数据的返回</p><p>欺骗是在本地进行一个冒名顶替</p></blockquote><h3 id="DDos攻击">DDos攻击</h3><blockquote><p>Dos攻击  是拒绝服务攻击</p><p>DDos 攻击是在Dos攻击的基础上，使用一种分布式的手段，更暴力的攻击手段</p></blockquote><p><img src="../assets/image-20230704170230574.png" alt="image-20230704170230574"></p><blockquote><p>域名服务器宕机之前 可以正常访问</p></blockquote><p><img src="../assets/image-20230704170259177.png" alt="image-20230704170259177"></p><blockquote><p>宕机之后不能访问,DNS解析过程不能正常服务</p></blockquote><p><img src="../assets/image-20230704170348102.png" alt="image-20230704170348102"></p><p><img src="../assets/image-20230704170537078.png" alt="image-20230704170537078"></p><blockquote><p>采购一些更安全的服务厂商的服务器或者提高本地服务器安全性能</p></blockquote><blockquote><p>个人角度 采取一些更加安全的DNS服务 降低个人被攻击的风险</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 计算机基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mysql-study</title>
      <link href="/project/posts/2aa7565.html"/>
      <url>/project/posts/2aa7565.html</url>
      
        <content type="html"><![CDATA[<h1>mysql学习</h1><h3 id="学习顺序">学习顺序</h3><p><img src="../assets/image-20230702110746243.png" alt="image-20230702110746243"></p><h2 id="mysql概述">mysql概述</h2><p><img src="../assets/image-20230626151232733.png" alt="image-20230626151232733"></p><p>Mysql概述</p><blockquote><p>mysql 是瑞典公司开发，2008年卖给sun （java）开发的公司后不断发展</p><p>Percona Server由领先的MySQL咨询公司Percona发布。 Percona Server是⼀款独⽴的数据库产品，其<br>可以完全与MySQL兼容，可以在不更改代码的情况了下将存储引擎更换成XtraDB  国内也有⼀些MySQL的分⽀或者⾃研的存储引擎，⽐如⽹易的InnoSQL，阿⾥的AliSQL。但是从github上⾯发现这两个数据库现在没有怎么更新, 不建议使⽤</p></blockquote><h2 id="引擎">引擎</h2><h3 id="Mysql架构">Mysql架构</h3><p>Mysql语句的执行过程</p><blockquote><p>mysql采用<strong>TCP协议通信</strong>  在数据传输之前要先建立连接  并保证数据控制和操控权限</p></blockquote><h3 id="Mysql存储引擎">Mysql存储引擎</h3><p>所谓存储引擎: 主要是指对于==MySQL数据库在磁盘或者内存中存储的组织⽅式==, 不同的存储引擎, 数据在磁<br>盘上的存储格式是不⼀样的</p><p>对于===MySQL数据库系统架构主要分为Server层和存储引擎层==,其中存储引擎采⽤的是插件机制来实现可插拔的操作, 可以让不同的表根据⾃身的业务特点选择不同的存储引擎, 其中常⻅的存储引擎有</p><p><strong>不同表 可以使用不同的存储引擎</strong></p><p>查看数据的存储⽬录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;datadir&#x27;</span></span><br></pre></td></tr></table></figure><p>MyLSAM 和InnoDB在服务器端的存储文件目录</p><p><img src="../assets/image-20230701114542536.png" alt="image-20230701114542536"></p><p>t1 是InnoDb 类型的存储引擎表【表数据和结构都在.ibd中】</p><p>t2是MyISAM类型的存储引擎表【数据在MYD中  MYI 存放的索引 .sdi存放的是表结构】</p><h4 id="两个引擎对比">两个引擎对比</h4><p><img src="../assets/image-20230701114853065.png" alt="image-20230701114853065"></p><h2 id="索引">索引</h2><h3 id="索引作用">索引作用</h3><blockquote><p>索引（Index）是帮助MySQL⾼效获取数据的数据结构。</p><p>索引是一种对列值进行排序的结构，方便快速访问数据</p></blockquote><blockquote><p>不同的数据存储在==磁盘==上的不同结构, 我们可以通过存==储引擎来进⾏区分==, 但是当数据存储到==⽂件==中,</p><p>我们通过==索引数据结构==, 可以让我们快速的从数据库中查询到对应的数据</p></blockquote><h4 id="提高查找速度">提高查找速度</h4><p><img src="../assets/image-20230702102140808.png" alt="image-20230702102140808"></p><p>在遍历查找 要查找某个值 可能要查找6次[值为89]</p><p>如果使用二叉树作为</p><p>索引 可以减少查找次数 查找2次即可</p><blockquote><p>因为索引使用了排序后 可以直接方便查找</p></blockquote><h4 id="案例">案例</h4><p><img src="../assets/image-20230702102732088.png" alt="image-20230702102732088"></p><p>在两个1000000条数据表中，一个使用索引，一个不适用索引，进行查询某条数据，最后结果会相差出几个数量级的时间</p><h3 id="索引的分类">索引的分类</h3><blockquote><p>常⻅的索引有 主键索引, 唯⼀索引, 普通索引</p><p>主键索引: 每个表有也只有⼀个主键索引, 强烈建议在创建表的时候指定主键索引, 如果没有指定主键索引, 对于表会默认⽣成⼀个==⾮空且唯⼀==的主键索引</p><p>【主键索引会帮助更快的搜索数据】</p><p>唯⼀索引:可以为空, 但是不能重复, 必须唯⼀. ⼀个表可以有多个唯⼀索引【针对不同的列创建不同的唯一索引】<br>普通索引: 可以为空, 可以重复, 如果普通索引可以满⾜对应的业务需求, ==推荐使⽤普通索引==【在删除和修改的时候 会比较好一点】</p></blockquote><p>主键索引    primary key (id)</p><p>唯⼀索引 unique key idx_name(name) 【 表示 索引名 (索引列)】</p><blockquote><p>其中 name 就是⼀个唯⼀索引, 通过使⽤ unique key 来指定唯⼀索引, idx_name 是索引名称如果索引名称没有指定, 或默认使⽤列名作为索引名称</p></blockquote><p>普通索引  key idx_name (name)</p><p>其中 name 就是⼀个普通索引, 通过使⽤ key 来指定唯⼀索引, idx_name 是索引名称</p><h3 id="添加索引">添加索引</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 给id 添加⼀个主键索引</span></span><br><span class="line"> <span class="keyword">alter</span> <span class="keyword">table</span> t4 <span class="keyword">add</span> <span class="keyword">primary</span> key (id);</span><br><span class="line"><span class="comment">-- 给name添加⼀个唯⼀索引</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t4 <span class="keyword">add</span> <span class="keyword">unique</span> key idx_name (name);</span><br><span class="line"><span class="comment">-- 给score 添加⼀个普通索引</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t4 <span class="keyword">add</span> key idx_score (score);</span><br><span class="line"><span class="keyword">show</span> index <span class="keyword">from</span> t4;</span><br></pre></td></tr></table></figure><p>展示所有的索引[show index from t4 ]</p><p><img src="../assets/image-20230702105656153.png" alt="image-20230702105656153"></p><blockquote><p>有时候我们在创建索引的时候,可能不只有⼀个列,需要把多个列同时创建⼀个索引的情况, 我们称之为复合索引具体语法可以参考官⽅⽂档: <a href="https://dev.mysql.com/doc/refman/8.0/en/alter-table.html">https://dev.mysql.com/doc/refman/8.0/en/alter-table.html</a><br>– 除了删除使⽤命令的⽅式创建, 我们也可以使⽤对应的可视化⼯具进⾏创建, ⽐如下⾯是Navicat的截图</p></blockquote><h3 id="复合索引">复合索引</h3><blockquote><p>有时候我们在创建索引的时候,可能不只有⼀个列,需要把多个列同时创建⼀个索引的情况, 我们称之为复合索引</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 复合索引 name, sn 创建⼀个唯⼀索引</span></span><br><span class="line"><span class="comment">-- 对于复合索引(联合索引) 索引是有顺序的</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t4 <span class="keyword">add</span> <span class="keyword">unique</span> key idx_name_sn (name, sn)</span><br><span class="line"><span class="keyword">show</span> index <span class="keyword">from</span> t4;</span><br><span class="line"><span class="comment">-- 删除复合索引</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t4 <span class="keyword">drop</span> key idx_name_sn</span><br><span class="line"><span class="keyword">show</span> index <span class="keyword">from</span> t4</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="索引算法">索引算法</h3><p><img src="../assets/image-20230704143637409.png" alt="image-20230704143637409"></p><h4 id="哈希算法">哈希算法</h4><blockquote><p>把索引的值做hash运算，并存放到hash表中,因为使⽤Hash表存储，按照常理，hash的性能⽐B-TREE效率⾼很多。</p></blockquote><p><img src="../assets/image-20230704143838877.png" alt="image-20230704143838877"></p><h5 id="示例">示例</h5><p>原来空的哈希表结构</p><p><img src="../assets/image-20230704144139263.png" alt="image-20230704144139263"></p><p>数据存储之后的表格</p><p><img src="../assets/image-20230704144406180.png" alt="image-20230704144406180"></p><blockquote><p>先将id 变为int类型 在进行哈希取模计算</p></blockquote><p><img src="../assets/image-20230704144502177.png" alt="image-20230704144502177"></p><blockquote><p>上表格中的就是根据id来创建的hash索引, 根据id%key 来获取对应的⼀个具体索引的位置<br>如果我们查询id=22的⼀个数据, 通过⼀次搜索就可以<br>根据id 进⾏hash运算, 获取到对应的hash值22<br>进⾏取模运算 22%7=1, 找到对应的位置【对哈希表长度7 进行取模运算】<br>判断位置上是否有值, 如果有, 则插⼊链表尾部, 如果没有, 插⼊对应的数据</p></blockquote><h5 id="哈希索引的缺点">哈希索引的缺点</h5><blockquote><p>但是我们会发现, 在索引中的数据, 查询单个值确实很快速, 但是整个数据是没有顺序的, 是散列插⼊.这个<br>就会有下⾯的⼏个问题<br>hash索引范围查询⽐较麻烦<br>hash索引结构没有顺序, 对数据排序需要重新进⾏排序<br>hash索引, 不能使⽤部分索引, ⽐如前缀索引, 以及像 like 'xxx%'模糊查询<br>如果⼤量的hash值相同, 会出现hash碰撞, 也会影响性能</p></blockquote><p><img src="../assets/image-20230704144944231.png" alt="image-20230704144944231"></p><h4 id="B树">B树</h4><h5 id="示例图">示例图</h5><p><img src="../assets/image-20230704145132319.png" alt="image-20230704145132319"></p><h5 id="特性">特性</h5><blockquote><p>n阶b树  每个节点最多可以有n个子节点，且当前节点可以存储n-1个数据</p></blockquote><p><img src="../assets/image-20230704145503767.png" alt="image-20230704145503767"></p><p>其中上⾯就是⼏个B-Tree,通过观察上⾯的B-Tree, 我们可以看出⼏个特点,这⾥以5阶B树为例：</p><ul><li>⼀个节点可以存放多个数据(值) , 其中最多可以是4个数据</li><li>这是⼀个多叉树, ⼀个节点可以有多个⼦节点, 其中最多是5个⼦节点</li><li>数据是有顺序的, 查找速度⽐较快</li><li>整个树是处于 ⼜胖⼜矮 的⼀个状态</li><li>树⽐较平衡, 每个节点的所有的⼦树⾼度⽐较⼀致</li></ul><blockquote><p>查找次数比较 b+树 查找次数比较少</p></blockquote><p><img src="../assets/image-20230704150127105.png" alt="image-20230704150127105"></p><blockquote><p>select * from t where id &gt;20   可以找到20节点 然后将20 节点左边的所有节点遍历出来 就可以。能够实现范围查询</p></blockquote><h4 id="B-树">B+树</h4><h5 id="示例图-2">示例图</h5><p><img src="../assets/image-20230704150811927.png" alt="image-20230704150811927"></p><h5 id="特性-2">特性</h5><blockquote><p>b树拥有b+树所有的特点</p><p>b+树的所有叶子子节点存储这 所有节点的data数据并且节点的链表是双向链表</p><p>所有的非叶子结点的数据 都会在叶子节点进行存储一份，方便进行直接访问数据【非叶子结点中不会存储数据】</p></blockquote><p><img src="../assets/image-20230704151528774.png" alt="image-20230704151528774"></p><p>查询过程</p><p><img src="../assets/image-20230704151652239.png" alt="image-20230704151652239"></p><blockquote><p>如果是等值查询  则从根节点开始查询，然后向下边叶子节点进行判断并不断找到叶子结点中的数据值</p><p>如果是范围查询 先找到对应的叶子结点值，然后变成链表查询方式，直接进行查询，速度比较快</p></blockquote><h4 id="b树与b-树对比">b树与b+树对比</h4><p>为了保证查找效率 不能让两种树的高度太高，所以需要尽量使一个节点中尽量存储多个数据节点 来保证树比较宽</p><p>通过⽐较B-Tree和B+树的存储数据的结构, 我们会发现B+树相对B树来说, 有以下⼏个好处</p><ul><li><p>范围查找不需多次遍历树, 只需要遍历到叶⼦节点, 通过链表的⽅式可以找到上⼀个或者下⼀个节点</p></li><li><p>对于B+树来说, ⾮叶⼦节点存储存储的数据个数要⽐B树要多, 可以很好的保证树的⼀个⾼度</p><blockquote><p>对于每个节点的磁盘空间⼤⼩是固定的, 默认情况下, MySQL中的每个节点的⼤⼩是16kb,<br>如果data数据的⼤⼩是1kb, 主键⼤⼩是8个字节, 指针⼤⼩6个字节<br>如果使⽤B树存储, 需要存储数据+指针+主键⼤⼩ ≈ 1kb, 每个节点最多存放16个数据 ⾼度为4的<br>话, 最多存储65535个记录</p><p>如果使⽤B+树存储, ⾮叶⼦节点主要存储的是指针+主键⼤⼩≈14字节 , 每个节点最多存放1170个<br>数据,叶⼦节点存放的数据 存储数据+指针+主键≈ 1kb, 存放16个节点, ⾼度为3(2层⾮叶⼦节点+1层叶<br>⼦节点)<img src="../assets/image-20230704152526086.png" alt="image-20230704152526086">也就是说⾼度为3, 就可以存储千万级别的数据了</p></blockquote></li></ul><p><img src="../assets/image-20230704152551120.png" alt="image-20230704152551120"></p><blockquote><p>一个节点大小为16kb， 并且为了保证树能够又胖又矮，增加查找效率，所以使用b+树</p></blockquote><h3 id="MyISAM-索引实现">MyISAM 索引实现</h3><h4 id="主键索引">主键索引</h4><blockquote><p>MyISAM引擎使⽤B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。</p></blockquote><p><img src="../assets/image-20230704152950183.png" alt="image-20230704152950183"></p><blockquote><p>MyISAM  是分文件存储 将数据 与索引值进行分开存放，他这里将索引的叶子节点中 存放数据的地址值，用来帮助找到数据文件中对应的数据值的位置，方便找到对应数据</p></blockquote><blockquote><p>先找主键索引，找到对应的数据值所在的位置地址，然后去数据中找对应的数据值</p></blockquote><blockquote><p>这⾥设表⼀共有三列，假设我们以Col1为主键，图解⼀个MyISAM表的主索引（Primary key）示意。可以<br>看出MyISAM的索引⽂件仅仅保存数据记录的地址</p></blockquote><h4 id="辅助索引">辅助索引</h4><blockquote><p>在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯⼀<br>的，⽽辅助索引的key可以重复。如果我们在Col2上建⽴⼀个辅助索引，则此索引的结构如图所示</p></blockquote><p><img src="../assets/image-20230704153337655.png" alt="image-20230704153337655"></p><h3 id="InnoDB-索引实现">InnoDB 索引实现</h3><blockquote><p>虽然InnoDB也使⽤B+Tree作为索引结构，但具体实现⽅式却与MyISAM截然不同。<br>第⼀个重⼤区别是==InnoDB的数据⽂件本身就是索引⽂件==。从上⽂知道，MyISAM索引⽂件和数据⽂件是分<br>离的，索引⽂件仅保存数据记录的地址。⽽在InnoDB中，表数据⽂件本身就是按B+Tree组织的⼀个索引<br>结构，这棵树的==叶节点data域保存了完整的数据记录==。这个索引的key是数据表的主键，因此InnoDB表数<br>据⽂件本身就是主索引</p></blockquote><h4 id="主键索引-2">主键索引</h4><p><img src="../assets/image-20230704153608186.png" alt="image-20230704153608186"></p><blockquote><p>因为InnoDB的数据⽂件本身要按主键聚集，所以==InnoDB要求表必须有主键（MyISAM可以没有）==，如果没<br>有显式指定，则MySQL系统会==⾃动选择⼀个可以唯⼀标识数据记录的列作为主键==，如果不存在这种列，则<br>MySQL⾃动为InnoDB表⽣成⼀个隐含字段(_rowid)作为主键，这个字段⻓度为6个字节，类型为⻓整形。</p></blockquote><blockquote><p>InnoDb 引擎的表 必须有一个主键 如果没有的话 会自动生成一个主键</p></blockquote><h4 id="辅助索引-2">辅助索引</h4><p><img src="../assets/image-20230704153743955.png" alt="image-20230704153743955"></p><blockquote><p>辅助索引以主键索引作为基础，在数据表中存放对应的主键，然后再根据主键去寻找对应的数据值</p></blockquote><blockquote><p>与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值⽽不是地址。换句话说，<br>InnoDB的所有辅助索引都引⽤主键作为data域。<br>聚集索引这种实现⽅式使得按主键的搜索⼗分⾼效，但是辅助索引搜索需要检索两遍索引：⾸先检索辅助<br>索引获得主键，然后⽤主键到主索引中检索获得记录</p></blockquote><h3 id="聚簇索引-非聚簇索引">聚簇索引&amp;&amp;非聚簇索引</h3><blockquote><p>聚簇索引: 聚簇索引是指的将索引和数据放到一个文件中，数据库表⾏中数据的物理顺序与键值的逻辑（索引）顺序相同</p></blockquote><p><img src="../assets/image-20230705160454694.png" alt="image-20230705160454694"></p><blockquote><p>InnoDB 就是聚簇索引 将索引与数据文件放到一起</p><p>MyISAM 是非聚簇索引  索引文件与数据文件单独存放，需要在索引中找到数据文件中对应的地址，再根据地址去数据文件中寻找对应的数据</p></blockquote><h3 id="索引的好与坏">索引的好与坏</h3><blockquote><p>通过索引我们可以快速的定位到我们需要查询的数据, 那是不是对于⼀个表来说, 索引越多越好呢? 答案是<br>否定的.创建索引, 有好的地⽅, 也有不⾜的地⽅, ⼀般情况, 对于⼀个表的索引会有限制</p></blockquote><blockquote><p>索引的好处:</p><ol><li>提⾼表数据的检索效率</li><li>如果排序的列是索引列，⼤⼤降低排序成本【索引已经经过b+树进行了排序】</li><li>在分组操作中如果分组条件是索引列，也会提⾼效率【索引已经经过b+树进行了排序，已经可以直接进行分组】</li></ol><p>索引的不⾜:</p><ol><li>增加数据的维护成本【维护索引树  要占用空间】</li><li>新增, 删除, 修改的时候, 需要对索引的结构进⾏修改维护</li></ol></blockquote><h4 id="索引的选择">索引的选择</h4><ul><li>较频繁的作为查询条件的字段应该创建索引</li><li>唯⼀性太差的字段不适合单独创建索引，即使频繁作为查询条件, ⽐如性别 【不具备区分性】</li><li>更新⾮常频繁的字段不适合创建索引【维护索引树 会降低性能】</li><li>不会出现在WHERE ⼦句中的字段不该创建索引</li></ul><h2 id="手敲sql代码">手敲sql代码</h2><p><img src="../assets/image-20230702092817163.png" alt="image-20230702092817163"></p><p>上边两个sql语句执行是不会执行的，[不能创建两张表]</p><p>而下边两个sql语句是可以直接创建两张表的</p><blockquote><p>原因: 因为在上边两个sql语句中，每行都给设定了 序号[2 3 4 5 ] 导致建表的时候</p><p>语句不会自动识别执行   将复制过来的序号当成了sql语句中自带的行号索引</p></blockquote><h3 id="测试两种引擎原子属性">测试两种引擎原子属性</h3><h4 id="Innodb引擎">Innodb引擎</h4><p><img src="../assets/image-20230702095446374.png" alt="image-20230702095446374"></p><blockquote><p>t1 是innodb 引擎 所以可以支持原子属性，在插入两条数据后，可以直接使用rollback; 命令，支持进行回滚，同时在没有使用commit命令的时候数据不会被提交。</p><p>但是回滚之后，并没有对数据表格进行修改，而是仍然是空表</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> autocommit<span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> @<span class="variable">@autocommit</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="operator">|</span> @<span class="variable">@autocommit</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="number">0</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.02</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">begin</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> t1(name) valuse(<span class="string">&#x27;nihao&#x27;</span>);</span><br><span class="line"><span class="number">1064</span> <span class="operator">-</span> You have an error <span class="keyword">in</span> your <span class="keyword">SQL</span> syntax; <span class="keyword">check</span> the manual that corresponds <span class="keyword">to</span> your MySQL server version <span class="keyword">for</span> the <span class="keyword">right</span> syntax <span class="keyword">to</span> use near <span class="string">&#x27;valuse(&#x27;</span>nihao<span class="string">&#x27;)&#x27;</span> <span class="keyword">at</span> line <span class="number">1</span></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> t1(name) <span class="keyword">values</span>(<span class="string">&#x27;nihao&#x27;</span>);</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> t1(name) <span class="keyword">values</span>(&quot;nihao&quot;);</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t1</span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="operator">|</span> id <span class="operator">|</span> name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">2</span> <span class="operator">|</span> nihao <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">3</span> <span class="operator">|</span> nihao <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.04</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">rollback</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t1;</span><br><span class="line"><span class="keyword">Empty</span> <span class="keyword">set</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> </span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="MyISAM引擎">MyISAM引擎</h4><p><img src="../assets/image-20230702095647902.png" alt="image-20230702095647902"></p><p><img src="../assets/image-20230702095732877.png" alt="image-20230702095732877"></p><p>t2数据表是使用的Myisam引擎， 在使用数据库表进行修改后，即使执行了rollback命令后，数据表中的数据仍然不会被撤销，会被直接进行commit</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">set</span> autocommit<span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> @<span class="variable">@autocommit</span></span><br><span class="line">    <span class="operator">-</span><span class="operator">&gt;</span> ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="operator">|</span> @<span class="variable">@autocommit</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="number">0</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.03</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> t2(name) <span class="keyword">values</span>(&quot;nihao&quot;);</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> t2(name) <span class="keyword">values</span>(&quot;nihao&quot;);</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t2;</span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="operator">|</span> id <span class="operator">|</span> name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">1</span> <span class="operator">|</span> nihao <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>  <span class="number">2</span> <span class="operator">|</span> nihao <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----+-------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.03</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">rollback</span>;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>==关于set autocommit=0 的命令解释==<a href="https://blog.csdn.net/LGD200008/article/details/79056645">(24条消息) MYSQL关于 autocommit 设置_LGD200008的博客-CSDN博客</a></p><p>set autocommit=0  命令 会将自动提交数据变成手动提交 但可能会有坑；</p><blockquote><p>下边链接闭坑  因为sql对数据修改的提交方式被修改，导致查询时候，写入的数据无法正常写入</p></blockquote><p><a href="https://blog.csdn.net/HopingWhite/article/details/5850461">mysql中的set autocommit =0_HopingWhite的博客-CSDN博客</a></p><h3 id="测试两种引擎的锁的粒度">测试两种引擎的锁的粒度</h3><blockquote><p>begin;  是开启一个事务；如果两个事务共同使用一个资源，或者对一个加锁数据进行修改，则必须在一个事务提交或者rollback之后，下一个事务才能进行操作。</p></blockquote><h4 id="Innodb-引擎">Innodb 引擎</h4><p><img src="../assets/image-20230702100837228.png" alt="image-20230702100837228"></p><p><img src="../assets/image-20230702100910328.png" alt="image-20230702100910328"></p><blockquote><p>事务锁是行锁，如果一个事务对表的一行数据进行上锁之后，其余事务是可以对该表的其他行数据进行修改的</p></blockquote><blockquote><p>注意 这里对数据进行修改后 必须使用commit 进行提交，因为上边修改了数据提交方式为手动提交</p></blockquote><h4 id="Myisam引擎">Myisam引擎</h4><p><img src="../assets/image-20230702101033586.png" alt="image-20230702101033586"></p><p><img src="../assets/image-20230702101052638.png" alt="image-20230702101052638"></p><blockquote><p>必须要释放一个表锁之后 另一个命令框 才能对改表 进行操作</p></blockquote><blockquote><p>必须使用一个表上锁，但一个事务进行上锁之后，其他的事务就不能对该表的任何一行数据进行修改</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChatGpt</title>
      <link href="/project/posts/41a5030d.html"/>
      <url>/project/posts/41a5030d.html</url>
      
        <content type="html"><![CDATA[<p>==ChatGpt课程实战技巧==</p><h1>第一章chatgpt是什么</h1><h3 id="第一课【基础】">第一课【基础】</h3><p>chatgpt  根据的是先验的训练数据 来进行的对话 同时可以根据上下文信息 来进行回答</p><p>借助chatgpt  来提高开发人员的效率，改变生活方式</p><h3 id="第二课">第二课</h3><h4 id="日常的情景：">日常的情景：</h4><p>chatGpt 是自然语言的方式，能够直接与使用人员进行交互</p><p>可以直接从工具上来获得信息</p><p>工作的人要将日常聊天或者工作中的关键字 关键点  变成周报 总结来进行汇报。</p><p>出门旅游 需要去查询攻略或者怎样安排</p><h4 id="情景总结">情景总结</h4><p>给工具输入信息，得到工具对应的反馈信息，并自己通过自己的总结 归纳能力，获得所需要的信息（攻略  周报 答辩信息）</p><h4 id="ChatGPT改变">ChatGPT改变</h4><blockquote><p>归纳整理能力</p></blockquote><p>以上场景借助ChatGPT可以直接帮助使用自然语言进行完成</p><blockquote><p>连续提问题和对话的能力</p></blockquote><p>ChatGPT可以联系上下文进行互动【多问多答】</p><p>能够记录上次提问中所携带的信息进行整理并汇总信息</p><p><img src="../assets/image-20230626093100203.png" alt="image-20230626093100203"></p><h4 id="已经落地">已经落地</h4><blockquote><p>客户服务（礼貌回答）</p><p>教育辅助(个性化家教)</p><p>医疗保健（大量知识库记录）</p><p>营销工作（方案设计）</p></blockquote><p><img src="../assets/image-20230626093126766.png" alt="image-20230626093126766"></p><h3 id="第三课【错误】">第三课【错误】</h3><h4 id="ChatGPT原理">ChatGPT原理</h4><p><img src="../assets/image-20230626093458141.png" alt="image-20230626093458141"></p><p>借助之前的论文  文字 各种文章作为输入 进行学习推理归纳，并根据输入的问题进行回答。</p><p>所有的回答都是之前的输入的文字的归纳或者复制（照猫画虎）</p><h3 id="第四课【优化错误-提升体验】">第四课【优化错误+提升体验】</h3><h4 id="问题总结">问题总结</h4><p><img src="../assets/image-20230626093823035.png" alt="image-20230626093823035"></p><h4 id="解决方式">解决方式</h4><p><img src="../assets/image-20230626094100286.png" alt="image-20230626094100286"></p><h4 id="实例">实例</h4><p><img src="../assets/image-20230626094149756.png" alt="image-20230626094149756"></p><h3 id="第五课【真实实例—周报】">第五课【真实实例—周报】</h3><blockquote><p>创建一个新的对话，专门用来完成周报的实现以及书写</p></blockquote><h4 id="周报助手实现">周报助手实现</h4><h5 id="1-限定身份先变成自己的周报助理">1 限定身份先变成自己的周报助理</h5><p><img src="../assets/image-20230626095432981.png" alt="image-20230626095432981"></p><blockquote><p>你现在是我的周报助理，我会跟你说我一周都做了什么，你先记录，并在我说生成周报的时候，帮我生成一周的工作报告，并根据我说的内容为准</p><p>那你能用列表的形式重复下我这周的工作吗？</p></blockquote><blockquote><p>在工作的内容说明中，增加一下子项的列举，每个工作内容至少举例子2项</p><p>请总结下我这周全部的工作内容吧</p></blockquote><blockquote><p>帮我把工作内容以typora的格式展示出来吧</p></blockquote><ol><li></li><li></li></ol><blockquote><p>后边先对ChatGPT进行一个角色限定，或者一些范围的限定，在此基础上进行问答，收紧ChatGPT的范围来提高ChatGPT的效率</p></blockquote><h5 id="2-每天发送一下自己的工作内容">2. 每天发送一下自己的工作内容</h5><p><img src="../assets/image-20230626095717973.png" alt="image-20230626095717973"></p><h5 id="3-举例扩充-格式调整">3.举例扩充+格式调整</h5><p><img src="../assets/image-20230626095806202.png" alt="image-20230626095806202"></p><blockquote><p>扩充内容</p><p><img src="../assets/image-20230626095832208.png" alt="image-20230626095832208"></p></blockquote><p><img src="../assets/image-20230626095845703.png" alt="image-20230626095845703"></p><blockquote><p>举例扩充</p></blockquote><p><img src="../assets/image-20230626095912457.png" alt="image-20230626095912457"></p><p><img src="../assets/image-20230626095924140.png" alt="image-20230626095924140"></p><p><img src="../assets/image-20230626095934688.png" alt="image-20230626095934688"></p><p>自己告诉进行格式调整即可</p><h1>第二章如何用好chatgpt</h1><h2 id="第六课【进阶】">第六课【进阶】</h2><p><img src="../assets/image-20230626095252722.png" alt="image-20230626095252722"></p><blockquote><p>prompt  提示 [输入的语言 ]</p></blockquote><p><img src="../assets/image-20230703091026653.png" alt="image-20230703091026653"></p><h3 id="prompt-的分类">prompt 的分类</h3><p><img src="../assets/image-20230703091214916.png" alt="image-20230703091214916"></p><p>列举和分类[给一个概念 进行分类]</p><p>解释一个概念[可以直接得到针对性概念]</p><p>扮演[作为某个固定的角色]作为一个英语翻译</p><p>推理和归纳[如果今天周一 那么后天是周几]</p><p>上下文[周报]</p><h3 id="prompt-工程">prompt 工程</h3><p><img src="../assets/image-20230703092254778.png" alt="image-20230703092254778"></p><p>对话前  【规范限制 】</p><p>对话中【设定引导 进行扩展 纠正】</p><p>分类不好 进行改进  内容再次进行扩充   分类在总结一下</p><h2 id="第七课-角色扮演">第七课 角色扮演</h2><h3 id="解释">解释</h3><p><img src="../assets/image-20230703092551709.png" alt="image-20230703092551709"></p><p>英语翻译 周报助手 微信机器人 【单纯的功能】</p><p>模拟一个操作系统 问答机  【固定的特征】</p><h3 id="实战">实战</h3><p><img src="../assets/image-20230703092734441.png" alt="image-20230703092734441"></p><h3 id="实例-2">实例</h3><p>扮演一个医生</p><p><img src="../assets/image-20230703092832587.png" alt="image-20230703092832587"></p><p><img src="../assets/image-20230703092926204.png" alt="image-20230703092926204"></p><p>chatgpt生成格言【个签更换】</p><p><img src="../assets/image-20230703092951007.png" alt="image-20230703092951007"></p><p><img src="../assets/image-20230703093009489.png" alt="image-20230703093009489"></p><h2 id="第八课-联系上下文-提高准确性">第八课 联系上下文 提高准确性</h2><p>ChatGPT 可以将前边说过的话进行记录，后边的回答会参考所有说过的话来进行</p><h3 id="上下文">上下文</h3><p><img src="../assets/image-20230703093239413.png" alt="image-20230703093239413"></p><p>在吃晚饭 和 吃完晚饭 所推荐的内容 是不一样的【可以根据输入进行判断】</p><p><img src="../assets/image-20230703093421491.png" alt="image-20230703093421491"></p><p>反复根据他的输出 来进行扩展 补充 分类等，不断进行训练</p><h3 id="实例-文章阅读">实例(文章阅读)</h3><p>接下来我会发送一篇文章给你，你要阅读和理解这篇文章，然后不要分送内容给我，直到我对你进行提问时，你需要根据文章的内容来回答我问的问题。</p><p><img src="../assets/image-20230703093515880.png" alt="image-20230703093515880"></p><blockquote><p>小学生阅读理解文章</p><p>理解阅读文章</p></blockquote><p><img src="../assets/image-20230710091924194.png" alt="image-20230710091924194"></p><blockquote><p>让ChatGPT以小学生的语气进行回答。答案review</p></blockquote><p><img src="../assets/image-20230710092052268.png" alt="image-20230710092052268"></p><blockquote><p>仿真回答 真的像阅读理解后给出的答案</p></blockquote><p><img src="../assets/image-20230710092147626.png" alt="image-20230710092147626"></p><blockquote><p>chatgpt 要比 苹果 小米的智能助手厉害的地方 就是他可以自己联系上下文 并且进行文章间的回答</p><p>能够实现理解文章内容的功能——本质原理是ChatGPT在训练过程中使用过太多的文章模型，所以就能够像人一样进行文章理解</p></blockquote><h2 id="第九课-设定身份实战">第九课 设定身份实战</h2><p>==prompt 的提示网站 以及相关提示语==</p><h3 id="数学老师">数学老师</h3><blockquote><p>ChatGPT 设定词</p></blockquote><p><img src="../assets/image-20230710092442455.png" alt="image-20230710092442455"></p><blockquote><p>讲解勾股定理 以及圆周率</p><p><img src="../assets/image-20230710092923154.png" alt="image-20230710092923154"></p></blockquote><blockquote><p>要求每次都要给出两道练习题</p></blockquote><p><img src="../assets/image-20230710093042466.png" alt="image-20230710093042466"></p><h3 id="java-开发面试官">java 开发面试官</h3><p><img src="../assets/image-20230710093207996.png" alt="image-20230710093207996"></p><blockquote><p>具体的提问</p></blockquote><p><img src="../assets/image-20230710093241575.png" alt="image-20230710093241575"></p><p><img src="../assets/image-20230710093400434.png" alt="image-20230710093400434"></p><blockquote><p>中途如果没有给出的话，会自动给出提示以及答案</p></blockquote><p><img src="../assets/image-20230710093448452.png" alt="image-20230710093448452"></p><blockquote><p>面试评价</p></blockquote><p>​<img src="../assets/image-20230710093512572.png" alt="image-20230710093512572"></p>]]></content>
      
      
      <categories>
          
          <category> ChatGPT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ChatGPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RocketMQ</title>
      <link href="/project/posts/69e6bbe1.html"/>
      <url>/project/posts/69e6bbe1.html</url>
      
        <content type="html"><![CDATA[<p>如果还不是很理解的话，可以去看我介绍 <code>Spring Cloud</code> 的那篇文章，其中介绍了 <code>Eureka</code> 注册中心。</p><p>你可以很明显地看出 <code>NameServer</code> 的所有节点是没有进行 <code>Info Replicate</code> 的</p><p>其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 <strong>Hash 取模法</strong> 来保证同一个订单在同一个队列中就行了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>English能力</title>
      <link href="/project/posts/5c16f2cb.html"/>
      <url>/project/posts/5c16f2cb.html</url>
      
        <content type="html"><![CDATA[<h2 id="备考技巧">备考技巧</h2><p>赶巧 在六级的驱使 和 期末考试(高级英语)和专业英语 以及未来的选择上，发现了英语的重要性，先拉长战线 慢慢把英语能力提升起来吧</p><p><img src="../assets/image-20230617091044735.png" alt="image-20230617091044735"></p><p><strong>视频中学长的英语底子</strong>，日拱一卒</p><p><a href="https://www.bilibili.com/video/BV1b7411w7BT/?vd_source=3d5976f4f96ce120bf88891647d386cb">60天爆肝587分，学长教你备考英语六级！_哔哩哔哩_bilibili</a></p><h2 id="词汇备考">词汇备考</h2><p><img src="../assets/image-20230617112714558.png" alt="image-20230617112714558"></p><p>背单词 练听力 学语法 三大战略上的准备上的懒惰 不能靠战术【刷题  改错】上的勤奋掩盖</p><p>讲押题 预测 刷题 的时候 还要继续背单词  背单词要背一辈子  【最少10年】</p><p>【不要把对将来的忧虑 变成当前的努力】不要去想象 先把手边的事做了</p><p>【喜欢的事情都不重要，重要的事情多不让人喜欢】—— 丘吉尔【吃宵夜 健身 游戏 娱乐 】【英语单词  恋爱的专一  写论文 自律】</p><p>只有自己强大起来 你才能去选择做你喜欢的事情，而不是把现在对一些事情的喜欢掩盖你自己的懒惰，那只是人性和任性罢了</p><p>【所有让人痛苦的事情不一定会让人成长，但是让人成长的事情一定是会让人痛苦的】——虎虎</p><p><img src="../assets/image-20230617113353020.png" alt="image-20230617113353020"></p><p>以上两个都是查漏补缺，为的是英语基础比较好的人准备的</p><p><img src="../assets/image-20230617113512832.png" alt="image-20230617113512832"></p><p>principal 【主要的  负责人】 王子抓住了权利 【所以是主要的负责人】——app不准确的记住了 词汇 拼写 【模糊记忆 基础不好反而会更差】</p><p>principle 【原则 准则】 王子 被抓住了 【触犯了原则】</p><p><img src="../assets/image-20230617113830305.png" alt="image-20230617113830305"></p><p>==自己从没有重视的一个地方： 重音的位置 哪怕背单词书也没重视过==</p><p>重音很重要</p><p>【勤标记  多重复】</p><p>标记 自己记不住那里  【是为什么记不住  拼写？ 词性？ 及物不及物？读音？】</p><p>divorce vt(及物  与某人离婚) vi</p><p>==背单词 要自己关注是不是词性  及物不及物 发音 以及拼写 自己关注==</p><p>不会的 标记后再背    【工具书  最好买新的  文学书 可以二手】</p><p>打钩的背 没打勾的重点背</p><p>那里不会的去打钩  【不会的单词 或者词性 重点打钩】</p><p>重看还不会的  要去在标记</p><p><img src="../assets/image-20230617120057650.png" alt="image-20230617120057650"></p><p>==勤标记 要分层次的标记==</p><p>【不同的标记  变得有趣  lv 爱玛士  ==城市 奇奇怪怪的标记==  想要的东西    打开书之后应该是美丽的世界】</p><p>背一个单词 就达成一个成就</p><p><img src="../assets/image-20230617115959652.png" alt="image-20230617115959652"></p><p>习惯了就好  头七很难   所以慢慢熬出来 习惯了就好  阈值慢慢提高</p><p>永远不要让别人告诉自己 你花期已过！！  励志精神还是要有的，you are more than what you have become now</p><p><img src="../assets/image-20230617120932320.png" alt="image-20230617120932320"></p><p>多重复   【两版 到三版 重复一次 】 虽然浪费时间 但是反而有效  慢慢来会更快</p><p>【基础都不想去触碰的话，可能永远都不会成长 】</p><p>艺术 考级 拉小提琴——学艺术的痛苦经历  农村man的被欺负生活经历</p><ol><li>直接看意思   不要读   想他的词性 以及近义词【意】<ol><li>名前动后  名词重音在前边  动词重音在后边  augment  重音在后边</li></ol></li><li>看拼写 如何导致意思  因为英语 是一个拼写的语言（词根词缀+联想法）【形】<ol><li>写七遍  联想拼写过程中 怎样实现的单词 脑子里去想他的读音</li></ol></li><li>检查读音  特别是重音【音】<ol><li>读的是错的 当然不能按照自己的读音去背【跟着音频读 不要自己去创读音】</li></ol></li><li>单词拟人化 哈哈哈我要干掉你  杀单词机器  我要去杀掉他  一种对抗的意识</li><li>随时随地重复 常忘词本     以及手写的单词本  使用a4纸 【重复单词背诵】</li></ol><p><img src="../assets/image-20230617133840764.png" alt="image-20230617133840764"></p><p>英语要努力的学好   将单词书背三遍</p><h2 id="听力备考">听力备考</h2><p><img src="../assets/image-20230626111123819.png" alt="image-20230626111123819"></p><p>有一个学习的难点——英语思维的转换</p><blockquote><p>听说读写都是会先将接触到的英语转换成为对应的中文 然后在中文思维上进行处理并进行英文输出，所以需要大量的训练听说读写 来实现英文思维的建立或者建立一个英文环境，进行英文的学习</p></blockquote><blockquote><p>难度排行</p></blockquote><p><img src="../assets/image-20230626111222695.png" alt="image-20230626111222695"></p><blockquote><p>听力 —— 保证语法和词汇的基础同时，训练英文接触的速度和准确性</p><p>翻译和写作 需要保证词汇和语法的基础并掌握相应技巧即可</p><p>阅读  保证词汇的积累+阅读技巧</p></blockquote><p><img src="../assets/image-20230626112101430.png" alt="image-20230626112101430"></p><p>==只要感觉学习是一件非常痛苦的事情，那么一定是姿势不对（学习方法错误）==</p><p>学习建立一座大山，一看搬过来一座大山，那么一定会很难。如果长远看，就像每天捡一块石头最后把石头堆积在一块那就会简单很多</p><blockquote><p>听力 每天练习半小时，将时间固定下来进行系统的训练</p></blockquote><p>==单词只要多背多复习不要等忘掉了再去复习，要多重复==</p><p>==好的记忆要建立好的学习习惯，不断去对抗要记忆变弱的情况，学习能力越强的人对抗的习惯越强，所以就是要不断的复习+不断的重复==</p><p>没努力 和努力没成果 【坚持一段时间后抱怨一下，重新思考是不是自己姿势变形，重新调整+努力】</p><p><img src="../assets/image-20230711100831424.png" alt="image-20230711100831424"></p><p><img src="../assets/image-20230711100955153.png" alt="image-20230711100955153"></p>]]></content>
      
      
      <categories>
          
          <category> 日拱一卒 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode2</title>
      <link href="/project/posts/af3ca302.html"/>
      <url>/project/posts/af3ca302.html</url>
      
        <content type="html"><![CDATA[<h1>1. 两数之和</h1><p><a href="https://leetcode.cn/problems/two-sum/">https://leetcode.cn/problems/two-sum/</a></p><p><img src="../assets/image-20230615222330774.png" alt="image-20230615222330774"></p><h4 id="思路">思路</h4><ol><li><p>暴力解法，两层for循环遍历数组，然后在数组中找和为target的元素 同时注意两个值不能为同一个数组元素【不能是同一个数组下标】因为题目中写道同一个元素不能在答案中重复出现</p></li><li><p>这里要将题目进行转换，求a+b = target  转换为 根据a 找到数组中有没有存在b=target-a  这个值的存在</p><ol><li>如果存在 需要暂存b对应的索引位置</li><li>这里不要求有序 但是不仅需要对应的数组value  还要活得该值在数组中的下边，所以考虑使用HashMap存储</li><li>使用map的话 要根据数组的value值寻找元素 所以要将数组的value作为map的key 同时将数组的index 作为map的value 存储</li><li>当没有找到对应的b时候，将数组的索引和value同时放入map中，并循环找下一个数组元素</li></ol></li><li><p>如下图解析</p><p><img src="../assets/image-20230615223043427.png" alt="image-20230615223043427"></p></li></ol><p><img src="../assets/image-20230615223050805.png" alt="image-20230615223050805"></p><blockquote><p>本题其实有四个重点：</p><ul><li>为什么会想到用哈希表</li><li>哈希表为什么用map</li><li>本题map是用来存什么的</li><li>map中的key和value用来存什么的</li></ul><p>把这四点想清楚了，本题才算是理解透彻了。</p></blockquote><h4 id="自己实现">自己实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> 哈希表;</span><br><span class="line"><span class="comment">//https://leetcode.cn/problems/two-sum/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_1_</span>两数之和 &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"><span class="type">int</span>[] nums =<span class="keyword">new</span> <span class="title class_">int</span>[] &#123;<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">6</span>&#125;;</span><br><span class="line"><span class="type">int</span>[] result = twoSum1(nums, <span class="number">11</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; result.length; i++) &#123;</span><br><span class="line">System.out.println(result[i]);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">    <span class="type">int</span>[] results = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; nums.length; j++) &#123;</span><br><span class="line"><span class="keyword">if</span> (nums[i]+nums[j]==target&amp;&amp;i!=j) &#123;</span><br><span class="line">results[<span class="number">1</span>]=i;</span><br><span class="line">results[<span class="number">0</span>]=j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> results;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 一个for循环+借助map进行查询集合 效率变得更快</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> nums</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> target</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] twoSum1(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line"><span class="comment">//    这里使用hashMap来进行查询和存储临时结果  map的key 存放对应的数组值  value存放对应数组元素的下标</span></span><br><span class="line">    <span class="type">int</span>[] results = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    HashMap&lt;Integer, Integer&gt; hashMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Integer, Integer&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line"><span class="keyword">if</span> (hashMap.containsKey(target-nums[i])) &#123;</span><br><span class="line">results[<span class="number">0</span>]= i;</span><br><span class="line">results[<span class="number">1</span>]=hashMap.get(target-nums[i]);</span><br><span class="line"><span class="comment">// 当两个key值相同的话，表明存储的是一样的值 也就是nums[] 中存了 2，2 这样的数</span></span><br><span class="line"><span class="comment">//直接返回对应两个2 的索引 仍然符合逻辑</span></span><br><span class="line"><span class="keyword">break</span>;<span class="comment">//数组中只有一组这样的数 直接退出循环就可以</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 下边不用写else 因为上边设定退出的话 可以直接排除正确的情况，否则其他的要全部加入到map中</span></span><br><span class="line">hashMap.put(nums[i],i);</span><br><span class="line"><span class="comment">//else &#123;</span></span><br><span class="line"><span class="comment">//hashMap.put(nums[i], i);</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">return</span> results;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>收获：</p><ol><li>hashMap 的操作</li><li>怎样使用哈希表+数组 对逻辑进行实现以及怎样去重</li></ol></blockquote><h4 id="参考">参考</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">    <span class="type">int</span>[] res = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span>(nums == <span class="literal">null</span> || nums.length == <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; nums.length; i++)&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> target - nums[i];   <span class="comment">// 遍历当前元素，并在map中寻找是否有匹配的key</span></span><br><span class="line">        <span class="keyword">if</span>(map.containsKey(temp))&#123;</span><br><span class="line">            res[<span class="number">1</span>] = i;</span><br><span class="line">            res[<span class="number">0</span>] = map.get(temp);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        map.put(nums[i], i);    <span class="comment">// 如果没找到匹配对，就把访问过的元素和下标加入到map中</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1>第15题. 三数之和</h1><p><a href="https://leetcode.cn/problems/3sum/">https://leetcode.cn/problems/3sum/</a></p><p><img src="../assets/image-20230616084339088.png" alt="image-20230616084339088"></p><blockquote><p>题目中要求和为0  且三个num[i] 不能重复</p><ol><li>可以使用a+b+c = 0 来进行判定 先确定a  在确定b+c= 0-a  使用三层for循环+哈希表来进行确定值【类比于两数之和】</li><li>但是要注意要求 是所有的三元组不能重复，需要判定不同的值 来将元素去重</li></ol></blockquote><h4 id="思路-2">思路</h4><ol><li>使用三层for循环 依次 来循环遍历数组元素  找到对应a b c 来求得不同的三元组<ol><li>对数组先进行排序，因为默认要求target=0 所以必须有正数和负数都存在（最大值小于0 或者最小值 大于0 都不符合题意）</li><li>可以使用哈希表来存储对应的下标和元素值</li><li>但是要求去重 可能会有很多边界条件限制</li></ol></li><li>使用双指针法，对排序数组 使用两次循环 ，降低时间复杂度</li><li>去重的逻辑【a b c  对应不同的操作】</li><li>==这里要求返回的是对应的元素值 而不是索引 所以可以对数组进行排序而不用保留索引值==</li></ol><h4 id="自己实现-2">自己实现</h4><blockquote><p>一上午的新路历城: 从早上9点开始接触三数之和 一直到下午两点 中间吃完午饭+午休后 开始总结一下</p><p>一开始代码随想录中 没有java解法，只有c++代码 ，就想着去公布的题解中找一个对应的java代码</p><ol><li>终于找到了: <a href="https://leetcode.cn/problems/3sum/solution/san-shu-zhi-he-javajian-ji-ti-jie-by-wang-zi-hao-z/">15. 三数之和，Java简洁题解（暴力、hash、双指针） - 三数之和 - 力扣（LeetCode）</a><ol><li>题解中有个hash的加速没成功 但是下一个评论中成功了 就开始想为什么会失败</li><li>最终找到了原因: 是因为没有把0,0,0， 这种情况考虑进来 把去重放到了外边导致一直死循环 需要在找到一个结果后再去重，而不是直接对第二个数字去重</li></ol></li><li>接着看到了其他的题解: 暴力解法  有一个题解是用的二分法查找 来减少第三个数据的时间 能提交通过</li><li>还有一个题解: 使用map来进行去重的同时 在map中存放 0-a-b 然后在遍历存放b的时候直接查找c 而不是存放c 在查找的时候找0-a-b</li><li>看到labuladong方法 总结了一下怎样解决n数之和 ，还需要自己动手写代码去提高双指针的思路<a href="https://labuladong.github.io/algo/di-ling-zh-bfe1b/yi-ge-fang-894da/">一个方法团灭 nSum 问题 :: labuladong的算法小抄</a></li></ol><p>​</p></blockquote><p>下边题解上一个关于hashmap去重的评论的解释</p><p><a href="https://leetcode.cn/problems/3sum/solution/san-shu-zhi-he-javajian-ji-ti-jie-by-wang-zi-hao-z/">15. 三数之和，Java简洁题解（暴力、hash、双指针） - 三数之和 - 力扣（LeetCode）</a></p><p><img src="../assets/image-20230616142239007.png" alt="image-20230616142239007"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>是java中一个List&lt;Integer&gt;，a,b,c是三个<span class="type">int</span>类型的数据，分析下下边的代码：List&lt;Integer&gt; <span class="built_in">list</span> = Arrays.asList(a, b, c);</span><br><span class="line">                <span class="built_in">list</span>.sort(Comparator.naturalOrder());</span><br><span class="line">这段代码创建了一个 List&lt;Integer&gt; 类型的对象 <span class="built_in">list</span>，其中包含三个 <span class="type">int</span> 类型的数据 a、b 和 c。</span><br><span class="line"></span><br><span class="line">Arrays.asList(a, b, c) 方法将三个 <span class="type">int</span> 类型的数据 a、b 和 c 转换为一个 List&lt;Integer&gt; 对象，并将这个对象赋值给了 <span class="built_in">list</span> 变量。</span><br><span class="line"></span><br><span class="line">接下来，<span class="built_in">list</span>.sort(Comparator.naturalOrder()) 方法会对 <span class="built_in">list</span> 中的元素进行升序排序。Comparator.naturalOrder() 方法返回一个自然顺序比较器，它会按照元素的自然顺序进行比较，因此 <span class="built_in">list</span> 中的元素将按照升序排列。</span><br><span class="line"></span><br><span class="line">最终，<span class="built_in">list</span> 中的元素会被按照升序排列，并且 <span class="built_in">list</span> 变量将引用这个排好序的 List&lt;Integer&gt; 对象。</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">list</span>是java中一个List&lt;Integer&gt;，a,b,c是三个<span class="type">int</span>类型的数据，分析下下边的代码：    Set&lt;List&lt;Integer&gt;&gt; result = new LinkedHashSet&lt;&gt;();                            </span><br><span class="line">  List&lt;Integer&gt; <span class="built_in">list</span> = Arrays.asList(a, b, c);</span><br><span class="line">                <span class="built_in">list</span>.sort(Comparator.naturalOrder());         </span><br><span class="line">                result.add(<span class="built_in">list</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">这段代码创建了一个 Set&lt;List&lt;Integer&gt;&gt; 类型的对象 result，并创建了一个 List&lt;Integer&gt; 类型的对象 <span class="built_in">list</span>，其中包含三个 <span class="type">int</span> 类型的数据 a、b 和 c。</span><br><span class="line"></span><br><span class="line">接下来，<span class="built_in">list</span>.sort(Comparator.naturalOrder()) 方法会对 <span class="built_in">list</span> 中的元素进行升序排序。</span><br><span class="line"></span><br><span class="line">然后，result.add(<span class="built_in">list</span>) 方法将 <span class="built_in">list</span> 对象添加到 result 集合中。由于 List 对象是可变对象，因此它们可以被改变。因此，如果稍后修改了 <span class="built_in">list</span> 对象中的元素，则 result 集合中的对象也会受到影响。为了避免这种情况，可以创建一个新的 List 对象并将其添加到 result 集合中，而不是直接将 <span class="built_in">list</span> 添加到 result 集合中。</span><br><span class="line"></span><br><span class="line">最终，result 集合中将包含一个 List&lt;Integer&gt; 对象，其中包含 a、b 和 c 这三个元素，并按照升序排列。由于 result 集合是一个 Set 集合，因此其中不会包含重复的 List&lt;Integer&gt; 对象。此外，它使用的是 LinkedHashSet 类型，保证了插入顺序与遍历顺序一致。</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>.sort(Comparator.naturalOrder());代码的底层原理</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">`<span class="built_in">list</span>.sort(Comparator.naturalOrder())` 这段代码的底层原理主要包括以下两个方面：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> `sort` 方法的实现原理：`sort` 方法是 `List` 接口中的一个默认方法，它的默认实现是使用归并排序算法进行排序。具体来说，`sort` 方法会将列表分为较小的子列表，对每个子列表进行排序，然后将排序后的子列表合并为一个大的排序列表。这个过程会递归进行，直到整个列表被排序。在 Java <span class="number">7</span> 中，`sort` 方法的实现是使用经过优化的合并排序算法；在 Java <span class="number">8</span> 中，它的实现改为了使用双轴快速排序算法。</span><br><span class="line"><span class="number">2.</span> `Comparator.naturalOrder()` 的实现原理：`naturalOrder()` 是 `Comparator` 接口中的一个静态方法，它返回一个自然顺序比较器。自然顺序是一种在数值和字母上都有定义的默认比较顺序，它遵循一些常见的规则，例如将数字按照升序排列，将字母按照字母表顺序排列等等。在 Java 中，`naturalOrder()` 方法的实现是通过调用 `Comparable` 接口中的 `compareTo()` 方法来实现的，这个方法定义了对象之间的自然顺序。如果要使用其他排序规则，可以通过编写自定义的比较器来实现。</span><br><span class="line"></span><br><span class="line">​</span><br><span class="line"></span><br><span class="line">###### </span><br></pre></td></tr></table></figure><h6 id="去重的逻辑">去重的逻辑</h6><blockquote><p>这里去重的时候 【-1，-1,2】 这种情况  当num[i] 指向第一个-1 的时候，应该判定后边的情况看是否符合a+b+c=0</p><p>当第一个-1判定结束之后，num[i] 指向第二个-1 应该判断前边的值-1 是不是已经检验过一次 此时前边也是-1  表明这种情况出现过 所以直接跳过</p><p>这里num[i] 是针对a 的情况  先确定a  在确定 b+c=0-a</p></blockquote><h1>Restart</h1><p><img src="../assets/image-20230702113117502.png" alt="image-20230702113117502"></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>成长那些事</title>
      <link href="/project/posts/330ccd1e.html"/>
      <url>/project/posts/330ccd1e.html</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试!" data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="da6f2663aaa43cb951e34bf74da1a8d19793d0b00b9e41eb8fa321cb18d51e3d">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-xray">      <input class="hbe hbe-input-field hbe-input-field-xray" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-xray" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-xray">这里记录我自己成长的历程，需要密码才能进入!</span>      </label>      <svg class="hbe hbe-graphic hbe-graphic-xray" width="300%" height="100%" viewBox="0 0 1200 60" preserveAspectRatio="none">        <path d="M0,56.5c0,0,298.666,0,399.333,0C448.336,56.5,513.994,46,597,46c77.327,0,135,10.5,200.999,10.5c95.996,0,402.001,0,402.001,0"></path>        <path d="M0,2.5c0,0,298.666,0,399.333,0C448.336,2.5,513.994,13,597,13c77.327,0,135-10.5,200.999-10.5c95.996,0,402.001,0,402.001,0"></path>      </svg>    </div>  </div></div><script data-pjax src="/project/lib/hbe.js"></script><link href="/project/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> myself </category>
          
      </categories>
      
      
        <tags>
            
            <tag> myself </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研一通宵记</title>
      <link href="/project/posts/e3464411.html"/>
      <url>/project/posts/e3464411.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>leetcode1</title>
      <link href="/project/posts/3635f2b8.html"/>
      <url>/project/posts/3635f2b8.html</url>
      
        <content type="html"><![CDATA[<h1>代码随想录每日刷题1-2</h1><h2 id="哈希表题目">哈希表题目</h2><h3 id="哈希表基础知识【java-VS-c-】">哈希表基础知识【java VS c++】</h3><p>Hash table（散列表）</p><blockquote><p>哈希表是根据关键码的值而直接进行访问的数据结构</p></blockquote><p>数组就是一张哈希表【关键码VS对应码值】</p><p>作用？ 快速查找某一数据是否出现过</p><h4 id="哈希函数">哈希函数</h4><h4 id="三种哈希结构">三种哈希结构</h4><p>数组</p><p>hash_set【set multiSet  unorderedSet】</p><p>hash_map【map multiMap unorderedMap】</p><p>java 的Collection和map结构分类</p><p><img src="../assets/image-20230613094154258.png" alt="image-20230613094154258"></p><p>Collection集合与map哈希的详解图</p><p><img src="../assets/image-20230613094242900.png" alt="image-20230613094242900"></p><p>待解决</p><p>==java的本地方法hashCode() 函数详解==https://tobebetterjavaer.com/basic-extra-meal/hashcode.html</p><h2 id="242-有效的字母异位词">_242_有效的字母异位词</h2><p><a href="https://leetcode.cn/problems/valid-anagram/">https://leetcode.cn/problems/valid-anagram/</a></p><blockquote><p>题目要求[要求字符的数量和对应符号必须全部相同]</p><p>给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。</p><p>示例 1: 输入: s = “anagram”, t = “nagaram” 输出: true</p><p>示例 2: 输入: s = “rat”, t = “car” 输出: false</p><p><strong>说明:</strong> 你可以假设字符串只包含小写字母。</p></blockquote><blockquote><p>思路： 一个数组记录出现的次数【增加次数】，另一个数组来遍历字符串减少次数，最后如果一致，则数组为{0}</p></blockquote><h4 id="自己实现">自己实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> 哈希表;</span><br><span class="line"><span class="comment">//https://leetcode.cn/problems/valid-anagram/</span></span><br><span class="line"><span class="comment">// 思路 使用数组 记录26个英文字母出现的次数【s字符串中某字母出现一次个数+1】 然后遍历另一个字符串【t字符串中】</span></span><br><span class="line"><span class="comment">// 对应的数组位置—1 最后判定数组位置是否全为0</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_242_</span>有效的字母异位词 &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">aString</span> <span class="operator">=</span><span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">bString</span> <span class="operator">=</span><span class="string">&quot;worldhello&quot;</span>;</span><br><span class="line">System.out.println(isAnagram(aString, bString));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isAnagram</span><span class="params">(String s, String t)</span> &#123;</span><br><span class="line"><span class="type">int</span> num[] =  <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">26</span>];<span class="comment">// int num[26] = &#123;0&#125;; 这是c的写法</span></span><br><span class="line"><span class="comment">//new 后 会直接进行初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i&lt;s.length(); i++) &#123;</span><br><span class="line"><span class="comment">//num[s[i]-&#x27;a&#x27;]++;</span></span><br><span class="line"><span class="comment">//s[i]=0;   这种写法也是c 或者c++ 的方式</span></span><br><span class="line">num[s.charAt(i) -<span class="string">&#x27;a&#x27;</span>]++;<span class="comment">// 并不需要记住字符a的ASCII，只要求出一个相对数值就可以了</span></span><br><span class="line"><span class="comment">//charAt() 方法用于返回指定索引处的字符。索引范围为从 0 到 length() - 1。</span></span><br><span class="line"><span class="comment">//上边的一句话 实现的将对应26个字母的从0-26 实现映射(将字母转为ASCII 数值)并记录其出现次数</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; t.length(); i++) &#123;</span><br><span class="line">num[t.charAt(i)-<span class="string">&#x27;a&#x27;</span>]--;<span class="comment">// 将t中出现的字母在num中去掉</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; num.length; i++) &#123;</span><br><span class="line"><span class="comment">//if (num[i] &gt;0) &#123;// 这里 不能写&gt;0  因为如果s 是&quot;a&quot; r = &quot;ab&quot; 则会有num[1]&lt;0 但仍会返回</span></span><br><span class="line"><span class="keyword">if</span> (num[i]!=<span class="number">0</span>) &#123;<span class="comment">//包括&gt; &lt; 0  都一样</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>收获：</p><ol><li>java的数组新建 用new  且会自动初始化 区别与c++</li><li>String字符串不是Iterable的 只有数组 才能用for(int n ： num)进行遍历</li><li>提交时候的bug 最后一个地方使用了错误的判定条件 应该用num[i] != 0 而不是&gt; 0【忽略了小于0 的情况】</li></ol></blockquote><h4 id="参考">参考</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 242. 有效的字母异位词 字典解法</span></span><br><span class="line"><span class="comment"> * 时间复杂度O(m+n) 空间复杂度O(1)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">isAnagram</span><span class="params">(String s, String t)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] record = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">26</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class="line">            record[s.charAt(i) - <span class="string">&#x27;a&#x27;</span>]++;     <span class="comment">// 并不需要记住字符a的ASCII，只要求出一个相对数值就可以了</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; t.length(); i++) &#123;</span><br><span class="line">            record[t.charAt(i) - <span class="string">&#x27;a&#x27;</span>]--;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> count: record) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count != <span class="number">0</span>) &#123;               <span class="comment">// record数组如果有的元素不为零0，说明字符串s和t 一定是谁多了字符或者谁少了字符。</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;                        <span class="comment">// record数组所有元素都为零0，说明字符串s和t是字母异位词</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>来自代码随想录</p></blockquote><h2 id="349-两个数组的交集">_349_两个数组的交集</h2><p><a href="https://leetcode.cn/problems/intersection-of-two-arrays/">https://leetcode.cn/problems/intersection-of-two-arrays/</a></p><p>题意：给定两个数组，编写一个函数来计算它们的交集。</p><p><img src="../assets/image-20230702120047241.png" alt="image-20230702120047241"></p><blockquote><p>思路</p><ol><li>暴力解法，两个数组先进行排序，直接遍历一个数组，另一个数组遍历查找看是否有交集<ol><li>这里有一个难点 就是将结果进行去重，【题目要求不用排序】——借助set 集合进行去重存储 最后把set转换为数组结果返回</li></ol></li><li>额外技巧: 数组先进行排序后， 使用双指针遍历两个数组的同时，因为结果集一定是递增进行加入结果集的，所以使用临时变量存储加入结果集中的数据，如果两个数组中公有且不等于临时变量值，表明不重复，==达到去重效果==</li><li>使用set存储一个数组【哈希表 】直接对一个数组去重，另一个数组也是用set进行去重，然后使用set 的O(1)的方式进行查找，并进行结果拼凑</li></ol></blockquote><p>题解中不断优化的过程:</p><blockquote><p>算暴力，执行用时 : 6 ms , 在所有 Java 提交中击败了 31.83% 的用户</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">        Set&lt;Integer&gt; result = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span> (nums1 == <span class="literal">null</span> || nums2 == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> <span class="variable">len1</span> <span class="operator">=</span> nums1.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">len2</span> <span class="operator">=</span> nums2.length;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; len1; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (checkIsExist(nums2, nums1[i])) &#123;</span><br><span class="line">                result.add(nums1[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span>[] finalResult = <span class="keyword">new</span> <span class="title class_">int</span>[result.size()];</span><br><span class="line">        <span class="type">int</span> <span class="variable">start</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : result) &#123;</span><br><span class="line">            finalResult[start] = item;</span><br><span class="line">            start++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> finalResult;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">checkIsExist</span><span class="params">(<span class="type">int</span>[] num2, <span class="type">int</span> cur)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; num2.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (num2[i] == cur) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>稍微优化了一下：执行用时 : 5 ms , 在所有 Java 提交中击败了 41.75% 的用户</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums1==<span class="literal">null</span> || nums2==<span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        Set&lt;Integer&gt; set1 = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        Set&lt;Integer&gt; set2 = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item: nums1) &#123;</span><br><span class="line">            set1.add(item);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item: nums2) &#123;</span><br><span class="line">            set2.add(item);</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;Integer&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item: set1) &#123;</span><br><span class="line">            <span class="keyword">if</span> (set2.contains(item)) &#123;</span><br><span class="line">                list.add(item);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span>[] result = <span class="keyword">new</span> <span class="title class_">int</span>[list.size()];</span><br><span class="line">        <span class="type">int</span> <span class="variable">start</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item: list) &#123;</span><br><span class="line">            result[start] = item;</span><br><span class="line">            start++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>再次优化，采用内置方法，set的retainAll:执行用时 : 4 ms , 在所有 Java 提交中击败了 63.82% 的用户</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums1 == <span class="literal">null</span> || nums2 == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Set&lt;Integer&gt; set1 = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        Set&lt;Integer&gt; set2 = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : nums1) &#123;</span><br><span class="line">            set1.add(item);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : nums2) &#123;</span><br><span class="line">            set2.add(item);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        set1.retainAll(set2);</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span>[] result = <span class="keyword">new</span> <span class="title class_">int</span>[set1.size()];</span><br><span class="line">        <span class="type">int</span> <span class="variable">start</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : set1) &#123;</span><br><span class="line">            result[start] = item;</span><br><span class="line">            start++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>减少一个set，少一次循环和调用内置方法：执行用时 : 3 ms , 在所有 Java 提交中击败了 98.08% 的用户</p></blockquote><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Solution &#123;</span><br><span class="line">    <span class="built_in">public</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums1 == <span class="keyword">null</span> || nums2 == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">Set</span>&lt;<span class="type">Integer</span>&gt; set1 = <span class="built_in">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="keyword">Set</span>&lt;<span class="type">Integer</span>&gt; set2 = <span class="built_in">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : nums1) &#123;</span><br><span class="line">            set1.<span class="keyword">add</span>(item);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : nums2) &#123;</span><br><span class="line">            <span class="keyword">if</span> (set1.contains(item)) &#123;</span><br><span class="line">                set2.<span class="keyword">add</span>(item);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span>[] result = <span class="built_in">new</span> <span class="type">int</span>[set2.size()];</span><br><span class="line">        <span class="type">int</span> start = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> item : set2) &#123;</span><br><span class="line">            result[<span class="keyword">start</span>] = item;</span><br><span class="line">            <span class="keyword">start</span>++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>最优解</p></blockquote><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Solution &#123;</span><br><span class="line">    <span class="built_in">public</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">        <span class="keyword">Set</span>&lt;<span class="type">Integer</span>&gt; <span class="keyword">set</span> = <span class="built_in">new</span> HashSet&lt;&gt;();</span><br><span class="line">        Map&lt;<span class="type">Integer</span>,<span class="type">Integer</span>&gt; map = <span class="built_in">new</span> HashMap();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums2.length; i++) &#123;</span><br><span class="line">            map.put(nums2[i],<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums1.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(<span class="keyword">set</span>.contains(nums1[i]))<span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">if</span>(map.<span class="keyword">get</span>(nums1[i])!=<span class="keyword">null</span>)<span class="keyword">set</span>.<span class="keyword">add</span>(nums1[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span>[] res = <span class="built_in">new</span> <span class="type">int</span>[<span class="keyword">set</span>.size()];</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">Integer</span> ii:<span class="keyword">set</span>)&#123;</span><br><span class="line">            res[i++]=i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="自己动手实现">自己动手实现</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> 哈希表;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="comment">//https://leetcode.cn/problems/intersection-of-two-arrays/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_349_</span>两个数组的交集 &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"><span class="type">int</span> a[] = <span class="keyword">new</span> <span class="title class_">int</span>[] &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="type">int</span> b[] = <span class="keyword">new</span> <span class="title class_">int</span>[] &#123;<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">1</span>&#125;;</span><br><span class="line"><span class="type">int</span>[] result =intersection1(a, b);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i : result) &#123;</span><br><span class="line">System.out.println(i);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这里使用set直接去重实现</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nums1</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nums2</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] intersection(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//    1. 使用集合set直接实现结果去重 </span></span><br><span class="line">    <span class="keyword">if</span>(nums1==<span class="literal">null</span>||nums1.length==<span class="number">0</span>||nums2==<span class="literal">null</span>||nums2.length==<span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">0</span>];   </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (nums1.length&gt;nums2.length) &#123;<span class="comment">//这里借助一个比较+递归 实现选择比较短的数组进行写入set中</span></span><br><span class="line"><span class="keyword">return</span> intersection(nums2,nums1);</span><br><span class="line">&#125;</span><br><span class="line">    Set&lt;Integer&gt; set1 = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;Integer&gt;();</span><br><span class="line">    Set&lt;Integer&gt; resultSet = <span class="keyword">new</span> <span class="title class_">HashSet</span>&lt;Integer&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i : nums1) &#123;</span><br><span class="line">set1.add(i);<span class="comment">// 直接将数组中的数据加入到set中去</span></span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i : nums2) &#123;</span><br><span class="line"><span class="keyword">if</span> (set1.contains(i)) &#123;<span class="comment">// 这里在遍历另一个数组的时候直接进行查找比较 看是否存在</span></span><br><span class="line">resultSet.add(i);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//    后边需要将结果set 集合转为数组</span></span><br><span class="line"><span class="comment">//    1. 直接将集合转为数组</span></span><br><span class="line"><span class="comment">//    return resultSet.stream().mapToInt(x -&gt;x).toArray();</span></span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span>[] result  = <span class="keyword">new</span> <span class="title class_">int</span>[resultSet.size()];</span><br><span class="line"><span class="comment">//    2.遍历set集合重新将数据写入到数组中并返回</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Integer integer : resultSet) &#123;</span><br><span class="line">result[i++] = integer;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 这里使用数组排序利用临时变量存储增量数据值+双指针对两个数组进行遍历</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> nums1</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> nums2</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">//    这里 两个数组的索引index1 和index2 同时作为指针 并且使用index 指向结果数组最后一个插入元素的位置</span></span><br><span class="line"><span class="comment">//    因为两个原数组是排序的，所以结果集中的数据是增序，只要新插入的数据不等于前一个插入的数据 就是结果中的一个</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] intersection1(<span class="type">int</span>[] nums1, <span class="type">int</span>[] nums2) &#123;</span><br><span class="line">    Arrays.sort(nums1);<span class="comment">// 数组排序方法</span></span><br><span class="line">    Arrays.sort(nums2);<span class="comment">// 数组排序方法</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">length1</span> <span class="operator">=</span> nums1.length, length2 = nums2.length;</span><br><span class="line">    <span class="type">int</span>[] intersection = <span class="keyword">new</span> <span class="title class_">int</span>[length1 + length2];<span class="comment">// 数组存储两个数组的元素</span></span><br><span class="line">    <span class="type">int</span> index=<span class="number">0</span>,index1=<span class="number">0</span>,index2=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(index1&lt;length1&amp;&amp;index2&lt;length2) &#123;<span class="comment">//两个指针都没有走向数组末尾，有一个走向结尾结束</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">num1</span> <span class="operator">=</span>nums1[index1];</span><br><span class="line">    <span class="type">int</span> <span class="variable">num2</span> <span class="operator">=</span>nums2[index2];</span><br><span class="line">    <span class="keyword">if</span> (num1==num2) &#123;</span><br><span class="line"><span class="comment">//    使用结果集最后一次插入的数据与新插入数据判等</span></span><br><span class="line">    <span class="keyword">if</span> (index==<span class="number">0</span>||intersection[index-<span class="number">1</span>]!=num1) &#123;</span><br><span class="line">    intersection[index++]=num1;</span><br><span class="line">&#125;</span><br><span class="line">    index1++;<span class="comment">//这里在一开始自己写的时候忘记</span></span><br><span class="line">    index2++;<span class="comment">//忘记让两个相等情况下的index1 和2 自增</span></span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (num1&gt;num2) &#123;</span><br><span class="line">index2++;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">index1++;</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> Arrays.copyOfRange(intersection, <span class="number">0</span>, index);<span class="comment">// 只取[0,index)</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>收获:</p><ol><li>使用递归对两个数组的length较大的一个数组进行写入集合操作 【上边代码31行】</li><li>Set集合的操作</li><li>怎样对两个数组进行去重操作【排序后对最近一次插入的数据进行记录并与新插入数据进行比较——排序的目的是保证结果是递增，才能只需要对最新插入数据比较即可】</li><li>怎样将set集合结果变成数组  resultSet.stream().mapToInt(x -&gt;x).toArray();</li><li>怎样在只用一个循环和一个set的情况下对数组进行快速判定元素是否出现过</li><li></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HelloWorld</title>
      <link href="/project/posts/77770c79.html"/>
      <url>/project/posts/77770c79.html</url>
      
        <content type="html"><![CDATA[<p>-------------------------------------------------------------------------------2023.8.23更新---------------------------------------------------------------------------------------------------------------</p><p><code>sticky: 2</code> 在设置栏中 标识的是是否置顶展示，数字越大，权重越高</p><p><img src="../assets/image-20230823112035025.png" alt="image-20230823112035025"></p><p><code>swiper_index: 7</code> 在设置栏中 标识的是是否轮播展示，数字越大，权重越高</p><p><img src="../assets/image-20230823112047021.png" alt="image-20230823112047021"></p><p>-------------------------------------------------------------------------------分割线---------------------------------------------------------------------------------------------------------------</p><blockquote><p>套用的是<a href="https://www.fomal.cc/">🥝 (fomal.cc)</a>  的模板，直接进行的自己改造</p></blockquote><p>2023.6.10 终于可以写自己网站上的博客，先把时间拉长，自己写几个文档，然后上传到上边去之后，慢慢的改进，借助时间的帮助不断把自己的个人博客封面以及内容完善起来</p><p>后边慢慢改进自己的小博客，先借着这个模板，能够自己提交图片+他的UI设计界面确实也挺好看的，后边自己魔改</p><p>​--------加油！ 马增龙，给自己前两周画一个圆满的句号。你终于成功了</p><p><img src="../assets/image-20230610171443769.png" alt=""></p><h3 id="搭建过程">搭建过程</h3><ol><li>以gitee作为仓库，进行配置</li><li><a href="https://github.com/fomalhaut1998/hexo-theme-Fomalhaut">fomalhaut1998/hexo-theme-Fomalhaut: 🥝的网站源码 (github.com)</a> 网站源码 作为基础</li><li>借助这个模板的图片功能，自己在gitee上传图片不能展示</li><li>按照他的模板，把自己的图片放到assert/ 文件夹下 可以直接使用</li><li>视频教程<a href="https://www.bilibili.com/video/BV16g411W7w7/?spm_id_from=333.788.top_right_bar_window_history.content.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">第13期：自定义彩色图标+渐变色版权美化_哔哩哔哩_bilibili</a></li><li>github仓库链接<a href="https://github.com/fomalhaut1998/hexo-theme-Fomalhaut">fomalhaut1998/hexo-theme-Fomalhaut: 🥝的网站源码 (github.com)</a></li></ol><h4 id="参考连接">参考连接:</h4><blockquote><p>官方文档 <a href="https://hexo.io/zh-cn/docs/asset-folders">资源文件夹 | Hexo</a></p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/420368532">hexo+gitee(码云)免费搭建个人静态博客（避开所有坑） - 知乎 (zhihu.com)</a></p><p><img src="../assets/image-20230610173005827.png" alt="image-20230610173005827"></p><p>最详细的链接 【自己全程参考 搭建到gitee上】<a href="https://blog.csdn.net/weixin_45631738/article/details/104716374">https://blog.csdn.net/weixin_45631738/article/details/104716374</a></p><p><img src="../assets/image-20230610172844161.png" alt="image-20230610172844161"></p><p>【下边是补充】</p><p><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029">https://blog.csdn.net/sinat_37781304/article/details/82729029</a></p><p><a href="https://blog.csdn.net/qingmuzhang/article/details/119906728#:~:text=1%E3%80%81%E9%A6%96%E5%85%88%E7%99%BB%E5%BD%95gitee%2C%20https%3A%2F%2Fgitee.com%2F%20%E5%88%9B%E5%BB%BA%E4%BB%93%E5%BA%93%202%E3%80%81%E9%85%8D%E7%BD%AEsshkey%20%E7%94%B1%E4%BA%8E%E4%B9%8B%E5%89%8D%E7%94%9F%E6%88%90%E8%BF%87%EF%BC%8C%E6%89%80%E4%BB%A5%E5%8F%AA%E9%9C%80%E8%A6%81%E6%89%BE%E5%88%B0%E4%B9%8B%E5%89%8D%E7%9A%84id_rsa.pub%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E5%A4%8D%E5%88%B6%E5%8D%B3%E5%8F%AF%20%E7%82%B9%E5%87%BBgitee%E8%AE%BE%E7%BD%AE%EF%BC%8C%E5%8E%BB%E9%85%8D%E7%BD%AEsshkey,%E7%82%B9%E5%87%BBSSH%E5%85%AC%E9%92%A5%EF%BC%8C%E8%BE%93%E5%85%A5%E6%A0%87%E9%A2%98%E5%92%8C%E5%A4%8D%E5%88%B6%E7%9A%84%E5%85%AC%E9%92%A5%E8%BF%9B%E8%A1%8C%E9%85%8D%E7%BD%AE%20hexo%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%20%E5%9C%A8hexo%E6%A0%B9%E7%9B%AE%E5%BD%95%E7%9A%84_config.yml%E4%B8%AD%E8%BF%9B%E8%A1%8C%E9%85%8D%E7%BD%AE%E4%BB%93%E5%BA%93%E5%9C%B0%E5%9D%80%20%E9%85%8D%E7%BD%AE%E5%AE%8C%E6%88%90%EF%BC%8C%E6%89%A7%E8%A1%8Chexo%20g%20-d%E5%B0%86%E5%8D%9A%E5%AE%A2%E6%8E%A8%E9%80%81%E5%88%B0GitHub%E5%92%8Cgitee%E3%80%82">https://blog.csdn.net/qingmuzhang/article/details/119906728#:~:text=1、首先登录gitee%2C https%3A%2F%2Fgitee.com%2F 创建仓库 2、配置sshkey 由于之前生成过，所以只需要找到之前的id_rsa.pub文件打开复制即可 点击gitee设置，去配置sshkey,点击SSH公钥，输入标题和复制的公钥进行配置 hexo项目配置 在hexo根目录的_config.yml中进行配置仓库地址 配置完成，执行hexo g -d将博客推送到GitHub和gitee。</a></p><h4 id="优质目标网站">优质目标网站</h4><h5 id="目标0">目标0</h5><p><a href="https://akilar.top/">Akilarの糖果屋 - Akilar.top</a></p><p><img src="../assets/image-20230610174812403.png" alt="image-20230610174812403"></p><p><a href="https://kaiter-plus.gitee.io/">Kaiter 的博客 (gitee.io)</a></p><h5 id="目标1">目标1</h5><p>带教程的网站</p><p><a href="https://fe32.top/articles/hexo1601/">https://fe32.top/articles/hexo1601/</a></p><p><img src="../assets/image-20230610172716750.png" alt="image-20230610172716750"></p><p>动画很好看</p><p><img src="../assets/image-20230610172731894.png" alt="image-20230610172731894"></p><p>一些坑</p><p><img src="../assets/image-20230610172741302.png" alt="image-20230610172741302"></p><h5 id="目标2">目标2</h5><p><a href="https://www.fomal.cc/">🐖抓到你啦～ (fomal.cc)</a></p><h5 id="目标3">目标3</h5><p><a href="https://zeabur.blog.ccknbc.cc/page/3/#content-inner">CC的部落格 - CC康纳百川 (ccknbc.cc)</a></p><p><a href="https://www.mintimate.cn/">雨月空间站 (mintimate.cn)</a></p><h5 id="目标4">目标4</h5><p><a href="https://wtlumos.github.io/archives/">归档 | W.T.的博客 (wtlumos.github.io)</a></p><h5 id="目标5">目标5</h5><p><a href="https://qingmuzhang.gitee.io/archives/">归档 | 钥零零の博客 (gitee.io)</a></p><h3 id="一些bug">一些bug</h3><blockquote><p>图片bug</p></blockquote><p>[<a href="https://zhuanlan.zhihu.com/p/405551874">Hexo博客搭建]第4章 Hexo图片显示问题 - 知乎 (zhihu.com)</a></p><p>图片问题 一直是自己搭建hexo的最大问题 又不想去弄图床</p><blockquote><p>git不够深入</p></blockquote><p><a href="https://blog.csdn.net/weixin_39876002/article/details/111547464?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-111547464-blog-126908436.235%5Ev38%5Epc_relevant_sort&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-111547464-blog-126908436.235%5Ev38%5Epc_relevant_sort&amp;utm_relevant_index=2">gitee怎么看用户名_多账号如何登录gitee_weixin_39876002的博客-CSDN博客</a></p><p>在使用git对gitee 和github 同时进行操作的时候，会经常出现问题：</p><ol><li>git的ssh链接方式 一直出现问题，因为自己的git已经配置好了obsidian 所以不能重装</li><li>git链接 借助代理 进行了转向，但是一直存在问题</li><li>链接deny 或者out time 链接</li><li>出现Permission denied, please try again</li><li><a href="https://blog.csdn.net/GX_1_11_real/article/details/80423409">https://blog.csdn.net/GX_1_11_real/article/details/80423409</a></li><li><img src="../assets/image-20230610174205030.png" alt="image-20230610174205030"></li></ol><p>git多用户处理</p><p><a href="https://blog.csdn.net/weixin_39876002/article/details/111547464?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-111547464-blog-126908436.235%5Ev38%5Epc_relevant_sort&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-111547464-blog-126908436.235%5Ev38%5Epc_relevant_sort&amp;utm_relevant_index=2">gitee怎么看用户名_多账号如何登录gitee_weixin_39876002的博客-CSDN博客</a></p><h2 id="总结">总结</h2><p>1.图片在md文件中使用官方文档的引用方式不能查看</p><h4 id="问题描述1">问题描述1</h4><p><img src="../assets/image-20230610175314173.png" alt="image-20230610175314173"></p><p>上边的官方文档中只能使用上边两种方式引入md文件的图片</p><h4 id="解决方式-1">解决方式: 1</h4><p>使用<a href="https://github.com/hexojs/hexo-renderer-marked">hexo-renderer-marked</a> 3.1.0 这个插件 可以直接进行图片的插入</p><p>将typora 的图片 设置在粘贴的时候 保存到上一层目录 下的asset目录下</p><p>然后可以直接使用md的编辑模式 插入图片</p><p><img src="%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84" alt=""></p><p><img src="../assets/image-20230610175054924.png" alt="image-20230610175054924"></p><p><img src="../assets/image-20230610175545476.png" alt="image-20230610175545476"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-marked --save</span><br><span class="line"><span class="comment">// 使用上边命令进行下载安装</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> helloworld </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown语法与外挂标签写法汇总</title>
      <link href="/project/posts/2013454d.html"/>
      <url>/project/posts/2013454d.html</url>
      
        <content type="html"><![CDATA[<h1>1.Markdown语法自带格式</h1><div class="note info flat"><p>参考：<a href="https://blog.csdn.net/u014061630/article/details/81359144">Markdown语法图文全面详解(10分钟学会)</a></p></div><div class="note warning flat"><p>注意：此页面偶尔会存在CSS冲突问题!</p></div><h2 id="1-1-代码块">1.1 代码块</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\```shell</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">VSCode终端</span></span><br><span class="line">hexo clean; hexo s</span><br><span class="line">hexo clean; hexo g; hexo d</span><br><span class="line">git add .; git commit -m &quot;npm publish&quot;; npm version patch; </span><br><span class="line">git push</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Cmder终端</span></span><br><span class="line">hexo clean &amp;&amp; hexo s</span><br><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br><span class="line">git add . &amp;&amp; git commit -m &quot;npm publish&quot; &amp;&amp; npm version patch</span><br><span class="line">git push</span><br><span class="line">\```</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">VSCode终端</span></span><br><span class="line">hexo clean; hexo s</span><br><span class="line">hexo clean; hexo g; hexo d</span><br><span class="line">git add .; git commit -m &quot;npm publish&quot;; npm version patch; </span><br><span class="line">git push</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Cmder终端</span></span><br><span class="line">hexo clean &amp;&amp; hexo s</span><br><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br><span class="line">git add . &amp;&amp; git commit -m &quot;npm publish&quot; &amp;&amp; npm version patch</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-2-多级标题">1.2 多级标题</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># H1</span></span><br><span class="line"><span class="section">## H2</span></span><br><span class="line"><span class="section">### H3</span></span><br><span class="line"><span class="section">#### H4</span></span><br><span class="line"><span class="section">##### H5</span></span><br><span class="line"><span class="section">###### H6</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><p>见本文章标题!</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-3-文字样式">1.3 文字样式</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">u</span>&gt;</span></span>下划线演示<span class="language-xml"><span class="tag">&lt;/<span class="name">u</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">文字<span class="strong">**加粗**</span>演示</span><br><span class="line"></span><br><span class="line">文字<span class="emphasis">*斜体*</span>演示</span><br><span class="line"></span><br><span class="line">文本<span class="code">`高亮`</span>演示</span><br><span class="line"></span><br><span class="line">文本~~删除~~线演示</span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">font</span> <span class="attr">size</span> = <span class="string">5</span>&gt;</span></span>5号字<span class="language-xml"><span class="tag">&lt;/<span class="name">font</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">font</span> <span class="attr">face</span>=<span class="string">&quot;黑体&quot;</span>&gt;</span></span>黑体<span class="language-xml"><span class="tag">&lt;/<span class="name">font</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">font</span> <span class="attr">color</span>=<span class="string">blue</span>&gt;</span></span>蓝色<span class="language-xml"><span class="tag">&lt;/<span class="name">font</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">table</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">MistyRose</span>&gt;</span></span>这里的背景色是：MistyRosen，此处输入任意想输入的内容<span class="language-xml"><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><p><u>下划线演示</u></p><p>文字<strong>加粗</strong>演示</p><p>文字<em>斜体</em>演示</p><p>文本<code>高亮</code>演示</p><p>文本<s>删除</s>线演示</p><p><font size = 5>5号字</font><br><font face="黑体">黑体</font><br><font color=blue>蓝色</font></p><table><tr><td bgcolor=MistyRose>这里的背景色是：MistyRosen，此处输入任意想输入的内容</td></tr></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><div class="note info flat"><p>上述要点可参考:<a href="https://blog.csdn.net/qq_43732429/article/details/108034518">【Markdown语法】字体颜色大小及文字底色设置</a></p></div><h2 id="1-4-引用">1.4 引用</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="quote">&gt;  Java</span></span><br><span class="line"><span class="quote">&gt; 二级引用演示</span></span><br><span class="line"><span class="quote">&gt; MySQL</span></span><br><span class="line"><span class="quote">&gt; &gt;外键</span></span><br><span class="line"><span class="quote">&gt; &gt;</span></span><br><span class="line"><span class="quote">&gt; &gt;事务</span></span><br><span class="line"><span class="quote">&gt; &gt;</span></span><br><span class="line"><span class="quote">&gt; &gt;<span class="strong">**行级锁**</span>(引用内部一样可以用格式)</span></span><br><span class="line"><span class="quote">&gt; </span></span><br><span class="line"><span class="quote">&gt; ....</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><blockquote><p>Java<br>二级引用演示<br>MySQL</p><blockquote><p>外键</p><p>事务</p><p><strong>行级锁</strong>(引用内部一样可以用格式)</p></blockquote><p>…</p></blockquote><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-5-分割线">1.5 分割线</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="strong">**<span class="emphasis">*</span></span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><hr><hr><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-6-列表-跟空格都可以">1.6 列表(*,+,-跟空格都可以)</h2><h3 id="1-6-1-无序列表">1.6.1 无序列表</h3><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">*</span> Java</span><br><span class="line"><span class="bullet">*</span> Python</span><br><span class="line"><span class="bullet">*</span> ...</span><br><span class="line"></span><br><span class="line"><span class="bullet">+</span> Java</span><br><span class="line"><span class="bullet">+</span> Python</span><br><span class="line"><span class="bullet">+</span> ...</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> Java</span><br><span class="line"><span class="bullet">-</span> Python</span><br><span class="line"><span class="bullet">-</span> ...</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ul><li>Java</li><li>Python</li><li>…</li></ul><ul><li>Java</li><li>Python</li><li>…</li></ul><ul><li>Java</li><li>Python</li><li>…</li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h3 id="1-6-2-有序列表">1.6.2 有序列表</h3><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 注意后面有空格</span></span><br><span class="line"><span class="bullet">1.</span> </span><br><span class="line"><span class="bullet">2.</span> </span><br><span class="line"><span class="bullet">3.</span> </span><br><span class="line"><span class="bullet">4.</span> </span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li></li><li></li><li></li><li></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-7-图片">1.7 图片</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 本地图片</span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;/assets/pusheencode.webp&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;示例图片&quot;</span> <span class="attr">style</span>=<span class="string">&quot;zoom:50%;&quot;</span> /&gt;</span></span></span><br><span class="line"><span class="section"># 在线图片</span></span><br><span class="line">![<span class="string">code</span>](<span class="link">https://cdn.jsdelivr.net/gh/fomalhaut1998/markdown_pic/img/code.png</span>)</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><p>本地图片:<br><img src="/assets/pusheencode.webp" alt="示例图片" style="zoom:50%;" /><br>在线图片:<br><img src="https://cdn.jsdelivr.net/gh/fomalhaut1998/markdown_pic/img/code.png" alt="code"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-8-表格">1.8 表格</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| 项目标号 | 资金     | 备注 |</span><br><span class="line">| -------- | -------- | ---- |</span><br><span class="line">| 1        | 100，000 | 无   |</span><br><span class="line">| 2        | 200，000 | 无   |</span><br><span class="line">| 3        | 300,600  | 重要 |</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><table><thead><tr><th>项目标号</th><th>资金</th><th>备注</th></tr></thead><tbody><tr><td>1</td><td>100，000</td><td>无</td></tr><tr><td>2</td><td>200，000</td><td>无</td></tr><tr><td>3</td><td>300,600</td><td>重要</td></tr></tbody></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="1-9-公式">1.9 公式</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\Gamma(z)=\int<span class="emphasis">_0^\infty t^&#123;z-1&#125;e^&#123;-t&#125;dt.</span></span><br><span class="line"><span class="emphasis">$$</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><p>$$<br>\Gamma(z)=\int_0^\infty t^{z-1}e^{-t}dt.<br>$$</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h1>2.Butterfly外挂标签</h1><div class="note info flat"><p>这部分参考安知鱼:<a href="https://anzhiy.cn/posts/7d58.html">基于Butterfly的外挂标签引入</a></p></div><h2 id="2-1-行内文本样式-text">2.1 行内文本样式 text</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% u 文本内容 %&#125;</span><br><span class="line">&#123;% emp 文本内容 %&#125;</span><br><span class="line">&#123;% wavy 文本内容 %&#125;</span><br><span class="line">&#123;% del 文本内容 %&#125;</span><br><span class="line">&#123;% kbd 文本内容 %&#125;</span><br><span class="line">&#123;% psw 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1.</span> 带 &#123;% u 下划线 %&#125; 的文本</span><br><span class="line"><span class="bullet">2.</span> 带 &#123;% emp 着重号 %&#125; 的文本</span><br><span class="line"><span class="bullet">3.</span> 带 &#123;% wavy 波浪线 %&#125; 的文本</span><br><span class="line"><span class="bullet">4.</span> 带 &#123;% del 删除线 %&#125; 的文本</span><br><span class="line"><span class="bullet">5.</span> 键盘样式的文本 &#123;% kbd command %&#125; + &#123;% kbd D %&#125;</span><br><span class="line"><span class="bullet">6.</span> 密码样式的文本：&#123;% psw 这里没有验证码 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><ol><li>带 <u>下划线</u> 的文本</li><li>带 <emp>着重号</emp> 的文本</li><li>带 <wavy>波浪线</wavy> 的文本</li><li>带 <del>删除线</del> 的文本</li><li>键盘样式的文本 <kbd>command</kbd> + <kbd>D</kbd></li><li>密码样式的文本：<psw>这里没有验证码</psw></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-2-行内文本-span">2.2 行内文本 span</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% span 样式参数(参数以空格划分), 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>字体</code>: logo, code</li><li><code>颜色</code>: red,yellow,green,cyan,blue,gray</li><li><code>大小</code>: small, h4, h3, h2, h1, large, huge, ultra</li><li><code>对齐方向</code>: left, center, right</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 彩色文字</span><br><span class="line">在一段话中方便插入各种颜色的标签，包括：&#123;% span red, 红色 %&#125;、&#123;% span yellow, 黄色 %&#125;、&#123;% span green, 绿色 %&#125;、&#123;% span cyan, 青色 %&#125;、&#123;% span blue, 蓝色 %&#125;、&#123;% span gray, 灰色 %&#125;。</span><br><span class="line"><span class="bullet">-</span> 超大号文字</span><br><span class="line">文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line">&#123;% span center logo large, Volantis %&#125;</span><br><span class="line">&#123;% span center small, A Wonderful Theme for Hexo %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><ul><li>彩色文字<br>在一段话中方便插入各种颜色的标签，包括：<span class='p red'>红色</span>、<span class='p yellow'>黄色</span>、<span class='p green'>绿色</span>、<span class='p cyan'>青色</span>、<span class='p blue'>蓝色</span>、<span class='p gray'>灰色</span>。</li><li>超大号文字<br>文档「开始」页面中的标题部分就是超大号文字。<br><span class='p center logo large'>Volantis</span><br><span class='p center small'>A Wonderful Theme for Hexo</span></li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-3-段落文本-p">2.3 段落文本 p</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% p 样式参数(参数以空格划分), 文本内容 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>字体</code>: logo, code</li><li><code>颜色</code>: red,yellow,green,cyan,blue,gray</li><li><code>大小</code>: small, h4, h3, h2, h1, large, huge, ultra</li><li><code>对齐方向</code>: left, center, right</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> 彩色文字</span><br><span class="line">在一段话中方便插入各种颜色的标签，包括：&#123;% p red, 红色 %&#125;、&#123;% p yellow, 黄色 %&#125;、&#123;% p green, 绿色 %&#125;、&#123;% p cyan, 青色 %&#125;、&#123;% p blue, 蓝色 %&#125;、&#123;% p gray, 灰色 %&#125;。</span><br><span class="line"><span class="bullet">-</span> 超大号文字</span><br><span class="line">文档「开始」页面中的标题部分就是超大号文字。</span><br><span class="line">&#123;% p center logo large, Volantis %&#125;</span><br><span class="line">&#123;% p center small, A Wonderful Theme for Hexo %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><ul><li>彩色文字<br>在一段话中方便插入各种颜色的标签，包括：<p class='p red'>红色</p>、<p class='p yellow'>黄色</p>、<p class='p green'>绿色</p>、<p class='p cyan'>青色</p>、<p class='p blue'>蓝色</p>、<p class='p gray'>灰色</p>。</li><li>超大号文字<br>文档「开始」页面中的标题部分就是超大号文字。</li></ul><p class='p center logo large'>Volantis</p><p class='p center small'>A Wonderful Theme for Hexo</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-4-引用note">2.4 引用note</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">通用配置</button></li><li class="tab"><button type="button" data-href="#分栏-2">语法格式</button></li><li class="tab"><button type="button" data-href="#分栏-3">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-4">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-5">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">note:</span><br><span class="line">  # Note tag style values:</span><br><span class="line">  #  - simple    bs-callout old alert style. Default.</span><br><span class="line">  #  - modern    bs-callout new (v2-v3) alert style.</span><br><span class="line">  #  - flat      flat callout style with background, like on Mozilla or StackOverflow.</span><br><span class="line">  #  - disabled  disable all CSS styles import of note tag.</span><br><span class="line">  style: simple</span><br><span class="line">  icons: false</span><br><span class="line">  border<span class="emphasis">_radius: 3</span></span><br><span class="line"><span class="emphasis">  # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6).</span></span><br><span class="line"><span class="emphasis">  # Offset also applied to label tag variables. This option can work with disabled note tag.</span></span><br><span class="line"><span class="emphasis">  light_</span>bg<span class="emphasis">_offset: 0</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 自带icon</span></span><br><span class="line">&#123;% note [class] [no-icon] [style] %&#125;</span><br><span class="line">Any content (support inline tags too.io).</span><br><span class="line">&#123;% endnote %&#125;</span><br><span class="line"><span class="section"># 外部icon</span></span><br><span class="line">&#123;% note [color] [icon] [style] %&#125;</span><br><span class="line">Any content (support inline tags too.io).</span><br><span class="line">&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.自带icon</p><table><thead><tr><th>参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td>class</td><td style="text-align:center">【可选】标识，不同的标识有不同的配色 （ default / primary / success / info / warning / danger ）</td></tr><tr><td>no-icon</td><td style="text-align:center">【可选】不显示 icon</td></tr><tr><td>style</td><td style="text-align:center">【可选】可以覆盖配置中的 style （simple/modern/flat/disabled）</td></tr></tbody></table><p>2.外部icon</p><table><thead><tr><th>参数</th><th style="text-align:center">用法</th></tr></thead><tbody><tr><td>class</td><td style="text-align:center">【可选】标识，不同的标识有不同的配色 （ default / blue / pink / red / purple / orange / green ）</td></tr><tr><td>no-icon</td><td style="text-align:center">【可选】可配置自定义 icon (只支持 fontawesome 图标, 也可以配置 no-icon )</td></tr><tr><td>style</td><td style="text-align:center">【可选】可以覆盖配置中的 style （simple/modern/flat/disabled）</td></tr></tbody></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><details class="folding-tag" blue><summary> 1.自带icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note simple %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default simple %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary simple %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success simple %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info simple %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning simple %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger simple %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>2.<code>modern</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note modern %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default modern %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary modern %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success modern %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info modern %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning modern %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger modern %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>3.<code>flat</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note flat %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default flat %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary flat %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success flat %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info flat %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning flat %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger flat %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>4.<code>disabled</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note disabled %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default disabled %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary disabled %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success disabled %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info disabled %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning disabled %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger disabled %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>5.<code>no-icon</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note no-icon %&#125;默认 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note default no-icon %&#125;default 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note primary no-icon %&#125;primary 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note success no-icon %&#125;success 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note info no-icon %&#125;info 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note warning no-icon %&#125;warning 提示块标签&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note danger no-icon %&#125;danger 提示块标签&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>              </div>            </details><details class="folding-tag" blue><summary> 2.外部icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; simple %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; simple %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; simple %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; simple%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; simple %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; simple %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; simple %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>2.<code>modern</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; modern %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; modern %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; modern%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; modern %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; modern %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; modern %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>3.<code>flat</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; flat %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; flat %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; flat %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; flat%&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; flat %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; flat %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; flat %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>4.<code>disabled</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note &#x27;fab fa-cc-visa&#x27; disabled %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue &#x27;fas fa-bullhorn&#x27; disabled %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink &#x27;fas fa-car-crash&#x27; disabled %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red &#x27;fas fa-fan&#x27; disabled %&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange &#x27;fas fa-battery-half&#x27; disabled %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple &#x27;far fa-hand-scissors&#x27; disabled %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green &#x27;fab fa-internet-explorer&#x27; disabled %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>5.<code>no-icon</code>样式</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note no-icon %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note blue no-icon %&#125;2021年快到了....&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note pink no-icon %&#125;小心开车 安全至上&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note red no-icon %&#125;这是三片呢？还是四片？&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note orange no-icon %&#125;你是刷 Visa 还是 UnionPay&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note purple no-icon %&#125;剪刀石头布&#123;% endnote %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% note green no-icon %&#125;前端最讨厌的浏览器&#123;% endnote %&#125;</span><br></pre></td></tr></table></figure>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-5"><details class="folding-tag" blue><summary> 1.自带icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><div class="note simple"><p>默认 提示块标签</p></div><div class="note default simple"><p>default 提示块标签</p></div><div class="note primary simple"><p>primary 提示块标签</p></div><div class="note success simple"><p>success 提示块标签</p></div><div class="note info simple"><p>info 提示块标签</p></div><div class="note warning simple"><p>warning 提示块标签</p></div><div class="note danger simple"><p>danger 提示块标签</p></div>2.`modern`样式<div class="note modern"><p>默认 提示块标签</p></div><div class="note default modern"><p>default 提示块标签</p></div><div class="note primary modern"><p>primary 提示块标签</p></div><div class="note success modern"><p>success 提示块标签</p></div><div class="note info modern"><p>info 提示块标签</p></div><div class="note warning modern"><p>warning 提示块标签</p></div><div class="note danger modern"><p>danger 提示块标签</p></div><p>3.<code>flat</code>样式</p><div class="note flat"><p>默认 提示块标签</p></div><div class="note default flat"><p>default 提示块标签</p></div><div class="note primary flat"><p>primary 提示块标签</p></div><div class="note success flat"><p>success 提示块标签</p></div><div class="note info flat"><p>info 提示块标签</p></div><div class="note warning flat"><p>warning 提示块标签</p></div><div class="note danger flat"><p>danger 提示块标签</p></div><p>4.<code>disabled</code>样式</p><div class="note disabled"><p>默认 提示块标签</p></div><div class="note default disabled"><p>default 提示块标签</p></div><div class="note primary disabled"><p>primary 提示块标签</p></div><div class="note success disabled"><p>success 提示块标签</p></div><div class="note info disabled"><p>info 提示块标签</p></div><div class="note warning disabled"><p>warning 提示块标签</p></div><div class="note danger disabled"><p>danger 提示块标签</p></div><p>5.<code>no-icon</code>样式</p><div class="note no-icon flat"><p>默认 提示块标签</p></div><div class="note default no-icon flat"><p>default 提示块标签</p></div><div class="note primary no-icon flat"><p>primary 提示块标签</p></div><div class="note success no-icon flat"><p>success 提示块标签</p></div><div class="note info no-icon flat"><p>info 提示块标签</p></div><div class="note warning no-icon flat"><p>warning 提示块标签</p></div><div class="note danger no-icon flat"><p>danger 提示块标签</p></div>              </div>            </details><details class="folding-tag" blue><summary> 2.外部icon </summary>              <div class='content'>              <p>1.<code>simple</code>样式</p><div class="note icon-padding simple"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding simple"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了…</p></div><div class="note pink icon-padding simple"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding simple"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding simple"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding simple"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding simple"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>2.<code>modern</code>样式</p><div class="note icon-padding modern"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了…</p></div><div class="note pink icon-padding modern"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding modern"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding modern"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding modern"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding modern"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>3.<code>flat</code>样式</p><div class="note icon-padding flat"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding flat"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了…</p></div><div class="note pink icon-padding flat"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding flat"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding flat"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding flat"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding flat"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>4.<code>disabled</code>样式</p><div class="note icon-padding disabled"><i class="note-icon fab fa-cc-visa"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue icon-padding disabled"><i class="note-icon fas fa-bullhorn"></i><p>2021年快到了…</p></div><div class="note pink icon-padding disabled"><i class="note-icon fas fa-car-crash"></i><p>小心开车 安全至上</p></div><div class="note red icon-padding disabled"><i class="note-icon fas fa-fan"></i><p>这是三片呢？还是四片？</p></div><div class="note orange icon-padding disabled"><i class="note-icon fas fa-battery-half"></i><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple icon-padding disabled"><i class="note-icon far fa-hand-scissors"></i><p>剪刀石头布</p></div><div class="note green icon-padding disabled"><i class="note-icon fab fa-internet-explorer"></i><p>前端最讨厌的浏览器</p></div><p>5.<code>no-icon</code>样式</p><div class="note no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note blue no-icon flat"><p>2021年快到了…</p></div><div class="note pink no-icon flat"><p>小心开车 安全至上</p></div><div class="note red no-icon flat"><p>这是三片呢？还是四片？</p></div><div class="note orange no-icon flat"><p>你是刷 Visa 还是 UnionPay</p></div><div class="note purple no-icon flat"><p>剪刀石头布</p></div><div class="note green no-icon flat"><p>前端最讨厌的浏览器</p></div>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-5-上标标签-tip">2.5 上标标签 tip</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip [参数，可选] %&#125;文本内容&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>样式</code>: success,error,warning,bolt,ban,home,sync,cogs,key,bell</li><li><code>自定义图标</code>: 支持fontawesome。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip %&#125;default&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip info %&#125;info&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip success %&#125;success&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip error %&#125;error&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip warning %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip bolt %&#125;bolt&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban %&#125;ban&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip home %&#125;home&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip sync %&#125;sync&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip cogs %&#125;cogs&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip key %&#125;key&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip bell %&#125;bell&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip fa-atom %&#125;自定义font awesome图标&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class="tip "><p>default</p></div><div class="tip info"><p>info</p></div><div class="tip success"><p>success</p></div><div class="tip error"><p>error</p></div><div class="tip warning"><p>warning</p></div><div class="tip bolt"><p>bolt</p></div><div class="tip ban"><p>ban</p></div><div class="tip home"><p>home</p></div><div class="tip sync"><p>sync</p></div><div class="tip cogs"><p>cogs</p></div><div class="tip key"><p>key</p></div><div class="tip bell"><p>bell</p></div><div class="tip fa-atom"><p>自定义font awesome图标</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-6-动态标签-anima">2.6 动态标签 anima</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip [参数，可选] %&#125;文本内容&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><div class="note info flat"><ol><li>将所需的CSS类添加到图标（或DOM中的任何元素）。</li><li>对于父级悬停样式，需要给目标元素添加指定CSS类，同时还要给目标元素的父级元素添加CSS类<code>faa-parent animated-hover</code>。（详情见示例及示例源码）<br>You can regulate the speed of the animation by adding the CSS class or . faa-fastfaa-slow</li><li>可以通过给目标元素添加CSS类<code>faa-fast</code>或<code>faa-slow</code>来控制动画快慢。</li></ol></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.On DOM load（当页面加载时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>2.调整动画速度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated faa-fast %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated faa-slow %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>3.On hover（当鼠标悬停时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-horizontal animated-hover %&#125;warning&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-flash animated-hover %&#125;ban&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><p>4.On parent hover（当鼠标悬停在父级元素时显示动画）</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tip warning faa-parent animated-hover %&#125;<span class="language-xml"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;faa-horizontal&quot;</span>&gt;</span></span>warning<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span>&#123;% endtip %&#125;</span><br><span class="line">&#123;% tip ban faa-parent animated-hover %&#125;<span class="language-xml"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;faa-flash&quot;</span>&gt;</span></span>ban<span class="language-xml"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span>&#123;% endtip %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.On DOM load（当页面加载时显示动画）</p><div class="tip warning faa-horizontal animated"><p>warning</p></div><div class="tip ban faa-flash animated"><p>ban</p></div>2.调整动画速度<div class="tip warning faa-horizontal animated faa-fast"><p>warning</p></div><div class="tip ban faa-flash animated faa-slow"><p>ban</p></div>3.On hover（当鼠标悬停时显示动画）<div class="tip warning faa-horizontal animated-hover"><p>warning</p></div><div class="tip ban faa-flash animated-hover"><p>ban</p></div>4.On parent hover（当鼠标悬停在父级元素时显示动画）<div class="tip warning faa-parent animated-hover"><p class="faa-horizontal">warning</p></div><div class="tip ban faa-parent animated-hover"><p class="faa-flash">ban</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-7-复选列表-checkbox">2.7 复选列表 checkbox</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% checkbox 样式参数（可选）, 文本（支持简单md） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>样式</code>: plus, minus, times</li><li><code>颜色</code>: red,yellow,green,cyan,blue,gray</li><li><code>选中状态</code>: checked</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% checkbox 纯文本测试 %&#125;</span><br><span class="line">&#123;% checkbox checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% checkbox red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% checkbox green checked, 绿色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox yellow checked, 黄色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox cyan checked, 青色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox blue checked, 蓝色 + 默认选中 %&#125;</span><br><span class="line">&#123;% checkbox plus green checked, 增加 %&#125;</span><br><span class="line">&#123;% checkbox minus yellow checked, 减少 %&#125;</span><br><span class="line">&#123;% checkbox times red checked, 叉 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class='checkbox'><input type="checkbox" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="checkbox" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="checkbox" />            <p>支持自定义颜色</p>            </div><div class='checkbox green checked'><input type="checkbox" checked="checked"/>            <p>绿色 + 默认选中</p>            </div><div class='checkbox yellow checked'><input type="checkbox" checked="checked"/>            <p>黄色 + 默认选中</p>            </div><div class='checkbox cyan checked'><input type="checkbox" checked="checked"/>            <p>青色 + 默认选中</p>            </div><div class='checkbox blue checked'><input type="checkbox" checked="checked"/>            <p>蓝色 + 默认选中</p>            </div><div class='checkbox plus green checked'><input type="checkbox" checked="checked"/>            <p>增加</p>            </div><div class='checkbox minus yellow checked'><input type="checkbox" checked="checked"/>            <p>减少</p>            </div><div class='checkbox times red checked'><input type="checkbox" checked="checked"/>            <p>叉</p>            </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-8-单选列表-radio">2.8 单选列表 radio</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% radio 样式参数（可选）, 文本（支持简单md） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>颜色</code>: red,yellow,green,cyan,blue,gray</li><li><code>选中状态</code>: checked</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% radio 纯文本测试 %&#125;</span><br><span class="line">&#123;% radio checked, 支持简单的 [<span class="string">markdown</span>](<span class="link">https://guides.github.com/features/mastering-markdown/</span>) 语法 %&#125;</span><br><span class="line">&#123;% radio red, 支持自定义颜色 %&#125;</span><br><span class="line">&#123;% radio green, 绿色 %&#125;</span><br><span class="line">&#123;% radio yellow, 黄色 %&#125;</span><br><span class="line">&#123;% radio cyan, 青色 %&#125;</span><br><span class="line">&#123;% radio blue, 蓝色 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class='checkbox'><input type="radio" />            <p>纯文本测试</p>            </div><div class='checkbox checked'><input type="radio" checked="checked"/>            <p>支持简单的 <a href="https://guides.github.com/features/mastering-markdown/">markdown</a> 语法</p>            </div><div class='checkbox red'><input type="radio" />            <p>支持自定义颜色</p>            </div><div class='checkbox green'><input type="radio" />            <p>绿色</p>            </div><div class='checkbox yellow'><input type="radio" />            <p>黄色</p>            </div><div class='checkbox cyan'><input type="radio" />            <p>青色</p>            </div><div class='checkbox blue'><input type="radio" />            <p>蓝色</p>            </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-9-时间轴-timeline">2.9 时间轴 timeline</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;% timeline 时间线标题（可选）[,color] %&#125;</span><br><span class="line">&lt;!-- timeline 时间节点（标题） --&gt;</span><br><span class="line">正文内容</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&lt;!-- timeline 时间节点（标题） --&gt;</span><br><span class="line">正文内容</span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>title</code>:标题/时间线</li><li><code>color</code>:<code>timeline</code>颜色:default(留空) / blue / pink / red / purple / orange / green</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;% timeline 时间轴样式,blue %&#125;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-07-24 [<span class="string">2.6.6 -&gt; 3.0</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases</span>) --&gt;</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 如果有 <span class="code">`hexo-lazyload-image`</span> 插件，需要删除并重新安装最新版本，设置 <span class="code">`lazyload.isSPA: true`</span>。</span><br><span class="line"><span class="bullet">2.</span> 2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <span class="code">`use_cdn: true`</span> 则需要删除。</span><br><span class="line"><span class="bullet">3.</span> 2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</span><br><span class="line"><span class="bullet">4.</span> 2.x 版本的置顶 <span class="code">`top: true`</span> 改为了 <span class="code">`pin: true`</span>，并且同样适用于 <span class="code">`layout: page`</span> 的页面。</span><br><span class="line"><span class="bullet">5.</span> 如果使用了 <span class="code">`hexo-offline`</span> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-05-15 [<span class="string">2.6.3 -&gt; 2.6.6</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6</span>) --&gt;</span><br><span class="line"></span><br><span class="line">不需要额外处理。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- timeline 2020-04-20 [<span class="string">2.6.2 -&gt; 2.6.3</span>](<span class="link">https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3</span>) --&gt;</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> 全局搜索 <span class="code">`seotitle`</span> 并替换为 <span class="code">`seo_title`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的索引规则有变，使用 group 组件的文章内，<span class="code">`group: group_name`</span> 对应的组件名必须是 <span class="code">`group_name`</span>。</span><br><span class="line"><span class="bullet">2.</span> group 组件的列表名优先显示文章的 <span class="code">`short_title`</span> 其次是 <span class="code">`title`</span>。</span><br><span class="line"></span><br><span class="line">&lt;!-- endtimeline --&gt;</span><br><span class="line"></span><br><span class="line">&#123;% endtimeline %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class="timeline blue"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p>时间轴样式</p></div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-07-24 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases">2.6.6 -&gt; 3.0</a></p></div></div><div class='timeline-item-content'><ol><li>如果有 <code>hexo-lazyload-image</code> 插件，需要删除并重新安装最新版本，设置 <code>lazyload.isSPA: true</code>。</li><li>2.x 版本的 css 和 js 不适用于 3.x 版本，如果使用了 <code>use_cdn: true</code> 则需要删除。</li><li>2.x 版本的 fancybox 标签在 3.x 版本中被重命名为 gallery 。</li><li>2.x 版本的置顶 <code>top: true</code> 改为了 <code>pin: true</code>，并且同样适用于 <code>layout: page</code> 的页面。</li><li>如果使用了 <code>hexo-offline</code> 插件，建议卸载，3.0 版本默认开启了 pjax 服务。</li></ol></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-05-15 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.6">2.6.3 -&gt; 2.6.6</a></p></div></div><div class='timeline-item-content'><p>不需要额外处理。</p></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>2020-04-20 <a href="https://github.com/volantis-x/hexo-theme-volantis/releases/tag/2.6.3">2.6.2 -&gt; 2.6.3</a></p></div></div><div class='timeline-item-content'><ol><li>全局搜索 <code>seotitle</code> 并替换为 <code>seo_title</code>。</li><li>group 组件的索引规则有变，使用 group 组件的文章内，<code>group: group_name</code> 对应的组件名必须是 <code>group_name</code>。</li><li>group 组件的列表名优先显示文章的 <code>short_title</code> 其次是 <code>title</code>。</li></ol></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-10-链接卡片-link">2.10 链接卡片 link</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link 标题, 链接, 图片链接（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% link 糖果屋教程贴, https://akilar.top/posts/615e2dec/, https://cdn.cbd.int/akilar-candyassets@1.0.36/image/siteicon/favicon.ico %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><div class="tag link"><a class="link-card" title="糖果屋教程贴" href="https://akilar.top/posts/615e2dec/"><div class="left"><img src="https://cdn.cbd.int/akilar-candyassets@1.0.36/image/siteicon/favicon.ico"/></div><div class="right"><p class="text">糖果屋教程贴</p><p class="url">https://akilar.top/posts/615e2dec/</p></div></a></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-11-按钮-btns">2.11 按钮 btns</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns 样式参数 %&#125;</span><br><span class="line">&#123;% cell 标题, 链接, 图片或者图标 %&#125;</span><br><span class="line">&#123;% cell 标题, 链接, 图片或者图标 %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li>圆角样式：rounded, circle</li><li>增加文字样式：可以在容器内增加 <code>&lt;b&gt;</code>标题<code>&lt;/b&gt;</code>和<code>&lt;p&gt;</code>描述文字<code>&lt;/p&gt;</code></li><li>布局方式：<br>默认为自动宽度，适合视野内只有一两个的情况。</li></ol><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>wide</td><td>宽一点的按钮</td></tr><tr><td>fill</td><td>填充布局，自动铺满至少一行，多了会换行</td></tr><tr><td>center</td><td>居中，按钮之间是固定间距</td></tr><tr><td>around</td><td>居中分散</td></tr><tr><td>grid2</td><td>等宽最多2列，屏幕变窄会适当减少列数</td></tr><tr><td>grid3</td><td>等宽最多3列，屏幕变窄会适当减少列数</td></tr><tr><td>grid4</td><td>等宽最多4列，屏幕变窄会适当减少列数</td></tr><tr><td>grid5</td><td>等宽最多5列，屏幕变窄会适当减少列数</td></tr></tbody></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.如果需要显示类似「团队成员」之类的一组含有头像的链接</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle grid5 %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% cell xaoxuu, https://xaoxuu.com, https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><p>2.或者含有图标的按钮</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns rounded grid5 %&#125;</span><br><span class="line">&#123;% cell 下载源码, /, fas fa-download %&#125;</span><br><span class="line">&#123;% cell 查看文档, /, fas fa-book-open %&#125;</span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><p>3.圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btns circle center grid5 %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p red, 专业版 %&#125;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_pro.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1&#x27;</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&#x27;fab fa-apple&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">b</span>&gt;</span></span>心率管家<span class="language-xml"><span class="tag">&lt;/<span class="name">b</span>&gt;</span></span></span><br><span class="line">  &#123;% p green, 免费版 %&#125;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_lite.png&#x27;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span></span><br><span class="line">&#123;% endbtns %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.如果需要显示类似「团队成员」之类的一组含有头像的链接</p><div class="btns circle grid5">            <a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a><a class="button" href='https://xaoxuu.com' title='xaoxuu'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/avatar/avatar.png'>xaoxuu</a>          </div>2.或者含有图标的按钮<div class="btns rounded grid5">            <a class="button" href='/' title='下载源码'><i class='fas fa-download'></i>下载源码</a><a class="button" href='/' title='查看文档'><i class='fas fa-book-open'></i>查看文档</a>          </div>3.圆形图标 + 标题 + 描述 + 图片 + 网格5列 + 居中<div class="btns circle center grid5">            <a href='https://apps.apple.com/cn/app/heart-mate-pro-hrm-utility/id1463348922?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p red'>专业版</p>  <img src='https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_pro.png'></a><a href='https://apps.apple.com/cn/app/heart-mate-lite-hrm-utility/id1475747930?ls=1'>  <i class='fab fa-apple'></i>  <b>心率管家</b>  <p class='p green'>免费版</p>  <img src='https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/qrcode/heartmate_lite.png'></a>          </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-12-github卡片-ghcard">2.12 github卡片 ghcard</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% ghcard 用户名, 其它参数（可选） %&#125;</span><br><span class="line">&#123;% ghcard 用户名/仓库, 其它参数（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><p>使用<code>,</code>分割各个参数。写法为：<code>参数名=参数值</code><br>以下只写几个常用参数值。</p><table><thead><tr><th><strong>参数名</strong></th><th>取值</th><th>释义</th></tr></thead><tbody><tr><td>hide</td><td>stars,commits,prs,issues,contribs</td><td>隐藏指定统计</td></tr><tr><td>count_private</td><td>true</td><td>将私人项目贡献添加到总提交计数中</td></tr><tr><td>show_icons</td><td>true</td><td>显示图标</td></tr><tr><td>theme</td><td>查阅:<a href="https://github.com/anuraghazra/github-readme-stats/blob/master/themes/README.md">Available Themes</a></td><td>主题</td></tr></tbody></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.用户信息卡片</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| &#123;% ghcard fomalhaut1998 %&#125; | &#123;% ghcard fomalhaut1998, theme=vue %&#125; |</span><br><span class="line">| -- | -- |</span><br><span class="line">| &#123;% ghcard fomalhaut1998, theme=buefy %&#125; | &#123;% ghcard fomalhaut1998, theme=solarized-light %&#125; |</span><br><span class="line">| &#123;% ghcard fomalhaut1998, theme=onedark %&#125; | &#123;% ghcard fomalhaut1998, theme=solarized-dark %&#125; |</span><br><span class="line">| &#123;% ghcard fomalhaut1998, theme=algolia %&#125; | &#123;% ghcard fomalhaut1998, theme=calm %&#125; |</span><br></pre></td></tr></table></figure><p>2.仓库信息卡片</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">| &#123;% ghcard volantis-x/hexo-theme-volantis %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=vue %&#125; |</span><br><span class="line">| -- | -- |</span><br><span class="line">| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=buefy %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=solarized-light %&#125; |</span><br><span class="line">| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=onedark %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=solarized-dark %&#125; |</span><br><span class="line">| &#123;% ghcard volantis-x/hexo-theme-volantis, theme=algolia %&#125; | &#123;% ghcard volantis-x/hexo-theme-volantis, theme=calm %&#125; |</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.用户信息卡片</p><table><thead><tr><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&show_owner=true"/></a></th><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=vue&show_owner=true"/></a></th></tr></thead><tbody><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=buefy&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=solarized-light&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=onedark&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=solarized-dark&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=algolia&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/fomalhaut1998"><img src="https://github-readme-stats.vercel.app/api/?username=fomalhaut1998&theme=calm&show_owner=true"/></a></td></tr></tbody></table><p>2.仓库信息卡片</p><table><thead><tr><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&show_owner=true"/></a></th><th><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=vue&show_owner=true"/></a></th></tr></thead><tbody><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=buefy&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-light&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=onedark&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=solarized-dark&show_owner=true"/></a></td></tr><tr><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=algolia&show_owner=true"/></a></td><td><a class="ghcard" rel="external nofollow noopener noreferrer" href="https://github.com/volantis-x/hexo-theme-volantis"><img src="https://github-readme-stats.vercel.app/api/pin/?username=volantis-x&repo=hexo-theme-volantis&theme=calm&show_owner=true"/></a></td></tr></tbody></table><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-13-github徽标-ghbdage">2.13 github徽标 ghbdage</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage [right],[left],[logo]||[color],[link],[title]||[option] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>left</code>：徽标左边的信息，必选参数。</li><li><code>right</code>: 徽标右边的信息，必选参数，</li><li><code>logo</code>：徽标图标，图标名称详见<a href="https://simpleicons.org/">simpleicons</a>，可选参数。</li><li><code>color</code>：徽标右边的颜色，可选参数。</li><li><code>link</code>：指向的链接，可选参数。</li><li><code>title</code>：徽标的额外信息，可选参数。主要用于优化SEO，但<code>object</code>标签不会像<code>a</code>标签一样在鼠标悬停显示<code>title</code>信息。</li><li><code>option</code>：自定义参数，支持<a href="https://shields.io/">shields.io</a>的全部API参数支持，具体参数可以参看上文中的拓展写法示例。形式为<code>name1=value2&amp;name2=value2</code>。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.基本参数,定义徽标左右文字和图标</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Theme,Butterfly %&#125;</span><br><span class="line">&#123;% bdage Frame,Hexo,hexo %&#125;</span><br></pre></td></tr></table></figure><p>2.信息参数，定义徽标右侧内容背景色，指向链接</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage CDN,JsDelivr,jsDelivr||abcdef,https://metroui.org.ua/index.html,本站使用JsDelivr为静态资源提供CDN加速 %&#125;</span><br><span class="line">//如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割</span><br><span class="line">&#123;% bdage Source,GitHub,GitHub||,https://github.com/ %&#125;</span><br></pre></td></tr></table></figure><p>3.拓展参数，支持shields的API的全部参数内容</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bdage Hosted,Vercel,Vercel||brightgreen,https://vercel.com/,本站采用双线部署，默认线路托管于Vercel||style=social&amp;logoWidth=20 %&#125;</span><br><span class="line">//如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割</span><br><span class="line">&#123;% bdage Hosted,Vercel,Vercel||||style=social&amp;logoWidth=20&amp;logoColor=violet %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.基本参数,定义徽标左右文字和图标</p><p><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Butterfly-Theme-orange?logo=&color=orange&link=&"></object><br><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Hexo-Frame-orange?logo=hexo&color=orange&link=&"></object></p><p>2.信息参数，定义徽标右侧内容背景色，指向链接</p><p><object class="ghbdage" style="margin-inline:5px" title="本站使用JsDelivr为静态资源提供CDN加速" standby="loading..." data="https://img.shields.io/badge/JsDelivr-CDN-orange?logo=jsDelivr&color=abcdef&link=https://metroui.org.ua/index.html&"></object><br>//如果是跨顺序省略可选参数，仍然需要写个逗号,用作分割<br><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/GitHub-Source-orange?logo=GitHub&color=orange&link=https://github.com/&"></object></p><p>3.拓展参数，支持shields的API的全部参数内容</p><p><object class="ghbdage" style="margin-inline:5px" title="本站采用双线部署，默认线路托管于Vercel" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=brightgreen&link=https://vercel.com/&style=social&logoWidth=20"></object><br>//如果是跨顺序省略可选参数组，仍然需要写双竖线||用作分割<br><object class="ghbdage" style="margin-inline:5px" title="" standby="loading..." data="https://img.shields.io/badge/Vercel-Hosted-orange?logo=Vercel&color=orange&link=&style=social&logoWidth=20&logoColor=violet"></object></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-14-网站卡片-sites">2.14 网站卡片 sites</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% sitegroup %&#125;</span><br><span class="line">&#123;% site 标题, url=链接, screenshot=截图链接, avatar=头像链接（可选）, description=描述（可选） %&#125;</span><br><span class="line">&#123;% site 标题, url=链接, screenshot=截图链接, avatar=头像链接（可选）, description=描述（可选） %&#125;</span><br><span class="line">&#123;% endsitegroup %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;% sitegroup %&#125;</span><br><span class="line">&#123;% site xaoxuu, url=https://xaoxuu.com, screenshot=https://i.loli.net/2020/08/21/VuSwWZ1xAeUHEBC.jpg, avatar=https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/avatar/avatar.png, description=简约风格 %&#125;</span><br><span class="line">&#123;% site inkss, url=https://inkss.cn, screenshot=https://i.loli.net/2020/08/21/Vzbu3i8fXs6Nh5Y.jpg, avatar=https://cdn.jsdelivr.net/gh/inkss/common@master/static/web/avatar.jpg, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site MHuiG, url=https://blog.mhuig.top, screenshot=https://i.loli.net/2020/08/22/d24zpPlhLYWX6D1.png, avatar=https://cdn.jsdelivr.net/gh/MHuiG/imgbed@master/data/p.png, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site Colsrch, url=https://colsrch.top, screenshot=https://i.loli.net/2020/08/22/dFRWXm52OVu8qfK.png, avatar=https://cdn.jsdelivr.net/gh/Colsrch/images/Colsrch/avatar.jpg, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% site Linhk1606, url=https://linhk1606.github.io, screenshot=https://i.loli.net/2020/08/21/3PmGLCKicnfow1x.png, avatar=https://i.loli.net/2020/02/09/PN7I5RJfFtA93r2.png, description=这是一段关于这个网站的描述文字 %&#125;</span><br><span class="line">&#123;% endsitegroup %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><div class="site-card-group"><a class="site-card" href="https://fomalhaut1998.com"><div class="img"><img src="https://i.loli.net/2020/08/21/VuSwWZ1xAeUHEBC.jpg"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/fomalhaut1998/cdn-assets/avatar/avatar.png"/><span class="title">fomalhaut1998</span><span class="desc">简约风格</span></div></a><a class="site-card" href="https://inkss.cn"><div class="img"><img src="https://i.loli.net/2020/08/21/Vzbu3i8fXs6Nh5Y.jpg"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/inkss/common@master/static/web/avatar.jpg"/><span class="title">inkss</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://blog.mhuig.top"><div class="img"><img src="https://i.loli.net/2020/08/22/d24zpPlhLYWX6D1.png"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/MHuiG/imgbed@master/data/p.png"/><span class="title">MHuiG</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://colsrch.top"><div class="img"><img src="https://i.loli.net/2020/08/22/dFRWXm52OVu8qfK.png"/></div><div class="info"><img src="https://cdn.jsdelivr.net/gh/Colsrch/images/Colsrch/avatar.jpg"/><span class="title">Colsrch</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a><a class="site-card" href="https://linhk1606.github.io"><div class="img"><img src="https://i.loli.net/2020/08/21/3PmGLCKicnfow1x.png"/></div><div class="info"><img src="https://i.loli.net/2020/02/09/PN7I5RJfFtA93r2.png"/><span class="title">Linhk1606</span><span class="desc">这是一段关于这个网站的描述文字</span></div></a></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-15-行内图片-inlineimage">2.15 行内图片 inlineimage</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% inlineimage 图片链接, height=高度（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>高度</code>：height=20px</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">这是 &#123;% inlineimage https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/0000.gif %&#125; 一段话。</span><br><span class="line"></span><br><span class="line">这又是 &#123;% inlineimage https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/5150.gif, height=40px %&#125; 一段话。</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>这是 <img no-lazy class="inline" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/0000.gif" style="height:1.5em"/> 一段话。</p><p>这又是 <img no-lazy class="inline" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/aru-l/5150.gif" style="height:40px;"/> 一段话。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-16-单张图片-image">2.16 单张图片 image</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image 链接, width=宽度（可选）, height=高度（可选）, alt=描述（可选）, bg=占位颜色（可选） %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li>图片宽度高度：width=300px, height=32px</li><li>图片描述：alt=图片描述（butterfly需要在主题配置文件中开启图片描述）</li><li>占位背景色：bg=#f2f2f2</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.添加描述：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg, alt=每天下课回宿舍的路，没有什么故事。 %&#125;</span><br></pre></td></tr></table></figure><p>2.指定宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg, width=400px %&#125;</span><br></pre></td></tr></table></figure><p>3.指定宽度并添加描述：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg, width=400px, alt=每天下课回宿舍的路，没有什么故事。 %&#125;</span><br></pre></td></tr></table></figure><p>4.设置占位背景色：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% image https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg, width=400px, bg=#1D0C04, alt=优化不同宽度浏览的观感 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.添加描述：</p><div class="img-wrap"><div class="img-bg"><img class="img" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg" alt="每天下课回宿舍的路，没有什么故事。"/></div><span class="image-caption">每天下课回宿舍的路，没有什么故事。</span></div>2..指定宽度<div class="img-wrap"><div class="img-bg"><img class="img" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg" style="width:400px;"/></div></div>3.指定宽度并添加描述：<div class="img-wrap"><div class="img-bg"><img class="img" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg" alt="每天下课回宿舍的路，没有什么故事。" style="width:400px;"/></div><span class="image-caption">每天下课回宿舍的路，没有什么故事。</span></div>4.设置占位背景色：<div class="img-wrap"><div class="img-bg" style="background:#1D0C04"><img class="img" src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper-minimalist/2020/025.jpg" alt="优化不同宽度浏览的观感" style="width:400px;"/></div><span class="image-caption">优化不同宽度浏览的观感</span></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-17-音频-audio">2.17 音频 audio</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% audio 音频链接 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% audio https://github.com/volantis-x/volantis-docs/releases/download/assets/Lumia1020.mp3 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><div class="audio"><audio controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/Lumia1020.mp3' type='audio/mp3'>Your browser does not support the audio tag.</audio></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-18-视频-video">2.18 视频 video</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% video 视频链接 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>对齐方向</code>：left, center, right</li><li><code>列数</code>：逗号后面直接写列数，支持 1 ～ 4 列。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.100%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br></pre></td></tr></table></figure><p>2.50%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 2 %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure><p>3.25%宽度</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;% videos, 4 %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG<span class="emphasis">_0341.mov %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% video https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_</span>0341.mov %&#125;</span><br><span class="line">&#123;% endvideos %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.100%宽度</p><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div>2.50%宽度<div class="videos" col='2'><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div></div>3.25%宽度<div class="videos" col='4'><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div><div class="video"><video controls preload><source src='https://github.com/volantis-x/volantis-docs/releases/download/assets/IMG_0341.mov' type='video/mp4'>Your browser does not support the video tag.</video></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-19-相册-gallery">2.19 相册 gallery</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><p>1.gallerygroup 相册图库</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;gallery-group-main&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line">&#123;% galleryGroup name description link img-url %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>2.gallery 相册</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% gallery %&#125;</span><br><span class="line">markdown 圖片格式</span><br><span class="line">&#123;% endgallery %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ul><li>gallerygroup 相册图库</li></ul><table><thead><tr><th>参数名</th><th>释义</th></tr></thead><tbody><tr><td>name</td><td>图库名字</td></tr><tr><td>description</td><td>图库描述</td></tr><tr><td>link</td><td>链接到对应相册的地址</td></tr><tr><td>img-url</td><td>图库封面</td></tr></tbody></table><ul><li><p>gallery 相册</p><p>区别于旧版的Gallery相册,新的Gallery相册会自动根据图片长度进行排版，书写也更加方便，与markdown格式一样。可根据需要插入到相应的md。无需再自己配置长宽。<strong>建议在粘贴时故意使用长短、大小、横竖不一的图片</strong>，会有更好的效果。（尺寸完全相同的图片只会平铺输出，效果很糟糕）</p></li></ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.gallerygroup 相册图库</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;gallery-group-main&quot;</span>&gt;</span></span></span><br><span class="line">&#123;% galleryGroup MC 在Rikkaの六花服务器里留下的足迹 &#x27;/gallery/MC/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg %&#125;</span><br><span class="line">&#123;% galleryGroup Gundam 哦咧哇gundam哒！ &#x27;/gallery/Gundam/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907110508327.png %&#125;</span><br><span class="line">&#123;% galleryGroup I-am-Akilar 某种意义上也算自拍吧 &#x27;/gallery/I-am-Akilar/&#x27; https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907113116651.png %&#125;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>2.gallery 相册</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% gallery %&#125;</span><br><span class="line">![](<span class="link">https://i.loli.net/2019/12/25/Fze9jchtnyJXMHN.jpg</span>)</span><br><span class="line">![](<span class="link">https://i.loli.net/2019/12/25/ryLVePaqkYm4TEK.jpg</span>)</span><br><span class="line">&#123;% endgallery %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.gallerygroup 相册图库</p><div class="gallery-group-main">  <figure class="gallery-group">  <img class="gallery-group-img no-lightbox" src='https://cdn.cbd.int/akilar-candyassets@1.0.36/image/1.jpg' alt="Group Image Gallery">  <figcaption>  <div class="gallery-group-name">MC</div>  <p>在Rikkaの六花服务器里留下的足迹</p>  <a href='/project/gallery/MC/'></a>  </figcaption>  </figure>  <figure class="gallery-group">  <img class="gallery-group-img no-lightbox" src='https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907110508327.png' alt="Group Image Gallery">  <figcaption>  <div class="gallery-group-name">Gundam</div>  <p>哦咧哇gundam哒！</p>  <a href='/project/gallery/Gundam/'></a>  </figcaption>  </figure>  <figure class="gallery-group">  <img class="gallery-group-img no-lightbox" src='https://cdn.cbd.int/akilar-candyassets@1.0.36/image/20200907113116651.png' alt="Group Image Gallery">  <figcaption>  <div class="gallery-group-name">I-am-Akilar</div>  <p>某种意义上也算自拍吧</p>  <a href='/project/gallery/I-am-Akilar/'></a>  </figcaption>  </figure></div>2.gallery 相册<div class="fj-gallery"><p><img src="https://i.loli.net/2019/12/25/Fze9jchtnyJXMHN.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/ryLVePaqkYm4TEK.jpg" alt=""></p>          </div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-20-折叠框-folding">2.20 折叠框 folding</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><p>1.gallerygroup 相册图库</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% folding 参数（可选）, 标题 %&#125;</span><br><span class="line">![](<span class="link">https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg</span>)</span><br><span class="line">&#123;% endfolding %&#125;</span><br></pre></td></tr></table></figure><!-- tab 参数配置 --><ol><li><p><code>颜色</code>：blue, cyan, green, yellow, red</p></li><li><p><code>状态</code>：状态填写 open 代表默认打开。</p></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&#123;% folding 查看图片测试 %&#125;</span><br><span class="line"></span><br><span class="line">![](<span class="link">https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg</span>)</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding cyan open, 查看默认打开的折叠框 %&#125;</span><br><span class="line"></span><br><span class="line">这是一个默认打开的折叠框。</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding green, 查看代码测试 %&#125;</span><br><span class="line">假装这里有代码块（代码块没法嵌套代码块）</span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding yellow, 查看列表测试 %&#125;</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> haha</span><br><span class="line"><span class="bullet">-</span> hehe</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding red, 查看嵌套测试 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding blue, 查看嵌套测试2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% folding 查看嵌套测试3 %&#125;</span><br><span class="line"></span><br><span class="line">hahaha <span class="language-xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/tieba/%E6%BB%91%E7%A8%BD.png&#x27;</span> <span class="attr">style</span>=<span class="string">&#x27;height:24px&#x27;</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% endfolding %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><details class="folding-tag" ><summary> 查看图片测试 </summary>              <div class='content'>              <p><img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg" alt=""></p>              </div>            </details><details class="folding-tag" cyan open><summary> 查看默认打开的折叠框 </summary>              <div class='content'>              <p>这是一个默认打开的折叠框。</p>              </div>            </details><details class="folding-tag" green><summary> 查看代码测试 </summary>              <div class='content'>              <p>假装这里有代码块（代码块没法嵌套代码块）</p>              </div>            </details><details class="folding-tag" yellow><summary> 查看列表测试 </summary>              <div class='content'>              <ul><li>haha</li><li>hehe</li></ul>              </div>            </details><details class="folding-tag" red><summary> 查看嵌套测试 </summary>              <div class='content'>              <details class="folding-tag" blue><summary> 查看嵌套测试2 </summary>              <div class='content'>              <details class="folding-tag" ><summary> 查看嵌套测试3 </summary>              <div class='content'>              <p>hahaha <span><img src='https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/tieba/%E6%BB%91%E7%A8%BD.png' style='height:24px'></span></p>              </div>            </details>              </div>            </details>              </div>            </details><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-21-分栏-tab">2.21 分栏 tab</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">配置参数</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs Unique name, [index] %&#125;</span><br><span class="line">&lt;!-- tab [Tab caption] [@icon] --&gt;</span><br><span class="line">Any content (support inline tags too).</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><p>Unique name :</p><ul><li><p>选项卡块标签的唯一名称，不带逗号。</p></li><li><p>将在#id中用作每个标签及其索引号的前缀。</p></li><li><p>如果名称中包含空格，则对于生成#id，所有空格将由破折号代替。</p></li><li><p>仅当前帖子/页面的URL必须是唯一的！</p></li></ul></li><li><p>[index]:</p><ul><li><p>活动选项卡的索引号。</p></li><li><p>如果未指定，将选择第一个标签（1）。</p></li><li><p>如果index为-1，则不会选择任何选项卡。</p></li><li><p>可选参数。</p></li></ul></li><li><p>[Tab caption]:</p><ul><li><p>当前选项卡的标题。</p></li><li><p>如果未指定标题，则带有制表符索引后缀的唯一名称将用作制表符的标题。</p></li><li><p>如果未指定标题，但指定了图标，则标题将为空。</p></li><li><p>可选参数。</p></li></ul></li><li><p>[@icon]:</p><ul><li><p>FontAwesome图标名称（全名，看起来像“ fas fa-font”）</p></li><li><p>可以指定带空格或不带空格；</p></li><li><p>例如’Tab caption @icon’ 和 ‘Tab caption@icon’.</p></li><li><p>可选参数。</p></li></ul></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.Demo 1 - 预设选择第一个【默认】</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><p>2.Demo 2 - 预设选择tabs</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test2, 3 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><p>3.Demo 3 - 没有预设值</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test3, -1 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 1.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 2.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line"><span class="strong">**This is Tab 3.**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><p>4.Demo 4 - 自定义Tab名 + 只有icon + icon和Tab名</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs test4 %&#125;</span><br><span class="line">&lt;!-- tab 第一个Tab --&gt;</span><br><span class="line"><span class="strong">**tab名字为第一个Tab**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab @fab fa-apple-pay --&gt;</span><br><span class="line"><span class="strong">**只有图标 没有Tab名字**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- tab 炸弹@fas fa-bomb --&gt;</span><br><span class="line"><span class="strong">**名字+icon**</span></span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.Demo 1 - 预设选择第一个【默认】</p><div class="tabs" id="test1"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test1-1">test1 1</button></li><li class="tab"><button type="button" data-href="#test1-2">test1 2</button></li><li class="tab"><button type="button" data-href="#test1-3">test1 3</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>2.Demo 2 - 预设选择tabs</p><div class="tabs" id="test2"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test2-1">test2 1</button></li><li class="tab"><button type="button" data-href="#test2-2">test2 2</button></li><li class="tab active"><button type="button" data-href="#test2-3">test2 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test2-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test2-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content active" id="test2-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>3.Demo 3 - 没有预设值</p><div class="tabs" id="test3"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#test3-1">test3 1</button></li><li class="tab"><button type="button" data-href="#test3-2">test3 2</button></li><li class="tab"><button type="button" data-href="#test3-3">test3 3</button></li></ul><div class="tab-contents"><div class="tab-item-content" id="test3-1"><p><strong>This is Tab 1.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-2"><p><strong>This is Tab 2.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test3-3"><p><strong>This is Tab 3.</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><p>4.Demo 4 - 自定义Tab名 + 只有icon + icon和Tab名</p><div class="tabs" id="test4"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#test4-1">第一个Tab</button></li><li class="tab"><button type="button" data-href="#test4-2"><i class="fab fa-apple-pay" style="text-align: center;"></i></button></li><li class="tab"><button type="button" data-href="#test4-3"><i class="fas fa-bomb"></i>炸弹</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="test4-1"><p><strong>tab名字为第一个Tab</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-2"><p><strong>只有图标 没有Tab名字</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="test4-3"><p><strong>名字+icon</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-22-诗词标签-poem">2.22 诗词标签 poem</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-2">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-3">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><ol><li><code>title</code>：诗词标题</li><li><code>author</code>：作者，可以不写</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;% poem 水调歌头,苏轼 %&#125;</span><br><span class="line">丙辰中秋，欢饮达旦，大醉，作此篇，兼怀子由。</span><br><span class="line">明月几时有？把酒问青天。</span><br><span class="line">不知天上宫阙，今夕是何年？</span><br><span class="line">我欲乘风归去，又恐琼楼玉宇，高处不胜寒。</span><br><span class="line">起舞弄清影，何似在人间？</span><br><span class="line"></span><br><span class="line">转朱阁，低绮户，照无眠。</span><br><span class="line">不应有恨，何事长向别时圆？</span><br><span class="line">人有悲欢离合，月有阴晴圆缺，此事古难全。</span><br><span class="line">但愿人长久，千里共婵娟。</span><br><span class="line">&#123;% endpoem %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><div class='poem'><div class='poem-title'>水调歌头</div><div class='poem-author'>苏轼</div><p>丙辰中秋，欢饮达旦，大醉，作此篇，兼怀子由。<br>明月几时有？把酒问青天。<br>不知天上宫阙，今夕是何年？<br>我欲乘风归去，又恐琼楼玉宇，高处不胜寒。<br>起舞弄清影，何似在人间？</p><p>转朱阁，低绮户，照无眠。<br>不应有恨，何事长向别时圆？<br>人有悲欢离合，月有阴晴圆缺，此事古难全。<br>但愿人长久，千里共婵娟。</p></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-23-阿里图标-icon">2.23 阿里图标 icon</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon [icon-xxxx],[font-size] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>icon-xxxx</code>：表示图标<code>font-class</code>,可以在自己的阿里矢量图标库项目的<code>font-class</code>引用方案内查询并复制。</li><li><code>font-size</code>：表示图标大小，直接填写数字即可，单位为<code>em</code>。图标大小默认值为<code>1em</code>。</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;% icon icon-rat<span class="emphasis">_zi %&#125;&#123;% icon icon-rat,2 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-ox_</span>chou,3 %&#125;&#123;% icon icon-ox,4 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-tiger<span class="emphasis">_yin,5 %&#125;&#123;% icon icon-tiger,6 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-rabbit_</span>mao,1 %&#125;&#123;% icon icon-rabbit,2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-dragon<span class="emphasis">_chen,3 %&#125;&#123;% icon icon-dragon,4 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-snake_</span>si,5 %&#125;&#123;% icon icon-snake,6 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-horse<span class="emphasis">_wu %&#125;&#123;% icon icon-horse,2 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-goat_</span>wei,3 %&#125;&#123;% icon icon-goat,4 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-monkey<span class="emphasis">_shen,5 %&#125;&#123;% icon icon-monkey,6 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-rooster_</span>you %&#125;&#123;% icon icon-rooster,2 %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% icon icon-dog<span class="emphasis">_xu,3 %&#125;&#123;% icon icon-dog,4 %&#125;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">&#123;% icon icon-boar_</span>hai,5 %&#125;&#123;% icon icon-boar,6 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rat_zi"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rat"></use></svg></p><p><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-ox_chou"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-ox"></use></svg></p><p><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-tiger_yin"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-tiger"></use></svg></p><p><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rabbit_mao"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rabbit"></use></svg></p><p><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-dragon_chen"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-dragon"></use></svg></p><p><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-snake_si"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-snake"></use></svg></p><p><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-horse_wu"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-horse"></use></svg></p><p><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-goat_wei"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-goat"></use></svg></p><p><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-monkey_shen"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-monkey"></use></svg></p><p><svg class="icon" style="width:1em; height:1em" aria-hidden="true"><use xlink:href="#icon-rooster_you"></use></svg><svg class="icon" style="width:2em; height:2em" aria-hidden="true"><use xlink:href="#icon-rooster"></use></svg></p><p><svg class="icon" style="width:3em; height:3em" aria-hidden="true"><use xlink:href="#icon-dog_xu"></use></svg><svg class="icon" style="width:4em; height:4em" aria-hidden="true"><use xlink:href="#icon-dog"></use></svg></p><p><svg class="icon" style="width:5em; height:5em" aria-hidden="true"><use xlink:href="#icon-boar_hai"></use></svg><svg class="icon" style="width:6em; height:6em" aria-hidden="true"><use xlink:href="#icon-boar"></use></svg></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-24-特效标签wow">2.24 特效标签wow</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-2">渲染演示</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow [animete],[duration],[delay],[offset],[iteration] %&#125;</span><br><span class="line">内容</span><br><span class="line">&#123;% endwow %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>animate</code>: 动画样式，效果详见<a href="https://animate.style/">animate.css参考文档</a></li><li><code>duration</code>: 选填项，动画持续时间，单位可以是<code>ms</code>也可以是<code>s</code>。例如<code>3s</code>，<code>700ms</code>。</li><li><code>delay</code>: 选填项，动画开始的延迟时间，单位可以是<code>ms</code>也可以是<code>s</code>。例如<code>3s</code>，<code>700ms</code>。</li><li><code>offset</code>: 选填项，开始动画的距离（相对浏览器底部）</li><li><code>iteration</code>: 选填项，动画重复的次数</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><p>1.flip动画效果。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__zoomIn,5s,5s,100,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`zoomIn`动画效果，持续`5s`，延时`5s`，离底部`100`距离时启动，重复`10`次</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>2.zoomIn动画效果，持续5s，延时5s，离底部100距离时启动，重复10次</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__zoomIn,5s,5s,100,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note blue &#x27;fas fa-bullhorn&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`zoomIn`动画效果，持续`5s`，延时`5s`，离底部`100`距离时启动，重复`10`次</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>3.slideInRight动画效果，持续5s，延时5s</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__slideInRight,5s,5s %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note orange &#x27;fas fa-car&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`slideInRight`动画效果，持续`5s`，延时`5s`。</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><p>4.heartBeat动画效果，延时5s，重复10次。此处注意不用的参数位置要留空，用逗号间隔。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% wow animate<span class="strong">__heartBeat,,5s,,10 %&#125;</span></span><br><span class="line"><span class="strong">&#123;% note red &#x27;fas fa-battery-half&#x27; modern%&#125;</span></span><br><span class="line"><span class="strong">`heartBeat`动画效果，延时`5s`，重复`10`次。</span></span><br><span class="line"><span class="strong">&#123;% endnote %&#125;</span></span><br><span class="line"><span class="strong">&#123;% endwow %&#125;</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>1.flip动画效果。</p><div class='wow animate__zoomIn' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset='100'  data-wow-iteration='10' ><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p><code>zoomIn</code>动画效果，持续<code>5s</code>，延时<code>5s</code>，离底部<code>100</code>距离时启动，重复<code>10</code>次</p></div></div><p>2.zoomIn动画效果，持续5s，延时5s，离底部100距离时启动，重复10次</p><div class='wow animate__zoomIn' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset='100'  data-wow-iteration='10' ><div class="note blue icon-padding modern"><i class="note-icon fas fa-bullhorn"></i><p><code>zoomIn</code>动画效果，持续<code>5s</code>，延时<code>5s</code>，离底部<code>100</code>距离时启动，重复<code>10</code>次</p></div></div><p>3.slideInRight动画效果，持续5s，延时5s</p><div class='wow animate__slideInRight' data-wow-duration='5s' data-wow-delay='5s' data-wow-offset=''  data-wow-iteration='' ><div class="note orange icon-padding modern"><i class="note-icon fas fa-car"></i><p><code>slideInRight</code>动画效果，持续<code>5s</code>，延时<code>5s</code>。</p></div></div><p>4.heartBeat动画效果，延时5s，重复10次。此处注意不用的参数位置要留空，用逗号间隔。</p><div class='wow animate__heartBeat' data-wow-duration='' data-wow-delay='5s' data-wow-offset=''  data-wow-iteration='10' ><div class="note red icon-padding modern"><i class="note-icon fas fa-battery-half"></i><p><code>heartBeat</code>动画效果，延时<code>5s</code>，重复<code>10</code>次。</p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-25-进度条-progress">2.25  进度条 progress</h2><div class="note info flat"><p>进度条标签参考<a href="https://rongbuqiu.com/jdt.html">沂佰孜猫-给HEXO文章添加彩色进度条</a>。<br>源样式提取自<a href="https://zwying0814.gitbook.io/cuteen/">Cuteen</a>主题。</p></div><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-2">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% progress [width] [color] [text] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>width</code>: 0到100的阿拉伯数字</li><li><code>color</code>: 颜色，取值有red,yellow,green,cyan,blue,gray</li><li><code>text</code>:进度条上的文字内容</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% progress 10 red 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 30 yellow 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 50 green 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 70 cyan 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 90 blue 进度条样式预览 %&#125;</span><br><span class="line">&#123;% progress 100 gray 进度条样式预览 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-red"  style="width: 10%" aria-valuenow="10" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-yellow"  style="width: 30%" aria-valuenow="30" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-green"  style="width: 50%" aria-valuenow="50" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-cyan"  style="width: 70%" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-blue"  style="width: 90%" aria-valuenow="90" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><div class="progress"><div class="progress-bar-animated progress-bar progress-bar-striped bg-gray"  style="width: 100%" aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"><p>进度条样式预览</p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-26-注释-notation">2.26 注释 notation</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% nota [label] , [text] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><p><code>label</code>: 注释词汇</p></li><li><p><code>text</code>: 悬停显示的注解内容</p></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% nota 把鼠标移动到我上面试试 ,可以看到注解内容出现在顶栏 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p><span class='nota' data-nota='可以看到注解内容出现在顶栏'>把鼠标移动到我上面试试</span></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-27-气泡注释-bubble">2.27 气泡注释 bubble</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% bubble [content] , [notation] ,[background-color] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>content</code>: 注释词汇</li><li><code>notation</code>: 悬停显示的注解内容</li><li><code>background-color</code>: 可选，气泡背景色。默认为“#71a4e3”</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最近我学到了不少新玩意儿（虽然对很多大佬来说这些已经是旧技术了），比如CSS的&#123;% bubble 兄弟相邻选择器,&quot;例如 h1 + p &#123;margin-top:50px;&#125;&quot; %&#125;，&#123;% bubble flex布局,&quot;Flex 是 Flexible Box 的缩写，意为&quot;弹性布局&quot;，用来为盒状模型提供最大的灵活性&quot;,&quot;#ec5830&quot; %&#125;，&#123;% bubble transform变换,&quot;transform 属性向元素应用 2D 或 3D 转换。该属性允许我们对元素进行旋转、缩放、移动或倾斜。&quot;,&quot;#1db675&quot; %&#125;，animation的&#123;% bubble 贝塞尔速度曲线,&quot;贝塞尔曲线(Bézier curve)，又称贝兹曲线或贝济埃曲线，是应用于二维图形应用程序的数学曲线。一般的矢量图形软件通过它来精确画出曲线，贝兹曲线由线段与节点组成，节点是可拖动的支点，线段像可伸缩的皮筋&quot;,&quot;#de4489&quot; %&#125;写法，还有今天刚看到的&#123;% bubble clip-path,&quot;clip-path属性使用裁剪方式创建元素的可显示区域。区域内的部分显示，区域外的隐藏。&quot;,&quot;#868fd7&quot; %&#125;属性。这些对我来说很新颖的概念狠狠的冲击着我以前积累起来的设计思路。</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>最近我学到了不少新玩意儿（虽然对很多大佬来说这些已经是旧技术了），比如CSS的<span class="bubble-content">兄弟相邻选择器</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#71a4e3;">例如 h1 + p {margin-top:50px;}</span></span>，<span class="bubble-content">flex布局</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#ec5830;">Flex 是 Flexible Box 的缩写，意为弹性布局&quot;，用来为盒状模型提供最大的灵活性&quot;</span></span>，<span class="bubble-content">transform变换</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#1db675;">transform 属性向元素应用 2D 或 3D 转换。该属性允许我们对元素进行旋转、缩放、移动或倾斜。</span></span>，animation的<span class="bubble-content">贝塞尔速度曲线</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#de4489;">贝塞尔曲线(Bézier curve)，又称贝兹曲线或贝济埃曲线，是应用于二维图形应用程序的数学曲线。一般的矢量图形软件通过它来精确画出曲线，贝兹曲线由线段与节点组成，节点是可拖动的支点，线段像可伸缩的皮筋</span></span>写法，还有今天刚看到的<span class="bubble-content">clip-path</span><span class="bubble-notation"><span class="bubble-item" style="background-color:#868fd7;">clip-path属性使用裁剪方式创建元素的可显示区域。区域内的部分显示，区域外的隐藏。</span></span>属性。这些对我来说很新颖的概念狠狠的冲击着我以前积累起来的设计思路。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-28-引用文献-reference">2.28 引用文献 reference</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% referto [id] , [literature] %&#125;</span><br><span class="line">&#123;% referfrom [id] , [literature] , [url] %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><p>referto 引用上标</p><ul><li><p><code>id</code>: 上标序号内容，需与referfrom标签的id对应才能实现跳转</p></li><li><p><code>literature</code>: 引用的参考文献名称</p></li></ul></li><li><p>referfrom 引用出处</p><ul><li><p><code>id</code>: 序号内容，需与referto标签的id对应才能实现 跳转</p></li><li><p><code>literature</code>: 引用的参考文献名称</p></li><li><p><code>url</code>: 引用的参考文献链接，可省略</p></li></ul></li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Akilarの糖果屋(akilar.top)是一个私人性质的博客&#123;% referto &#x27;[1]&#x27;,&#x27;Akilarの糖果屋群聊简介&#x27; %&#125;，从各类教程至生活点滴，无话不谈。建群的目的是提供一个闲聊的场所。博客采用Hexo框架&#123;% referto &#x27;[2]&#x27;,&#x27;Hexo中文文档&#x27; %&#125;，Butterfly主题&#123;% referto &#x27;[3]&#x27;,&#x27;Butterfly 安装文档(一) 快速开始&#x27; %&#125;</span><br><span class="line"></span><br><span class="line">本项目参考了Volantis&#123;% referto &#x27;[4]&#x27;,&#x27;hexo-theme-volantis 标签插件&#x27; %&#125;的标签样式。引入<span class="code">`[tag].js`</span>，并针对<span class="code">`butterfly`</span>主题修改了相应的<span class="code">`[tag].styl`</span>。在此鸣谢<span class="code">`Volantis`</span>主题众开发者。</span><br><span class="line">主要参考内容包括各个volantis的内置标签插件文档&#123;% referto &#x27;[5]&#x27;,&#x27;Volantis文档:内置标签插件&#x27; %&#125;</span><br><span class="line">Butterfly主题的各个衍生魔改&#123;% referto &#x27;[6]&#x27;,&#x27;Butterfly 安装文档:标签外挂（Tag Plugins&#x27; %&#125;&#123;% referto &#x27;[7]&#x27;,&#x27;小弋の生活馆全样式预览&#x27; %&#125;&#123;% referto &#x27;[8]&#x27;,&#x27;l-lin-font-awesome-animation&#x27; %&#125;&#123;% referto &#x27;[9]&#x27;,&#x27;小康的butterfly主题使用文档&#x27; %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% referfrom &#x27;[1]&#x27;,&#x27;Akilarの糖果屋群聊简介&#x27;,&#x27;https://jq.qq.com/?<span class="emphasis">_wv=1027&amp;k=pGLB2C0N&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[2]&#x27;,&#x27;Hexo中文文档&#x27;,&#x27;https://hexo.io/zh-cn/docs/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[3]&#x27;,&#x27;Butterfly 安装文档(一) 快速开始&#x27;,&#x27;https://butterfly.js.org/posts/21cfbf15/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[4]&#x27;,&#x27;hexo-theme-volantis 标签插件&#x27;,&#x27;https://volantis.js.org/v5/tag-plugins/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[5]&#x27;,&#x27;Volantis文档:内置标签插件&#x27;,&#x27;https://volantis.js.org/tag-plugins/&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[6]&#x27;,&#x27;Butterfly 安装文档:标签外挂（Tag Plugins&#x27;,&#x27;https://butterfly.js.org/posts/4aa8abbe/#%E6%A8%99%E7%B1%A4%E5%A4%96%E6%8E%9B%EF%BC%88Tag-Plugins%EF%BC%89&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[7]&#x27;,&#x27;小弋の生活馆全样式预览&#x27;,&#x27;https://lovelijunyi.gitee.io/posts/c898.html&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[8]&#x27;,&#x27;l-lin-font-awesome-animation&#x27;,&#x27;https://github.com/l-lin/font-awesome-animation&#x27; %&#125;</span></span><br><span class="line"><span class="emphasis">&#123;% referfrom &#x27;[9]&#x27;,&#x27;小康的butterfly主题使用文档&#x27;,&#x27;https://www.antmoe.com/posts/3b43914f/&#x27; %&#125;</span></span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>Akilarの糖果屋(akilar.top)是一个私人性质的博客<span class="hidden-anchor" id="referto_[1]"></span><sup class="reference"><a href="#referfrom_[1]">[1]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Akilarの糖果屋群聊简介</span><span class="reference-title">参考资料</span></span></span>，从各类教程至生活点滴，无话不谈。建群的目的是提供一个闲聊的场所。博客采用Hexo框架<span class="hidden-anchor" id="referto_[2]"></span><sup class="reference"><a href="#referfrom_[2]">[2]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Hexo中文文档</span><span class="reference-title">参考资料</span></span></span>，Butterfly主题<span class="hidden-anchor" id="referto_[3]"></span><sup class="reference"><a href="#referfrom_[3]">[3]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Butterfly 安装文档(一) 快速开始</span><span class="reference-title">参考资料</span></span></span></p><p>本项目参考了Volantis<span class="hidden-anchor" id="referto_[4]"></span><sup class="reference"><a href="#referfrom_[4]">[4]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">hexo-theme-volantis 标签插件</span><span class="reference-title">参考资料</span></span></span>的标签样式。引入<code>[tag].js</code>，并针对<code>butterfly</code>主题修改了相应的<code>[tag].styl</code>。在此鸣谢<code>Volantis</code>主题众开发者。<br>主要参考内容包括各个volantis的内置标签插件文档<span class="hidden-anchor" id="referto_[5]"></span><sup class="reference"><a href="#referfrom_[5]">[5]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Volantis文档:内置标签插件</span><span class="reference-title">参考资料</span></span></span><br>Butterfly主题的各个衍生魔改<span class="hidden-anchor" id="referto_[6]"></span><sup class="reference"><a href="#referfrom_[6]">[6]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">Butterfly 安装文档:标签外挂（Tag Plugins</span><span class="reference-title">参考资料</span></span></span><span class="hidden-anchor" id="referto_[7]"></span><sup class="reference"><a href="#referfrom_[7]">[7]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">小弋の生活馆全样式预览</span><span class="reference-title">参考资料</span></span></span><span class="hidden-anchor" id="referto_[8]"></span><sup class="reference"><a href="#referfrom_[8]">[8]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">l-lin-font-awesome-animation</span><span class="reference-title">参考资料</span></span></span><span class="hidden-anchor" id="referto_[9]"></span><sup class="reference"><a href="#referfrom_[9]">[9]</a></sup><span class="reference-bubble"><span class="reference-item"><span class="reference-literature">小康的butterfly主题使用文档</span><span class="reference-title">参考资料</span></span></span></p><div class="reference-source"><span class="hidden-anchor" id="referfrom_[1]"></span><a class="reference-anchor" href="#referto_[1]">[1]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://jq.qq.com/?_wv=1027&k=pGLB2C0N">Akilarの糖果屋群聊简介</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[2]"></span><a class="reference-anchor" href="#referto_[2]">[2]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://hexo.io/zh-cn/docs/">Hexo中文文档</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[3]"></span><a class="reference-anchor" href="#referto_[3]">[3]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://butterfly.js.org/posts/21cfbf15/">Butterfly 安装文档(一) 快速开始</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[4]"></span><a class="reference-anchor" href="#referto_[4]">[4]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://volantis.js.org/v5/tag-plugins/">hexo-theme-volantis 标签插件</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[5]"></span><a class="reference-anchor" href="#referto_[5]">[5]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://volantis.js.org/tag-plugins/">Volantis文档:内置标签插件</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[6]"></span><a class="reference-anchor" href="#referto_[6]">[6]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://butterfly.js.org/posts/4aa8abbe/#%E6%A8%99%E7%B1%A4%E5%A4%96%E6%8E%9B%EF%BC%88Tag-Plugins%EF%BC%89">Butterfly 安装文档:标签外挂（Tag Plugins</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[7]"></span><a class="reference-anchor" href="#referto_[7]">[7]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://lovelijunyi.gitee.io/posts/c898.html">小弋の生活馆全样式预览</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[8]"></span><a class="reference-anchor" href="#referto_[8]">[8]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://github.com/l-lin/font-awesome-animation">l-lin-font-awesome-animation</a></div><div class="reference-source"><span class="hidden-anchor" id="referfrom_[9]"></span><a class="reference-anchor" href="#referto_[9]">[9]<div class="reference-anchor-up fa-solid fa-angles-up"></div></a><a class="reference-link" href="https://www.antmoe.com/posts/3b43914f/">小康的butterfly主题使用文档</a></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-29-PDF展示">2.29 PDF展示</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf 文件路径 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li><code>文件路径</code>: 可以是相对路径或者是在线链接</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 1.本地文件:在md文件路径下创建一个同名文件夹，其内放pdf文件名为xxx.pdf的文件</span></span><br><span class="line">&#123;% pdf xxx.pdf %&#125;</span><br><span class="line"><span class="section"># 2.在线链接</span></span><br><span class="line">&#123;% pdf https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/pdf/小作文讲义.pdf %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><p>2.在线链接(要放到最外层才能起作用)</p><pre><code>&lt;div class=&quot;row&quot;&gt;&lt;embed src=&quot;https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/pdf/小作文讲义.pdf&quot; width=&quot;100%&quot; height=&quot;550&quot; type=&quot;application/pdf&quot;&gt;&lt;/div&gt;</code></pre><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-30-Hexo-tag-map-插件">2.30 Hexo-tag-map 插件</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% + 标签值 + 经度 + 纬度 + 文本 + 缩放等级 + 宽 + 高 + 默认图层 + %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><table><thead><tr><th style="text-align:center">地图名</th><th style="text-align:center">标签值 &lt;必填&gt;</th><th style="text-align:center">宽 (默认 100%) / 高 (默认 360px)</th><th style="text-align:center">缩放等级 (默认 14)</th><th style="text-align:center">宽 (默认 100%) / 高 (默认 360px)</th><th style="text-align:center">默认图层 (默认 1)</th></tr></thead><tbody><tr><td style="text-align:center">混合地图</td><td style="text-align:center">map</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 3~18</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~7</td></tr><tr><td style="text-align:center">谷歌地图</td><td style="text-align:center">googleMap</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~20</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~3</td></tr><tr><td style="text-align:center">高德地图</td><td style="text-align:center">gaodeMap</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 3~18</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~3</td></tr><tr><td style="text-align:center">百度地图</td><td style="text-align:center">baiduMap</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 4~18</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~2</td></tr><tr><td style="text-align:center">Geoq 地图</td><td style="text-align:center">geoqMap</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~18</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~5</td></tr><tr><td style="text-align:center">openstreet 地图</td><td style="text-align:center">openstreetMap</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">取值 1~18</td><td style="text-align:center">百分数或具体值 (100% 或 360px)</td><td style="text-align:center">不支持此参数</td></tr></tbody></table><ol><li>参数之间，用英文逗号相隔</li><li>参数必须按上述事例顺序输入，不得为空</li><li>同一个页面，同一组经纬度值，只能插入一个相同标签值的地图 (若有需要，可以将第二个地图上，经度或纬度末尾删除一两个数)</li><li>参数取值必须在上述范围内</li><li>默认图层：即地图叠加层的值，默认常规地图还是卫星地图，可按地图显示顺序取值</li><li>缩放等级，数字越大，地图比例尺越小，显示的越精细</li><li>除标签值外，其他参数选填，但 每个参数的左边的参数必填</li><li>谷歌地图需要外网才能加载查看</li></ol><p>坐标获取：<a href="https://lbs.amap.com/tools/picker">高德地图坐标拾取系统</a> 、<a href="https://api.map.baidu.com/lbsapi/getpoint/index.html">百度地图坐标拾取系统</a></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% map 120.101101,30.239119 %&#125;</span><br><span class="line">&#123;% googleMap 120.101101,30.239119, 这里是西湖灵隐寺，据说求姻缘很灵验哦！ %&#125;</span><br><span class="line">&#123;% geoqMap 120.101101,30.239119, 这里是西湖灵隐寺，据说求姻缘很灵验哦！, 13, 90%, 320px, 3 %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><link rel="stylesheet" href="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.css"><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.js"></script><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet.ChineseTmsProviders@1.0.4.js"></script><div class="map-box" style="margin: 0.8rem 0 1.6rem 0;"><div id="map-120.101101-30.239119" style="max-width:100%; height:360px;display: block;margin:0 auto;z-index:1;border-radius: 5px;"></div></div><script type="text/javascript">var normalm=L.tileLayer.chinaProvider('GaoDe.Normal.Map',{maxZoom:20,minZoom:1,attribution:'高德地图'});var imgm=L.tileLayer.chinaProvider('GaoDe.Satellite.Map',{maxZoom:20,minZoom:1,attribution:'高德地图'});var imga=L.tileLayer.chinaProvider('GaoDe.Satellite.Annotion',{maxZoom:20,minZoom:1,attribution:'高德地图'});var normalMap=L.tileLayer.chinaProvider('Google.Normal.Map',{maxZoom:20,minZoom:1,attribution:'Google Maps'}),satelliteMap=L.tileLayer.chinaProvider('Google.Satellite.Map',{maxZoom:21,minZoom:1,attribution:'Google Maps'});routeMap=L.tileLayer.chinaProvider('Google.Satellite.Annotion',{maxZoom:21,minZoom:1});var normalMap=L.tileLayer.chinaProvider('Google.Normal.Map',{maxZoom:21,minZoom:1,attribution:'Google Maps'}),satelliteMap=L.tileLayer.chinaProvider('Google.Satellite.Map',{maxZoom:21,minZoom:1,attribution:'Google Maps'}),routeMap=L.tileLayer.chinaProvider('Google.Satellite.Annotion',{maxZoom:21,minZoom:1,attribution:'Google Maps'});var normalm1=L.tileLayer.chinaProvider('Geoq.Normal.Map',{maxZoom:21,minZoom:1,attribution:'GeoQ'});var normal=L.layerGroup([normalm]),image=L.layerGroup([imgm,imga]);var baseLayers={"高德地图":normal,"智图地图":normalm1,"谷歌地图":normalMap,"高德卫星地图":imgm,"谷歌卫星地图":satelliteMap,"高德卫星标注":image,"谷歌卫星标注":routeMap};var mymap=L.map('map-120.101101-30.239119',{center:[30.239119,120.101101],zoom:14,layers:[normal],zoomControl:false});L.control.layers(baseLayers,null).addTo(mymap);L.control.zoom({zoomInTitle:'放大',zoomOutTitle:'缩小'}).addTo(mymap);</script><br><link rel="stylesheet" href="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.css"><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.js"></script><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet.ChineseTmsProviders@1.0.4.js"></script><div id="googleMap-120.101101-30.239119" style="max-width:100%; height:360px;display: block;margin:0 auto;z-index:1;border-radius: 5px;"></div><script type="text/javascript">var normalMap=L.tileLayer.chinaProvider('Google.Normal.Map',{maxZoom:22,minZoom:1,attribution:'Google Maps'}),satelliteMap=L.tileLayer.chinaProvider('Google.Satellite.Map',{maxZoom:22,minZoom:1,attribution:'Google Maps'}),routeMap=L.tileLayer.chinaProvider('Google.Satellite.Annotion',{maxZoom:22,minZoom:1,attribution:'Google Maps'});var baseLayers={"谷歌地图":normalMap,"谷歌卫星图":satelliteMap,"谷歌卫星标注": routeMap};var overlayLayers={};var mymap=L.map("googleMap-120.101101-30.239119",{center:[30.239119,120.101101],zoom:14,layers:[normalMap],zoomControl:false});L.control.layers(baseLayers,null).addTo(mymap);L.control.zoom({zoomInTitle:'放大',zoomOutTitle:'缩小'}).addTo(mymap);var marker = L.marker(['30.239119','120.101101']).addTo(mymap);marker.bindPopup("这里是西湖灵隐寺，据说求姻缘很灵验哦！").openPopup();</script><br><link rel="stylesheet" href="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.css"><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet@1.7.1.js"></script><script data-pjax src="//unpkg.com/hexo-tag-map/lib/leaflet.ChineseTmsProviders@1.0.4.js"></script><div id="geoqMap-120.101101-30.239119" style="max-width:90%; height:320px;display: block;margin:0 auto;z-index:1;border-radius: 5px;"></div><script type="text/javascript">var normalm1=L.tileLayer.chinaProvider('Geoq.Normal.Map',{maxZoom:20,minZoom:1,attribution:'GeoQ'});var normalm2=L.tileLayer.chinaProvider('Geoq.Normal.PurplishBlue',{maxZoom:20,minZoom:1,attribution:'GeoQ'});var normalm3=L.tileLayer.chinaProvider('Geoq.Normal.Gray',{maxZoom:20,minZoom:1,attribution:'GeoQ'});var normalm4=L.tileLayer.chinaProvider('Geoq.Normal.Warm',{maxZoom:20,minZoom:1,attribution:'GeoQ'});var normalm5=L.tileLayer.chinaProvider('Geoq.Theme.Hydro',{maxZoom:20,minZoom:1,attribution:'GeoQ'});var normal=L.layerGroup([normalm1,normalm2,normalm3,normalm4,normalm5]);var baseLayers={"智图地图":normalm1,"午夜蓝":normalm2,"灰色":normalm3,"暖色":normalm4,"水系":normalm5};var mymap=L.map("geoqMap-120.101101-30.239119",{center:[30.239119,120.101101],zoom:13,layers:[normalm3],zoomControl:false});L.control.layers(baseLayers,null).addTo(mymap);L.control.zoom({zoomInTitle:'放大',zoomOutTitle:'缩小'}).addTo(mymap);var marker = L.marker(['30.239119','120.101101']).addTo(mymap);marker.bindPopup("这里是西湖灵隐寺，据说求姻缘很灵验哦！").openPopup();</script><br><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div><h2 id="2-31-隐藏块">2.31 隐藏块</h2><div class="tabs" id="分栏"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分栏-1">标签语法</button></li><li class="tab"><button type="button" data-href="#分栏-2">参数配置</button></li><li class="tab"><button type="button" data-href="#分栏-3">示例源码</button></li><li class="tab"><button type="button" data-href="#分栏-4">渲染演示</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分栏-1"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% hideBlock display,bg,color %&#125;</span><br><span class="line">content</span><br><span class="line">&#123;% endhideBlock %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-2"><ol><li>content：要隐藏的内容</li><li>display：展示前按钮显示的文字（可选）</li><li>bg：按钮的背景颜色（可选）</li><li>color：按钮显示的文字的颜色（可选）</li></ol><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-3"><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% hideBlock 点我预览, blue %&#125;</span><br><span class="line">这里有张图片：</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://s1.vika.cn/space/2022/10/30/b35fce448bc9404a8d65c3ce1e6e46eb&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;image (1)&quot;</span> <span class="attr">style</span>=<span class="string">&quot;zoom:67%;&quot;</span> /&gt;</span></span></span><br><span class="line">&#123;% endhideBlock %&#125;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分栏-4"><div class="hide-block"><button type="button" class="hide-button" style="background-color:  blue;">点我预览    </button><div class="hide-content"><p>这里有张图片：<br><img src="https://s1.vika.cn/space/2022/10/30/b35fce448bc9404a8d65c3ce1e6e46eb" alt="image (1)" style="zoom:67%;" /></p></div></div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>]]></content>
      
      
      <categories>
          
          <category> 演示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> 外挂标签 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
