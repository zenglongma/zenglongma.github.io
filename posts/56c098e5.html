<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>文献阅读7 | Fomalhaut🥝</title><meta name="keywords" content="Java,MySQL,算法,代码,博客,Butterfly,Hexo,Fomalhaut🥝,Fomalhaut"><meta name="author" content="Fomalhaut🥝"><meta name="copyright" content="Fomalhaut🥝"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比这篇论文旨在进行基于骨骼的无监督的动作表示学习，并提出了一种新的分层对比（HiCo）框架。与现有的基于对比的解决方案不同，通常将输入骨骼序列表示为实例级特征并在整体上执行对比，我们提出的HiC">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读7">
<meta property="og:url" content="https://www.fomal.cc/posts/56c098e5.html">
<meta property="og:site_name" content="Fomalhaut🥝">
<meta property="og:description" content="Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比这篇论文旨在进行基于骨骼的无监督的动作表示学习，并提出了一种新的分层对比（HiCo）框架。与现有的基于对比的解决方案不同，通常将输入骨骼序列表示为实例级特征并在整体上执行对比，我们提出的HiC">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://source.fomal.cc/img/default_cover_14.webp">
<meta property="article:published_time" content="2023-09-25T01:42:20.000Z">
<meta property="article:modified_time" content="2023-12-20T03:01:34.503Z">
<meta property="article:author" content="Fomalhaut🥝">
<meta property="article:tag" content="Java,MySQL,算法,代码,博客,Butterfly,Hexo,Fomalhaut🥝,Fomalhaut">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://source.fomal.cc/img/default_cover_14.webp"><link rel="shortcut icon" href="/"><link rel="canonical" href="https://www.fomal.cc/posts/56c098e5"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js',
      css: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读7',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-20 11:01:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="blurNum"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Fomalhaut🥝" type="application/atom+xml">
</head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/02/avatar.webp" onerror="onerror=null;src='/assets/r1.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Fomalhaut🥝</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg><span> 搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-tupian1"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon">       </use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读7</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于 </span><time class="post-meta-date-created" datetime="2023-09-25T01:42:20.000Z" title="发表于 2023-09-25 09:42:20">2023-09-25</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-20T03:01:34.503Z" title="更新于 2023-12-20 11:01:34">2023-12-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">2.3w</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>82分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读7"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Hierarchical-Contrast-for-Unsupervised-Skeleton-based-Action-Representation-Learning基于无监督骨架的动作表示学习的层次对比"><a href="#Hierarchical-Contrast-for-Unsupervised-Skeleton-based-Action-Representation-Learning基于无监督骨架的动作表示学习的层次对比" class="headerlink" title="Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比"></a>Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比</h1><p>这篇论文旨在进行<strong>基于骨骼的无监督的动作表示学习</strong>，并提出了一种新的<strong>分层对比（HiCo）框架</strong>。与现有的基于对比的解决方案不同，通常将输入骨骼序列表示为<strong>实例级特征并在整体上执行对比</strong>，我们提出的<strong>HiCo将输入表示为多个层次的特征</strong>，<strong>并以分层方式进行对比</strong>。具体而言，给定一个人体骨骼序列，我们通过序列到序列（S2S）编码器和统一的下采样模块，<strong>将其表示为来自时间和空间域的不同粒度的多个特征向量</strong>。此外，<strong>分层对比是根据四个级别进行的：实例级、域级、剪辑级和部分级</strong>。此外，HiCo与S2S编码器正交，允许我们灵活地融合最先进的<strong>S2S编码器</strong>。在四个数据集上进行的大量实验，即NTU-60、NTU-120、PKU-MMD I和II，显示HiCo在两个下游任务，包括<strong>动作识别和检索中</strong>，实现了<strong>无监督骨骼动作表示学习的最新状态</strong>，并且其学习到的动作表示具有良好的可迁移性。此外，我们还展示了我们的<strong>框架对于半监督骨骼动作识别是有效的</strong>。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/HuiGuanLab/HiCo">https://github.com/HuiGuanLab/HiCo</a> 上获得。</p>
<p>摘要总结： 这篇论文提出了一个新的Hierarchical Contrast（HiCo）框架，用于<strong>无监督骨骼动作表示学习</strong>。与传统的对比方法不同，<strong>HiCo将输入骨骼序列分层表示，并在多个层次进行对比。它通过序列到序列编码器和统一的下采样模块生成不同粒度的特征向量，然后在实例级、域级、剪辑级和部分级进行分层对比</strong>。实验结果表明，HiCo在动作识别和检索任务中实现了无监督骨骼动作表示学习的最新成果，并且具有良好的迁移性能，同时也在半监督骨骼动作识别方面表现出效果。其代码可在GitHub上找到。</p>
<h3 id="int"><a href="#int" class="headerlink" title="int"></a>int</h3><p>这篇论文介绍了人类动作识别在人机交互、智能监控、视频内容分析、游戏控制等领域具有广泛应用。近年来，基于3D骨骼的动作识别在深度学习网络的推动下取得了显著进展。然而，如何学习更具区分性的骨骼表示仍然是骨骼动作识别的一个未解之谜。为了解决这个问题，早期的大多数工作采用了完全监督的方式来训练网络，这需要大量标记的3D骨骼数据，因此成本高昂且耗时。近年来，出现了一种新趋势，即提出无监督的基于骨骼的动作表示学习（SARL）方法，以减轻注释工作量。本文也关注无监督的SARL。</p>
<p>现有的无监督SARL方法大致分为三类：编码器-解码器方法、对比学习方法和混合方法。编码器-解码器方法首先将输入的骨骼序列编码为潜在特征，然后在各种手工制作的前提任务的指导下解码这些潜在特征，例如骨骼重建、骨骼上色预测和骨骼位移预测。对比学习方法通常将输入的骨骼序列增强为两个增强实例，并训练编码器使同一骨骼的实例具有比不同骨骼的实例更相似的表示。混合方法将编码器-解码器和对比学习的思想结合起来。近年来，由于其简单的机制和出色的性能，对比学习方法占据主导地位。</p>
<p>本文提出了一个新的Hierarchical Contrast（HiCo）框架，用于无监督SARL。HiCo具有分层编码器网络，将骨骼编码为不同粒度的部分级、剪辑级、域级和实例级表示，并进行分层对比。具体来说，分层编码器网络有两个分支，对应于时间域和空间域。对于时间分支，它将骨骼序列编码为不同时间粒度的多个剪辑级特征，通过提取不同长度的剪辑的特征。类似地，空间分支从不同大小的人体部位提取特征以获得部分级特征。此外，从时间和空间域导出的剪辑级特征和部分级特征逐渐用于组成域级表示和实例级表示。另外，鉴于多级表示，进行了四个级别的分层对比。我们的假设是这种分层对比与人体骨骼的分层性质一致，并提供更多对于无监督学习至关重要的监督。</p>
<p>总结： 本文提出了HiCo框架，该框架使用分层对比来进行无监督的骨骼动作表示学习。HiCo具有分层编码器网络，能够将骨骼序列编码为不同粒度的特征表示，并进行多级别的分层对比。实验结果表明，HiCo在无监督骨骼动作表示学习中取得了最新的最新成果，并具有良好的可迁移性能。此外，我们还展示了该方法对于半监督的基于骨骼的动作识别同样有效。</p>
<p>这篇论文旨在解决无监督骨骼动作表示学习的问题，并提出了一种新的<strong>分层对比（HiCo）框架</strong>。不同于现有的基于对比的解决方案通常将输入的骨骼序列表示为实例级特征并在整体上进行对比，我们提出的<strong>HiCo将输入表示为多级特征</strong>，<strong>并以分层方式进行对比</strong>。具体来说，对于给定的人体骨骼序列，我们<strong>通过序列到序列（S2S）编码器和统一的下采样模块</strong>，将其表示为来自时间和空间领域的不同粒度的多个特征向量。此外，分层对比从四个层面进行：实例级、域级、剪辑级和部分级。此外，==HiCo与S2S编码器是正交的==，这使我们能够灵活地应用最先进的S2S编码器。在四个数据集上进行的大量实验表明，HiCo在<strong>无监督骨骼动作表示学习的两个下游任务（动作识别和检索）中取得了最新的最新成果</strong>，并且其学到的动作表示具有良好的可迁移性。此外，我们还展示了我们的框架对于半监督骨骼动作识别同样有效。</p>
<h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>1 引言 人体动作识别在人机交互、智能监视、视频内容分析、游戏控制等领域有广泛应用（Gao、Zhang和Xu 2019; Li等2019; Dong等2022）。近年来，基于3D骨架的动作识别在深度学习网络的推动下取得了显著进展（Zhang 2012; Liu等2019; Wang、Ni和Yang 2020）。然而，<strong>如何学习更具辨识性的骨骼表示仍然是骨骼动作识别领域的一个未解决问题。</strong></p>
<p>为了解决这个问题，早期的大多数工作（Ni、Wang和Moulin 2011; Wang等2012; Vemulapalli和Chellapa 2016; Zhang等2017; Liu等2019; Cheng等2020）采用了<strong>全面监督的方式训练网络</strong>，这需要大量标记的3D骨架数据，因此成本高昂且耗时。最近，我们注意到提出<strong>无监督骨骼动作表示学习（SARL）方法的新趋势</strong>，以减轻注释工作的负担（Zheng等2018; Su、Liu和Shlizerman 2020; Lin等2020）。本文也关注<strong>无监督的SARL</strong>。</p>
<p>目前的无监督SARL研究可以大致分为三类：<strong>编码器-解码器方法</strong>（Zheng等2018; Nie、Liu和Liu 2020），<strong>对比学习方法</strong>（Rao等2021; Thoker、Doughty和Snoek 2021; Guo等2022）和<strong>混合方法</strong>（Su、Lin和Wu 2021; Chen等2022）。==编码器-解码器方法首先将输入的骨骼序列编码成潜在特征，然后在各种手工制作的前提任务的指导下解码潜在特征，例如骨骼重建（Zheng等2018）、骨骼着色预测（Yang等2021）和骨骼位移预测（Kim等2022）。对比学习方法通常将输入的骨骼序列增强为两个增强实例，并训练编码器使相同骨骼的实例具有更相似的表示，而不同骨骼的实例则不然。混合方法融合了编码器-编码器和对比学习的思想==其中，对比学习方法在近年来占据主导地位，因为它们具有简单的机制和出色的性能（Li等2021; Guo等2022）。<strong>第一个无监督SARL的对比学习方法由（Rao等2021）提出，它改编了最初设计用于无监督图像表示学习的MoCo（He等2020）来进行无监督骨骼表示学习</strong>。此后，通过建模骨骼的不确定性（Su等2021）、探索更多正样本对（Li等2021）和利用更好的骨骼特定增强（Thoker、Doughty和Snoek 2021）等方式提出了==一些改进的对比学习方法==。正如图1(a)所示，这些对比学习方法<strong>通常首先将骨骼序列表示为实例级特征，然后以整体的方式执行实例级对比</strong>。尽管对比学习方法已经表现出更好的性能，但这种整体的实例级对比可能不够优化，考虑到人体骨架天然具有分层结构。<strong>骨骼序列可以在时间上常被看作是整个骨骼（帧）的列表，或在空间上被看作是骨骼关节的列表</strong>。此外，帧或关节都是时间或空间域中的基本元素，可以构建为较大粒度的元素，例如帧剪辑或身体部分。</p>
<p>受人体骨骼分层性质的启发，我们提出了一种<strong>新的无监督SARL框架</strong>——Hierarchical Contrast（HiCo）。HiCo具有一个分层编码器网络，用于<strong>将骨骼编码为部分级、剪辑级、域级和实例级表示，并进行多层次对比</strong>，如图1(b)所示。具体而言，分层编码器网络有两个分支，分别对应时间域和空间域。对于时间分支，<strong>它将骨骼序列编码为不同时间粒度的多个剪辑级特征，通过从不同长度的剪辑中提取特征</strong>。同样，<strong>空间分支从不同尺寸的人体部位提取特征以获得部分级特征。</strong>此外，从时间和空间领域导出的<strong>剪辑级特征和部分级特征逐渐用于构建域级表示和实例级表示</strong>。另外，<strong>考虑到多层次表示，进行了四个级别的分层对比</strong>。<strong>我们的假设是，这种分层对比与人体骨架的分层性质一致，并为无监督学习提供了更多关键的监督。</strong>总之，本文的贡献如下。</p>
<p> • 我们提出了一种分层编码器网络，通过序列到序列（S2S）编码器和统一的下采样模块，将骨骼序列表示为来自时间和空间领域的不同粒度的多个特征向量。此外，我们的框架与S2S编码器正交，允许我们灵活应用最先进的S2S编码器。</p>
<p> • 基于人体骨骼的多级表示，我们提出了一种新的分层对比损失，用于无监督学习，在无监督SARL的情况下，它比实例级表示上的简单对比损失更有效。</p>
<p> • 在四个数据集上进行的广泛实验表明，我们的HiCo在两个下游任务中取得了无监督SARL的最新成果，其学到的表示具有良好的可迁移性。此外，我们还展示了我们的方法对于半监督的基于骨骼的动作识别同样有效。</p>
<h2 id="related"><a href="#related" class="headerlink" title="related"></a>related</h2><p><strong>Hierarchical Modeling on Skeletons 骨架的层次化建模</strong></p>
<ol>
<li>There are also some previous works that modeled human skeletons hierarchically. 也有一些以前的研究对人类骨架进行了分层建模。</li>
<li>For instance, (Du, Wang, and Wang 2015; Cheng et al. 2021; Wang, Ni, and Yang 2020) modeled the spatial hierarchical structure with hand-crafted designs, but they did not consider the temporal hierarchical structure. 例如，（杜，王和王2015年；程等人2021年；王，倪和杨2020年）使用手工设计对空间层次结构进行了建模，但没有考虑时间层次结构。</li>
<li>(Chen et al. 2022) exploited the temporal hierarchical clues from the frame, clip, and video levels. （陈等人2022年）从帧、剪辑和视频级别利用了时间层次线索。</li>
<li>By contrast, we jointly model the temporal and spatial hierarchical structure, which can be readily implemented using our proposed more unified framework. 相比之下，我们联合建模了时间和空间的层次结构，这可以很容易地使用我们提出的更统一的框架来实现。</li>
<li>Besides, different from the previous models that present skeletons at the instance level, we present skeletons at the instance level, domain level, clip level, and part level. 此外，不同于以前将骨架呈现在实例级别的模型，我们在实例级别、域级别、剪辑级别和部分级别呈现骨架。</li>
</ol>
<p>总结： 本章介绍了以前的一些关于对人体骨架进行分层建模的研究，其中一些模型侧重于空间层次结构，而另一些侧重于时间层次结构。与以前的模型不同，本研究提出了一个更统一的框架，可以同时对时间和空间的层次结构进行建模，并将骨架呈现在不同的层次上，包括实例级别、域级别、剪辑级别和部分级别。</p>
<h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><p>3.1 分层编码器网络 分层编码器网络由一个时间分支和一个空间分支组成，分别从时间域和空间域对输入骨骼序列进行编码。<strong>时间分支</strong>对骨骼序列进行多个时间粒度的编码，产生一个<strong>剪辑级别的表示</strong>。类似地，<strong>空间分支</strong>对骨骼序列进行多个空间粒度的编码，产生一个<strong>部分级别的表示</strong>。<strong>剪辑级别的表示和部分级别的表示都逐渐用于组装域级别和实例级别的表示。</strong></p>
<p>总结：这一部分描述了分层编码器网络的结构，它包括时间分支和空间分支，用于对骨骼序列进行编码。时间分支和空间分支分别处理骨骼序列的时间和空间信息，产生剪辑级别和部分级别的表示。这些不同粒度的表示逐渐组合以构建域级别和实例级别的表示。这个网络的设计充分考虑了人体骨架的分层性质，有助于提高无监督学习的性能。</p>
<h4 id="Clip-level-Representation"><a href="#Clip-level-Representation" class="headerlink" title="Clip-level Representation."></a><strong>Clip-level Representation</strong>.</h4><p>裁剪级别表示。 Given a skeleton sequence X ∈ RT×J×3 that has T frames with J joints, and each joint consists of three spatial coordinates, 我们首先将它在以时间为主的领域中重塑为一个帧列表 Xc = {xc i }T i=1 ∈ RT×3J, where xc i 表示实际上是一个完整的人体骨架的第i帧。此外，我们使用帧嵌入将输入投影到C维密集特征空间，通过两个全连接层。形式上，从xc i 获得的投影特征如下： x c i = W2(σ(W1xc i + b1)) + b2， 其中W1 ∈ RC×3J和W2 ∈ RC×C是变换矩阵，b1 ∈ RC和b2 ∈ RC表示偏置向量，σ是ReLU激活函数。所有投影特征联合表示为Xc 1 = { xc i }T i=1 ∈ RT×C。为了方便参考，我们将这些特征称为初始帧表示。==由于初始帧特征是独立表示的，它们自然缺乏对于动作表示至关重要的时间依赖性。因此，我们建议使用多个时间粒度来建模它们的时间依赖性。我们首先生成不同时间粒度的剪辑，然后利用S2S编码器来建模它们的时间依赖性==</p>
<p><strong>总结：这一部分介绍了剪辑级别的表示方法。首先，输入的骨骼序列被转化为帧列表，并通过帧嵌入映射到一个C维的特征空间，形成初始帧表示。然后，=== 为了引入时间依赖性===，作者生成不同时间粒度的剪辑，并使用S2S编码器来建模它们的时间依赖性。这样，得到了包含不同时间粒度信息的剪辑级别表示。</strong></p>
<h4 id="Clip-Construction-剪辑构建。"><a href="#Clip-Construction-剪辑构建。" class="headerlink" title="Clip Construction. 剪辑构建。"></a><strong>Clip Construction. 剪辑构建。</strong></h4><p>In order to generate clips of different temporal granularities, we propose a Unified Downsampling Module (UDM), which sequentially merges multiple consecutive frame/clip features to obtain more coarse-grained clips. 为了生成不同时间粒度的剪辑，我们提出了一个统一的下采样模块 (UDM)，它依次合并多个连续的帧/剪辑特征，以获得更粗粒度的剪辑。</p>
<p>The structure of UDM is illustrated in Figure 2(b), which can be formally defined as: UDM(·) = MaxPool1D(LN(σ(Conv1D(·)))), 其中UDM的结构如图2(b)所示，可以正式定义为：UDM(·) = MaxPool1D(LN(σ(Conv1D(·)))),， 这里 Conv1D 表示一个具有核大小为5和步幅大小为1的1D卷积层，LN表示层归一化，而MaxPool1D是具有核大小2的1D最大池化操作。这里的1D卷积层用于收集附近的上下文信息，而最大池化则用于信息聚合。需要注意的是，UDM可以串联堆叠，从而导致粒度更粗的剪辑。此外，我们将帧视为粒度为1的单帧剪辑，以方便描述。借助UDM的帮助，给定粒度为n的剪辑，可以轻松获得更大粒度的剪辑，如下所示： Xc n+1 = UDM(Xc n)。通过连续使用UDM L-1 次，我们能够获得不同时间粒度的剪辑{Xc 1, Xc 2, . . . , Xc L}。</p>
<p>Summary: This section discusses the construction of clips at different temporal granularities. The Unified Downsampling Module (UDM) is introduced, which merges consecutive frame/clip features to create coarser-grained clips. UDM consists of a 1D convolution layer, Layer Norm (LN), and 1D max pooling, and it can be stacked in series to obtain clips of varying granularities. This approach allows for the generation of clips with different levels of temporal detail.</p>
<p>摘要：本节讨论不同时间粒度的剪辑的构造。 引入了统一下采样模块（UDM），该模块合并连续的帧/剪辑特征以创建更粗粒度的剪辑。  UDM 由 1D 卷积层、Layer Norm (LN) 和 1D max pooling 组成，可以串联堆叠以获得不同粒度的剪辑。 这种方法允许生成具有不同时间细节级别的剪辑。</p>
<h4 id="Temporal-Dependency-Modeling-时间依赖性建模。"><a href="#Temporal-Dependency-Modeling-时间依赖性建模。" class="headerlink" title="Temporal Dependency Modeling. 时间依赖性建模。"></a><strong>Temporal Dependency Modeling. 时间依赖性建模。</strong></h4><p>For each granularity of clips, we further model the temporal dependency by an S2S encoder. 具体来说，对于每个剪辑的粒度Xc n，我们将其馈送到一个S2S编码器，例如GRU，然后经过一个时间最大池化层（TMP）进行特征聚合，粒度为n的剪辑级特征vc n如下获取： vc n = TMP(S2S(Xc n))。 (4) 这里，使用了S2S编码器所有时间步的输出。因此，给定L个粒度的{Xc 1,Xc 2, . . . ,Xc L}，我们能够获得包含L个特征向量的最终剪辑级表示Vc如下： Vc = {vc 1, vc 2, . . . , vc L}。 (5) 需要注意的是，我们的网络与S2S编码器正交，理论上可以使用任何S2S编码器。在我们的实现中，我们尝试了GRU、LSTM和Transformer。</p>
<p>Summary: In this section, temporal dependency modeling is discussed. For each granularity of clips, a temporal dependency model is applied using an S2S encoder, which can be GRU, LSTM, or Transformer. This modeling captures the temporal relationships within the clips and results in clip-level features that are aggregated using a temporal max pooling layer (TMP). These features are then used to construct the final clip-level representation.</p>
<p>摘要：本节将讨论时态依赖关系建模。对于剪辑的每个粒度，使用 S2S 编码器（可以是 GRU、LSTM 或转换器）应用时态依赖关系模型。此建模可捕获剪辑内的时间关系，并生成使用时间最大池化图层 （TMP） 聚合的剪辑级要素。然后，这些特征用于构造最终的剪辑级表示。</p>
<h4 id="Part-level-Representation"><a href="#Part-level-Representation" class="headerlink" title="Part-level Representation."></a><strong>Part-level Representation.</strong></h4><p>Obtaining the part-level representation is similar to obtaining the clip-level representation, so we mainly specify choices that are unique at the part-level. 获得部分级表示与获得剪辑级表示类似，因此我们主要说明在部分级别独特的选择。↳</p>
<p>Given a skeleton sequence X ∈ RT×J×3, here we reshape it in the space-m</p>
<p>Similarly, a joint embedding is employed to obtain the initial joint representation Xp 1 ∈ RJ×C. 类似地，使用关节嵌入来获取初始关节表示Xp 1 ∈ RJ×C。</p>
<p>Additionally, we generate human parts of different spatial granularities by UDM. 此外，我们通过UDM生成不同空间粒度的人体部分。</p>
<p>The UDM here is employed to merge nearby joints/parts to obtain parts of larger sizes. 这里的UDM被用来合并附近的关节/部分，以获取更大尺寸的部分。</p>
<p>Similarly, by jointly employing UDM L−1 times in series, we are able to obtain parts of different spatial granularities {Xp 1 ,Xp 2 , . . . ,Xp L}. 类似地，通过连续共同使用UDM L−1次，我们可以获得不同空间粒度的部分{Xp 1 ,Xp 2 , . . . ,Xp L}。</p>
<p>Furthermore, for each granularity, an S2S encoder with a max pooling layer is used to capture the spatial dependency. 此外，对于每个粒度，使用带有最大池化层的S2S编码器来捕捉空间依赖性。</p>
<p>Consequently, we are able to obtain the final part-level representation containing clues of multiple spatial granularities as Vp = {vp 1, vp 2, . . . , vp L}. 因此，我们能够获得包含多个空间粒度线索的最终部分级表示Vp = {vp 1, vp 2, . . . , vp L}。</p>
<p>Summary: In the Part-level Representation section, the process of obtaining part-level representation is explained. It involves reshaping the skeleton sequence to focus on joints, using a joint embedding for initial representation, and employing a Unified Downsampling Module (UDM) to generate parts of different spatial granularities. Multiple spatial granularities of parts are obtained, and for each granularity, an S2S encoder with a max pooling layer is used to capture spatial dependencies. This results in a final part-level representation that incorporates information from various spatial granularities.</p>
<p>摘要：在部件级表示部分中，解释了获取部件级表示的过程。 它涉及重塑骨架序列以关注关节，使用关节嵌入进行初始表示，并采用统一下采样模块（UDM）生成不同空间粒度的部分。 获得零件的多个空间粒度，并且对于每个粒度，具有最大池化层的S2S编码器用于捕获空间依赖性。 这会产生最终的部件级表示，其中包含来自各种空间粒度的信息</p>
<p>领域级别和实例级别表示 领域级别和实例级别表示。到目前为止，我们已经具有多个<strong>时间粒度的剪辑级别</strong>表示Vt以及多个<strong>空间粒度的部分级别表示</strong>Vs。我们进一步逐步将它们组合起来，以获得领域级别表示和实例级别表示。 对于领域级别表示，我们融合所有L个剪辑级别表示的特征向量，以得出时间域表示vt，并融合所有相应的部分级别表示的特征向量，以得出空间域表示vs： vt = F(Vt) = F(vc 1, vc 2, . . . , vc L), vs = F(Vp) = F(vp 1, vp 2, . . . , vp L), (6) 其中F(·)表示对多个特征向量的融合运算符。在我们的实现中，使用了串联操作。 此外，时间域和空间域表示被联合组合成实例级别表示vi： vi = F(vt, vs). (7) 值得指出的是，尽管已经获得了多级表示，但最终骨架序列表示仅使用实例级别表示，而其他表示用于分层对比，将在下一部分中描述。</p>
<p>总结：在领域级别和实例级别表示部分，我们将多个时间粒度的剪辑级别表示（Vt）和多个空间粒度的部分级别表示（Vs）逐步组合，以获得领域级别和实例级别表示。领域级别表示包括时间域（vt）和空间域（vs）表示。这些领域级别表示然后联合组合成实例级别表示（vi）。值得注意的是，对于最终的骨架序列表示，仅使用实例级别表示，而其他表示用于分层对比，将在接下来的部分中解释。</p>
<h4 id="Hierarchical-Contrast"><a href="#Hierarchical-Contrast" class="headerlink" title="Hierarchical Contrast"></a><strong>Hierarchical Contrast</strong></h4><p>分层对比 不同于以前的对比学习方法，仅在实例级特征上进行对比，我们提出了一种分层对比方法，该方法在实例级、领域级、剪辑级和部分级特征上进行对比。对于对比的方式，我们采用了MoCo的方式（He等人，2020年），即查询编码器和关键编码器与动态字典队列和移动平均更新机制一起使用。此外，根据（Chen等人，2020年）的方法，我们在对比之前使用了一个两层的多层感知器进行特征投影。为了简洁描述，我们使用了与原始特征相同的投影特征的符号。</p>
<p>总结：在分层对比部分，作者介绍了一种不同于以前的对比学习方法，即分层对比方法，该方法在实例级、领域级、剪辑级和部分级特征上进行对比。他们采用了MoCo的方式，并使用多层感知器进行特征投影。这种方法旨在提高对比学习的效果，以更好地捕捉骨架序列的层次结构特征。</p>
<h4 id="Instance-level-Contrast"><a href="#Instance-level-Contrast" class="headerlink" title="Instance-level Contrast"></a>Instance-level Contrast</h4><p>根据（Rao等人，2021年）的方法，我们使用了噪声对比估计损失InfoNCE（Van den Oord等人，2018年）进行对比。在实例级别上，对比损失计算如下： L实例 = −log exp(vi · ˆvi/τ) exp(vi · ˆvi/τ) +  mi j∈Mi exp(vi · mi j/τ) ， (8) 其中τ是温度超参数，mi j表示来自先进先出队列Mi的前一次投影实例级别特征的第j个负样本。</p>
<p>总结：在实例级对比部分，作者采用了InfoNCE损失函数，用于衡量正样本和负样本之间的对比。这有助于训练模型，使其能够更好地区分不同的实例级特征。</p>
<p>无监督方法在各种任务中越来越受到关注（Yang等人，2018年；Deng等人，2019年；Gao等人，2020年；Liu等人，2022年），因为它具有在没有人工标注的情况下进行训练的良好特性。现有的无监督骨骼动作表示学习（SARL）方法可以大致分为三类：编码-解码方法、对比学习方法和混合方法。</p>
<h4 id="Hierarchical-Modeling-on-Skeletons"><a href="#Hierarchical-Modeling-on-Skeletons" class="headerlink" title="Hierarchical Modeling on Skeletons"></a>Hierarchical Modeling on Skeletons</h4><ol>
<li>There are also some previous works that modeled human skeletons hierarchically. 也有一些以前的研究对人类骨架进行了分层建模。</li>
<li>For instance, (Du, Wang, and Wang 2015; Cheng et al. 2021; Wang, Ni, and Yang 2020) modeled the spatial hierarchical structure with hand-crafted designs, but they did not consider the temporal hierarchical structure. 例如，（杜，王和王2015年；程等人2021年；王，倪和杨2020年）使用手工设计对空间层次结构进行了建模，但没有考虑时间层次结构。</li>
<li>(Chen et al. 2022) exploited the temporal hierarchical clues from the frame, clip, and video levels. （陈等人2022年）从帧、剪辑和视频级别利用了时间层次线索。</li>
<li>By contrast, we jointly model the temporal and spatial hierarchical structure, which can be readily implemented using our proposed more unified framework. 相比之下，我们联合建模了时间和空间的层次结构，这可以很容易地使用我们提出的更统一的框架来实现。</li>
<li>Besides, different from the previous models that present skeletons at the instance level, we present skeletons at the instance level, domain level, clip level, and part level. 此外，不同于以前将骨架呈现在实例级别的模型，我们在实例级别、域级别、剪辑级别和部分级别呈现骨架。</li>
</ol>
<p>总结： 本章介绍了以前的一些关于对人体骨架进行分层建模的研究，其中一些模型侧重于空间层次结构，而另一些侧重于时间层次结构。与以前的模型不同，本研究提出了一个更统一的框架，可以同时对时间和空间的层次结构进行建模，并将骨架呈现在不同的层次上，包括实例级别、域级别、剪辑级别和部分级别。</p>
<h1 id="Multilevel-Spatial–Temporal-Excited-Graph-Network-for-Skeleton-Based-Action-Recognition"><a href="#Multilevel-Spatial–Temporal-Excited-Graph-Network-for-Skeleton-Based-Action-Recognition" class="headerlink" title="Multilevel Spatial–Temporal Excited Graph Network for Skeleton-Based Action Recognition"></a>Multilevel Spatial–Temporal Excited Graph Network for Skeleton-Based Action Recognition</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节 INTRODUCTION：</p>
<p>Although it improves the capacity of modeling the joint<br>relationships, the pipeline of GCN+TCN still suffers from<br>several problems. From the perspective of the GCN, the<br>physical skeleton graph is predefined according to the intrinsic<br>connectivity of the human body and thus lacks flexibility in<br>network training. It cannot handle the various joint relations<br>of different samples, especially when they are performing<br>different actions. For example, as Figure 1 shows, the relation<br>between ‘finger’ and ‘head’ is important for distinguishing<br>between ‘wave hand’ and ‘touch head’. However, it is difficult<br>for the fixed graph to explore the dependency between these<br>two parts. Moreover, most current methods [11], [12], [17]<br>utilize only joint-level relations to generate spatial features<br>but ignore high-level cues such as part-level and body-<br>level relations. Notice that such high-level interactions often</p>
<p>carry crucial information for identifying complicated actions.<br>For example, the motion of ‘jump up’ in Figure 1 is typically<br>interpreted based on the interplay of arms and legs at a<br>relatively high level, rather than the detailed locations of<br>fingers and toes.</p>
<p>尽管它增强了对关节关系建模的能力，但GCN+TCN的管道仍然存在一些问题。从GCN的角度看，<strong>物理骨骼图是根据人体内在连接性预定义的，因此在网络训练中缺乏灵活性。</strong>它<strong>不能处理不同样本的各种关节关系</strong>，尤其是<strong>当它们执行不同动作时</strong>。例如，正如图1所示，’手指’和’头部’之间的关系对于区分’挥手’和’触摸头’非常重要。然而，<strong>对于固定的图来探索这两部分之间的依赖关系是困难的。</strong>此外，大多数当前方法[11]、[12]、[17]仅<strong>利用关节级别的关系来生成空间特征</strong>，但忽略了<strong>高级别的线索，如</strong>部分级别和整体级别的关系。请注意，这种高级交互通常携带了识别复杂动作所需的重要信息。例如，图1中的’跳起’动作通常是基于相对较高级别上手臂和腿的相互作用来解释，而不是基于手指和脚趾的详细位置。</p>
<p><strong>总结：</strong></p>
<p>这段介绍指出了传统的GCN+TCN方法的局限性，包括固定的骨骼图和无法适应不同样本的不同关节关系。它还提到了目前的方法通常只使用关节级别的关系来生成空间特征，而忽略了高级别的部分和整体关系，而这些高级别的关系通常携带着识别复杂动作所需的重要信息。</p>
<p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节 INTRODUCTION： HUMAN action recognition becomes increasingly impor- tant in various application scenarios, such as video surveillance and human-computer interaction [1], [2], [3]. Many researchers exploit image-based methods to learn unique spatial-temporal contexts inherited in videos. Although great advances have been made, the RGB-based representations learned by these methods are not robust against changes in body scales, camera viewpoints, and background interference. In contrast to the RGB image, skeleton data only contain a time series of 2D or 3D positions of multiple human joints, providing extremely abstract and well-structured information. Moreover, this information can be easily acquired in many ways, such as human pose estimation methods [4] and mobile sensors such as Kinect [5], successfully meeting the requirement of real-time action recognition. The development of skeleton-based action recognition in deep learning can be roughly divided into two phases. The first type employs a Recurrent Neural Network (RNN) [6], [7], [8] or Convolutional Neural Network (CNN) [9], [10] to investigate short-term and long-term temporal dynamics. These networks treat skeleton data as joint-coordinate sequences or pseudoimages, which are then input into RNNs or CNNs to predict the action category. While effective to a certain extent, these approaches overlook the structural pattern of the human body as well as the spatial interactions between adjacent joints. The skeleton is naturally a graph structure in non-Euclidean space, with joints serving as nodes and their connections serving as edges. For this reason, recent works prefer to build skeleton graphs based on the physical links of human anatomy [11], [12], [13], [14], [15], [16]. The relations between human joints are modeled by graphs, and a ‘graph convolution network (GCN) + temporal convolution network (TCN)’ pipeline is often used to extract spatiotemporal features. Although it improves the capacity of modeling the joint relationships, the pipeline of GCN+TCN still suffers from several problems. From the perspective of the GCN, the physical skeleton graph is predefined according to the intrinsic connectivity of the human body and thus lacks flexibility in network training. It cannot handle the various joint relations of different samples, especially when they are performing different actions. For example, as Figure 1 shows, the relation between ‘finger’ and ‘head’ is important for distinguishing between ‘wave hand’ and ‘touch head’. However, it is difficult for the fixed graph to explore the dependency between these two parts. Moreover, most current methods [11], [12], [17] utilize only joint-level relations to generate spatial features but ignore high-level cues such as part-level and body- level relations. Notice that such high-level interactions often carry crucial information for identifying complicated actions. For example, the motion of ‘jump up’ in Figure 1 is typically interpreted based on the interplay of arms and legs at a relatively high level, rather than the detailed locations of fingers and toes. </p>
<p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节(不用展示英文原文) INTRODUCTION：  In temporal modeling, the action is described as a spatial human body moving along the temporal dimension. Since the motion changes are often concentrated in a few key areas, TCN, which simply uses temporal convolution, may not be able to fully explore the locations with significant changes. Some recent methods [16], [18] attempt to alleviate this issue by utilizing attention-based multistream models or 3D graph convolution. Although they improve the recognition accuracy, the high computational cost severely limits their real-world applications. To address the above limitations, we design a multilevel spatial-temporal excited graph network (ML-STGNet), which contains a multilevel GCN (ML-GCN) network, a spatial data- driven excitation (SDE) module, a temporal motion excitation (TME) module and a simplified multiscale TCN (MS-TCN) network. Note that when observing a comprehensive action, the human visual system usually first identifies its generality and then performs fine-grained recognition based on the personality of the action. To mimic this process, we decouple the learning of the skeleton graph into two modules, ML- GCN and SDE, which correspond to the general graph and individual graph, respectively. Specifically, ML-GCN leverages hierarchical graphs to represent both low-level joint linkages and high-level body interactions. These graphs are initialized based on the topological priors of the human body and have adaptive, trainable parameters.</p>
<p>帮我逐句翻译下我下边输入的段落并根据输入内容总结一下introduction章节(不用展示英文原文) INTRODUCTION：</p>
<p> SDE further leverages a transformer-like structure to learn exclusive data- dependent topologies for different samples. Different from the self-attention mechanism, SDE fully exploits positional information, which is critical to capture spatial structures in vision tasks. Compared with the previous methods, this decoupling method is more flexible in graph construction and more adaptive to various samples. Furthermore, TME is also proposed to capture salient motion information by applying the concept of temporal difference in optical flow [19] into a systematic and efficient temporal module. Instead of treating the motion velocities as another input modality [20], our module could integrate the motion modeling into the whole feature learning process. Concretely, the feature-level motion difference is calculated between adjacent frames. These motion features are then utilized to produce weights for exciting the motion-sensitive features. In this way, the network is forced to enhance the informative features that capture salient information. Based on these features, we further leverage a simplified MS-TCN [16] to enrich the representation capacity of temporal features. To verify the effectiveness of ML-STGNet, we report experimental results on four popular skeleton-based action recognition datasets, including NTU-RGB+D 120 [21], NTU-RGB+D 60 [22], Kinetics Skeleton 400 [23] and Toyota Smarthome [24]. The experimental results show that ML-STGNet outperforms most state-of-the-art methods in terms of recognition accuracy. The main contributions of this paper are as follows: 1. We propose a novel ML-STGNet for skeleton-based action recognition, which attempts to exploit the comprehensive joint relations in various samples and enhance the motion dynamics. 2. We decouple the learning of the skeleton graph into the general and individual parts. ML-GCN is proposed to model the general graph, which is responsible for capturing the hierarchical joint relations in complicated action. SDE is proposed to model the individual graph, which is responsible for exploring the topological relations among skeleton joints of different samples. 3. We design an efficient temporal module TME to select the most salient information at the feature level. This module is easy to implement and facilitates temporal modeling. 4. Experimental results show that the proposed ML-STGNet achieves impressive performance on popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, Kinetics Skeleton 400 and Toyota Smarthome.</p>
<p>以下是对Introduction部分的逐句翻译和总结：</p>
<p><strong>Introduction引言：</strong></p>
<p><strong>In temporal modeling, the action is described as a spatial human body moving along the temporal dimension. Since the motion changes are often concentrated in a few key areas, TCN, which simply uses temporal convolution, may not be able to fully explore the locations with significant changes.</strong> 在时域建模中，动作被描述为在时间维度上移动的空间人体。<strong>由于运动变化通常集中在一些关键区域</strong>，==仅使用时间卷积的TCN可能无法充分探索具有显著变化的位置==。</p>
<p><strong>Some recent methods [16], [18] attempt to alleviate this issue by utilizing attention-based multistream models or 3D graph convolution. Although they improve the recognition accuracy, the high computational cost severely limits their real-world applications.</strong> 一些最近的方法<strong>尝试通过使用基于注意力的多流模型或3D图卷积来缓解这个问题</strong>。尽管它们提高了识别精度，但高昂的计算成本严重限制了它们在现实世界中的应用。</p>
<p><strong>To address the above limitations, we design a multilevel spatial-temporal excited graph network (ML-STGNet), which contains a multilevel GCN (ML-GCN) network, a spatial data-driven excitation (SDE) module, a temporal motion excitation (TME) module and a simplified multiscale TCN (MS-TCN) network.</strong> 为了解决上述限制，==我们设计了一个<strong>多级时空激发图网络</strong>（ML-STGNet），其中包含多级GCN（ML-GCN）网络、空间数据驱动激发（SDE）模块、时间运动激发（TME）模块和简化的多尺度TCN（MS-TCN）网络==</p>
<p><strong>Note that when observing a comprehensive action, the human visual system usually first identifies its generality and then performs fine-grained recognition based on the personality of the action. To mimic this process, we decouple the learning of the skeleton graph into two modules, ML-GCN and SDE, which correspond to the general graph and individual graph, respectively.</strong> 请注意，在观察综合动作时，人类视觉系统通常首<strong>先识别其一般性</strong>，然后根据动作的<strong>个性进行细粒度识别</strong>。为模仿这个过程，我们将骨骼图的学习解耦为两个模块，ML-GCN和SDE，分别对应于一般图和个体图。</p>
<p><strong>Specifically, ML-GCN leverages hierarchical graphs to represent both low-level joint linkages and high-level body interactions. These graphs are initialized based on the topological priors of the human body and have adaptive, trainable parameters. SDE further leverages a transformer-like structure to learn exclusive data-dependent topologies for different samples.</strong> 具体来说，ML-GCN利用分层图来表示低级关节连接和高级身体相互作用。这些图是基于人体的拓扑先验初始化的，并具有自适应的可训练参数。SDE进一步利用类似Transformer的结构来学习不同样本的独占数据依赖拓扑结构。</p>
<p><strong>Different from the self-attention mechanism, SDE fully exploits positional information, which is critical to capture spatial structures in vision tasks. Compared with the previous methods, this decoupling method is more flexible in graph construction and more adaptive to various samples.</strong> 与自注意机制不同，<strong>SDE充分利用位置信息</strong>，这对于捕捉视觉任务中的空间结构至关重要。<strong>与以前的方法相比，这种解耦方法在图构建方面更加灵活，对各种样本更具适应性。</strong></p>
<p><strong>Furthermore, TME is also proposed to capture salient motion information by applying the concept of temporal difference in optical flow [19] into a systematic and efficient temporal module. Instead of treating the motion velocities as another input modality [20], our module could integrate the motion modeling into the whole feature learning process.</strong> 此外，还提出了TME，通过<strong>将光流中的时间差概念</strong>应用到<strong>系统化和高效的时间模块中来捕获显著的运动信息</strong>。与将运动速度视为另一输入模态的方法不同，我们的模块可以<strong>将运动建模整合到整个特征学习过程中</strong>。</p>
<p><strong>Concretely, the feature-level motion difference is calculated between adjacent frames. These motion features are then utilized to produce weights for exciting the motion-sensitive features. In this way, the network is forced to enhance the informative features that capture salient information. Based on these features, we further leverage a simplified MS-TCN [16] to enrich the representation capacity of temporal features.</strong> 具体来说，<strong>计算相邻帧之间的特征级运动差异</strong>。然后<strong>利用这些运动特征生成权重来激发运动敏感特征</strong>。通过这种方式，<strong>网络被迫增强捕捉显著信息的信息特征</strong>。基于这些特征，我们进一步利用了<strong>简化的MS-TCN来丰富时间特征的表示能力</strong>。</p>
<p><strong>To verify the effectiveness of ML-STGNet, we report experimental results on four popular skeleton-based action recognition datasets, including NTU-RGB+D 120 [21], NTU-RGB+D 60 [22], Kinetics Skeleton 400 [23] and Toyota Smarthome [24]. The experimental results show that ML-STGNet outperforms most state-of-the-art methods in terms of recognition accuracy.</strong> 为了验证ML-STGNet的有效性，我们报告了在四个常见的基于骨骼的动作识别数据集上的实验结果，包括NTU-RGB+D 120、NTU-RGB+D 60、Kinetics Skeleton 400和Toyota Smarthome。实验结果表明，ML-STGNet在识别准确性方面优于大多数最先进的方法。</p>
<p><strong>The main contributions of this paper are as follows:</strong> 本文的主要贡献如下：</p>
<ol>
<li>We propose a novel ML-STGNet for skeleton-based action recognition, which attempts to exploit the comprehensive joint relations in various samples and enhance the motion dynamics.</li>
<li>我们提出了一种新颖的ML-STGNet，用于基于骨骼的动作识别，旨在<strong>利用各种样本中的全面关节关系并增强运动动态性</strong>。</li>
<li>We decouple the learning of the skeleton graph into the general and individual parts. ML-GCN is proposed to model the general graph, which is responsible for capturing the hierarchical joint relations in complicated action. SDE is proposed to model the individual graph, which is responsible for exploring the topological relations among skeleton joints of different samples.</li>
<li>我们将骨骼图的学习<strong>解耦为一般和个体部分</strong>。ML-GCN被用来模拟通用图，负责捕捉复杂动作中的分层关节关系。SDE被用来模拟个体图，负责探索不同样本的骨骼关节之间的拓扑关系。</li>
<li>We design an efficient temporal module TME to select the most salient information at the feature level. This module is easy to implement and facilitates temporal modeling.</li>
<li>我们设计了一个<strong>高效的时间模块TME</strong>，以在特征级别选择最显著的信息。该模块易于实施，并有助于时间建模。</li>
<li>Experimental results show that the proposed ML-STGNet achieves impressive performance on popular datasets: NTU-RGB+D 60, NTU-RGB+D 120, Kinetics Skeleton 400 and Toyota Smarthome.</li>
<li>实验结果表明，所提出的ML-STGNet在流行数据集（NTU-RGB+D 60、NTU-RGB+D 120、Kinetics Skeleton 400和Toyota Smarthome）上取得了令人印象深刻的性能。</li>
</ol>
<h2 id="2-related-work"><a href="#2-related-work" class="headerlink" title="2.related work"></a>2.related work</h2><h3 id="A-Skeleton-Based-Action-Recognition"><a href="#A-Skeleton-Based-Action-Recognition" class="headerlink" title="A  Skeleton-Based Action Recognition"></a>A  Skeleton-Based Action Recognition</h3><p>章节 基于骨架的动作识别 针对骨架数据的传统动作识别方法通常从骨架序列中提取手工设计的特征[25]，[26]，[27]。然而，它们主要依赖于关节之间的旋转或平移，并且特征设计复杂。最近，随着深度学习的发展，基于神经网络的方法在基于骨架的动作识别中展现出强大的能力，可以根据网络架构归纳为三类：基于RNN的[6]，[7]，[8]，基于CNN的[9]，[10]，[28]，以及基于GCN的[11]，[12]，[13]，[14]，[15]，[16]，[18]，[29]，[30]，[31]，[32]。 基于RNN的方法通常将骨架数据建模为矢量序列，其中每个矢量代表人体关节的坐标。杜等人[6]将人体骨架划分为不同部分，然后将它们分别馈送到分层RNN中。张等人[7]设计了一个自适应RNN架构，使网络能够确定最佳视点。 基于CNN的方法通常将骨架转化为伪图像，并利用CNN将图像分类为动作类别。李等人[9]设计了一个新的骨架变换模块，并将关节坐标以及骨架运动馈送到一个7层CNN中，以进行双流预测。尽管CNN比RNN更受欢迎且更容易训练，但它们都无法描述相关关节之间的空间依赖性，因为骨架数据自然表示图拓扑而不是基于网格的图像或序列。</p>
<p>【</p>
<p>受益于图卷积在结构化数据建模方面的强大能力，Yan等人首次将图卷积网络（GCN）引入骨架动作识别中，提出ST-GCN，以同时建模关节的空间和时间关系。在这之后，2s-AGCN结合了固定物理图和可学习掩码，自适应地学习不同骨架样本的拓扑结构。SGN则将高层语义信息（关节类型和帧索引）作为网络输入的一部分，以提高特征学习能力。而MS-G3D进一步提出了一个统一的多尺度时空模块，结合多邻接GCN和MS-TCN，以利用时空中的直接信息流。然而，上述方法多多少少存在以下问题：它们可能无法灵活有效地学习骨架图结构，无法建模高层身体部位之间的连接，以及可能存在昂贵的计算成本。为了解决这些问题，我们首先正式将骨架图的学习分为两个部分：通用部分和个体部分。我们使用ML-GCN在通用图中捕获复杂动作中的分层关节关系，并使用类似于变换器的结构在个体图中学习独特的数据相关拓扑结构。这种实现方式模块化且即插即用，易于构建且不需要大量参数。</p>
<p>】</p>
<p>该章节讨论了基于骨架的动作识别方法。传统方法通常从骨架序列中提取手工设计的特征，但这些方法主要依赖于关节之间的旋转或平移，存在复杂的特征设计。近年来，深度学习方法在骨架动作识别中取得了显著进展，主要分为基于RNN、CNN和GCN的三种网络架构。</p>
<ul>
<li>基于RNN的方法将骨架数据视为矢量序列，每个矢量代表关节的坐标。这些方法通常使用分层RNN或自适应RNN架构。</li>
<li>基于CNN的方法将骨架数据转化为伪图像，然后使用CNN分类器。尽管CNN更容易训练，但它们难以描述关节之间的空间依赖性。</li>
<li>基于GCN的方法利用图卷积网络的能力，首次引入GCN用于骨架动作识别，以同时建模关节的空间和时间关系。后续的研究进一步改进了GCN的性能，例如2s-AGCN、SGN和MS-G3D。但这些方法存在某些缺点，如难以灵活有效地学习骨架图、难以建模高级身体部件之间的连接以及计算成本较高。</li>
</ul>
<p>为解决这些问题，该研究采用两部分模块化方法来学习骨架图：通用部分和个体部分。通用部分使用ML-GCN捕获复杂动作中的层次关系，而个体部分使用类似Transformer的结构为不同样本学习独立的数据相关拓扑结构。这种实现方式简单且不需要大量参数，使模型更具灵活性和效率。</p>
<h3 id="B-Attention-Mechanism-注意力机制"><a href="#B-Attention-Mechanism-注意力机制" class="headerlink" title="B. Attention Mechanism 注意力机制"></a>B. Attention Mechanism 注意力机制</h3><p>这一章节讨论了注意力机制。注意力机制可视为一种基于输入图像特征的动态权重调整过程，在各种视觉任务中取得了巨大成功。SENet首次提出了通道注意力，提出了一个压缩激发块来收集通道级全局信息。随着自注意力在自然语言处理中的流行，Wang等人提出了非局部操作以捕获像素对之间的长程空间依赖关系。在骨架动作识别领域，AGC-LSTM提出了增强型注意力图卷积LSTM来有效捕获有区别的时空特征。Chen等人设计了交叉头部注意力机制，使模型能够聚焦于关键运动信息。然而，纯粹的自注意力结构缺乏位置信息，而在视觉任务中描述空间结构非常重要。最近，Dosovitskiy等人通过引入视觉变换器（ViT）来解决这个问题。受到ViT的启发，该研究设计了SDE来模拟个体图。此外，还将查询位置编码的思想扩展到所有键-值对，以获取更精确的位置信息。</p>
<p>总结该章节： 该章节讨论了注意力机制在骨架动作识别中的应用。传统的注意力机制已在视觉任务和自然语言处理中取得成功。针对骨架动作识别，之前的研究已提出各种不同的注意力机制，如通道注意力、自注意力、交叉头部注意力等。然而，这些方法在描述空间结构时缺乏位置信息。因此，该研究受到视觉变换器（ViT）的启发，设计了SDE模型，并引入了查询位置编码，以提供更精确的位置信息。</p>
<h3 id="C-Temporal-Difference-Modeling"><a href="#C-Temporal-Difference-Modeling" class="headerlink" title="C. Temporal Difference Modeling"></a>C. Temporal Difference Modeling</h3><p>时间差异建模 时间差异操作在动作识别中用于提取运动信息，例如RGB差异和特征差异。前者计算输入级别图像之间的差异以提供运动信息。ResGCN将这一思想扩展到骨架领域，并设计了一个多分支架构，分别从关节、速度和骨骼模态接收输入。然而，这些方法的主要缺点在于它们将RGB差异仅视为另一种视频模态，并训练一个独立的网络将其与主流网络融合。后者采用差异操作进行网络设计。例如，STM提出了一个逐通道的运动模块来编码运动特征。TEA设计了一个运动激发模块，以促使网络发现信息丰富的时间瞬间。受到TEA的启发，我们将特征差异操作扩展到基于骨架的动作识别中，并设计TME来激发显著的运动敏感特征。</p>
<p>总结：本节介绍了在动作识别中的时间差异建模。时间差异操作可用于提取运动信息，包括RGB差异和特征差异。先前的方法将RGB差异视为另一种视频模态，与主流网络分开训练。而后者使用差异操作来设计网络，例如通过逐通道运动模块或运动激发模块来编码运动特征。本文受到后者的启发，将特征差异操作引入基于骨架的动作识别中，设计了TME来激发显著的运动敏感特征。</p>
<h2 id="III-MULTILEVEL-SPATIAL-TEMPORAL-EXCITED-GRAPH-NETWORK"><a href="#III-MULTILEVEL-SPATIAL-TEMPORAL-EXCITED-GRAPH-NETWORK" class="headerlink" title="III. MULTILEVEL SPATIAL-TEMPORAL EXCITED GRAPH NETWORK"></a>III. MULTILEVEL SPATIAL-TEMPORAL EXCITED GRAPH NETWORK</h2><p>跟随“GCN+TCN”流程，我们提出的ML-STGNet是GC-Blocks的集成，每个块包含四个组件：<strong>多级GCN（ML-GCN）网络，空间数据驱动激发（SDE）模块，时间运动激发（TME）模块和简化的多尺度TCN（MS-TCN）网络。</strong></p>
<p>首先，我们将<strong>骨架图的学习</strong>分为==通用部分和个体部分==，<strong>以模拟人类视觉系统</strong>。<strong>ML-GCN被引入作为通用图</strong>，用于探索关节之间复杂动作的层次关联。如图2所示，我们基于人体拓扑结构启发式地设计了三个层次的图，即<strong>低级关节图，中级部位图和高级人体图</strong>。为了促进网络的灵活性，这些图都有可学习的参数。<strong>SDE进一步利用类似变换器的结构充分探索不同样本的个体图</strong>。与普通的自注意力或变换器结构相比，<strong>SDE具有更精确的位置信息</strong>。此外，我们<strong>引入了TME来突出显示对运动敏感的特征，这易于实现，并有助于MS-TCN中的时间建模</strong>。总体流程如图3所示。在接下来的章节中，我们介绍一些相关符号。然后，我们提供了ML-STGNet框架的详细技术描述。</p>
<p>总结：本章介绍了ML-STGNet的结构，它采用了多级GCN网络、空间数据驱动激发模块、时间运动激发模块和多尺度TCN网络。ML-GCN用于捕捉复杂动作之间的关联，SDE用于探索不同样本的个体图，TME用于突出显示运动敏感特征。整体框架分为通用部分和个体部分，以模拟人类视觉系统。这一结构的设计旨在提高网络的灵活性和性能。</p>
<h3 id="A-Preliminaries"><a href="#A-Preliminaries" class="headerlink" title="A. Preliminaries"></a>A. Preliminaries</h3><ol>
<li>符号：人体骨架可以表示为一个图拓扑结构。我们可以将其形式化为G = (V, E)，其中V = {v1, v2, . . . , vN} 是表示关节的N个节点的集合。E以邻接矩阵A ∈ RN×N的形式给出了边的代表性描述，其中元素ai j取1或0的值，指示ai和aj是否相邻。给定一个骨架序列，我们首先计算可以表示为X ∈ RC×T×N的节点特征X = {xt,n|1 ≤ t ≤ T, 1 ≤ n ≤ N; n, t ∈ Z}，其中xt,n = Xt,n表示帧t上节点vn的C维特征。</li>
<li>图卷积网络：每个i-th关节在t-th帧上的层次GCN的更新规则可以写成： fout (vti ) = X vt j ∈B(vti ) 1 Z[l(vt j )] fin(vt j )w[l(vt j )]， 其中fin和fout代表特征映射。B(vti ) = {vti |d(vti , vt j ) ≤ 1}表示vti的1距离邻居集。<strong>l(·)是为B(vti)中的每个顶点分配从1到K的标签的标签函数</strong>。按照ST-GCN [11]中提出的空间配置分割策略，K被设置为3，将B(vti )分为3个子集：顶点本身、向心子集和离心子集，如图2（a）所示。w(·)是可学习的1×1卷积的权重。Z是归一化项。</li>
</ol>
<p>为了便于计算，方程1可以转换成矩阵计算： Xt = σ Kv X k AkXtWk ， 其中Ak = 3 −1 2 k Ak c k3 −1 2 k，B(vti )的每个k-th分区表示为cAk，3i</p>
<p><strong>总结：</strong> 本章介绍了关于骨架的预备知识，包括符号表示和图卷积网络。人体骨架被表示为一个图，其中节点表示关节，边表示它们之间的连接。图卷积网络用于对骨架数据进行特征提取，其中每个关节的特征是通过与邻近关节的特征加权组合得到的。矩阵形式的计算更有利于处理这些图卷积操作。</p>
<h3 id="B-Multilevel-Graph-Convolution"><a href="#B-Multilevel-Graph-Convolution" class="headerlink" title="B. Multilevel Graph Convolution"></a>B. Multilevel Graph Convolution</h3><p>通用图表捕获了包含在动作中的常见模式。如图2所示，为了充分挖掘综合性动作，我们定义了三个分层图Ajoint、Apart和Abody。在关节级别图中，Ajointk与ST-GCN [11]中的Ak完全相同。在部位级别图中，我们根据人体拓扑结构将关节手动分类为<strong>10个类别</strong>。然后，我们定义Apart = 3 −1 2 partRpart[ApartRT part3 −1 2 part，其中Rpart是一个25×10的关系矩阵，<strong>描述了新的部位级别图中每个关节属于哪个类别</strong>（如果第i个关节属于第j个部位，则Ri j设置为1；否则，Ri j设置为0），RT part是它的转置。[Apart是一个10×10的部位级别邻接矩阵，显示了不同部位之间是否存在连接。3ii part = P j Ai j part是一个对角线矩阵。类似于部位级别图，我们将关节分成5个不同的身体部分，并将身体级别图定义为Abody = 3 −1 2 bodyRbody\AbodyRT body3 −1 2 body。在训练过程中，所有这些图都会自适应调整以捕捉灵活的多级身体关系。 然后，如图4（a）所示，给定X ∈ RC×T×N，我们首先基于上述图计算图特征： Xjoint = Kv X k Ajointk XWk， Xpart = ApartXWpart， Xbody = AbodyXWbody， (3) 我们选择1×1卷积层作为嵌入函数，其中Wk、Wpart和Wbody∈ RCout×Cin。为简化起见，我们假设Cout = Cin = C。</p>
<p><strong>总结： 本章介绍了一种通用图表，该图表捕获了动作中的常见模式。作者通过定义关节级别、部位级别和身体级别的分层图来捕获多级身体关系。在训练过程中，这些图会自适应调整以更好地适应多级关系。作者还介绍了如何计算图特征，这些特征用于后续的分析。</strong></p>
<h3 id="C-Spatial-Data-Driven-Excitation"><a href="#C-Spatial-Data-Driven-Excitation" class="headerlink" title="C. Spatial Data-Driven Excitation"></a>C. Spatial Data-Driven Excitation</h3><p>个体图<strong>以数据驱动的方式处理不同样本的多样化关节关系</strong>。如图4（b）所示，我们设计了一个类似于Transformer的结构来利用这些信息。让我们<strong>考虑编码后的通用图特征X ∈ RCin×T×N</strong>。首先，<strong>为了减少计算成本并更加强调空间信息</strong>，我们使用<strong>时间池化来压缩时间维度</strong>：Xs ∈ RCin×1×N。然后，我们从自注意力开始，介绍了如何构建我们的SDE模块。在自注意力中，输出Y ∈ RCout×1×N是通过<strong>对投影输入进行池化计算得到的</strong>： Yi = XN p=1 softmax(qT i kp)vp， (6) 其中，查询项qi = WqXs i，键项ki = WkXs i，值项vi = WvXs i都是输入Xs i的线性投影，其中i ∈ {1, 2, . . . N}。Wq，Wk ∈ RCq×Cin 和Wv ∈ RCout×Cin 都是可学习参数。然而，这个机制在计算非局部上下文时没有使用任何位置信息。在视觉模型中，位置信息通常有助于捕捉对象的结构。</p>
<p><strong>总结： 本章讨论了个体图的构建，该图以数据驱动的方式处理不同样本的关节关系。为了更好地处理这些关系，作者介绍了一个自注意力机制，但指出这一机制在计算非局部上下文时未使用任何位置信息。此位置信息在视觉模型中通常用于捕捉对象的结构。</strong></p>
<p>变换器模块通过以下公式缓解了这个问题： Yi = XN p=1 softmax(qT i kp + qT i rp−i )vp， (7) 其中rp−i是<strong>添加的相对位置编码</strong>。内积qT i rp−i测量从第p个关节到第i个关节的兼容性。请注意，此结构的位置信息仅取决于查询，但键和值也传达关于要关注的位置的信息。<strong>将变换器模块应用于语义信息丰富的RGB数据时，单一的查询位置编码可能足以描述对象的结构</strong>。然而，骨架数据要稀疏得多，我们可能<strong>需要更多的位置编码以协作更好地描述关节的空间位置</strong>。 因此，在我们的SDE中，我们<strong>将查询位置编码扩展到所有键-值对</strong>： Yi = XN p=1 softmax(qT i kp + qT i rq p−i + kT p rk p−i )(vp + rv p−i )， (8) 其中rk p−i和rv p−i<strong>是键和值的位置编码。</strong>在实践中，可以将<strong>等式8中的单头注意力扩展为多头注意力</strong>，以捕获各种相关性。多头注意力通过将G个单头注意力应用于Xs i（每个头的g-th head采用不同的Wg q，Wg k，Wg v∀g ∈ {1, 2, . . . G}），然后<strong>通过连接每个头部（G经验性地设置为8）获得输出Yi</strong>。最后，<strong>Y被映射回T帧，被视为激发X的注意权重</strong>。在SDE中，采用了残差连接以稳定训练。</p>
<p><strong>总结：</strong> 本章介绍了一个SDE模块，该模块使用Transformer结构来处理骨骼数据中的位置信息。作者指出，与RGB数据不同，骨骼数据更稀疏，因此<strong>需要更多的位置编码来更好地描述关节的空间位置</strong>。在该模块中，查询位置编码被扩展到所有键-值对，并采用了多头注意力以捕获多种相关性。残差连接被用于稳定训练。</p>
<h3 id="D-Temporal-Motion-Excitation"><a href="#D-Temporal-Motion-Excitation" class="headerlink" title="D. Temporal Motion Excitation"></a>D. Temporal Motion Excitation</h3><p>运动是<strong>两个连续帧之间的位移测量，反映了真实的动作</strong>。如图4（c）所示，我们设计了TME<strong>以突出运动敏感特征</strong>。给定编码的图形特征X ∈ RCin×T×N，首先使用<strong>1×1卷积层来提高效率的降低特征通道数</strong>： Xr = WrX，其中Wr ∈ R Cin η ×Cin <strong>是一个可学习的卷积权重</strong>，η经验性地设置为4。在<strong>时间步骤t上计算特征级运动表示</strong>，它是<strong>相邻两帧Xr t和Xr t+1之间的差异</strong>。如TEA [42]中的实践，对（t + 1）-th帧应用了一个<strong>通道级的1×1卷积来减轻不匹配问题</strong>： Mt = WmXr t+1 −Xr t 1 ≤ t ≤ T − 1， 其中Wm是1×1通道级的转换，<strong>Mt ∈ R Cin η ×N是时间t的运动特征</strong>。为了对齐时间维度，我们<strong>将MT = 0表示最后时间步的运动特征</strong>，最后将这些特征连接成M ∈ R Cin η ×T×N。 然后，我们使用全局平均池化（GAP）来总结空间信息，并利用另一个1×1卷积层将通道维度扩展到原始大小Cin。 可以进一步计算运动关注权重β ∈ RCin×T×1如下： β = sigmoid(WeGAP(M))， </p>
<p><strong>总结：</strong> 本章节介绍了关于运动特征建模的部分。运动在骨骼动作识别中起到关键作用，因为它是连续帧之间的位移测量，反映了真实的动作。TME模块用于突出运动敏感特征。该模块通过1x1卷积层降低特征通道数，计算相邻帧之间的运动差异，最后使用全局平均池化和卷积层处理运动特征，以计算运动注意权重。这有助于提取并强调骨骼动作中的运动信息。</p>
<p>最后，<strong>该模块的目标是突出运动敏感特征：</strong> X ← βX+X。 这里使用了残差连接以稳定训练。</p>
<p>基于运动敏感特征，我们<strong>进一步利用TCN来学习最终的表示</strong>。我们的TCN是MS-TCN [16]的简化版本，主要区别在于我们<strong>使用较少的分支</strong>（dilations={1, 2}）和<strong>更大的时间核大小（核大小=5）以实现高效的时间建模</strong>。</p>
<p>在TCN之后，使用GAP（全局平均池化）层来总结时空信息，然后使用FC（全连接）层将隐藏空间映射到类别概率。ML-STGNet的损失函数采用了预测值和真实标签之间的交叉熵损失。</p>
<p>需要注意的是，这些模块都设计为灵活的组件，可以轻松嵌入到GC-Block中。如图5所示，我们主要探讨了三个示例：ML-STGNet-c，ML-STGNet-p和ML-STGNet-r。这些组合将在消融研究中进行详细讨论，如果没有特别说明，所有实验都使用ML-STGNet-c。</p>
<p>这一章节描述了一个模块，其目标是突出运动敏感特征，然后通过TCN和其他层来学习最终的表示。不同的模块组合可以用于不同的实验设置。</p>
<h1 id="FSAR-Federated-Skeleton-based-Action-Recognition-with-Adaptive-Topology-Structure-and-Knowledge-Distillation"><a href="#FSAR-Federated-Skeleton-based-Action-Recognition-with-Adaptive-Topology-Structure-and-Knowledge-Distillation" class="headerlink" title="FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation"></a>FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</h1><h2 id="1-Introduction-1"><a href="#1-Introduction-1" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>这一章节是论文的引言，讨论了以下主要内容：</p>
<ul>
<li>骨骼动作识别是一个有广泛应用领域的重要研究领域，包括人机交互、智能安全监控和视频理解。</li>
<li><strong>深度神经网络</strong>在骨骼序列中<strong>学习区分性空间和时间特征方面</strong>取得了显著进展。然而，这些方法通常依赖于集中收集人体骨骼视频，存在<strong>隐私问题</strong>。</li>
<li>论文提出了<strong>利用联邦学习（</strong>Federated Learning）进行骨骼动作识别<strong>以保护骨骼数据隐私</strong>的方法。</li>
<li><strong>==联邦学习是一种协作学习方法，旨在从多个分布式的边缘设备或客户端共同学习，同时保护本地数据的安全和隐私==</strong>。</li>
<li>然而，现有的联邦学习方法主要针对图像任务，鲜有应用于骨骼动作识别。所以，==本文尝试将联邦学习应用于骨骼动作识别任务==。</li>
<li>骨骼数据的非欧几里得性质使得图卷积网络（Graph Convolution Networks）成为骨骼任务的有效模型。</li>
<li>论文介绍了 Federated Skeleton-based Action Recognition (FSAR) 作为一个==新的联邦学习骨骼动作识别任务的基准==。FSAR 使用自适应拓扑结构（ATS）来提高训练的稳定性，并采用==多粒度知识蒸馏==（MKD）机制降低客户端模型的差异。</li>
<li>作者通过ATS和MKD取得了显著的性能，并提供了骨骼动作识别中联邦学习的实际解决方案。</li>
<li>论文的贡献总结如下：1）首次将联邦学习引入骨骼动作识别领域；2）提出自适应拓扑结构（ATS）来解决骨骼<strong>数据的训练不稳定性</strong>问题；3）开发多粒度知识蒸馏（MKD）机制来<strong>提高准确性</strong>；4）通过大量实验证明了FSAR的有效性。</li>
</ul>
<p>这一章节主要介绍了论文的背景、研究问题、方法和贡献，为后续章节提供了基础。</p>
<h2 id="逐句翻译"><a href="#逐句翻译" class="headerlink" title="逐句翻译"></a>逐句翻译</h2><p>Introduction 骨骼动作识别是一个有广泛应用领域的重要研究领域，包括人机交互、智能安全监控和视频理解。</p>
<p>Recent advancements in deep neural networks [41, 38] have shown significant progress in learning discriminative spatial and temporal features from skeleton sequences. 最近深度神经网络的进展（引用了参考文献41和38）在从骨骼序列中学习区分性的时空特征方面取得了显著进展。</p>
<p>Though successful, these methods heavily rely on massively centralizing human skeleton videos, which directly and effectively pose privacy concerns due to the exposure of human-related formation, like motion patterns, behavior tendencies, and personal identity. 尽管这些方法取得了成功，但它们严重依赖于集中收集人体骨骼视频，这直接而有效地引发了隐私问题，因为这些==视频暴露了与人相关的信息，如运动模式、行为倾向和个人身份==。</p>
<p>For instance, Liao et al. [24, 23] have developed algorithms to identify individuals based on their body structures and walking styles. 例如，Liao等人已经开发了一种算法，可以<strong>根据个体的身体结构和步行风格来识别个人</strong>。</p>
<p>Therefore, centralized collection of such sensitive data exacerbates the risk of privacy disclosure for each local user site. 因此，对这种敏感数据进行集中收集增加了每个本地用户站点的隐私披露风险。</p>
<p>In response to the increasing awareness of personal data protection, decentralized training techniques are being developed, with federated learning being a powerful approach. 为了响应对个人数据保护意识的增强，==正在开发分布式训练技术，而联邦学习是一个强大的方法==。</p>
<blockquote>
<p>现在的骨架动作识别基于骨骼数据，但是现有的方法依赖的这些数据会导致个人信息的暴露</p>
<p>同时又有技术已经开始针对这些数据进行个人信息的识别</p>
<p>所以要倾向于保户个人数据 利用分布式训练技术 获取数据 引出连邦学习</p>
<p>连邦学习是一个非常强大的分布式训练服方法</p>
</blockquote>
<p>Thus in this work, we explore the application of federated learning to skeleton-based action recognition for privacy-preserving of skeleton data, which has rarely been investigated before. 因此，在这项研究中，我们探索了==将联邦学习应用于骨骼动作识别，以保护骨骼数据的隐私，这之前很少有研究==</p>
<p>Federated Learning (FL) is a collaborative learning approach that aims to collectively learn from multiple decentralized edge devices or clients while preserving the security and privacy of local data [30, 22]. 联邦学习（FL）是一种协作学习方法，旨在通过保护本地数据的安全和隐私，从多个分布式的边缘设备或客户端集体学习[30, 22]。</p>
<p>However, many effective FL techniques are image-based tasks, such as person re-identification [44], medical image segmentation [8], and vision language navigation [48]. 然而，许多有效的FL技术是基于图像的任务，如人员再识别[44]、医学图像分割[8]和视觉语言导航[48]。</p>
<p>To introduce FL schemes into the skeleton-based action recognition task, we follow the standard client-server architecture [30, 12] and construct a vanilla Federated Skeleton-based Action Recognition (Vanilla FSAR) paradigm. 为了将FL方案引入基于骨骼的动作识别任务，我们<strong>遵循标准的客户端-服务器架构[30, 12]，并构建了一个普通的基于骨骼的联邦动作识别（Vanilla FSAR）范例</strong>。</p>
<p>As shown in Fig. 1, we consider multiple local clients having independent and non-overlapping category labels. 如图1所示，我们考虑了<strong>多个具有独立和非重叠类别标签的本地客户端</strong>。</p>
<p>Each client is optimized with non-shared local data under the orchestration of a central server. 在中央服务器的协调下，每个客户端都<strong>使用非共享的本地数据进行优化</strong>。</p>
<p>The generalized server model is learned by aggregating the local model parameters without accessing sensitive local data. 综合的服务器模型是<strong>通过聚合本地模型参数学习的，而不访问敏感的本地数据。</strong></p>
<p>Under this paradigm, vanilla FSAR optimizes a generic feature space from multiple and non-shared data silos while maintaining data privacy. 在这种范例下，普通FSAR==从多个非共享的数据储存空间中优化了通用的特征空间，同时保护数据隐私==</p>
<p>Due to the non-euclidean nature of skeleton data, Graph Convolution Networks (GCNs) have been effective models for skeleton-based tasks. 由于骨骼数据的非欧几里得性质，图卷积网络（Graph Convolution Networks）已经成为骨骼任务的有效模型。</p>
<p>One of the notable efforts is ST-GCN [46] which models spatial temporal graph of skeleton sequences based on human topology structures. 其中一项值得注意的工作是ST-GCN[46]，它基于人体拓扑结构对骨骼序列的时空图进行建模。</p>
<p>Compared to CNN- or RNN-based methods, ST-GCN and its variants [3, 19, 28] achieve superior performance on skeleton-based action recognition. 与基于CNN或RNN的方法相比，<strong>ST-GCN及其变种</strong>[3, 19, 28]在基于骨骼的动作识别上实现了更出色的性能。</p>
<p>Without loss of generality, we apply FL approaches to the ST-GCN model via the above client-server collaboration training paradigm as the straightforward solution to address the privacy and security concerns. 毫不失一般性，我们<strong>通过上述客户端-服务器协作培训范例将FL方法应用于ST-GCN模型，作为解决隐私和安全问题的直接方法</strong>。</p>
<p>Nevertheless, the direct combination can suffer from slow convergence and considerable fluctuations, as shown in Fig. 2, which obstructs the model from generating feature representation suitable for efficient deployment with privacy protection. 然而，直接组合可能会导致收敛速度较慢和相当大的波动，如图2所示，这<strong>阻碍了模型生成适用于隐私保护的高效部署的特征表示</strong>。</p>
<p>Apart from the non-Independently Identically Distribution (non-IID) [47] of data from different clients, we identify the heterogeneous graph topologies across clients as a critical trigger for this phenomenon. 除了来自不同客户端的数据的非独立同分布性（non-IID）[47]，我们确定<strong>不同客户端之间的异构图拓扑</strong>是这一现象的重要触发因素。</p>
<p>This structured element is dataset-specific, causing local models of different clients to gradually diverge from each other during local training. 这种结构化元素是特定于数据集的，<strong>导致不同客户端的本地模型在本地训练期间逐渐发散</strong>。</p>
<p>It is known as the client drift problem in traditional FL [13], which can drastically damage the training performance of the global model when the data similarity decreases. 这在传统的FL[13]中被称为客户端漂移问题，当数据相似性降低时，它可能会严重损害全局模型的训练性能。</p>
<p>In light of this, we introduce FSAR, a novel benchmark for Federated Skeleton-based Action Recognition, to address the aforementioned client drift issues, especially for skeleton data. 鉴于此，我们引入了FSAR，这是一个新的联邦骨骼动作识别的基准，以解决前述的客户端漂移问题，特别是针对骨骼数据。</p>
<p>In FSAR, instead of finding one global model that fits the data distribution of all clients, an Adaptive Topology Structure (ATS) is proposed to inject modulated and customized elements into each client model. 在FSAR中，我们<strong>不是寻找适应所有客户端数据分布的全局模型，而是提出了一个自适应拓扑结构（ATS），将调制和定制的元素注入到每个客户端模型中</strong>。</p>
<p>The ATS learns the commonality of shared structure to improve the stability of FL training, and preserves the unique structure of their own data to prevent current clients from being affected by other clients with different dataset scales, respectively. ATS学习共享结构的共性，以提高FL培训的稳定性，并保留自己数据的独特结构，以防止当前客户端受到其他具有不同数据集规模的客户端的影响。</p>
<p>Moreover, we adopt learnable factors to automatically balance both topology structures on each client data since smaller datasets are more susceptible to large-scale datasets. 此外，我们采用<strong>可学习的因子来自动平衡每个客户端数据上的两种拓扑结构</strong>，因为较小的数据集更容易受到大规模数据集的影响。</p>
<p>Apart from this, data heterogeneity across clients caused by various source domains also jeopardize the training stability and accuracy. 除此之外，由<strong>各种来源领域引起的客户端之间</strong>的<strong>数据异质性也危害了培训的稳定性和准确性</strong>。</p>
<p>Generally, the features extracted by the shallower layer contain universal information (e.g., the connection between different joints), and the deeper features hold semantic information related to action labels (which is personalized and client-specific). 通常，浅层提取的特征包含通用信息（例如不同关节之间的连接），而深层特征包含与动作标签相关的语义信息（这是个性化的和特定于客户端的）。</p>
<p>Therefore, a Multi-grain Knowledge Distillation (MKD) mechanism is further developed to reduce the feature variation of shallow layers, which decreases client divergences with respect to the server model and facilitates client-server communication on generalized messages. 因此，我们进一步开发了<strong>多粒度知识蒸馏（MKD）机制</strong>，<strong>以减少浅层特征的变化，从而减小了客户端与服务器模型的差异，促进了客户端-服务器上的广义消息通信</strong>。</p>
<p>By leveraging both ATS and MKD, FSAR achieves remarkable performance and provides practical solutions for federated learning in skeleton-based action recognition. 通过充分利用ATS和MKD，FSAR取得了显著的性能，并为基于骨骼的动作识别的联邦学习提供了实际的解决方案。</p>
<blockquote>
<p>ATS 自适应的异质图拓扑结构 保证所有客户端之间的互相影响   解决了客户端飘移问题</p>
<p>多粒度的知识蒸馏机制  减少浅层的 类似于节点间连接  和 特征变换等类似问题  减少客户端和服务器间模型差异  <strong>从而减小了客户端与服务器模型的差异，促进了客户端-服务器上的广义消息通信</strong></p>
</blockquote>
<p>The contributions are summarized as follows: 总结贡献如下：</p>
<p>• We take the lead in introducing federated learning into skeleton-based action recognition and present a novel benchmark FSAR to address privacy concerns in this field for the first time. • 我们引领将联邦学习引入基于骨骼的动作识别，并首次提出了一个新的基准FSAR，以解决这一领域的隐私问题。</p>
<p>• We identify the heterogeneous graph topology structure as a major obstacle that causes training instability for skeleton data, and explore a novel Adaptive Topology Structure (ATS) to facilitate collaborative training between decentralized clients by learning domain-invariant and domain-specific topologies. • 我们确定<strong>异构图拓扑结构是导致骨骼数据训练不稳定性的主要障碍</strong>，并探索了一<strong>种新颖的自适应拓扑结构（ATS），通过学习领域不变和领域特定的拓扑结构来</strong>促进分散客户端之间的协作培训。</p>
<p>• The innovative Multi-grain Knowledge Distillation (MKD) mechanism is then explored by aligning shallow features to mitigate client-server divergence and further boost the accuracy. • 通过将浅层特征与客户端-服务器的差异来减少差异，从而进一步提高精度。↳</p>
<p>• Extensive experiments validate the effectiveness of FSAR and demonstrate that it achieves significant improvements over SOTA FL-based methods on several benchmark datasets with local data privacy protected. • 大量实验证实了FSAR的有效性，并表明它在多个具有本地数据隐私保护的基准数据集上取得了显著的改进。</p>
<h1 id="LAC-Latent-Action-Composition-for-Skeleton-based-Action-Segmentation"><a href="#LAC-Latent-Action-Composition-for-Skeleton-based-Action-Segmentation" class="headerlink" title="LAC - Latent Action Composition for Skeleton-based Action Segmentation"></a>LAC - Latent Action Composition for Skeleton-based Action Segmentation</h1><ul>
<li>Human-centric activity recognition is a crucial task in real-world video understanding. 人类中心的活动识别是实际视频理解中的重要任务。↳</li>
<li>In this context, skeleton data that can be represented by 2D or 3D human keypoints plays an important role, as it is complementary to other modalities such as RGB and optical flow. 在这一背景下，可以<strong>由2D或3D人体关键点表示的骨骼数据起着重要作用</strong>，因为它是与其他形式（如RGB和光流）互补的。</li>
<li>The study of recognizing activities directly from 2D/3D skeletons has gained increasing attention. 直接从2D/3D骨骼中识别活动的研究引起了越来越多的关注。</li>
<li>In untrimmed videos, activities are composable, and expressive skeleton features are required to model long-term dependency among different actions. 在未经修剪的视频中，<strong>活动是可组合的</strong>，需要有表现力的骨骼特征来建模不同活动之间的长期依赖性。</li>
<li>Current approaches obtain such features through visual encoders pre-trained on trimmed datasets, but their performance is not satisfactory for classifying complex actions. <strong>==当前的方法通过在经过修剪的数据集上预训练的视觉编码器获得这些特征，但对于分类复杂的动作，它们的性能不令人满意==</strong></li>
<li>To address this issue, the paper proposes to construct synthesized composable skeleton data for training a more effective visual encoder. 为了解决这个问题，本文提出<strong>构建合成可组合骨骼数据，以训练更有效的视觉编码器</strong>。</li>
<li>The proposed framework, Latent Action Composition (LAC), leverages synthesized composable motion data for self-supervised action representation learning. 提出的框架，潜在动作合成（LAC），利用合成的可组合动作数据进行自监督的动作表示学习。</li>
<li>LAC learns action representations in two steps: a first action composition step is followed by a contrastive learning step. LAC分两步学习动作表示：==首先是动作合成步骤，然后是对比学习步骤==。</li>
<li>Action composition involves training a generative module to generate new skeleton sequences by combining multiple videos using a Linear Action Decomposition (LAD) mechanism. 动作合成包括训练一个生成模块，使用线性动作分解（LAD）机制结合多个视频生成新的骨骼序列。</li>
<li>The contrastive learning step is designed to train a skeleton visual encoder in a self-supervised manner, maximizing the similarity of different skeleton sequences across datasets. 对比学习步骤旨在以自监督方式训练骨骼视觉编码器，最大化不同骨骼序列之间的相似性。</li>
<li>Experimental results show that LAC significantly improves the expressive power of the visual encoder for action segmentation. 实验结果表明，LAC显著提高了用于动作分割的视觉编码器的表现力。</li>
</ul>
<p>总结： 该章节介绍了文章的背景，指出了骨骼数据在人类中心活动识别中的重要性。文章提出了一种新的方法，即Latent Action Composition（LAC），用于自监督的动作表示学习。LAC包括两个步骤：动作合成和对比学习。动作合成利用线性动作分解（LAD）机制合成可组合的骨骼数据，对比学习用于训练骨骼视觉编码器。实验结果表明，LAC显著提高了骨骼数据的表现能力，特别是在动作分割任务中。</p>
<ol>
<li><p>Introduction 人类中心的活动识别在实际视频理解中是一项关键任务。</p>
<p>在这个背景下，可以由2D或3D人类关键点表示的骨骼数据发挥着重要作用，因为它与其他类型，如RGB和光流，具有互补性。</p>
<p>随着人类骨骼模态在相机视角和主体外观相关内容变化方面的稳健性大幅提升，直接从2D/3D骨架中识别活动的研究引起了越来越多的关注。</p>
<p>虽然先前的方法取得了显著的成功，但这些方法通常专注于包含单个动作的经修剪视频，这构成了一种高度简化的情境。与此不同，本文致力于基于未经修剪的视频中的骨骼序列进行动作分割，这是一种具有挑战性的设置。</p>
<p>在未经修剪的视频中，活动是可组合的，也就是说，一个人的动作通常包括多个动作（共现），每个动作持续几秒钟。</p>
<p>为了建模不同动作之间的长期依赖性，需要具有表现力的骨骼特征。</p>
<p>目前的方法通过视觉编码器（例如AGCNs）获得这些特征，这些编码器在经修剪的数据集上进行了预训练。然而，由于修剪样本中的<strong>动作信息有限</strong>，这些特征在分类复杂动作方面的性能远未令人满意。</p>
<p>为了解决这个问题，我们提出构建综合可组合的骨骼数据，以训练具有强大表现力的视觉编码器，用于动作分割。</p>
</li>
<li><p>总结 本文介绍了一种名为<strong>LAC的新框架</strong>，旨在利用<strong>综合可组合的运动数据进行自监督动作表示学习</strong>。</p>
<p>与当前的自监督方法不同，LAC分两步学习动作表示：首先进行动作合成，然后进行对比学习。</p>
<p>动作合成是一种新颖的初始化步骤，用于<strong>训练生成模块，该模块可以通过组合多个视频生成新的骨骼序列</strong>。</p>
<p>高级运动很难通过联合坐标（例如“喝水”和“坐下”）直接组合，因此LAC在自动编码器内部==引入了一种新颖的线性动作分解（LAD）机制==。</p>
<p>LAD旨在学习一个<strong>动作字典，以离散方式表达微妙的动作分布。这种动作字典在潜在编码空间中包含两组方向</strong>。</p>
<p>第一组称为“静态”，包括代表骨骼序列的静态信息的方向，例如视角和体型。</p>
<p>另一组称为“运动”，包括代表骨骼序列的时间信息的方向，例如主体执行的动作的基本动态。</p>
<p>新的骨骼序列通过线性组合学习的“静态”和“运动”方向来生成。我们<strong>采用运动重定向来训练自动编码器和字典</strong>，使用了从3D合成数据构建的<strong>“静态”和“运动”信息的骨骼序列</strong>。</p>
<p>一旦构建了动作字典，在接下来的对比学习步骤中，不需要“静态”/“运动”信息和动作标签，可以从多个输入骨骼序列中生成可组合的动作，通过组合它们的潜在“运动”集。</p>
<p><strong>对比学习步骤的目标是以自监督方式训练骨骼视觉编码器</strong>，如UNIK，无需动作标签。</p>
<p>它旨在<strong>使最终的视觉编码器能够最大程度地增加来自相同原始序列的数据增强获取的不同骨骼序列的相似性</strong>，</p>
<blockquote>
<p>对比学习 增大 两个来自相同的原始骨骼序列的相似性  但是两个序列经过不同的数据增强后变的不一样 但经过对比学习后会增强两个骨架序列的相似性</p>
</blockquote>
<p>这些数据增强涵盖了大规模数据集。与目前的方法不同，<strong>这里的对比学习还在帧空间中执行</strong>，<strong>以精细地增加正样本之间的每帧相似性。</strong></p>
<p>随后，<strong>在动作分割数据集上对经过此培训的帧级骨骼视觉编码器进行转移和重新训练。</strong></p>
<p>为了评估LAC的性能，我们在大规模数据集Posetics上训练骨骼视觉编码器，并通过对未经修剪的动作分割数据集（如TSU、Charades、PKU-MMD）进行微调来评估学到的骨骼表示的质量。实验分析证实，<strong>动作合成和对比学习可以显著增强视觉编码器的表现力。</strong></p>
<p>总之，本文的贡献包括：（i）我们引入了LAC，这是一种新颖的<strong>生成和对比框架</strong>，旨在综合复杂的运动并提高骨骼动作表示的能力。（ii）在生成步骤中，我们<strong>引入了一种新颖的线性动作分解（LAD）机制</strong>，通过<strong>正交基表示高级运动特征</strong>。因此，可以<strong>通过潜在空间操作线性组合多个骨骼序列</strong>。（iii）在对比学习步骤中，我们建议<strong>在视频和帧空间中学习骨骼表示</strong>，以<strong>提高面向帧的动作分割任务的泛化能力</strong>。（iv）我们进行实验分析，并显示在Posetics上进行预训练并将其传输到未经修剪的目标视频数据集是一种用于动作分割的通用且有效的方法。</p>
</li>
</ol>
<h1 id="Parallel-Attention-Interaction-Network-for-Few-Shot-Skeleton-based-Action-Recognition"><a href="#Parallel-Attention-Interaction-Network-for-Few-Shot-Skeleton-based-Action-Recognition" class="headerlink" title="Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition"></a>Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</h1><p>当然，我会逐句为您翻译这一段文字并总结 Introduction（引言）章节：</p>
<ol>
<li><p>骨骼动作识别[48, 2]在近年来引起了越来越多的关注，它是许多领域的主要话题，从人机互动到虚拟现实，由于其聚焦于动作本质和紧凑性而备受青睐[6]。</p>
<p>这一句说明了骨骼动作识别的重要性和它在多个领域的广泛应用，强调了它引起了人们的关注。</p>
</li>
<li><p>然而，<strong>如何识别新颖的动作</strong>仍然是一个未解决的问题。为了克服这个问题，越来越多的工作已经集中在<strong>少样本动作识别上</strong>，这可以<strong>减轻对稀有类别造成的性能下降问题</strong>[10, 27, 20, 43, 12]。特别是，这些方法探讨了<strong>无标签查询和标记支持集</strong>，以便<strong>学习具有区分性的特征表示</strong>，将<strong>查询动作与由少数支持样本表示的类别相匹配</strong>[35, 9]。</p>
<p>这段讨论了关于如何应对新颖动作识别问题以及少样本动作识别的方法，特别是强调了它们在解决性能下降问题上的作用。</p>
<blockquote>
<p>这些方法同时探索了无标签查询和有标签支持集，从而学习一种判别特征表示，将查询动作与由少数支持样本表示的类别相匹配</p>
</blockquote>
</li>
<li><p>如图1所示，现有方法侧重于<strong>如何利用骨骼内部或骨骼间的关系，而忽略了这两种范例之间的互补性</strong>。他们在挑战性场景中容易失败，例如<strong>相似的空间外观或不一致的时间依赖性</strong>。</p>
<p>这部分提到了现有方法的局限性，即它们倾向于忽略了骨骼内部和骨骼间关系的互补性，导致在特定场景下的失败。</p>
</li>
<li><p>尽管取得了显著进展，但我们仍然认为只考虑序列内部依赖性或序列间关联性是不足够的，这需要并行对齐上述样本。</p>
<p>这句指出了作者的观点，即现有方法还有改进的空间，特别是需要在并行方面进行样本的对齐。</p>
</li>
<li><p>受此启发，我们提出了一种新的少样本骨骼动作识别框架，称为Parallel Attention Interaction Network（PAINet）。我们认为，适应骨骼内部和骨骼间的本地关节特征是完善空间匹配的不可或缺的方式。</p>
<p>这句说明了作者提出的新方法，PAINet，以及他们认为的改进方法。</p>
</li>
<li><p>我们的贡献可以总结如下：</p>
<ul>
<li>我们提出了一种新颖的PAINet，用于少样本骨骼动作识别，在匹配过程中减轻了类似空间外观和不一致时间依赖性等挑战。</li>
<li>我们进一步设计了一个拓扑编码模块，以捕获关节和身体部位之间的协同运动，以及关节之间的内在语义关系。此外，我们提出了一个方向平均对称表面度量，用于发现最接近的时间关系。</li>
<li>我们在NTU-T、NTU-S和Kinetics等数据集上进行了广泛的实验，结果表明我们的模型明显优于现有的最先进方法。</li>
</ul>
<p>这段总结了作者的贡献，说明了他们的新方法PAINet的优势，以及他们的实验结果支持了他们的观点。</p>
</li>
</ol>
<p>总的来说，引言章节强调了骨骼动作识别的重要性和当前的挑战，然后引入了作者提出的新方法PAINet，以解决这些挑战。作者还指出了他们的贡献和实验结果。</p>
<p>正如图1所示，当前的方法主要关注如何利用骨架内部或骨架间的关系，同时忽略了这两种范例之间的互补性。它们容易在具有挑战性的情境中失败，例如相似的空间外观或不一致的时间依赖性。</p>
<p>一方面，一些研究尝试<strong>学习序列内的区分性特征</strong>。例如，==[3, 52, 26, 21, 31] 利用<strong>骨架内的局部关节特征来捕捉不同的模式</strong>==。特别是，[3] 提出了一个<strong>基于部分的空间区域聚合</strong>【Part-aware prototypical graph network for one-shot skeleton-based action recognition】，而[52]【Adaptive local-component-aware graph convolutional network for one-shot skeleton-based action recognition.】 采用了<strong>基于身体部位的局部嵌入的选择性求和以获得个体表示</strong>。</p>
<p>另一方面，一些研究探讨了跨序列调整局部特征以识别相似性和差异性。例如，[25] 利用交叉注意机制来激活它们的空间信息，而[40] 在查询和支持之间的时间和相机视角空间中实现了最佳对齐。尽管已经取得了显著的进展，我们仍然认为仅考虑序列内的依赖性或序列间的关联性是不足够的，这需要同时对齐上述样本。</p>
<p>如图1(a)所示，当分类具有相似外观的样本，如摘下耳机和摘下眼镜时，手部和肘部关节的空间位置中微差异有助于识别。序列间的交互可以通过优先考虑区分性关节来放大类别特定信息，因此只需要进行最佳的时间集匹配来进行识别。另外，如图1(b)中所示，当分类具有不一致运动模式的样本，如举手，肘部和躯干关节在两个图中具有不同的路径，但具有坚实的语义连接。在这种情况下，需要通过序列内的上下文聚合来增强实例特定信息。随着动作特征的丰富，序列之间的时间自适应交互可以进一步改善对齐。</p>
<p>受此启发，我们提出了一种新颖的少样本骨架动作识别框架，称为“Parallel Attention Interaction Network”（PAINet）。我们认为，适应骨架内部和骨架间的局部关节特征是完善空间匹配的不可或缺的方法。与以前的方法相比，我们的方法涉及了两个并行分支内的空间和时间域的对齐，从而实现了对骨架序列中信息丰富区域的互补注意。具体来说，我们提出了一个拓扑编码模块，利用拓扑和物理信息来增强两个分支中交互部分和关节对的建模。在交叉空间对齐分支中，我们使用了一个空间交叉注意模块来建立跨序列的关节关联。随后，我们引入了一个方向平均对称表面度量，考虑了所有可能的子序列对，并选择具有最大相似性的对。在交叉时间对齐分支中，我们提出了一个空间自注意模块，用于在序列内汇总空间上下文。随后，我们遵循基于视频的方法TRX [27]，通过时间交叉注意匹配器来汇总对齐的距离。我们的贡献总结如下：</p>
<p> • 我们提出了一种新颖的PAINet框架，用于少样本骨架动作识别，从而在匹配过程中解决了相似的空间外观和不一致的时间依赖性所带来的挑战。</p>
<p> • 我们进一步设计了一个拓扑编码模块，以捕捉关节和身体部分之间的共同运动，以及关节之间的内在语义关系。此外，我们提出了一种方向平均对称表面度量，用于发现最接近的时间关系。</p>
<p> • 对NTU-T、NTU-S和Kinetics等广泛的实验结果表明，我们的模型在性能上明显优于现有的方法。</p>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>文献阅读7</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.fomal.cc/posts/56c098e5.html">https://www.fomal.cc/posts/56c098e5.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Fomalhaut🥝</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-09-25</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2023-12-20</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a"></audio><script defer="defer" src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/9ed60f01.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">代码详解1</div></div></a></div><div class="next-post pull-right"><a href="/posts/ca7eff4.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献略读6</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:bold">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hierarchical-Contrast-for-Unsupervised-Skeleton-based-Action-Representation-Learning%E5%9F%BA%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E9%AA%A8%E6%9E%B6%E7%9A%84%E5%8A%A8%E4%BD%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B1%82%E6%AC%A1%E5%AF%B9%E6%AF%94"><span class="toc-text">Hierarchical Contrast for Unsupervised Skeleton-based Action  Representation Learning基于无监督骨架的动作表示学习的层次对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#int"><span class="toc-text">int</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction"><span class="toc-text">introduction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related"><span class="toc-text">related</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text">method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Clip-level-Representation"><span class="toc-text">Clip-level Representation.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Clip-Construction-%E5%89%AA%E8%BE%91%E6%9E%84%E5%BB%BA%E3%80%82"><span class="toc-text">Clip Construction. 剪辑构建。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Temporal-Dependency-Modeling-%E6%97%B6%E9%97%B4%E4%BE%9D%E8%B5%96%E6%80%A7%E5%BB%BA%E6%A8%A1%E3%80%82"><span class="toc-text">Temporal Dependency Modeling. 时间依赖性建模。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Part-level-Representation"><span class="toc-text">Part-level Representation.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-Contrast"><span class="toc-text">Hierarchical Contrast</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Instance-level-Contrast"><span class="toc-text">Instance-level Contrast</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-Modeling-on-Skeletons"><span class="toc-text">Hierarchical Modeling on Skeletons</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Multilevel-Spatial%E2%80%93Temporal-Excited-Graph-Network-for-Skeleton-Based-Action-Recognition"><span class="toc-text">Multilevel Spatial–Temporal Excited Graph Network for Skeleton-Based Action Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-related-work"><span class="toc-text">2.related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Skeleton-Based-Action-Recognition"><span class="toc-text">A  Skeleton-Based Action Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Attention-Mechanism-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">B. Attention Mechanism 注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Temporal-Difference-Modeling"><span class="toc-text">C. Temporal Difference Modeling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#III-MULTILEVEL-SPATIAL-TEMPORAL-EXCITED-GRAPH-NETWORK"><span class="toc-text">III. MULTILEVEL SPATIAL-TEMPORAL EXCITED GRAPH NETWORK</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Preliminaries"><span class="toc-text">A. Preliminaries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Multilevel-Graph-Convolution"><span class="toc-text">B. Multilevel Graph Convolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Spatial-Data-Driven-Excitation"><span class="toc-text">C. Spatial Data-Driven Excitation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-Temporal-Motion-Excitation"><span class="toc-text">D. Temporal Motion Excitation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#FSAR-Federated-Skeleton-based-Action-Recognition-with-Adaptive-Topology-Structure-and-Knowledge-Distillation"><span class="toc-text">FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction-1"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%90%E5%8F%A5%E7%BF%BB%E8%AF%91"><span class="toc-text">逐句翻译</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LAC-Latent-Action-Composition-for-Skeleton-based-Action-Segmentation"><span class="toc-text">LAC - Latent Action Composition for Skeleton-based Action Segmentation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Parallel-Attention-Interaction-Network-for-Few-Shot-Skeleton-based-Action-Recognition"><span class="toc-text">Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-color: transparent;"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</div><div class="btn-xz-box"><a class="btn-xz" target="_blank" rel="noopener" href="https://stellarium.org/">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/posts/eec9786.html">魔改指南</a><a href="/box/nav/">网址导航</a></li><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/personal/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/box/Gallery/">我的画廊</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/site/time/">建设进程</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a href="https://www.fomal.cc/" title="Fomalhaut🥝"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/60e5d4e39da7c077.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div></div></div></div><div class="copyright"><span><b>&copy;2022-2024</b></span><span><b>&nbsp;&nbsp;By Fomalhaut🥝</b></span></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v6.3.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Frame-Hexo-blue.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Theme-Butterfly-6513df.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" title="本站采用多线部署，主线路托管于Vercel"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Hosted-Vercel-brightgreen.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://user.51.la/" style="margin-inline:5px" title="本站数据分析得益于51la技术支持"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Analytics-51la-3db1eb.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/?keyword=20226665" style="margin-inline:5px" title="本站已加入萌ICP豪华套餐，萌ICP备20226665号"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/萌ICP备-20226665-fe1384.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://bitiful.dogecast.com/buckets" style="margin-inline:5px" title="本网站经Service Worker分流至缤纷云对象存储"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Bucket-缤纷云-9c62da.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://www.netdun.net/" style="margin-inline:5px" title="本站使用网盾星球提供CDN加速与防护"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/CDN-网盾星球-fff2cc.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Source-Github-d021d6.svg" alt=""/></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())
setTimeout(function(){preloader.endLoading();}, 5000);
document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: '',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: '',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.staticfile.org/twikoo/1.6.8/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script async src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script><script defer type="text/javascript" src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script defer src="https://cdn1.tianli0.top/gh/nextapps-de/winbox/dist/winbox.bundle.min.js"></script><script async src="//at.alicdn.com/t/c/font_3586335_hsivh70x0fm.js"></script><script async src="//at.alicdn.com/t/c/font_3636804_gr02jmjr3y9.js"></script><script async src="//at.alicdn.com/t/c/font_3612150_kfv55xn3u2g.js"></script><script async src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax","#bibi","body > title","#app","#tag-echarts","#posts-echart","#categories-echarts"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/c0bca80f.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-17</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/c0bca80f.html&quot;);" href="javascript:void(0);" alt="">boss_and_job</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/c0bca80f.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/cea7f2ca.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-09-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/cea7f2ca.html&quot;);" href="javascript:void(0);" alt="">社科人文认知突破</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/cea7f2ca.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/ec0c0ea5.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-08-24</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/ec0c0ea5.html&quot;);" href="javascript:void(0);" alt="">LeetCode2</a><div class="blog-slider__text">帅地训练营day2</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/ec0c0ea5.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-08-14</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt="">LeetCode1</a><div class="blog-slider__text">帅地训练营day1</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-10</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">HelloWorld</a><div class="blog-slider__text">这里是第一个马增龙写的文档，用来记录自己套用模板的全部过程</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 320px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/site/census/'|| '/site/census/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("/api?null",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'null')
    }
  </script><!-- hexo injector body_end end --></body></html>