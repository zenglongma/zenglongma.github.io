<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>masked | Fomalhaut🥝</title><meta name="keywords" content="Java,MySQL,算法,代码,博客,Butterfly,Hexo,Fomalhaut🥝,Fomalhaut"><meta name="author" content="Fomalhaut🥝"><meta name="copyright" content="Fomalhaut🥝"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="【代码待更新】Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence abstract  自我监督的预训练范式在基于机器人的动作识别领域得到了广泛的研究。特别是基于掩码预测的方法将预训练的性能推向了一个新的高度。然而，这些方">
<meta property="og:type" content="article">
<meta property="og:title" content="masked">
<meta property="og:url" content="https://www.fomal.cc/posts/cea7f2ca.html">
<meta property="og:site_name" content="Fomalhaut🥝">
<meta property="og:description" content="【代码待更新】Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence abstract  自我监督的预训练范式在基于机器人的动作识别领域得到了广泛的研究。特别是基于掩码预测的方法将预训练的性能推向了一个新的高度。然而，这些方">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://source.fomal.cc/img/default_cover_14.webp">
<meta property="article:published_time" content="2023-10-16T09:20:54.000Z">
<meta property="article:modified_time" content="2024-04-28T04:02:22.388Z">
<meta property="article:author" content="Fomalhaut🥝">
<meta property="article:tag" content="Java,MySQL,算法,代码,博客,Butterfly,Hexo,Fomalhaut🥝,Fomalhaut">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://source.fomal.cc/img/default_cover_14.webp"><link rel="shortcut icon" href="/"><link rel="canonical" href="https://www.fomal.cc/posts/cea7f2ca"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js',
      css: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'masked',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-28 12:02:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="blurNum"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Fomalhaut🥝" type="application/atom+xml">
</head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/02/avatar.webp" onerror="onerror=null;src='/assets/r1.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Fomalhaut🥝</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg><span> 搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-tupian1"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon">       </use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">masked</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于 </span><time class="post-meta-date-created" datetime="2023-10-16T09:20:54.000Z" title="发表于 2023-10-16 17:20:54">2023-10-16</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-28T04:02:22.388Z" title="更新于 2024-04-28 12:02:22">2024-04-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">3.6w</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>122分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="masked"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>【代码待更新】Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence</h1>
<h2 id="abstract">abstract</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240303111656152.png" alt="image-20240303111656152"></p>
<p>自我监督的预训练范式在基于机器人的动作识别领域得到了广泛的研究。特别是基于掩码预测的方法将预训练的性能推向了一个新的高度。然而，这些方法采取低级别的功能，如原始的关节坐标或时间运动，作为预测目标的掩蔽区域，这是次优的。在本文中，我们表明，使用高层次的上下文特征作为预测目标，可以实现上级性能。具体来说，我们提出了一个简单而有效的自监督3D动作表示学习框架，它利用基于transformerbased的教师编码器，以未掩蔽的训练样本作为输入，以创建潜在的上下文表示作为预测目标。受益于自注意机制，由教师编码器生成的潜在表示可以包含整个训练样本的全局上下文，从而导致更丰富的训练任务。此外，考虑到骨架序列中的高时间相关性，我们提出了一种运动感知的管掩蔽策略，该策略将骨架序列划分为几个管，并根据运动先验在每个管内执行持久掩蔽，从而迫使模型建立长距离时空连接并专注于动作语义丰富的区域。在NTU-60、NTU-120和PKU-MMD数据集上的大量实验表明，我们提出的方法优于以前的方法，并取得了最先进的结果。可以在https://github.com/ Ruizhuo-Xu/Xueton 2 vec上获得Xueton 2 vec的源代码。</p>
<h2 id="introduction">introduction</h2>
<p>引言</p>
<p>人类动作识别在现实世界中有着重要的应用，例如安全、人机交互和虚拟现实。深度传感器的发展以及姿态估计算法的进步[4, 12, 41]已将基于骨架的动作识别推向热门研究主题，这归功于它的计算效率、背景鲁棒性和隐私保护。一系列完全监督的基于骨架的人类动作识别方法已经被开发，这些方法使用了CNNs [10, 19]、RNNs [24, 46]和GCNs [5, 43]。尽管它们展示了有希望的性能，但这些方法依赖于大量手动标注的数据，这些数据昂贵、耗时且劳动强度大。这种情况激励我们去探索3D动作的自监督表示学习。</p>
<p>早期工作[21, 29, 33, 47]已经使用各种前置任务，如运动预测、拼图识别和掩码重建，来学习3D动作表示。最近，对比学习方法[15, 22, 28, 30]变得突出。然而，这些方法通常需要精心设计的数据增强，并倾向于鼓励编码器学习更全局的表示，从而忽视了局部时空信息。随着变压器模型[37]的兴起，基于掩码预测任务的自监督预训练方法在视觉表示学习[15, 22, 28, 30]中已成为主流。如SkeletonMAE [39, 42]和MAMP [27]等作品已尝试将MAE [17]方法转移到3D动作表示学习领域，取得了有希望的结果。然而，这些类MAE方法通过关注原始关节坐标或时间运动等低级高频细节作为学习目标，效率低下地利用了模型容量，这对于建模高级时空结构是次优的。我们相信，使用更高级别的预测目标将指导模型学习更好的表示，并提高预训练性能。</p>
<p>受此思想的启发，我们提出了Skeleton2vec，一个简单而高效的3D动作表示学习的自监督框架。Skeleton2vec解决了现有类MAE方法的局限性，如图1所示，它利用了上下文化的预测目标。我们遵循data2vec[1, 2]的工作，使用一个教师编码器来处理未掩码的训练样本，以生成作为目标的潜在上下文化表示。然后我们使用一个学生编码器，以掩码版本的样本为输入，并结合非对称解码器来预测掩码位置的数据表示。整个模型基于标准的变压器架构。自注意机制确保构建的目标是上下文化的，包含来自整个样本的信息，使它们比孤立目标（例如原始关节坐标）或基于局部上下文的目标（例如时间运动）更丰富。</p>
<p>此外，考虑到3D骨架序列中强大的时空相关性，我们提出了一个感知运动的管状掩码策略。最初，我们将输入的骨架序列沿时间轴分成多个管状部分，在每个管状部分内的帧共享一个掩码图，以避免相邻帧之间的信息泄露。这迫使模型从遥远的时间步骤中提取信息以进行更好的预测。然后我们根据身体关节在每个管状部分内的空间运动强度来指导掩码关节的采样。运动强度较高的关节将以更高的概率被掩码，允许模型更多地关注具有丰富动作语义的时空区域。与随机掩码相比，我们的方法更好地利用了3D骨架序列的时空特性和运动先验，有效地提高了预训练性能。</p>
<p>总结一下，本工作的主要贡献有三个方面： • 我们提出了Skeleton2vec框架，它使用来自教师编码器的上下文化表示作为预测目标，使学习到的表示具有更强的语义关联。 • 我们引入了一个感知运动的管状掩码策略，该策略根据空间运动强度对管状部分内的关节进行持久掩码，迫使模型构建更好的长距离时空连接并关注更富有语义的区域。 • 我们在三个大规模的基于3D骨架的动作识别数据集上验证了我们方法的有效性，并取得了最先进的结果。</p>
<h3 id="总结">总结</h3>
<ol>
<li>人类动作识别具有安全、人机交互和虚拟现实等多种实际应用价值。深度传感器和姿态估计算法的进步使基于骨架的动作识别成为一个热门研究方向，因为它高效、鲁棒且能保护隐私。</li>
<li>尽管现有的完全监督的基于骨架的动作识别方法取得了一定的成果，但这些方法依赖大量手动标注数据，这一过程成本高且耗时。</li>
<li>因此，研究者开始探索自监督学习方法来学习3D动作的表示，早期工作尝试了多种前置任务，而最近的研究开始利用对比学习。</li>
<li>然而，对比学习方法常常需要复杂的数据增强，并且可能忽视局部时空信息。受到变压器模型和自监督预训练方法的影响，研究者开始采用掩码预测任务。</li>
<li>Skeleton2vec是一个自监督框架，用于学习3D动作表示，它克服了现有方法的局限并使用上下文化的预测目标，通过教师编码器生成丰富的表示作为预测目标。</li>
<li>为了更好地利用骨架序列的时空特性，提出了一种新的运动感知管状掩码策略，通过掩码强度较高的关节来引导模型关注动作信息丰富的区域。</li>
<li>这项工作的主要贡献包括提出新的框架、引入新的掩码策略，并且在三个大规模3D骨架动作识别数据集上验证了方法的有效性，取得了领先的结果。</li>
</ol>
<h1>【未开源】SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition用于自监督骨架动作识别的时空掩蔽自动编码器</h1>
<blockquote>
<p>[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr<br>
Doll´ar, and Ross Girshick. Masked autoencoders are scalable<br>
vision learners. In Proceedings ofthe IEEE/CVF Conference<br>
on Computer Vision and Pattern Recognition, pages 16000–<br>
16009, 2022.</p>
</blockquote>
<p>随着深度学习技术的蓬勃发展，基于<strong>骨骼的全监督动作识别技术</strong>取得了长足的进步。然而，这些方法需要足够的标注数据，而这些数据不易获得。相比之下，基于<strong>骨骼的自我监督动作识别则吸引了更多的关注</strong>。利用未标记的数据，可以学习更多的可泛化特征来缓解过拟合问题，减少对大量标记训练数据的需求。在MAE[15]的启发下，我们提出了一个用于<strong>基于骨骼的自监督3D动作识别的时空掩藏自动编码器框架(骨架MAE)</strong>。在==<strong>MAE的掩蔽和重建管道的基础上，我们利用一个基于骨架的编码器-解码器transformer结构来重建掩蔽骨架序列</strong>==。针对骨架序列，分别从<strong>关节级和帧级两方面提出了一种新的掩蔽策略——时空掩蔽策略</strong>。这种预训练策略使得<strong>编码器输出具有空间和时间依赖性的可泛化骨架特征</strong>。<strong>给定未隐藏的骨架序列，编码器被微调为动作识别任务</strong>。大量实验表明，在NTU RGB+D和NTU RGB+D 120数据集上，我们的骨架mae都取得了显著的性能，并且优于目前最先进的方法。</p>
<h2 id="introduction-2">introduction</h2>
<p>人体动作识别是计算机视觉领域的一个基本研究课题，旨在理解人类行为并区分不同的动作[38]。随着深度学习和人体姿势估计方法的蓬勃发展[2, 39, 37]，<strong>人类骨骼数据可以被高效提取为一种高级但轻量级的表示</strong>，因此引起了对人类行为和动作分析的广泛关注。因此，基于3D骨骼的动作识别已经成为人体动作识别中的一个重要研究领域。</p>
<p>大多数最近的方法侧重于使用全监督学习算法来构建它们的框架：基于卷积神经网络（CNN）的方法[11, 48]，基于循环神经网络（RNN）的方法[43, 34]，基于图卷积网络（GCN）的方法[46, 33, 49, 6]以及基于Transformer的方法[29, 30]广泛应用于骨骼动作识别，并取得了非常好的结果。然而，<strong>完全监督的动作识别容易出现过拟合。<strong>此外，它</strong>需要大量的标记训练数据</strong>，这既昂贵又耗时。为了缓解这些问题，在骨骼动作识别中，自监督学习方法越来越盛行，这些方法利用未标记的数据来学习数据表示。<strong>一些自监督方法考虑了骨骼表示学习的预设任务，使用未标记的骨骼数据，例如动作重建[7]和拼图[22]</strong>。然而，这些<strong>基于预设任务的方法侧重于局部特征，如同一帧内的关节相关性和骨骼比例，并未充分探索时间信息</strong>。最近的一些工作[20, 14]<strong>通过构建不同视图下的骨骼序列，通过数据增强和正负对，训练了基于对比学习框架的对比模型</strong>。尽管这些基于对比学习的方法==强调了高级上下文信息，但它们严重依赖于用于提取骨骼特征的关节对的数量，并忽略了不同帧之间的关节相关信息==。</p>
<p>自监督任务的发展路径</p>
<blockquote>
<p>基于重建不同视图下的骨架数据，使用数据增强和正负对</p>
<p>Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition.</p>
<p>3d human action representation learning via cross-view consistency pursuit</p>
<p>基于未标记数据使用拼图和动作重建</p>
</blockquote>
<p>最近，一种名为&quot;Masked Autoencoders (MAE)&quot;的新的自监督学习方法[15]展示出在计算机视觉任务中具有强大的泛化能力，表现出色。<strong>MAE会对输入图像的大部分区域进行遮盖，然后强制模型仅使用未遮盖的部分来重建原始图像。</strong></p>
<p>然而，由于以下原因，MAE[15]不能直接用于自监督骨骼动作识别：</p>
<ul>
<li>MAE[15]使用了视觉变换器（ViT）[10]结构来处理图像输入。<strong>与不包含时间信息的图像不同，人体骨架序列是从包含丰富语义信息的高信息密度视频中提取出来的：在空间层面，关节特征包含了同一帧内不同关节之间的关系；在时间层面，帧特征代表了同一关节在不同帧之间的运动。</strong></li>
<li>MAE的遮盖策略仅关注空间域。在处理人体骨架序列数据时，需要一种<strong>空间-时间遮盖策略</strong>。</li>
</ul>
<p>为了解决这些问题，我们引入了一种名为&quot;SkeletonMAE&quot;的新型骨架自监督学习方法，旨在进行骨架的<strong>空间-时间表示学习</strong>：1) 遮盖输入序列是从原始骨架序列中生成的，其中包含了关节坐标（空间）信息和帧（时间）信息；2) 借助空间-时间遮盖策略和编码-解码规则，SkeletonMAE通过基于transformer的编码器和解码器处理遮盖序列，从而获得重建序列，其中空间和时间信息得到了很好的处理（transformer在长序列数据的空间-时间表示学习中具有巨大潜力）。</p>
<p>SkeletonMAE的框架如图1所示。具体来说，整个SkeletonMAE流程是根据以下原则设计的。在预训练阶段，使用了空间-时间遮盖策略（具有预设的帧遮盖和关节遮盖比率）来遮盖输入骨架序列的部分部分，包括帧级别和关节级别（Sec. 3.2）。为了找到最佳的空间-时间表示学习的权衡点，我们讨论了关<strong>节遮盖和帧遮盖比率的作用</strong>，找到了最佳的比率组合。<strong>编码器用于学习可泛化的特征表示</strong>，而<strong>解码器则用于重建丢失的骨架</strong>。由于我们处理的是骨架序列，我们<strong>使用了专为处理骨架序列而开发的&quot;空间-时间元组变换器（STTFormer）</strong>&quot;[30]作为我们的网络骨干，而不是ViT [10]。在微调阶段，我们仅使用编码器和简单的输出层来预测动作。动作识别结果表明，我们的方法在没有额外数据的情况下胜过了最先进的自监督学习方法。</p>
<p><strong>总结：</strong> 本段介绍了一种名为&quot;SkeletonMAE&quot;的新型自监督学习方法，专门用于骨骼动作识别。该方法解决了现有方法的局限性，同时在标准数据集上取得了自监督设置下的最先进性能。</p>
<p><strong>Summary</strong></p>
<p>这个章节介绍了人体动作识别的背景和问题。随着深度学习和人体姿势估计技术的发展，骨骼数据成为了一种重要的高级表示方式，用于动作识别。目前的方法主要集中在全监督学习算法上，但这些方法容易过拟合，而且需要大量标记数据。为了克服这些问题，自监督学习方法变得越来越受欢迎，但现有方法主要关注局部特征，而<strong>忽略了时间信息和不同帧之间的关节相关性</strong>。为了解决这些问题，引入了一种新的方法，<strong>该方法使用自监督学习和对比学习框架来改进骨骼动作识别</strong>，特别是在处理类似的空间外观和不一致的时间依赖性时，具有更好的性能。该方法引入了拓扑编码和方向平均对称表面度量等新颖概念，以提高模型的性能。实验结果表明，这一方法在现有方法的基础上取得了显著的改进。</p>
<h2 id="Related-work">Related work</h2>
<h3 id="2-1-Supervised-skeleton-based-action-recognition">2.1. Supervised skeleton-based action recognition</h3>
<p>以下是对输入段落的逐句翻译：</p>
<p>在深度学习出现之前，骨骼动作识别工作中使用手工制作的技术来提取空间-时间特征[44, 40, 41]。近年来，由于其强大的特征提取和表示学习能力，深度学习已广泛用于骨骼动作识别领域。而其中大多数是完全监督的。基于循环神经网络（如LSTMs）[12, 50, 24]的方法被广泛用于处理骨架数据。与此同时，基于卷积神经网络（CNNs）的方法[35, 18, 45]也被引入到骨骼动作识别中。然而，由RNN或CNN提取的数据表示过于简单，无法全面呈现骨骼数据的空间-时间特征。因此，自然而然地引入了基于图卷积网络（GCN）的方法[46, 33, 49, 6]，以模拟来自骨架数据的拓扑图特征。最近，随着视觉变换器（ViT）的成功[10]，基于变换器的模型由于其学习全局表示的能力而成为顺序骨架数据分析的强大架构[29, 30, 52, 47, 5, 51, 27]。因此，我们在研究中采用了基于骨骼的变换器（STTFormer [30]）作为骨架序列处理的骨干网络。</p>
<p><strong>总结：</strong> 这一段描述了在深度学习之前，骨骼动作识别领域使用手工制作的技术来提取空间-时间特征。近年来，深度学习在这个领域得到广泛应用，但大多数方法都是完全监督的。然后介绍了基于循环神经网络（RNN）和卷积神经网络（CNN）的方法，但它们提取的数据表示太简单，不能充分表现骨骼数据的复杂空间-时间特征。因此，引入了基于图卷积网络（GCN）的方法，以更好地建模骨架数据中的拓扑图特征。最近，基于变换器的模型，如视觉变换器（ViT），由于其全局表示学习的能力，成为顺序骨架数据分析的强大架构，因此在研究中采用了基于骨骼的变换器（STTFormer）作为骨干网络。</p>
<h3 id="2-2-Self-supervised-skeleton-based-action-recognition">2.2. Self-supervised skeleton-based action recognition</h3>
<p>自监督学习旨在在不使用标记数据的情况下提取特征表示，并在基于图像和视频的表示学习中取得了令人满意的性能[36, 26, 9, 13]。更多自监督表示学习方法采用所谓的对比学习方式[16, 4, 21, 3]来提高性能。受到对比学习结构的启发，最近的骨骼表示学习工作在自监督骨骼动作识别中取得了一些令人鼓舞的进展。MS2L [22]引入了一种多任务自监督学习框架，通过运动预测和拼图识别来提取关节表示。CrosSCLR [20]开发了一种基于对比学习的框架，用于从骨架数据中学习单视图和跨视图表示。在CrosSCLR之后，AimCLR [14]采用了极端的数据增强策略，增加了额外的硬对比对，旨在从骨架数据中学习更通用的表示。</p>
<p><strong>总结：</strong> 这一段介绍了自监督学习的概念，即在没有标记数据的情况下提取特征表示，并指出它在图像和视频表示学习中取得了良好的性能。此外，提到了采用对比学习方式的自监督表示学习方法，这种方法在骨骼表示学习领域也取得了一些进展。例如，MS2L引入了一个多任务的自监督学习框架，用于从骨架数据中提取关节表示。CrosSCLR开发了一个基于对比学习的框架，用于学习单视图和跨视图的骨架表示。AimCLR则采用了极端的数据增强策略，以学习更通用的骨架数据表示。</p>
<p>==Lilang Lin, Sijie Song,Wenhan Yang, and Jiaying Liu. Ms2l:<br>
Multi-task self-supervised learning for skeleton based action<br>
recognition. In Proceedings of the 28th ACM International<br>
Conference on Multimedia, pages 2490–2498, 2020.==</p>
<h3 id="2-3-Masked-Autoencoder">2.3&quot;Masked Autoencoder&quot;</h3>
<p>“Masked Autoencoder” 是一种自编码器（Autoencoder）的变种，用于学习数据的压缩表示，特别是在自然语言处理领域中广泛使用。这种自编码器之所以称为 “masked”，<strong>是因为它的训练过程通常涉及到将输入数据中的某些部分（通常是随机选择的一些元素）掩盖或遮挡，然后尝试通过模型来预测这些被掩盖的部分</strong>。</p>
<p>下面是一些关于 Masked Autoencoder 的关键概念和工作原理：</p>
<ol>
<li><strong>自编码器（Autoencoder）</strong>：自编码器是一种神经网络结构，它有一个编码器和一个解码器。编码器将输入数据映射到低维表示（潜在空间），然后解码器将潜在表示映射回原始数据的尺寸。自编码器的目标是学习一种紧凑的数据表示，以最小化输入和重构之间的差异。</li>
<li><strong>Masked Autoencoder的特点</strong>：在 Masked Autoencoder 中，输入数据的一部分通常会被随机掩盖或遮挡，这部分被称为 “mask”。模型的目标是从被掩盖的数据中预测出原始数据，同时仍要学习到有意义的表示。这使得模型更有能力学习到数据的局部特征，而不仅仅是整体表示。</li>
<li><strong>应用领域</strong>：Masked Autoencoder 在自然语言处理中非常有用。例如，对于语言建模任务，可以使用 Masked Autoencoder 预测一句话中的某些单词，而其他单词被掩盖。这有助于模型学习词语之间的关系和句子的结构。</li>
<li><strong>变种</strong>：基于 Masked Autoencoder 的模型有多种变种，其中最著名的是 BERT（Bidirectional Encoder Representations from Transformers）。BERT 使用双向掩码语言建模作为预训练任务，它的预训练表示在多种自然语言处理任务中都表现出色。</li>
</ol>
<p>总之，Masked Autoencoder 是一种用于学习数据表示的神经网络模型，通过随机掩盖部分输入数据并预测它们，模型能够学习到有意义的特征表示。它在自然语言处理中有广泛应用，但也可用于其他领域的序列数据建模和特征学习。</p>
<h2 id="3-Methodology">3. Methodology</h2>
<p>在这一节中，我们首先在<strong>3.1节中介绍了骨架mae的初步内容</strong>。然后，在第<strong>3.2节中，我们设计了一种骨架数据的时空掩蔽策略</strong>。接下来，我们在第3.3节中分析我们的骨骼mae用于动作识别。最后，我们在第3.4节中介绍了我们的微调过程。</p>
<h3 id="3-1-Preliminaries">3.1. Preliminaries</h3>
<p>这里介绍了MAE（Masked Autoencoder）模型。MAE采用了一种不对称的编码器和解码器结构。需要注意的是，解码器的结构与编码器不同，<strong>这意味着可以根据需要自定义解码器以构建高效的预训练模型</strong>。具体来说，MAE中的编码器基于ViT（Vision Transformer），但仅处理未掩盖的图像：首先，==图像块经线性投影编码，并加上位置嵌入以形成图像令牌，然后这些令牌经过多个Transformer块的处理。仅加载了一小部分未掩盖的令牌（75％的块被掩盖，其余的作为输入）。至于MAE解码器，它根据原始图像块上的位置信息来解码已掩盖的令牌以进行重构。然后，在像素空间中计算了已掩盖和重构令牌之间的均方误差（MSE==。在预训练后，应用具有简单分类头的预训练编码器进行图像分类任务。</p>
<p>总结：该部分介绍了MAE模型的结构和工作方式。MAE包括一个编码器和一个解码器，它们以不对称的方式构建。编码器基于ViT进行图像编码，但仅处理未掩盖的图像部分，而解码器负责重构已掩盖的图像部分。在预训练后，预训练编码器可用于图像分类任务。这种结构的不对称性为构建高效的预训练模型提供了灵活性。</p>
<p>STTFormer [30]。与MAE不同，<strong>MAE在编码器中应用了ViT，而在解码器中应用了Transformer块进行图像重构</strong>，而我们<strong>选择使用STTFormer来构建编码器和解码器</strong>，因为它具有基于骨架的Transformer结构。<strong>与ViT基于没有时间信息的图像块不同，STTFormer是一种基于骨架数据的Transformer，对于处理时空数据具有很大的潜力。<strong>具体而言，</strong>== STTFormer将骨架数据分成多个元组（不重叠的部分），并提供了名为Spatio-Temporal Tuples Attention（STTA）的自注意模块，用于提取相邻帧之间的多关节表示==</strong>。然后，在STTA块之后提出了一种名为Inter-Frame Feature Aggregation（IFFA）的特征聚合模块，用于<strong>提高相似动作识别的学习能力</strong>。STTFormer的结构如图1所示。</p>
<p>总结：该部分介绍了STTFormer模型，该模型与MAE不同，采用了骨架数据驱动的Transformer结构，适用于处理骨架数据的时空信息。STTFormer的结构包括Spatio-Temporal Tuples Attention（STTA）自注意模块和Inter-Frame Feature Aggregation（IFFA）特征聚合模块，有助于提取和集成骨架数据的多关节表示，从而提高相似动作的识别能力。这一部分强调了STTFormer相对于ViT的优势，特别适用于骨架数据的处理。</p>
<h3 id="3-2-Spatial-temporal-masking-strategy">3.2. Spatial-temporal masking strategy</h3>
<p>我们提出了一种时空遮蔽方法，<strong>用于对骨架序列输入的一部分进行遮蔽</strong>，我们的遮蔽策略流程如图2所示。</p>
<p>时空遮蔽方法。图2显示了我们的帧级遮蔽方法。根据预设的帧遮蔽比率，<strong>随机去除一部分帧并存储它们的索引，然后剩下的帧将通过关节级别的空间遮蔽方法进行处理</strong>。</p>
<p>空间遮蔽方法。如图2所示，在所有输入帧上实施了时空遮蔽方法后，<strong>剩下的帧将通过空间遮蔽策略进行处理</strong>。根据预设的关节遮蔽比率，我们<strong>会随机地在每个未遮蔽的帧中遮蔽部分关节</strong>。值得注意的是，在这个随机的空间遮蔽方法中，<strong>被遮蔽的关节的索引不是固定的</strong>，这意味着<strong>不同帧中的相同关节可能被遮蔽或不被遮蔽</strong>。这种简单的方法如图3(b)所示。除了这种遮蔽方法，我们<strong>还引入了一个具有固定索引的关节遮蔽策略，如图3©所示</strong>。<strong>不同帧中具有相同索引的关节将根据关节遮蔽比率一起被遮蔽或不被遮蔽</strong>。我们在第4.3节中进行了实验，比较了这两种遮蔽策略。</p>
<p>总结：这一部分描述了作者提出的骨架序列的时空遮蔽方法。这种方法分为时序遮蔽和空间遮蔽两个阶段。时序遮蔽是在帧级别进行的，随机去除一部分帧。然后，在剩余的帧中，进行空间遮蔽，其中部分关节也会被随机遮蔽。作者还介绍了两种空间遮蔽策略，一种是随机遮蔽关节的策略，另一种是基于固定索引的关节遮蔽策略。这两种策略在后续实验中进行了比较。</p>
<h3 id="3-3-SkeletonMAE-architecture">3.3. SkeletonMAE architecture</h3>
<p>我们描述了SkeletonMAE的主要组件，例如编码器、解码器、重构序列、损失函数和骨架动作识别的微调流程。流程和SkeletonMAE的结构如图1所示。</p>
<p>SkeletonMAE编码器。我们的编码器基于STTFormer，<strong>只处理可见的骨架标记</strong>。给定一个骨架序列作为输入，我们分别应用了<strong>帧遮蔽和关节遮蔽方法</strong>。这些在空间和时间上未遮蔽的标记被馈送到<strong>SkeletonMAE编码器，将输入映射到时空嵌入特征</strong>。</p>
<p>SkeletonMAE解码器。我们的<strong>解码器也采用了STTFormer的结构</strong>。与MAE中的解码器相同，SkeletonMAE解码器中的<strong>时空嵌入特征被处理以重构原始序列</strong>。同时，<strong>为了保留用于重构的位置信息，还引入了位置嵌入。解码器的输出是重构序列，应与未遮蔽的原始序列相同。</strong></p>
<p>重构。我们使用均方误差（MSE）损失来衡量重构的结果。在这种情况下，我们<strong>计算原始骨架序列与重构序列之间的MSE损失</strong>，如下所示：</p>
<p>MSE = 1 N ΣN i=1 |Si − S∗ i |2， 其中i是帧的索引，N是样本数量，S是输入序列，S∗是重构序列。</p>
<p>总结：这一部分描述了SkeletonMAE的架构，包括编码器、解码器和重构方法。编码器和解码器都基于STTFormer结构，用于处理骨架序列的重构。重构的质量通过均方误差损失来衡量，以衡量原始骨架序列与重构序列之间的差异。</p>
<h3 id="3-4-Fine-tuning-for-skeleton-action-recognition">3.4. Fine-tuning for skeleton action recognition</h3>
<p>为了评估SkeletonMAE学习骨架表示的能力，我们<strong>加载了从预训练中获得的参数权重</strong>，**用所有的训练数据对模型进行微调，然后使用识别准确度来预测每个动作的标签。**微调的过程如图1(b)所示。与最新的基于对比的自监督骨架动作识别方法[20, 14]不同，<strong>这些方法通过线性评估协议验证模型，我们只关注骨架动作识别任务的端到端微调</strong>。</p>
<p>总结：这一部分描述了如何评估SkeletonMAE对骨架表示的学习能力。首先，他们使用从预训练中获得的参数权重对模型进行微调，然后使用准确度来评估每个动作的标签预测。与其他方法不同，他们侧重于端到端微调而不是线性评估协议。这是评估SkeletonMAE性能的关键步骤。</p>
<h2 id="详细介绍">详细介绍</h2>
<p>SkeletonMAE的框架如图1所示。具体来说，整个SkeletonMAE流程是根据以下原则设计的。在预训练阶段，使用了空间-时间遮盖策略（具有预设的帧遮盖和关节遮盖比率）来遮盖输入骨架序列的部分部分，包括帧级别和关节级别（Sec. 3.2）。为了找到最佳的空间-时间表示学习的权衡点，我们讨论了关<strong>节遮盖和帧遮盖比率的作用</strong>，找到了最佳的比率组合。<strong>编码器用于学习可泛化的特征表示</strong>，而<strong>解码器则用于重建丢失的骨架</strong>。由于我们处理的是骨架序列，我们<strong>使用了专为处理骨架序列而开发的&quot;空间-时间元组变换器（STTFormer）</strong>&quot;[30]作为我们的网络骨干，而不是ViT [10]。在微调阶段，我们仅使用编码器和简单的输出层来预测动作。动作识别结果表明，我们的方法在没有额外数据的情况下胜过了最先进的自监督学习方法。</p>
<p><strong>总结：</strong> 本段介绍了一种名为&quot;SkeletonMAE&quot;的新型自监督学习方法，专门用于骨骼动作识别。该方法解决了现有方法的局限性，同时在标准数据集上取得了自监督设置下的最先进性能。</p>
<p><strong>Summary</strong></p>
<p>这个章节介绍了人体动作识别的背景和问题。随着深度学习和人体姿势估计技术的发展，骨骼数据成为了一种重要的高级表示方式，用于动作识别。目前的方法主要集中在全监督学习算法上，但这些方法容易过拟合，而且需要大量标记数据。为了克服这些问题，自监督学习方法变得越来越受欢迎，但现有方法主要关注局部特征，而<strong>忽略了时间信息和不同帧之间的关节相关性</strong>。为了解决这些问题，引入了一种新的方法，<strong>该方法使用自监督学习和对比学习框架来改进骨骼动作识别</strong>，特别是在处理类似的空间外观和不一致的时间依赖性时，具有更好的性能。该方法引入了拓扑编码和方向平均对称表面度量等新颖概念，以提高模型的性能。实验结果表明，这一方法在现有方法的基础上取得了显著的改进。</p>
<h2 id="方法详解">方法详解</h2>
<p>在这一节中，我们首先在<strong>第1节中介绍了骨架mae的初步内容</strong>。然后，在第****第2*<em>节中，我们设计了一种骨架数据的时空掩蔽策略</em>*。接下来，我们在第****第3*<em>节<strong>中分析我们的骨骼mae用于动作识别。最后，我们在第</strong></em>*第4**节**节中介绍了我们的微调过程。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/640" alt="Image"></p>
<h3 id="参数">参数</h3>
<p>这里介绍了MAE（Masked Autoencoder）模型。MAE采用了一种不对称的编码器和解码器结构。需要注意的是，解码器的结构与编码器不同，<strong>这意味着可以根据需要自定义解码器以构建高效的预训练模型</strong>。具体来说，MAE中的编码器基于ViT（Vision Transformer），但仅处理未掩盖的图像：首先，图像块经线性投影编码，并加上位置嵌入以形成图像令牌，然后这些令牌经过多个Transformer块的处理。仅加载了一小部分未掩盖的令牌（75％的块被掩盖，其余的作为输入）。至于MAE解码器，它根据原始图像块上的位置信息来解码已掩盖的令牌以进行重构。然后，在像素空间中计算了已掩盖和重构令牌之间的均方误差（MSE。在预训练后，应用具有简单分类头的预训练编码器进行图像分类任务。</p>
<p>总结：该部分介绍了MAE模型的结构和工作方式。MAE包括一个编码器和一个解码器，它们以不对称的方式构建。编码器基于ViT进行图像编码，但仅处理未掩盖的图像部分，而解码器负责重构已掩盖的图像部分。在预训练后，预训练编码器可用于图像分类任务。这种结构的不对称性为构建高效的预训练模型提供了灵活性。</p>
<p>STTFormer [30]。与MAE不同，<strong>MAE在编码器中应用了ViT，而在解码器中应用了Transformer块进行图像重构</strong>，而我们<strong>选择使用STTFormer来构建编码器和解码器</strong>，因为它具有基于骨架的Transformer结构。<strong>与ViT基于没有时间信息的图像块不同，STTFormer是一种基于骨架数据的Transformer，对于处理时空数据具有很大的潜力。<strong>具体而言，<strong>STTFormer将骨架数据分成多个元组（不重叠的部分），并提供了名为Spatio-Temporal Tuples Attention（STTA）的自注意模块，用于提取相邻帧之间的多关节表示</strong>。然后，在STTA块之后提出了一种名为Inter-Frame Feature Aggregation（IFFA）的特征聚合模块，用于</strong>提高相似动作识别的学习能力</strong>。STTFormer的结构如图1所示。</p>
<p>总结：该部分介绍了STTFormer模型，该模型与MAE不同，采用了骨架数据驱动的Transformer结构，适用于处理骨架数据的时空信息。STTFormer的结构包括Spatio-Temporal Tuples Attention（STTA）自注意模块和Inter-Frame Feature Aggregation（IFFA）特征聚合模块，有助于提取和集成骨架数据的多关节表示，从而提高相似动作的识别能力。这一部分强调了STTFormer相对于ViT的优势，特别适用于骨架数据的处理。</p>
<h3 id="时空遮蔽策略">时空遮蔽策略</h3>
<p>我们提出了一种时空遮蔽方法，<strong>用于对骨架序列输入的一部分进行遮蔽</strong>，我们的遮蔽策略流程如图2所示。</p>
<p>时空遮蔽方法。图2显示了我们的帧级遮蔽方法。根据预设的帧遮蔽比率，<strong>随机去除一部分帧并存储它们的索引，然后剩下的帧将通过关节级别的空间遮蔽方法进行处理</strong>。</p>
<p>空间遮蔽方法。如图2所示，在所有输入帧上实施了时空遮蔽方法后，<strong>剩下的帧将通过空间遮蔽策略进行处理</strong>。根据预设的关节遮蔽比率，我们<strong>会随机地在每个未遮蔽的帧中遮蔽部分关节</strong>。值得注意的是，在这个随机的空间遮蔽方法中，<strong>被遮蔽的关节的索引不是固定的</strong>，这意味着<strong>不同帧中的相同关节可能被遮蔽或不被遮蔽</strong>。这种简单的方法如图3(b)所示。除了这种遮蔽方法，我们<strong>还引入了一个具有固定索引的关节遮蔽策略，如图3©所示</strong>。<strong>不同帧中具有相同索引的关节将根据关节遮蔽比率一起被遮蔽或不被遮蔽</strong>。我们在第4.3节中进行了实验，比较了这两种遮蔽策略。</p>
<p>总结：这一部分描述了作者提出的骨架序列的时空遮蔽方法。这种方法分为时序遮蔽和空间遮蔽两个阶段。时序遮蔽是在帧级别进行的，随机去除一部分帧。然后，在剩余的帧中，进行空间遮蔽，其中部分关节也会被随机遮蔽。作者还介绍了两种空间遮蔽策略，一种是随机遮蔽关节的策略，另一种是基于固定索引的关节遮蔽策略。这两种策略在后续实验中进行了比较。</p>
<h3 id="SkeletonMAE框架结构">SkeletonMAE框架结构</h3>
<p>我们描述了SkeletonMAE的主要组件，例如编码器、解码器、重构序列、损失函数和骨架动作识别的微调流程。流程和SkeletonMAE的结构如图1所示。</p>
<p>SkeletonMAE编码器。我们的编码器基于STTFormer，<strong>只处理可见的骨架标记</strong>。给定一个骨架序列作为输入，我们分别应用了<strong>帧遮蔽和关节遮蔽方法</strong>。这些在空间和时间上未遮蔽的标记被馈送到<strong>SkeletonMAE编码器，将输入映射到时空嵌入特征</strong>。</p>
<p>SkeletonMAE解码器。我们的<strong>解码器也采用了STTFormer的结构</strong>。与MAE中的解码器相同，SkeletonMAE解码器中的<strong>时空嵌入特征被处理以重构原始序列</strong>。同时，<strong>为了保留用于重构的位置信息，还引入了位置嵌入。解码器的输出是重构序列，应与未遮蔽的原始序列相同。</strong></p>
<p>重构。我们使用均方误差（MSE）损失来衡量重构的结果。在这种情况下，我们<strong>计算原始骨架序列与重构序列之间的MSE损失</strong>，如下所示：</p>
<p>MSE = 1 N ΣN i=1 |Si − S∗ i |2， 其中i是帧的索引，N是样本数量，S是输入序列，S∗是重构序列。</p>
<p>总结：这一部分描述了SkeletonMAE的架构，包括编码器、解码器和重构方法。编码器和解码器都基于STTFormer结构，用于处理骨架序列的重构。重构的质量通过均方误差损失来衡量，以衡量原始骨架序列与重构序列之间的差异。</p>
<h3 id="动作识别微调">动作识别微调</h3>
<p>为了评估SkeletonMAE学习骨架表示的能力，我们<strong>加载了从预训练中获得的参数权重</strong>，**用所有的训练数据对模型进行微调，然后使用识别准确度来预测每个动作的标签。**微调的过程如图1(b)所示。与最新的基于对比的自监督骨架动作识别方法[20, 14]不同，<strong>这些方法通过线性评估协议验证模型，我们只关注骨架动作识别任务的端到端微调</strong>。</p>
<p>总结：这一部分描述了如何评估SkeletonMAE对骨架表示的学习能力。首先，他们使用从预训练中获得的参数权重对模型进行微调，然后使用准确度来评估每个动作的标签预测。与其他方法不同，他们侧重于端到端微调而不是线性评估协议。这是评估SkeletonMAE性能的关键步骤。</p>
<h1>Masked Motion Predictors are Strong 3D Action Representation Learners<strong>蒙面动作预测器是强大的3D动作表示学习者</strong></h1>
<h2 id="Abstract">Abstract</h2>
<p>在3D人体动作识别中，有限的监督数据使充分发挥强大网络（如变换器）的建模潜力变得具有挑战性。因此，研究人员一直积极探索<strong>有效的自监督预训练策略</strong>。在这项工作中，我们展示了<strong>与按照常见的假设任务对人体关节进行遮挡自部件重建不同</strong>，==明确的上下文运动建模对于学习3D动作识别的有效特征表示成功至关重要==。形式上，我们提出了Masked Motion Prediction (MAMP) 框架。具体来说，提出的MAMP以经过遮挡的时空骨骼序列作为输入，并<strong>预测了被遮挡的人体关节的对应时间运动</strong>。考虑到骨骼序列的高时间冗余性，我们的MAMP中的<strong>运动信息还充当经验语义丰富性先验，指导了遮挡过程</strong>，促进更好地关注语义丰富的时间区域。在NTU-60、NTU-120和PKU-MMD数据集上进行的大量实验表明，提出的MAMP预训练大大提高了采用的普通变换器的性能，实现了无花哨的最先进结果。我们的MAMP源代码可在https://github.com/maoyunyao/MAMP上找到。</p>
<p>要点总结：</p>
<ul>
<li>3D人体动作识别中受限的监督数据使建模成为挑战。</li>
<li>研究人员研究了自监督预训练策略。</li>
<li>本工作提出了Masked Motion Prediction (MAMP) 框架，与先前的自监督任务不同，它<strong>专注于明确的上下文运动建模</strong>。</li>
<li>MAMP使用<strong>遮挡的骨骼序列作为输入，预测遮挡的关节的时间运动</strong>。</li>
<li>MAMP利用骨骼序列的时间冗余性，并将运动信息作为先验，以更好地关注语义丰富的时间区域。</li>
<li>实验证明，MAMP预训练大大提高了3D动作识别的性能，达到最先进的结果。源代码可在https://github.com/maoyunyao/MAMP上找到。</li>
</ul>
<p>// 添加阿里云 maven 地址<br>
maven { url’<a target="_blank" rel="noopener" href="http://maven.aliyun.com/nexus/content/groups/public/">http://maven.aliyun.com/nexus/content/groups/public/</a>’ }<br>
maven { url’<a target="_blank" rel="noopener" href="http://maven.aliyun.com/nexus/content/repositories/jcenter">http://maven.aliyun.com/nexus/content/repositories/jcenter</a>’ }<br>
maven { url’<a target="_blank" rel="noopener" href="https://maven.aliyun.com/repository/public/">https://maven.aliyun.com/repository/public/</a>’ }<br>
maven { url’<a target="_blank" rel="noopener" href="https://maven.aliyun.com/repository/google/">https://maven.aliyun.com/repository/google/</a>’ }<br>
maven { url’<a target="_blank" rel="noopener" href="https://maven.aliyun.com/repository/jcenter/">https://maven.aliyun.com/repository/jcenter/</a>’ }<br>
maven { url’<a target="_blank" rel="noopener" href="https://maven.aliyun.com/repository/central/">https://maven.aliyun.com/repository/central/</a>’ }<br>
maven { url’htttps://repo.spring.io/plugins-release’}<br>
// ————————————————<br>
// 版权声明：本文为CSDN博主「TMS320VC5257H」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>
// 原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/youngwah292/article/details/110743407">https://blog.csdn.net/youngwah292/article/details/110743407</a></p>
<h2 id="1-Introduction">1. Introduction</h2>
<ol>
<li>如何准确识别人类动作一直是计算机视觉领域的长期挑战。最近，随着深度感知和姿态估计技术的进步，基于<strong>骨架的3D人体动作识别</strong>已经成为一个新兴问题，对于人机互动、视频监控、虚拟现实等一系列应用具有重要意义。尽管骨架在计算效率和背景稳健性方面具有优势，<strong>但现有的监督式3D动作识别方法严重依赖于良好注释的训练序列，而获取这些注释需要耗费大量时间和精力。</strong></li>
</ol>
<p>要点总结：</p>
<ul>
<li>
<p>准确识别人类动作一直是计算机视觉领域的挑战。</p>
</li>
<li>
<p>基于骨架的3D人体动作识别因深度感知和姿态估计技术的进步而成为新兴问题，适用于多种应用。</p>
</li>
<li>
<p>==现有的监督式方法需要大量的已注释训练数据，这些数据获取耗时且需要大量工作==</p>
</li>
<li>
<p>Furthermore, limited supervision also leads to the overfitting issue in general models, especially for transformers that are with weak inductive bias and high model capacity. 还有，有限的监督还会导致通用模型，特别是对于**==感应偏置较弱、模型容量较大的transformers，出现过拟合问题==**</p>
</li>
<li>
<p>These facts motivate the exploration of self-supervised 3D action representation learning. 这些事实激发了对自监督3D动作表示学习的探索。</p>
</li>
<li>
<p>在文献中，最初为图像开发的流行托词任务已被用于3D动作表示学习，如着色[58]、重建[62,46,27]、对比学习[23,48,49]等</p>
</li>
<li>
<p>Among them, contrastive learning once dominated 3D action representation learning with its concise framework and promising performance. 在其中，<strong>==对比学习==曾经以其简洁的框架和有希望的性能主导了3D动作表示学习。</strong></p>
</li>
<li>
<p>Nevertheless, as a global representation learner, it still suffers from certain limitations, such as the lack of explicit constraints for temporal context modeling and the over-reliance on heuristic action data augmentations, impeding its further exploration of 3D actions. 然而，作为全局表示学习器，它仍然存在一些限制，比如**==缺乏明确的时间上下文建模约束以及对启发式动作数据增强的过分依赖==，这阻碍了其对3D动作的进一步探索。**</p>
</li>
<li>
<p>Recently, as transformers flourish in computer vision, masked autoencoder (MAE) has attracted a surge of research interest for its exceptional performance. 最近，随着变换器在计算机视觉领域的繁荣，**遮蔽自动编码器（MAE）**因其卓越的性能引起了大量研究兴趣。</p>
</li>
<li>
<p>Given that a 3D skeleton serves as an abstract representation of human behaviors, there has been growing interest in applying the MAE concept to 3D action representation learning, to capture the underlying spatio-temporal dynamics of skeleton sequences. 鉴于3D骨架充当了人类行为的抽象表示，人们越来越有兴趣<strong>将MAE概念应用于3D动作表示学习，以捕捉骨架序列的潜在时空动态。</strong></p>
</li>
<li>
<p>Early attempts generally followed the practice of images, employing masked self-reconstruction of human joints as the pre-training pretext. 早期的尝试通常<strong>遵循图像的惯例</strong>，采用人体关节的遮蔽自重建作为预训练方法。</p>
</li>
<li>
<p>Despite considerable effort, we argue that the network is not effectively directed to prioritize contextual motion modeling in such a self-reconstruction objective, which is, however, crucial for comprehending 3D actions as the appearance information is greatly erased in human skeletons. 尽管付出了相当大的努力，我们认为网络在这种==<strong>自重建目标</strong>中<strong>没有有效地优先考虑上下文动作建模</strong>==，然而，这对于理解3D动作至关重要，因为<strong>人类骨架中的外观信息被大大抹去</strong>。==如何更好地在自监督3D动作表示学习中探索上下文运动线索是一个有效的问题==</p>
</li>
<li>
<p>由于有限的监督数据，强大网络（如变换器）在3D人体动作识别中很难发挥潜力。</p>
</li>
<li>
<p>自监督预训练策略成为解决这个问题的方法，特别是对于3D动作识别。</p>
</li>
<li>
<p>以前为图像开发的预文本任务已经用于3D动作识别，如上色、重建、对比学习等。</p>
</li>
<li>
<p><strong>对比学习曾经主导3D动作识别</strong>，但存在一些限制，如<strong>时间上下文建模不足以及对启发式动作数据增强的过度依赖</strong>。</p>
</li>
<li>
<p><strong>遮蔽自动编码器</strong>（MAE）引起了人们的关注，因为它在计算机视觉中表现出色。</p>
</li>
<li>
<p>3D骨架作为人类行为的抽象表示，吸引了<strong>将MAE概念应用于3D动作识别学习</strong>的兴趣。</p>
</li>
<li>
<p>尽管付出了努力，<strong>自重建的网络没有有效地优先考虑上下文运动建模</strong>，这对于理解3D动作至关重要，因为人类骨架中的外观信息被大大抹去。</p>
</li>
<li>
<p>By consolidating this idea, we introduce Masked Motion Prediction (MAMP), a simple yet effective framework to address the problem of self-supervised 3D action representation learning. 通过巩固这一想法，我们引入了<strong>Masked Motion Prediction（MAMP）</strong>，这是一个简单但有效的框架，用于解决自监督的3D动作表示学习问题。</p>
</li>
<li>
<p>Specifically, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and turns to predict the corresponding temporal motion of the masked</p>
</li>
<li>
<p><strong>mmap以掩蔽的时空骨骼序列为输入，转向预测掩蔽的人体关节相应的时间运动。</strong></p>
</li>
<li>
<p>In this way, the network is directly encouraged for contextual motion modeling. 这样，<strong>网络被直接鼓励进行上下文运动建模。</strong></p>
</li>
<li>
<p>Moreover, given the observation that moments with significant motion are often critical for human action understanding, in our MAMP, the temporal motion is used not only as the pre-training objective but also as an empirical semantic richness prior that effectively guiding the skeleton masking process. 此外，<strong>鉴于观察到==重要的动作时刻通常对于理解人体动作至关重要==，在我们的MAMP中，时间运动不仅用作预训练目标，还用作经验性的语义丰富性先验，有效地引导骨架遮蔽过程。</strong></p>
</li>
<li>
<p>Compared to the random version, the proposed motion-aware masking strategy takes additional temporal motion intensity as input. It first converts the input intensity into a probability distribution and then utilizes the re-parameterization technique for efficient probability-guided masked token sampling. 与随机版本相比，提出的<strong>具有运动感知的遮蔽策略以额外的时间运动强度作为输入</strong>。<strong>它首先将输入强度转换为概率分布，然后利用重新参数化技术进行高效的概率引导的遮蔽标记抽样。</strong></p>
</li>
<li>
<p>As a result, joints with significant motion are masked with a higher probability, facilitating better attention to semantically rich temporal regions. 结果，<strong>具有显著运动的关节以更高的概率被遮蔽，有助于更好地关注语义丰富的时间区域。</strong></p>
</li>
<li>
<p>As illustrated in Figure 1, compared to masked self-reconstruction of human joints, masked motion prediction acts as a more effective pretext task for 3D action representation learning. 如图1所示，<strong>与对人体关节进行遮蔽的自重建相比，遮蔽的运动预测作为3D动作表示学习的更有效的 pretext task</strong>。</p>
</li>
<li>
<p>It substantially alleviates the problem that the transformers cannot fully unleash their modeling potential for human actions due to the scarcity of annotated 3D skeletons. 这在很大程度上<strong>缓解了变换器不能充分发挥其对人体动作建模潜力的问题，因为缺乏已注释的3D骨架。</strong></p>
</li>
<li>
<p>The adopted vanilla transformer sets a series of state-of-the-art records in 3D action recognition after MAMP pre-training, without the need for bells and whistles such as multi-stream ensembling. 采用的普通变换器在MAMP预训练后在3D动作识别中创造了一系列最新记录，而无需多通道集成等繁琐操作。</p>
</li>
<li>
<p>Specifically, compared to training from scratch, our MAMP demonstrates significant absolute performance improvements of 10.0% and 13.2% on the challenging cross-subject protocol of NTU RGB+D 60 and NTU RGB+D 120 datasets, resulting in top-1 accuracy of 93.1% and 90.0%, respectively. 具体来说，与从零开始训练相比，我们的MAMP在NTU RGB+D 60和NTU RGB+D 120数据集的<strong>具有挑战性的跨主题协议上</strong>表现出显著的绝对性能改进，分别达到了93.1%和90.0%的top-1准确度。</p>
</li>
<li>
<p>We hope this simple yet effective framework will serve as a strong baseline that facilitates</p>
</li>
</ul>
<p>要点总结：</p>
<ul>
<li>介绍了<strong>Masked Motion Prediction</strong>（MAMP），这是一个简单而有效的框架，用于自监督3D动作表示学习。</li>
<li>MAMP通过<strong>遮蔽的骨架序列预测遮蔽的人体关节的时间运动，以鼓励上下文运动建模</strong>。</li>
<li>引入了<strong>具有运动感知的遮蔽策略，这策略以额外的时间运动强度为输入，用于提高关注语义丰富的时间区域</strong>。</li>
<li>与随机版本相比，此策略以更高的概率遮蔽具有显著运动的关节。</li>
<li>相对于自重建，遮蔽的运动预测对于3D动作表示学习是更有效的借口任务。</li>
<li>MAMP极大地改进了3D动作的<strong>建模能力</strong>，使得普通变换器在MAMP预训练后创造了最新的记录。</li>
<li>MAMP在具有挑战性的数据集上表现出显著的绝对性能改进，为未来3D动作预训练研究提供了强大的基准。</li>
<li>我们提出了<strong>掩蔽运动预测来学习3D动作表示</strong>，这大大<strong>缓解了传统掩蔽自我重建范式中上下文运动建模不足</strong>的问题。</li>
<li>我们设计了运动感知掩蔽策略，该策略**==将运动强度作为自适应关节掩蔽的经验语义丰富度==**。</li>
<li>我们在三个普遍的基准上进行了广泛的实验，以验证我们方法的有效性。值得一提的是，通过我们提出的MAMP, vanilla transformer首次实现了3D动作识别的最佳记录。</li>
</ul>
<blockquote>
<p>缺乏上下文的运动建模  就直接鼓励进行上下文运动建模</p>
<p>时间运动建模还用作经验性的语义丰富性先验，有效地引导骨架遮蔽过程</p>
<p>具有运动感知的遮蔽策略以额外的时间运动强度作为输入？？</p>
</blockquote>
<h2 id="2-Related-work">2.Related work</h2>
<h3 id="Supervised-3D-Action-Recognition">Supervised 3D Action Recognition</h3>
<ul>
<li>监督式3D动作识别</li>
<li>How to better model the dynamic skeletons for supervised action recognition is an extensively studied problem.</li>
<li><strong>如何更好地对监督式动作识别建模动态骨骼</strong>是一个广泛研究的问题。</li>
<li>In many early works, RNNs are favored for their excellent sequential modeling capability, such as the hierarchical RNN model proposed in [14] and the 2D Spatio-Temporal LSTM in [30, 29].</li>
<li>在许多早期研究中，由于其出色的时序建模能力，例如[14]中提出的分层RNN模型和[30, 29]中的2D时空LSTM，RNNs备受青睐。</li>
<li>In view of the great success of CNNs [21, 18] in image understanding, some methods also try to apply it to 3D action recognition.</li>
<li>鉴于CNNs在图像理解中取得的巨大成功，一些方法也尝试将其应用于3D动作识别。</li>
<li>To cater for the input format, [13] and [22] treat the skeleton sequence as a three-channel (x, y, and z coordinates) pseudo-image, with the number of frames and joints as height and width, respectively.</li>
<li>为适应输入格式，[13]和[22]将骨骼序列视为三通道（x、y和z坐标）的伪图像，其中帧数和关节数分别作为高度和宽度。</li>
<li>Considering the natural connections between joints, ST-GCN [55] introduces the Graph Neural Networks (GCNs) for skeleton modeling, where the convolution kernels are elaborately designed according to the skeleton topology.</li>
<li>考虑到关节之间的自然联系，ST-GCN [55]引入了图神经网络（GCNs）用于骨骼建模，其中卷积核根据骨骼拓扑结构精心设计。</li>
<li>The astonishing performance of ST-GCN has led the trend of GCN-based 3D action recognition, with numerous subsequent improvements emerging in input streams [51, 26, 40], kernel design [5, 61, 39, 32], etc.</li>
<li>ST-GCN的出色性能引领了基于GCN的3D动作识别趋势，出现了众多后续改进，涵盖了输入流[51, 26, 40]、卷积核设计[5, 61, 39, 32]等多个方面。</li>
<li>Recent approaches [36, 35, 41] try to introduce the popular vision transformer into 3D action recognition. However, under limited training data, vanilla transformers with weak inductive bias cannot be fully trained.</li>
<li>最近的方法[36, 35, 41]尝试将流行的视觉变换器引入3D动作识别。然而，在有限的训练数据下，具有弱感应偏差的普通变换器无法充分训练。</li>
<li>Therefore, many customized designs are required in existing supervised attempts, such as temporal convolution [36], graph convolution [35, 36], space-time separation [41], etc.</li>
<li>因此，<strong>在现有的监督尝试中需要许多定制的设计，例如时间卷积[36]、图卷积[35, 36]、时空分离[41]等。</strong></li>
<li>In our approach, we demonstrate that pre-training with masked motion prediction is key to the success of transformers in 3D action recognition.</li>
<li>在我们的方法中，我们证明了<strong>使用遮蔽运动预测进行预训练是Transformer在3D动作识别中取得成功的关键</strong>。</li>
<li>The proposed MAMP framework endows the vanilla transformer with unrivaled performance.</li>
<li>提出的MAMP框架使普通变换器具有无与伦比的性能。</li>
</ul>
<p>要点总结：</p>
<ul>
<li>介绍了监督式3D动作识别的问题和不同方法。</li>
<li>提到了RNNs和CNNs以及GCNs等不同的方法和技术，这些方法都用于尝试更好地建模3D动作。</li>
<li>引入了视觉变换器，并指出在有限的训练数据下，标准变换器需要许多定制设计。</li>
<li>作者提出了一种名为MAMP的方法，它通过预训练和遮蔽运动预测来改进变换器在3D动作识别中的性能。</li>
</ul>
<h3 id="Self-supervised-3D-Action-Recognition">Self-supervised 3D Action Recognition</h3>
<ul>
<li>自监督3D动作识别</li>
<li>Self-supervised representation learning aims to capture the domain priors from unlabeled data so as to facilitate the application of the model in downstream tasks.</li>
<li>自监督表示学习旨在从未标记的数据中捕获领域先验知识，以便在下游任务中应用该模型。</li>
<li>In 3D human action recognition, many pretext tasks have been utilized to explore the action context that resides in the skeleton sequence.</li>
<li>在3D人体动作识别中，<strong>使用了许多前提任务来探索骨骼序列中存在的动作上下文</strong>。</li>
<li>Among them, LongT GAN [62] and P&amp;C [46] try to learn 3D action representation by autoencoder-based sequence reconstruction, where the decoder in P&amp;C is further weakened to promote the learning of the feature encoder.</li>
<li>在其中，LongT GAN [62]和P&amp;C [46]尝试通过<strong>基于自动编码器的序列重建来学习3D动作表示</strong>，其中P&amp;C中的<strong>解码器被进一步削弱以促进特征编码器的学习</strong>。</li>
<li>In Colorization [58], the skeleton sequences are treated as point clouds and action representation is learned by colorizing each joint based on its spatial and temporal orders.</li>
<li>在Colorization [58]中，<strong>骨骼序列被视为点云，并且根据其空间和时间顺序对每个关节进行着色以学习动作表示</strong>。</li>
<li>Recently, many contrastive learning-based approaches [27, 48, 23, 49, 59, 34] have emerged, showing superior performance compared to earlier works.</li>
<li>最近，出现了<strong>许多基于对比学习的方法 [27, 48, 23, 49, 59, 34]，表现出优于早期方法的性能</strong>。</li>
<li>To learn better 3D action representation, they either try to dig helpful supervision across different skeleton modalities [23, 34], or explore better action data augmentation [49] and positive sample mining strategies [59].</li>
<li>为了学习更好的3D动作表示，它们要么尝试在不同的骨骼模态之间获取有用的监督 [23, 34]，要么探索更好的动作数据增强 [49] 和正样本挖掘策略 [59]。</li>
<li>Nevertheless, as a global feature learner originally designed for images, contrastive learning lacks explicit constraints on the exploration of temporal motion context, limiting its further development for 3D actions.</li>
<li><strong>尽管对比学习最初是为图像设计的全局特征学习器，但它缺乏对探索时序动作上下文的明确约束，从而限制了其在3D动作领域的进一步发展。</strong></li>
<li>SkeletonMAE [53] first introduces the idea of MAE[17] into transformer-based 3D action representation learning, where the original joint coordinates of masked regions are predicted.</li>
</ul>
<p>Wenhan Wu, Yilei Hua, Ce zheng, Shiqian Wu, Chen Chen,<br>
and Aidong Lu.==SkeletonMAE: Spatial-temporal masked au-<br>
toencoders for self-supervised skeleton action recognition.==<br>
arXiv preprint arXiv:2209.02399, 2022. 1, 3, 4, 6, 7, 8</p>
<p>==Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr<br>
Doll´ar, and Ross Girshick. Masked autoencoders are scalable<br>
vision learners. In Proceedings ofthe IEEE/CVF Conference<br>
on Computer Vision and Pattern Recognition (CVPR), pages<br>
16000–16009, 2022. 2, 3, 4, 5, 8==</p>
<ul>
<li>SkeletonMAE [53]首次将MAE[17]的思想==引入基于变换器的3D动作表示学习中，其中预测遮蔽区域的原始关节坐标==。</li>
<li>In our approach, we demonstrate that such a self-reconstruction objective is sub-optimal for learning 3D action representation.</li>
<li>在我们的方法中，我们证明了这种自重建目标不是学习3D动作表示的最佳选择。</li>
<li>Therefore, we introduce the Masked Motion Prediction (MAMP) framework for explicit contextual motion modeling, resulting in significantly better performance compared to raw skeleton reconstruction.</li>
<li>因此，我们引入了Masked Motion Prediction（MAMP）框架，==用于明确的上下文动作建模==，与原始骨骼重建相比，性能显著更好。</li>
</ul>
<p>总结：</p>
<p>该段描述了自监督3D动作识别领域的研究，其中各种前提任务和方法被用来探索骨骼序列中的动作上下文。虽然许多方法已经出现，包括对比学习等，但文章指出对比学习对于探</p>
<h3 id="Masked-Visual-Prediction">Masked Visual Prediction</h3>
<ul>
<li>掩盖的视觉预测</li>
<li>With the development of vision transformers [4, 12, 50], the masked prediction derived from the autoencoder [1] has revived again.</li>
<li>随着视觉变换器的发展 [4, 12, 50]，从自动编码器 [1] 导出的掩盖预测再次复活。</li>
<li>Similar to the BERT [11] pre-training in NLP, the input tokens are randomly masked and corresponding objectives are predicted, which can be the raw pixels [17], HOG features [52], or token ids from offline learned dVAEs [2].</li>
<li>类似于自然语言处理中的BERT [11] 预训练，输入令牌被随机遮蔽，然后预测相应的目标，这些目标可以是原始像素 [17]、HOG特征 [52] 或来自离线学习的dVAEs [2] 的令牌ID。</li>
<li>Recently, there have also been attempts [45, 57] to use optical flow or temporal difference of images as the auxiliary reconstruction objectives, but inferior performance is observed when they are applied alone.</li>
<li>最近，也有尝试 [45, 57] 使用光流或图像的时间差作为辅助重建目标，但当它们单独应用时性能较差。</li>
<li>This is largely attributed to the high redundancy of the raw images, where the key foreground motion is difficult to be pre-extracted accurately.</li>
<li>这在很大程度上归因于原始图像的高冗余性，其中关键的前景运动难以准确预先提取。</li>
<li>In our approach, we employ the idea of masked visual prediction for 3D action representation learning, with the temporal skeleton motion adopted as the only reconstruction target.</li>
<li>在我们的方法中，我们采用了==掩盖的视觉预测的思想==来学习3D动作表示，其中==采用了时间骨架运动作为唯一的重建目标==。</li>
<li>Different from images, the explicit temporal correspondence of joints in the human skeleton sequence enables the ready extraction of their accurate motion context.</li>
<li>与图像不同，人体骨架序列中<strong>关节的明确时间对应关系使得能够轻松提取其准确的运动背景</strong>。</li>
<li>Furthermore, we also incorporate motion intensity as the semantic richness prior to guide the masking process.</li>
<li>此外，我们还将运动强度作为语义丰富性的先验，以引导遮盖过程。</li>
</ul>
<p>总结：</p>
<p>该段介绍了&quot;Masked Visual Prediction&quot;，一种用于3D动作表示学习的方法。这个方法在视觉变换器的发展中得以复兴，类似于NLP中的BERT预训练，采用了随机遮蔽的输入令牌，以及相应的目标预测。不同于图像，3D骨架序列中的关节具有明确的时间对应关系，使得能够更容易地提取准确的运动背景。此外，==运动强度也被纳入作为语义丰富性的先验来引导遮盖过程==。</p>
<h2 id="Our-Method">Our Method</h2>
<h3 id="3-1-Overview">3.1. Overview</h3>
<p>图2展示了我们提出的遮罩运动预测（MAMP）框架的总体流程。</p>
<p>它以骨架序列S ∈ RTs×V×Cs作为输入，该序列是从原始数据中随机裁剪并调整为固定的时间长度Ts。其中，V和Cs分别是关节数量和坐标通道数量。</p>
<p>输入的运动序列M ∈ RTs×V×Cs也被提取出来，其定义为时间维度上的差分（第一帧手动填充）。</p>
<p>与大多数视觉变换器一样，输入关节被线性映射为关节嵌入E ∈ RTe×V×Ce。</p>
<p>然后，应用了运动感知遮罩策略，根据时间运动强度的指导，对大多数嵌入特征进行遮罩处理。</p>
<p>剩下的特征由编码器-解码器架构处理，其中变换器编码器从未被遮罩的关节嵌入中学习表示，变换器解码器基于可学习的遮罩标记和变换器编码器的潜在表示执行上下文建模。</p>
<p>与MAE [17]不同，后者用于表示学习并重建原始信号，MAMP采用了运动预测头，该头以解码后的特征作为输入并预测输入骨架序列的时间运动。</p>
<p>在上述的预训练之后，只有关节嵌入层和变换器编码器被保留供下游应用使用。</p>
<p><strong>总结：</strong> 该段落描述了MAMP框架的总体流程，包括输入骨架序列的处理、关节嵌入、运动感知遮罩策略以及运动预测，以及在预训练后保留的用于下游应用的层。这个框架旨在通过自监督学习提取骨架序列的有用表示。</p>
<h3 id="3-2-Joint-Embedding">3.2. Joint Embedding</h3>
<p>在大多数基于Transformer的尝试中[35, 41, 36]，每个时空骨架关节都被单独嵌入，导致了大量的输入标记。然而，在我们的方法中，考虑到时间上的冗余，我们将输入的骨架序列S ∈ RTs×V×Cs分为在时间上不重叠的段S′ ∈ RTe×V×l×Cs，其中l是每个段的长度，Te = Ts/l。在每个段中，具有相同空间位置的关节被一起嵌入：E = \texttt {JointEmbed}(S’) ∈ \mathbb {R}^{T_e \times V \times C_e}，其中Ce是嵌入特征的维度。与原始的骨架序列相比，嵌入E的时间分辨率降低了l倍，从而提高了计算效率。</p>
<p>总结：这段文字描述了一个在骨架动作识别中的创新方法，采用了分段嵌入策略，将原始的骨架序列分成不重叠的时间段，将具有相同空间位置的关节一起嵌入，以减少输入标记数量和提高计算效率。这一方法有望在处理大规模骨架数据时更加高效。</p>
<h3 id="3-3-Motion-Extraction">3.3. Motion Extraction</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/image-20231023200018904.png" alt="image-20231023200018904"></p>
<h3 id="3-4-Motion-Aware-Masking">3.4. Motion-Aware Masking</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/image-20231023200157110.png" alt="image-20231023200157110"></p>
<h3 id="3-5-Masked-Motion-Prediction">3.5. Masked Motion Prediction</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/image-20231023200246428.png" alt="image-20231023200246428"></p>
<h1>Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning    提示与masked运动建模的对比:  面向通用的3D动作表示学习</h1>
<h2 id="1introduction">1introduction</h2>
<p>我们提出了用于多粒度表示的PCM3，它以一种<strong>互惠的方式集成了掩蔽骨架预测和对比学习范式</strong>。我们使用<strong>掩蔽预测网络来产生更多不同的正向运动视图来进行对比学习</strong>。同时，<strong>生成的梯度利用高级语义信息依次传播和引导掩蔽预测学习</strong>。</p>
<p>•考虑到<strong>不同的数据视图和预训练任务可能会产生相互干扰</strong>，我们在<strong>多任务预训练中引入了领域特异性提示和任务特异性提示</strong>。这些<strong>可训练的提示</strong>使模型能<strong>够对不同的骨架实现更有区别的表示</strong>。</p>
<p>•我们执行严格的<strong>定量实验</strong>，<strong>评估最先进的自我监督的3D动作表示学习技术的泛化效果</strong>，在5个下游任务，包括识别，检索，检测和运动预测，在未损坏和损坏的骨骼。我们的研究为研究界提供了一个全面的基准，我们相信它可以提供有价值的见解，并有助于该领域的未来研究。</p>
<p>INTRODUCTION</p>
<ul>
<li>引言</li>
</ul>
<p>Human activity understanding is a crucial problem in multi-media processing on account of its significant role in real-life applications, such as human-robotics interaction [18], healthcare [25] and autonomous driving [2]</p>
<ul>
<li>人类活动理解是多媒体处理中的一个关键问题，因为它在现实生活中的应用中起着重要作用，如人机交互[18]、医疗[25]和自动驾驶[2]等。</li>
</ul>
<p>As a highly efficient representation for human activity understanding, 3D skeletons represent the human form by 3D coordinates of key body joints. In comparison to other representations such as RGB videos and depth data, skeletons are lightweight, compact, and privacy-preserving. Owing to these competitive advantages, skeletons have become widely used in human action analysis.</p>
<ul>
<li>作为人类活动理解的高效表示，<strong>3D骨架通过关键身体关节的3D坐标来表示人体形态</strong>。与其他表示方法（如RGB视频和深度数据）相比，骨架轻巧、紧凑且具有隐私保护性。由于这些竞争优势，骨架已广泛用于人体动作分析。</li>
</ul>
<p>Many efforts have been made on the supervised skeleton-based action learning [6, 9, 35, 47]. However, the performance of these methods heavily relies on a huge amount of labeled data, which requires time-consuming and expensive data annotation work. This inherent shortcoming of full supervision limits their applications in the real world. Therefore, more and more attention has been paid to self-supervised 3D action representation learning recently to learn meaningful features from unlabeled data.</p>
<ul>
<li>在监督的基于骨架的动作学习方面已经做出了很多努力[6, 9, 35, 47]。然而，**这些方法的性能严重依赖于大量的已标记数据，**这需要耗时且昂贵的数据注释工作。**全面监督的固有缺陷限制了它们在现实世界中的应用。**因此，最近越来越多的关注已经转向了自监督的3D动作表示学习，以从未标记的数据中学习有意义的特征。</li>
</ul>
<p>Self-supervised 3D action representation learning research has mainly focused on two paradigms: reconstruction-based and contrastive learning-based methods. Reconstruction-based methods leverage an encoder-decoder architecture to learn representations by predicting masked skeletons (i.e., masked modeling) or reconstructing original data. These methods focus on joint-level feature modeling and capture spatial-temporal relationships. In contrast, contrastive learning-based methods use data augmentations to construct positive/negative pairs, and apply an instance discrimination task to learn sequence-level semantic features.</p>
<ul>
<li><strong>自监督3D动作表示学习的研究主要集中在两种范式上：<strong>基于重建和基于对比学习的方法。基于重建的方法</strong>利用编码器-解码器架构通过预测掩盖的骨架</strong>（<strong>即掩模建模</strong>）或重建原始数据来学习表示。这些方法<strong>专注于关节级特征建模并捕获时空关系</strong>。相比之下，<strong>基于对比学习的方法使用数据增强来构建正负对，并应用实例判别任务来学习序列级语义特征</strong>。</li>
</ul>
<p>However, it is noticed that most recent representation learning methods focus on the single paradigm to model the joint-level (by masked skeleton modeling) [45, 53] or sequence-level (by contrastive learning) [8, 20, 26, 52] features solely. As a result, it is difficult for these methods to generalize well to different downstream tasks, e.g., recognition task and motion prediction, because they cannot learn representations of different granularity simultaneously and effectively. Although some works [21, 40, 43] make valuable efforts to combine the above two approaches to learn richer representations, only mediocre improvement is observed. It is because simply combining them ignores the interference due to the gaps between feature modeling mechanisms of masked prediction and contrastive learning [30], and fails to utilize the potential synergy. These problems limit the generalization power of the model, and versatile 3D action representation learning remains a challenging and under-explored area.</p>
<ul>
<li>但需要注意的是，大多数最近的<strong>表示学习方法专注于单一范式，<strong><strong><strong>仅对关节级（通过掩蔽骨架建模）</strong></strong>[45, 53] 或</strong>序列级（通过对比学习）</strong>[8, 20, 26, 52] <strong>特征进行建模</strong>。因此，这些方法难以很好地推广到不同的下游任务，<strong>例如识别任务和动作预测</strong>，因为它们<strong>不能同时有效地学习不同粒度的表示</strong>。尽管一些工作[21, 40, 43]付出了努力，<strong>将上述两种方法结合起来学习更丰富的表示</strong>，但只有一般的改进。这是因为简单地将它们结合起来**忽略了由于掩蔽预测和对比学习的特征建模机制之间差距而产生的干扰，未能充分利用潜在的协同作用。**这些问题限制了模型的泛化能力，<strong>多功能的3D动作表示学习仍然是一个具有挑战性且未被充分探讨的领域。</strong></li>
</ul>
<p>To this end, we propose the prompted contrast with masked motion modeling, PCM3, which explores the mutual collaboration between the above two paradigms for versatile 3D action representation learning as shown in Figure 1. Specifically, the well-designed inter-intra-contrastive learning and topology-based masked skeleton prediction are first proposed as the basic pipelines. Furthermore, we connect the two tasks and explore the synergy between them. The views in masked prediction training are utilized as novel positive samples for contrastive learning. In turn, the masked prediction branch is also updated via the gradients from the contrastive learning branch for higher-level semantic guidance. Meanwhile, to reduce the distraction of learning between different pretext tasks and data views, we</p>
<ul>
<li>为此，我们提出了PCM3，即<strong>采用具有掩盖运动建模的提示对比方法</strong>，它<strong>探索了上述两种范式之间的相互协作</strong>，用图1所示的方式进行多功能3D动作表示学习。具体而言，我们首次<strong>提出了精心设计的内外对比学习和基于拓扑的掩模骨架预测作为基本流程</strong>。此外，我们将这两个任务连接起来，<strong>探索它们之间的协同作用</strong>。<strong>掩模预测训练中的视图被用作对比学习的新颖正样本</strong>。反过来，<strong>掩模预测分支也通过对比学习分支的梯度进行更新</strong>，<strong>以进行更高级别的语义引导</strong>。同时，为了<strong>减少不同前提任务和数据视图之间的学习干扰</strong>，我们提出了<strong>双提示多任务预训练策略</strong>。应用了两种类型的提示，即<strong>领域特定提示和任务特定提示</strong>，以明确指导模型从不同的数据视图/任务中学习。我们进行了广泛的实验，涵盖了五个下游任务，以提供全面的评估。所提出的方法与最先进的方法相比，展现出有前途的泛化能力。</li>
</ul>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>我们的贡献可以总结如下：</li>
</ul>
<p>• We propose PCM3 for multi-granularity representations, which integrates masked skeleton prediction and contrastive learning paradigms in a mutually beneficial manner. • 我们提出了PCM3，用于<strong>多粒度表示</strong>，它<strong>以相互有益的方式将掩模骨架预测和对比学习范式相结合</strong>。</p>
<p>• We employ the masked prediction network to generate more diverse positive motion views for contrastive learning. • 我们<strong>使用掩模预测网络生成更多多样化的正向动作视图，用于对比学习</strong>。</p>
<p>• Meanwhile, the generated gradients are propagated and guide masked prediction learning in turn with high-level semantic information. • 与此同时，<strong>生成的梯度被传播并通过高级语义信息反过来指导掩模预测学习</strong>。</p>
<p>• Considering that different data views and pretraining tasks can cause mutual interference, we introduce domain-specific prompts and task-specific prompts for the multi-task pretraining. • 考虑到不同的数据视图和预训练任务可能会相互干扰，我们<strong>引入了领域特定提示和任务特定提示进行多任务预训练</strong>。</p>
<p>• We perform rigorous quantitative experiments to assess the generalization efficacy of state-of-the-art self-supervised 3D action representation learning techniques across five downstream tasks, including recognition, retrieval, detection, and motion prediction, on both uncorrupted and corrupted skeletons. • 我们进行了严格的定量实验，评估了最先进的自监督3D动作表示学习技术在五个下游任务中的泛化效能，包括识别、检索、检测和动作预测，在未损坏和损坏的骨骼上进行。</p>
<p>• Our study serves as a comprehensive benchmark for the research community, and we believe it can provide valuable insights and aid future investigation in this field. • 我们的研究为研究社区提供了全面的基准，我们相信它可以提供有价值的见解并帮助未来的研究。</p>
<h2 id="2-RELATEDWORKS">2 RELATEDWORKS</h2>
<h3 id="2-1-Skeleton-based-Action-Recognition">2.1 Skeleton-based Action Recognition</h3>
<p>随着深度学习的显著进步，我们研究了用于基于骨架的动作识别的循环神经网络（RNN）、卷积神经网络（CNN）、图卷积网络（GCN）和变换器（transformer）等方法。</p>
<p>RNN已广泛用于模拟时间依赖性和捕捉基于骨架的动作识别中的运动特征。</p>
<p>在[9]中的工作中，使用RNN来处理骨架数据，将其视为序列数据。</p>
<p>之后，Song等人在[37, 38]中提出了<strong>使用注意机制和多模态信息来增强特征表示。</strong></p>
<p>其他一些工作[15, 24]将每个<strong>骨架序列转换为类似图像</strong>的表示，并应用CNN模型来提取空间-时间信息。</p>
<p>最近，由于人体的自然拓扑结构，GCN方法引起了更多关注。</p>
<p>许多工作[6, 35, 47]<strong>将GCN应用于空间和时间维度</strong>[47]，在监督的基于骨架的动作识别中取得了显著的成果。</p>
<p>同时，<strong>变换器模型</strong>[29, 36]也显示出有希望的结果，因为它可以<strong>通过注意机制学习长期时间依赖性。</strong></p>
<p>然而，这些监督性作品依赖于大规模标记数据来训练模型。在本文中，我们研究了自监督的三维动作表示学习。</p>
<p><strong>总结：</strong> 该段落讨论了用于基于骨架的动作识别的不同方法，包括RNN、CNN、GCN和transformer。它提到了这些方法的应用领域以及它们的一些优势和限制，特别强调了监督方法需要大量标记数据，而本文将探讨自监督学习方法。</p>
<h3 id="2-2-Contrastive-Learning-for-Skeleton">2.2 Contrastive Learning for Skeleton</h3>
<p>对比学习已被证明对于骨架表示学习非常有效。</p>
<p>一种流行的研究观点是<strong>骨架增强</strong>，这对于学到的表示质量至关重要。</p>
<p>郭等人在当前的<strong>对比学习管道中探索了极端的增强</strong>。</p>
<p>张等人提出了<strong>分层一致性对比学习，以利用更强大的增强</strong>。</p>
<p>另一个视角是探索骨架中不<strong>同视图的知识。</strong></p>
<p>ISC使用图像、图和序列表示进行交叉对比学习。</p>
<p>李等人挖掘了潜在的正例，利用了不同的骨架模态，即关节、骨骼、运动，并根据相似度重新加权训练样本。</p>
<p>毛等人在不同视图之间进行了相互蒸馏。</p>
<p>与上述作品不同，我们提出将基于遮罩的建模前提任务与对比学习相结合，以对关节级别和序列级别的特征进行建模，以获得更通用的表示学习。</p>
<p><strong>总结：</strong> 该段落讨论了在骨架表示学习中应用对比学习的有效性，并提到了不同方法和观点，包括极端增强、分层一致性对比学习、交叉对比学习等。最后，介绍了该工作的方法，即将基于遮罩的建模与对比学习相结合，以获得更通用的表示学习。</p>
<h3 id="2-3-Masked-Image-Skeleton-Modeling">2.3 Masked Image/Skeleton Modeling</h3>
<p><strong>2.3 基于图像/骨架的遮罩建模</strong></p>
<p><strong>遮罩建模已在堆叠去噪自动编码器中得到探索</strong>[42]，其中<strong>遮罩操作被视为向原始数据添加噪音。</strong></p>
<p>最近，<strong>遮罩建模</strong>在图像表示学习的自监督学习[13, 46]中取得了显著成功。</p>
<p>对于骨架数据，LongT GAN [53]直接使用了一个基于自编码器的模型，通过附加的对抗训练策略进行优化。</p>
<p>一些作品[21, 40]将运动预测前提任务应用于学习骨架序列中的时间依赖性。</p>
<p>受到遮罩自动编码器[13]的启发，吴等人[45]提出了一种遮罩骨架自动编码器，用于学习空间-时间关系。</p>
<p>在本文中，我们探索了遮罩建模和对比学习之间的协同作用，并提出了一种基于拓扑结构的遮罩策略，以进一步增强表示学习。</p>
<p><strong>总结：</strong> 该段落讨论了遮罩建模在图像和骨架领域的应用，以及一些相关的自监督学习方法。它还介绍了本文的方法，即将遮罩建模与对比学习相结合，并提出了一种新的基于拓扑结构的遮罩策略以增强表示学习。</p>
<h2 id="3-THE-PROPOSED-METHOD-PCM3">3 THE PROPOSED METHOD: PCM3</h2>
<p>在这一部分中，我们首先描述了我们设计的对比学习管道(章节3.1)以及我们提出的基于拓扑的掩码建模方法(章节3.2)。然后，我们在第3.3节中进一步介绍了这两个任务之间的协同探索。基于提示的前训练策略和整个模型在3.4节给出。</p>
<p>3.1节 提出的整体基线</p>
<p>3.2节 提出的基于拓扑的掩码建模方法</p>
<p>3.3节 提出的两个pretext tasks 之间的协同合作</p>
<p>3.4节 提出的基于prompted预训练任务 和整个模型</p>
<h3 id="3-1-Skeleton-Contrastive-Learning">3.1 Skeleton Contrastive Learning</h3>
<p>出于清晰起见，对于图像/骨架的对比学习的经典设计是根据以前的工作[1, 3, 14]提供的，通常包括以下组件：</p>
<p>• 数据增强模块包含一系列手动数据转换，用于构建原始数据的不同视图，这些视图被视为共享相同语义的正样本。</p>
<p>• 编码器 𝑓(·) 用作从输入空间到潜在特征空间的映射函数。</p>
<p>• 嵌入投影 ℎ(·) 在编码器 𝑓(·) 之后连续应用，将编码特征映射到一个嵌入空间，其中应用自监督损失。</p>
<p>• 自监督损失旨在<strong>最大化正样本之间的相似性</strong>，<strong>执行特征聚类操作以获得可区分的表示空间。</strong></p>
<p>我们的==对比学习设计基于MoCo v2== [4]。具体来说，我们<strong>引入了骨架内部和骨架间的变换以及关系知识蒸馏，以帮助模型捕获不同的运动模式并提高表示学习</strong>。</p>
<p><strong>总结：</strong> 这一段描述了对比学习的经典设计，包括数据增强模块、编码器、嵌入投影和自监督损失。对比学习的设计基于MoCo v2，并引入了骨架内部和骨架间的变换以及关系知识蒸馏，以增强模型对多样化运动模式的捕获和表示学习。这些组件和方法用于构建骨架的自监督学习框架，以提高骨架表示的质量。</p>
<h4 id="1-Intra-skeleton-transformation-learning">1) Intra-skeleton transformation learning.</h4>
<ol>
<li>**骨架内部变换学习。**我们利用以下变换：时间裁剪-调整大小，剪切，关节抖动，遵循以前的研究[26, 41]。具体来说，给定骨架序列 𝑥，通过上述变换构建正样本对 (𝑠𝑖𝑛𝑡𝑟𝑎, 𝑠′)。然后，通过查询/关键编码器 𝑓𝑞 (·)/𝑓𝑘 (·) 和嵌入投影 ℎ𝑞 (·)/ℎ𝑘 (·)，获得相应的特征表示 (𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′)。同时，维护一个存储大量负样本的内存队列 M 用于对比学习。我们通过 InfoNCE 目标 [28] 优化整个网络： L𝐼𝑛𝑡𝑟𝑎 𝐼𝑛𝑓𝑜 = − log exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑧′/𝜏) exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑧′/𝜏) + Í 𝑖=1 exp(𝑧𝑖𝑛𝑡𝑟𝑎 · 𝑚𝑖/𝜏) , (1) 其中 𝑚𝑖 是 M 中与第 𝑖 个负样本对应的特征，𝜏 是温度超参数。在训练步骤后，    根据先进先出的策略，批次中的样本将更新到 M 作为负样本。关键编码器是查询编码器的动量更新版本，即 𝜃𝑘 ← 𝛼𝜃𝑘 + (1 − 𝛼)𝜃𝑞，其中 𝜃𝑞 和 𝜃𝑘 是查询编码器和关键编码器的参数，𝛼 ∈ [0, 1) 是动量系数。</li>
</ol>
<p><strong>==总结： 这一段描述了骨架内部变换学习的过程。首先，通过一系列变换构建正样本对，然后使用查询/关键编码器和嵌入投影获得特征表示。同时，维护一个存储负样本的内存队列，并通过 InfoNCE 目标来优化网络。关键编码器是查询编码器的动量更新版本，用于提高训练的稳定性==</strong></p>
<h4 id="2-Inter-skeleton-transformation-learning">2) Inter-skeleton transformation learning.</h4>
<ol>
<li>**骨架间转换学习。**受到==混合增强在自监督学习中的成功应用的启发==[17, 19, 34, 49]，我们将 CutMix [48]、ResizeMix [31] 和 Mixup [50] 引入到我们的骨架对比学习中。这些骨架间的变换利用两个不同的样本来生成混合增强的视图。具体来说，给定两个骨架序列 𝑠1, 𝑠2，我们随机选择上述的混合方法，并按以下方式获得混合的骨架数据 𝑠𝑖𝑛𝑡𝑒𝑟：
<ol>
<li>• Mixup [50]：我们根据采样的混合比例 𝜆 对两个骨架序列进行插值，即，𝑠𝑖𝑛𝑡𝑒𝑟 = (1 − 𝜆)𝑠1 + 𝜆𝑠2。</li>
<li>• CutMix [48]：随机选择的两个骨架序列的区域在时空维度上被剪切和粘贴。𝜆 被定义为替换的关节数与总关节数的比例。</li>
<li>• ResizeMix [31]：这与 CutMix 类似，但在混合之前先在时序维度上对 𝑠2 进行降采样。 随后，我们可以通过 𝑧𝑖𝑛𝑡𝑒𝑟 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑖𝑛𝑡𝑒𝑟) 获得与混合数据对应的嵌入，然后优化以下损失：</li>
</ol>
</li>
</ol>
<p><strong>总结：</strong> 这一段描述了骨架间转换学习的过程，其中引入了不同的混合增强方法（Mixup、CutMix和ResizeMix）来利用两个不同样本生成混合骨架数据。通过这些混合数据，可以获得相应的嵌入表示，并进行损失优化。==这种方法有助于模型捕获多样的运动模式并提高表示学习的效果==。</p>
<h4 id="3-Relational-Knowledge-Distillation">3) Relational Knowledge Distillation.</h4>
<blockquote>
<p>关系知识蒸馏项 被引入 使得增强对比学习的细粒度表示</p>
<p>蒸馏项引入了更多的锚点来提高表示质量，从而提高了模型的性能</p>
</blockquote>
<ol>
<li>关系知识蒸馏。为了进一步为对比学习提供精细的语义一致性监督，我们引入了<strong>一种关系知识自蒸馏损失到正样本中</strong>。受到 [26, 44, 52] 的工作的启发，关系知识被建模为𝑧′/𝑧′ 𝑖𝑛𝑡𝑒𝑟与<strong>存储在内存队列 M 中的特征锚点之间的余弦相似性</strong>。关系分布，即与负锚点样本的相似性，被强制保持在每个正样本对之间一致。以与前述的内部变换对应的嵌入对 (𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′) 为例，该损失可以表示为： L𝐼𝑛𝑡𝑟𝑎 𝐾𝐿 = −𝑝 � 𝑧′, 𝜏𝑘</li>
</ol>
<p>log 𝑝 � 𝑧𝑖𝑛𝑡𝑟𝑎, 𝜏𝑞</p>
<p>， 𝑝 𝑗 (𝑧, 𝜏) = exp(𝑧 · 𝑚 𝑗/𝜏) Í 𝑖=1 exp(𝑧 · 𝑚𝑖/𝜏) ， (3) 其中 𝑚𝑖 是 M 中存储的第 𝑖 个特征锚点。 𝜏𝑘 和 𝜏𝑞 是温度超参数，分别设置为 0.05 和 0.1。 这个蒸馏项引入了更多的锚点来挖掘细粒度和语义感知的相似关系[44]，提高了表示质量。</p>
<p><strong>总结：</strong> 这一段介绍了一种关系知识蒸馏方法，它为对比学习提供了更精细的语义一致性监督。这种方法使用余弦相似度来度量正样本对之间的关系知识，并使其与负锚点样本的相似性保持一致。这个蒸馏项引入了更多的锚点来提高表示质量，从而提高了模型的性能。</p>
<h3 id="3-2-Masked-Skeleton-Prediction">3.2 Masked Skeleton Prediction</h3>
<p>以下是对输入段落的逐句翻译：</p>
<p><strong>3.2 Masked Skeleton Prediction</strong> <strong>3.2 遮蔽骨骼预测</strong></p>
<p>To further enrich the learned representations by the model, we integrate masked skeleton modeling, and the joint-level feature learning is baked into the training process. This further improves generalization ability, especially for dense prediction downstream tasks, as compared with using only instance-wise discrimination tasks, i.e., contrastive learning. 为了通过模型进一步丰富学到的表示，我们将遮蔽骨骼建模和关节级特征学习融入训练过程中。这进一步提高了泛化能力，特别是对于<strong>密集预测下游任务</strong>，与仅使用逐例判别任务，即对比学习相比。</p>
<p>First, in terms of the masking strategy, previous works utilize Random Mask to randomly select the masked joints in the spatial-temporal dimension. However, given the redundancy of skeleton sequences, the masked joints can be easily inferred by copying adjacent joints in the spatial or temporal dimension, which is not conducive to modeling meaningful relationships in skeletons. To this end, we propose a Topology-based masking strategy, which masks the skeleton at the body-part level instead of the joint level, i.e., trunk, right-hand, left-hand, right-leg, and left-leg. Meanwhile, we divide the sequences into different clips in the temporal dimension, and the same parts are all masked in a clip, as shown in Figure 3. 首先，==在遮蔽策略方面，先前的工作使用随机遮罩在时空维度中随机选择被遮蔽的关节。然而，鉴于骨骼序列的冗余性，可以通过复制时空维度中相邻的关节轻松地推断遮蔽的关节，这对于建模骨骼中有意义的关系是不利的。因此，我们提出了一种基于拓扑的遮蔽策略，它在身体部位级别而不是关节级别上对骨骼进行遮蔽，即躯干、右手、左手、右腿和左腿。同时，我们将序列分成不同的时间片段，在一个片段中相同的部位都被遮蔽，如图3所示==</p>
<p>Based on the above masking strategy, we mask the original skeleton 𝑥 and then feed the masked skeleton 𝑠𝑚𝑎𝑠𝑘 into the encoder 𝑓𝑞 (·) to obtain the corresponding features. To predict the masked skeleton region, we employ a decoder 𝑑𝑒𝑐(·), which takes the encoded features as input and outputs the reconstructed skeleton. The MSE loss between the original data 𝑥 and the predicted data 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 is optimized in the masked region: 基于上述遮蔽策略，我们对原始骨骼𝑥进行遮蔽，然后将遮蔽的骨骼𝑠𝑚𝑎𝑠𝑘输入编码器𝑓𝑞(·)以获取相应的特征。为了预测被遮蔽的骨骼区域，我们使用解码器𝑑𝑒𝑐(·)，它以编码特征为输入并输出重建的骨骼。在遮蔽区域中，原始数据𝑥和预测数据𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡之间的均方误差损失被优化：</p>
<p>L𝑀𝑎𝑠𝑘 = 1 𝑁 𝑁 ∑︁ ||(𝑥 - 𝑑𝑒𝑐 ◦ 𝑓𝑞(𝑠𝑚𝑎𝑠𝑘)) ⊙ (1 - 𝑀)||2, (4) 其中𝑁是所有被遮蔽关节的数量。𝑀是二值遮蔽掩码，其中1和0分别对应于可见关节和被遮蔽关节。1是与𝑀相同形状的全1矩阵。</p>
<p><strong>总结：</strong> 本节讨论了遮蔽骨骼预测方法，用于丰富模型学到的表示。作者提出了一种基于拓扑的遮蔽策略，该策略在身体部位级别而非关节级别上进行遮蔽。遮蔽的骨骼数据由编码器处理并由解码器进行重建。在被遮蔽的区域内，通过均方误差损失来优化原始数据和预测数据之间的一致性。这种方法有助于提高骨骼数据的泛化能力，特别适用于密集预测任务。</p>
<h3 id="3-3-Collaboration-between-Contrastive-Learning-and-Masked-Modeling">3.3 Collaboration between Contrastive Learning and Masked Modeling</h3>
<p>尽管我们提出了用于对比学习和掩蔽骨架预测的新管道，但我们发现，简单地整合这两个范例只会产生平庸的性能收益，如表7所示。这是由于两个任务[30]的特性建模范式之间存在固有的差距，模型不能直接利用它们之间潜在的协同作用。因此，在本部分中，我们将探索合作，并以互利的方式将两个任务联系起来。</p>
<h4 id="1-Novel-Positive-Pairs-as-Connection">1) Novel Positive Pairs as Connection.</h4>
<p><strong>First, we utilize special data views in masked prediction training to provide more diverse positive samples for contrastive learning.</strong> 首先，在遮蔽预测训练中，我们利用特殊的数据视图，为对比学习提供更多不同的正样本。↳</p>
<p><strong>Considering that the masked skeleton naturally simulates occlusion for skeletons, we take masked skeleton 𝑠𝑚𝑎𝑠𝑘 views as challenging positives, to learn the underlying semantic consistency and enhance robustness to occlusion.</strong> 考虑到遮蔽的骨骼自然地模拟了骨骼的遮挡，==我们将遮蔽的骨骼视图视为具有挑战性的正样本==   以学习潜在的语义一致性并增强对遮挡的鲁棒性。</p>
<p><strong>Meanwhile, we also boost contrastive learning by taking the predicted skeletons output by decoder 𝑑𝑒𝑐(·) as positive samples.</strong> 同时，我们还=通过将解码器 𝑑𝑒𝑐(·) 输出的预测骨骼作为正样本来增强对比学习=。</p>
<p><strong>Compared with the masked views, the predicted views contain the inherent noise, uncertainty, and diversity brought by continuous training of the model, which contributes to encoding more diverse movement patterns and thus improves generalization capacity.</strong> 与遮蔽视图相比，预测视图包含了由模型的连续训练引入的固有噪声、不确定性和多样性，这有助于编码更多不同的运动模式，从而提高泛化能力。</p>
<p><strong>In a nutshell, we utilize the masked view 𝑠𝑚𝑎𝑠𝑘 and predicted view 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 as positive samples to connect masked modeling with contrastive learning.</strong> 简而言之，==我们利用遮蔽视图𝑠𝑚𝑎𝑠𝑘和预测视图𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡作为正样本，将遮蔽建模与对比学习连接起来==</p>
<p><strong>We present all positive (embedding) pairs as follows: {(𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′), (𝑧𝑖𝑛𝑡𝑒𝑟, 𝑧′ 𝑖𝑛𝑡𝑒𝑟), (𝑧𝑚𝑎𝑠𝑘, 𝑧′), (𝑧𝑝𝑟𝑒𝑑𝑖𝑐𝑡, 𝑧′)}.</strong> 我们将所有正（嵌入）对表示为{(𝑧𝑖𝑛𝑡𝑟𝑎, 𝑧′), (𝑧𝑖𝑛𝑡𝑒𝑟, 𝑧′ 𝑖𝑛𝑡𝑒𝑟), (𝑧𝑚𝑎𝑠𝑘, 𝑧′), (𝑧𝑝𝑟𝑒𝑑𝑖𝑐𝑡, 𝑧′)}。</p>
<p><strong>They are obtained by the query/key encoder and projector, respectively. Each positive pair is applied to calculate Eq. 1 for contrastive loss and Eq. 3 for distillation loss. Taking (𝑠𝑚𝑎𝑠𝑘, 𝑠′) as an example, it replaces the 𝑧𝑖𝑛𝑡𝑟𝑎 with 𝑧𝑚𝑎𝑠𝑘 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑚𝑎𝑠𝑘 ) for optimization.</strong> 它们分别由查询/键编码器和投影器获得。每个正对都用于计算对比损失的公式1和蒸馏损失的公式3。以(𝑠𝑚𝑎𝑠𝑘, 𝑠′)为例，它用𝑧𝑚𝑎𝑠𝑘 = ℎ𝑞 ◦ 𝑓𝑞 (𝑠𝑚𝑎𝑠𝑘 )替换𝑧𝑖𝑛𝑡𝑟𝑎以进行优化。</p>
<p><strong>Note that we use L𝐶𝑜𝑛 and L𝐾𝐿 to represent the total contrastive loss and distillation loss, respectively, which comprise component losses in the form of Eq. 1 and Eq. 3 calculated for all four positive pairs defined in Eq. 5.</strong> 请注意，我们使用L𝐶𝑜𝑛和L𝐾𝐿分别表示总对比损失和蒸馏损失，它们包括公式1和公式3的组件损失，这些损失是为公式5中定义的所有四个正对计算的。</p>
<p><strong>总结：</strong> 本段讨论了如何利用遮蔽骨骼数据和预测骨骼数据作为正样本来增强对比学习。遮蔽的骨骼模拟了骨骼的遮挡，这有助于学习潜在的语义一致性并提高鲁棒性。与遮蔽视图相比，预测视图包含了模型连续训练带来的固有噪声、不确定性和多样性，这有助于编码更多不同的运动模式，从而提高泛化能力。这些正样本用于对比损失和蒸馏损失的计算，从而提高了训练的效果。</p>
<h4 id="2-High-Level-Semantic-Guidance-高级语义指导">2) High-Level Semantic Guidance.高级语义指导</h4>
<p><strong>On the other hand, the gradients of 𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡 from the contrastive learning branch are propagated to update the reconstructed decoder 𝑑𝑒𝑐(·) as shown in Figure 2.</strong> 另一方面，来自对比学习分支的𝑠𝑝𝑟𝑒𝑑𝑖𝑐𝑡的梯度被传播以更新重建解码器 𝑑𝑒𝑐(·)，如图2所示。</p>
<p><strong>It provides the high-level semantic guidance for the skeleton prediction together with the MSE loss in Eq. 4 which serves as joint-level supervision, leading to better masked prediction learning and higher quality of 𝑠𝑝𝑟𝑑𝑖𝑐𝑡 as positive samples.</strong>==这为骨骼预测提供了高级语义指导，与公式4中的均方误差损失一起，作为关节级别的监督，从而带来更好的遮蔽预测学习和更高质量的𝑠𝑝𝑟𝑑𝑖𝑐𝑡作为正样本==</p>
<p><strong>With the above synergetic designs, the masked prediction task provides novel positive samples as meaningful supplements to the contrastive learning. Meanwhile, with the gradients of contrastive learning propagating to the masked modeling branch, the masked prediction task can be conversely assisted via the high-level semantic guidance provided by contrastive learning task. These designs connect the two tasks and yield better representation quality.</strong> 通过上述协同设计，遮蔽预测任务为对比学习提供了新颖的正样本，作为对比学习的有意义补充。同时，通过对比学习的梯度传播到遮蔽建模分支，遮蔽预测任务可以通过对比学习任务提供的高级语义指导进行相反的协助。这些设计连接了这两个任务并产生更好的表示质量。</p>
<p><strong>总结：</strong> 本段讨论了如何利用对比学习分支的梯度来更新重建解码器，提供高级语义指导以及均方误差损失，以改进遮蔽预测学习，提高正样本质量。这些设计连接了两个任务，产生更好的表示质量。</p>
<h3 id="3-4-Dual-Prompted-Multi-Task-Pretraining-双提示多任务提示的预训练过程">3.4 Dual-Prompted Multi-Task Pretraining 双提示多任务提示的预训练过程</h3>
<blockquote>
<p>对于两种视图 一个是骨架视图 一个是增强的视图</p>
</blockquote>
<p>对于自监督的预训练，<strong>整个模型以多任务的方式对对比学习和掩蔽预测任务进行优化</strong>  然而，对于不同的pretext tasks，如对比学习和掩蔽预测，输入数据来自不同的分布(域)，如增强视图和掩蔽视图。之前的作品直接将它们输入编码器以学习各自的表示。这可能会导致歧义，并干扰从不同数据/任务中学习的特征建模。为此，我们提出了一种新的双提示多任务预训练策略来明确地指导模型从不同的领域/任务中学习。具体而言，设计了领域提示和任务提示两类提示，并将它们作为可培训向量实现，提供培训指导。</p>
<h4 id="1-Domain-Specific-Prompt">1) Domain-Specific Prompt.</h4>
<p><strong>1) Domain-Specific Prompt. To deal with different domains of input, we maintain domain-specific prompts for each input view, i.e., 𝑝𝑖𝑛𝑡𝑒𝑟, 𝑝𝑖𝑛𝑡𝑟𝑎, 𝑝𝑚𝑎𝑠𝑘 , and 𝑝𝑝𝑟𝑟𝑒𝑑𝑖𝑐𝑡 , of which the dimension equals to the skeleton spatial size. Then, these domain-specific prompts are added to the corresponding input data (𝑠∗ means any view): 𝑠∗ = 𝑠∗ + 𝑝∗. (6) These decorated skeletons are fed into the encoder for self-supervised pretraining. The trainable prompts enable the model to learn domain-specific knowledge and achieve better representations.</strong></p>
<ol>
<li>针对不同领域的输入，我们为每个输入视图维护特定于领域的提示，即 𝑝𝑖𝑛𝑡𝑒𝑟，𝑝𝑖𝑛𝑡𝑟𝑎，𝑝𝑚𝑎𝑠𝑘 和𝑝𝑝𝑟𝑟𝑒𝑑𝑖𝑐𝑡，其维度等于骨架空间大小。然后，将这些特定于领域的提示添加到相应的输入数据（𝑠∗表示任何视图）：𝑠∗ = 𝑠∗ + 𝑝∗。这些装饰后的骨架被送入编码器进行自监督预训练。可训练的提示使模型能够学习领域特定的知识并实现更好的表示。</li>
</ol>
<h4 id="2-Task-Specific-Prompt">2) Task-Specific Prompt.</h4>
<p><strong>For task-specific prompts, we apply deep-feature prompt after encoder instead of input-wise prompt to encourage the encoder to extract more general features for various tasks. After obtaining the representations 𝑓𝑒𝑎𝑡∗ = 𝑓𝑞/𝑓𝑘 (𝑠∗) ∈ Rd, we add the task-specific prompts 𝑝𝑐𝑙 or 𝑝𝑚𝑝 to the 𝑓𝑒𝑎𝑡∗. Specifically, 𝑝𝑐𝑙, 𝑝𝑚𝑝 ∈ Rr where the dimension r &lt; d for efficiency [11], are added to the randomly selected r-dimensional channels from the original feature 𝑓𝑒𝑎𝑡∗. If the feature is to feed into the projector ℎ𝑞/ℎ𝑘 (·) for contrastive learning, the 𝑝𝑐𝑙 is added, otherwise, the 𝑝𝑚𝑝 is added for masked prediction. These prompts can effectively learn task-specific knowledge and reduce interference between different pretext tasks.</strong> 2) 针对任务特定提示，我们在编码器之后应用<strong>深度特征提示</strong>，而不是按输入方式的提示，以鼓励编码器为各种任务提取更通用的特征。在获得表示形式 𝑓𝑒𝑎𝑡∗ = 𝑓𝑞/𝑓𝑘 (𝑠∗) ∈ Rd 后，我们将任务特定的提示 𝑝𝑐𝑙 或 𝑝𝑚𝑝 添加到 𝑓𝑒𝑎𝑡∗。具体来说，𝑝𝑐𝑙，𝑝𝑚𝑝 ∈ Rr，其中 r &lt; d 以提高效率，被添加到原始特征 𝑓𝑒𝑎𝑡∗ 中随机选择的 r 维通道。如果特征将被馈送到用于对比学习的投影仪 ℎ𝑞/ℎ𝑘 (·)，则添加 𝑝𝑐𝑙，否则，添加 𝑝𝑚𝑝 用于遮蔽预测。这些提示可以有效地学习任务特定的知识并减少不同预设任务之间的干扰。</p>
<p><strong>Overall, the following objective is applied to the whole model as shown in Figure 2: L = L𝐶𝑜𝑛 + 𝜆𝑚L𝑀𝑎𝑠𝑘 + 𝜆𝑘𝑙L𝐾𝐿, (7) where the loss weight 𝜆𝑚 and 𝜆𝑘𝑙 are set to 40.0, 1.0 in implementation. Note that the prompts are tuned only in the pretraining stage since they are targeted for self-supervised pretext tasks rather than downstream tasks. Therefore, we simply drop all prompts after the pretraining stage.</strong> 总的来说，如图2所示，上述目标适用于整个模型：L = L𝐶𝑜𝑛 + 𝜆𝑚L𝑀𝑎𝑠𝑘 + 𝜆𝑘𝑙L𝐾𝐿，其中损失权重 𝜆𝑚 和 𝜆𝑘𝑙 在实施中设置为40.0和1.0。请注意，提示仅在预训练阶段进行调整，因为它们针对的是自监督预设任务，而不是下游任务。因此，我们在预训练阶段后简单地删除所有提示。</p>
<p><strong>总结：</strong> 这一段讨论了两种类型的提示：特定于领域和特定于任务的提示。特定于领域的提示用于处理不同输入领域，而特定于任务的提示用于鼓励编码器提取更通用的特征以适应不同任务。整个模型的目标函数是这两种提示的组合。这些提示在预训练阶段进行调整，因为它们主要用于自监督预设任务。</p>
<h1>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training  基于图的骨架序列预训练的掩蔽自动编码器</h1>
<p>Abstract 2D poses Skeleton sequence representation learning has shown great advantages for action recognition due to its promis- ing ability to model human joints and topology. How- ever, the current methods usually require sufficient labeled data for training computationally expensive models, which is labor-intensive and time-consuming. Moreover, these methods ignore how to utilize the fine-grained dependen- cies among different skeleton joints to pre-train an effi- cient skeleton sequence learning model that can general- ize well across different datasets. In this paper, we pro- pose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehen- sively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmet- ric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into Graph Convolutional Network (GCN) and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained Skele- tonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and out- performs the state-of-the-art self-supervised skeleton-based action recognition methods on FineGym, Diving48, NTU60 and NTU 120 datasets. Additionally, we obtain comparable performance to some fully supervised methods. The code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/HongYan1123/">https://github.com/HongYan1123/</a> SkeletonMAE</p>
<p><strong>二维骨骼序列</strong>表示学习由于具有<strong>良好的关节建模和拓扑结构建模能力</strong>，在动作识别方面显示出了极大的优势。然而，目前的方法通常需要足够的标记数据来训练计算昂贵的模型，这是劳动密集型和耗时的。此外，这些方法<strong>忽略了如何利用不同骨骼关节之间的细粒度依赖关系来预训练一个高效的骨骼序列学习模型</strong>，该模型能够<strong>很好地跨不同数据集泛化</strong>。在本文中，我们提出了一个有效的骨架序列学习框架，称为<strong>骨架序列学习(SSL)**<strong>Skeleton Sequence Learning</strong>。为了全面捕捉人体姿态并获得具有判别性的骨骼序列表示，我们构建了一个非对称的基于图的编码器-解码器预训练架构，名为SkeletonMAE，该算法</strong>将骨架关节序列嵌入到图卷积网络**(Graph Convolutional Network, GCN)中，并<strong>根据人类先验拓扑知识对隐藏的骨架关节和边缘进行重构</strong>。然后，将预训练的SkeletonMAE编码器与****时空表示学习(STRL)模块<strong>集成【<strong>Spatial-Temporal Representation Learning</strong>】</strong>，以构建SSL框架。大量的实验结果表明，我们的SSL在不同的数据集上具有很好的泛化性，并且在FineGym、Diving48、NTU60和NTU 120数据集上的性能优于目前最先进的基于骨骼的自监督动作识别方法。此外，我们获得了与一些完全监督方法相当的性能。代码可以在https://github.com/HongYan1123/ SkeletonMAE上找到</p>
<h2 id="introduction-3">introduction</h2>
<ol>
<li>
<p>人体动作识别在视频理解领域引起了越来越多的关注[88, 7, 48, 63, 43, 45, 46]，由于其在人机交互[41, 40, 84]、智能监控安全[42, 99]、虚拟现实等方面的广泛应用[2, 14, 64, 54, 55, 89, 30, 80]。从视觉感知的角度来看[24, 47]，即使没有外观信息，人们也可以通过观察关节的运动来识别动作类别。<strong>与 RGB 视频不同[3, 14, 13, 44]，骨架序列只包含人体关键关节的坐标信息，这是高级别的、轻量级的，并且对复杂的背景和各种条件（包括视角、尺度和运动速度）具有鲁棒性[11, 72]</strong>。此外，随着人体姿态估计算法的发展[8, 1]，人体关节（即关键点）的定位方法取得了很大的进展，可以获取准确的骨架序列。目前，现有的 2D 姿态估计方法比 3D 姿态估计方法更准确、更稳健[11]。在图1(a)中，我们使用 HRNet [70] 对 FineGym 数据集[62] 上的两个动作类别进行可视化呈现 2D 姿态。可以看出，2D 姿态能够准确捕捉人体骨架和运动细节。</p>
<p>由于骨架序列具有建模<strong>人体运动中的多粒度和大变化的</strong>有希望的能力，相较于 RGB 数据，它<strong>更适合区分具有细微差异的相似动作</strong>。为了捕捉具有区分性的空时运动模式，大多数现有的基于骨架的动作识别方法[11, 88, 5]是完全监督的，通常需要大量带标签的数据来训练精心设计的模型，这是耗时且劳动密集的。为了缓解有限标签训练数据的问题，最近<strong>自监督骨架动作识别方法</strong>[32, 18, 68]受到了越来越多的关注。一些对比学习方法[32, 18]<strong>采用数据增强生成正负样本对</strong>，但它们在很大程度上依赖于对比对的数量。随着编码器-解码器[67, 51]的流行，一些方法[94, 68]通过<strong>链路重建重构了掩蔽的骨架序列</strong>，以<strong>鼓励按照图形编码器-解码器的范例保持拓扑接近性</strong>。然而，这些方法<strong>通常擅长链路预测和节点聚类，但在节点和图分类方面表现不佳</strong>。对于==准确的动作识别，不同骨架关节之间的细粒度依赖关系（即图分类）是必不可少的==。因此，==先前<strong>基于自监督学习的方法倾向于忽略不同骨架关节之间的细粒度依赖关系</strong>，这限制了自监督骨架表示的泛化性。如图1(b)-©所示，<strong>手臂关节和边缘对于区分这两个相似的动作是至关重要的</strong>。与 MAE [20] 的随机遮罩策略不同，我们的遮罩策略是与动作相关的，并重构给定动作类别中占主导地位的特定肢体或身体部分。我们的 SkeletonMAE 利用先验的人体拓扑知识来引导遮罩骨架关节和边缘的重构，以实现对关节、拓扑和动作的全面感知==</p>
<p>为了解决上述挑战，我们提出了一种高效的骨架序列表示学习框架，称为Skeleton Sequence Learning (SSL)。为了<strong>充分发现不同骨架关节之间的细粒度依赖关系，我们构建了一种新颖的不对称图编码器-解码器预训练架构，称为SkeletonMAE，<strong>它</strong>将骨架关节序列嵌入到图卷积网络（GCN）中</strong>。SkeletonMAE旨在<strong>基于先验的人体拓扑知识重构被遮罩的人体骨架关节和边缘</strong>，<strong>以便推断骨架的底层拓扑结构</strong>，<strong>并获得对人体动作的全面认知</strong>。为了<strong>学习区分性的空时骨架表示</strong>，<strong>预训练的SkeletonMAE编码器与空时表示学习（STRL）模块相结合，以学习空时依赖关系</strong>。最后，SSL在动作识别数据集上进行微调。FineGym、Diving48、NTU 60和NTU 120等广泛的实验结果表明，我们的SSL在不同数据集上具有良好的泛化性能，并在很大程度上优于现有方法。我们的贡献总结如下：</p>
</li>
</ol>
<ul>
<li>为了全面捕捉人体姿态并获得有区别的骨架序列表示，我们提出了一种名为SkeletonMAE的基于图的编码器-解码器预训练架构，将骨架关节序列嵌入GCN，并<strong>利用先验的人体拓扑知识</strong> <strong>指导底层遮罩关节和拓扑的重构</strong>。</li>
<li>为了<strong>学习骨架序列的全面空时依赖关系</strong>，我们提出了一种<strong>高效的骨架序列学习框架</strong>，称为Skeleton Sequence Learning（SSL），它将预训练的SkeletonMAE编码器与空时表示学习（STRL）模块相结合。</li>
<li>在FineGym、Diving48、NTU 60和NTU 120等数据集上的广泛实验结果表明，我们的SSL在不同数据集上具有良好的泛化性能，并在很大程度上优于现有的自监督骨架动作识别方法，与现有的完全监督方法相比，性能相当。</li>
</ul>
<h2 id="related-work">related work</h2>
<h3 id="Self-supervised-Learning-for-Skeleton-Sequence">Self-supervised Learning for Skeleton Sequence.</h3>
<p><strong>Self-supervised Learning for Skeleton Sequence.</strong></p>
<p>针对未标记的骨架数据，最近自监督学习取得了令人鼓舞的进展，以学到更有效的表示。对于对比学习方法，AS-CAL [58] 和 SkeletonCLR [32] 使用动量编码器进行对比学习，采用了单流骨架序列。AimCLR [18] 采用极端的数据增强策略，添加了额外的困难对比对。<strong>大多数对比学习方法采用数据增强生成正负样本对，但它们在很大程度上依赖于对比对的数量</strong>。对于<strong>生成学习方法</strong>，LongT GAN [94] 提出了编码器-解码器来重构掩蔽的输入序列骨架。基于 LongT GAN，P&amp;C [68] 强化了编码器并削弱了解码器以获取特征表示。Wu 等人 [85] 提出了一种用于自监督3D骨架动作识别的空间-时间掩蔽自编码器框架。Colorization [90] 使用了三对编码器-解码器框架，从骨架点云中学习空间-时间特征。由于<strong>重构标准的限制</strong>，先前的生成方法通常<strong>未能充分发现不同骨架关节之间的细粒度空时依赖关系</strong>。在我们的SkeletonMAE中，我们利用先验的人体拓扑知识推断骨架序列，并获得对动作的全面认知。</p>
<p>根据输入内容总结 “Self-supervised Learning for Skeleton Sequence.” 章节：</p>
<p>在这一章节中，作者讨论了最近在未标记骨架数据上取得的自监督学习的进展。他们提到了对比学习方法，如AS-CAL和SkeletonCLR，以及生成学习方法，如LongT GAN和P&amp;C。对比学习方法采用了数据增强生成正负样本对，而生成学习方法则使用编码器-解码器框架进行骨架序列的重构。然而，先前的生成方法由于重构标准的限制，<strong>未能完全发现不同骨架关节之间的细粒度空时依赖关系</strong>。为了解决这个问题，作者介绍了他们的方法SkeletonMAE，该方法利用先验的人体拓扑知识来推断骨架序列，以获得对动作的全面认知。这一方法旨在克服以往方法的局限性，提供更全面的空时表示。</p>
<h2 id="3-Methodology-2">3. Methodology</h2>
<p>在本节中，我们介绍了骨架序列学习(SSL)的细节，它包含两部分:1)预训练的骨架序列学习和2)基于预训练的骨架序列学习对下游数据集进行微调。</p>
<h3 id="3-1Pre-training-SkeletonMAE">3.1Pre-training SkeletonMAE</h3>
<p>在本节中，我们将介绍<strong>基于图的非对称编码-解码器预训练架构</strong>skeleton mae，在没有监督的情况下学习人体骨骼序列表示。由于图同构网络(GIN)[87]【How powerful are graph neural networks?】提供了更好的归纳偏差，因此它更适合于学习更广义的自监督表示[22]。因此，我们采用GIN作为骨架mae的骨干。此外，我们在表4中评估了骨架mae的不同backbones，包括GIN[87]、GCN[28]和GAT[75]。</p>
<h4 id="3-1-1SkeletonMAE-Structure">3.1.1SkeletonMAE Structure</h4>
<ol>
<li></li>
</ol>
<p>受到在自然语言处理 [9]、图像识别 [20] 和视频识别 [73] 中掩蔽自编码器（MAE）有效表示学习的启发，我们专注于人体骨架序列，并构建了一种名为SkeletonMAE的<strong>不对称图形编码器-解码器预训练架构，将骨架序列及其先验拓扑知识嵌入到GIN中</strong>。SkeletonMAE的实现遵循图生成自监督学习的范例。</p>
<ol start="2">
<li>
<hr>
<p>我们遵循Kinetics Skeleton数据集的关节标签 [63]。具体而言，如图2(d)所示，我们根据身体的自然部位将所有的N = 17个关节分为R = 6个本地区域：V0, …, V5。值得注意的是，与MAE [20] 的随机遮罩策略选择骨架关节相比，我们的<strong>遮罩策略是动作敏感的</strong>，并且<strong>重构特定支配给定动作类的肢体或身体部位</strong>。然后，我们<strong>遮罩这些骨架区域，并让SkeletonMAE根据相邻关节重构遮罩关节特征及其边缘。通过重构遮罩的骨架关节和边缘，SkeletonMAE可以推断关节的==底层拓扑结构==，获得对动作的全面认知</strong>。</p>
</li>
<li></li>
</ol>
<p>如图2所示，SkeletonMAE是一种不对称的编码器-解码器架构，包括一个编码器和一个解码器。<strong>编码器由LD GIN层组成</strong>，将输入的2D骨架数据映射到隐藏特征。解码器仅包含一个GIN层，<strong>根据重构标准重新构建隐藏特征</strong>。</p>
<ol start="4">
<li></li>
</ol>
<p>根据先前的人体骨架知识，人体骨架可以表示为一个<strong>以关节为顶点、以肢体为边的图形，我们将人体骨架形式化为以下图形结构</strong>。</p>
<ol start="5">
<li></li>
</ol>
<p>**N个人体骨架关节和T帧的二维坐标的骨架序列经过以下方式预处理。**具体而言，我们将所有骨架关节（节点特征）及其拓扑(<strong>邻接矩阵</strong>)嵌入到一个结构G中，骨架结构和关节特征融合以获得关节序列矩阵S ∈ RN×T×2。然后，<strong>使用可学习的参数将S线性变换为S ∈ RN×T×D</strong>。我们经验性地将T和D设置为64。</p>
<p>对于来自S的每个骨架帧X ∈ RN×D，让G = (V,A,X)表示一个骨架，其中V = {v1, v2, …, vN}是包含所有骨架关节的节点集合，N = |V|是关节的数量。关节数量为N = 17。A ∈ {0, 1}N×N是一个邻接矩阵，其中Ai,j = 1表示关节i和j物理上连接，否则为0。vi的特征表示为xi ∈ R1×D。GE、GD分别表示GIN编码器和GIN解码器。</p>
<p>根据输入内容总结 “SkeletonMAE Structure” 章节：</p>
<p>在这一章节中，作者介绍了SkeletonMAE结构，这是一种不对称图形编码器-解码器预训练架构，专注于学习人体骨架序列的有效表示。该架构包括一个编码器和一个解码器，其中编码器由LD GIN层组成，用于将输入的2D骨架数据映射到隐藏特征，而解码器仅包含一个GIN层，用于在重构标准的监督下重新构建隐藏特征。<strong>骨架序列被处理为图形结构，其中骨架关节被嵌入到结构G中，形成一个以关节为顶点、以肢体为边的图形。作者还详细描述了骨架序列的预处理步骤和图形结构的表示，以及相应的GIN编码器和解码器。</strong></p>
<h4 id="3-1-2-Skeleton-Joints-Masking-and-Reconstruction">3.1.2 Skeleton Joints Masking and Reconstruction</h4>
<p>鉴于先前的人体骨架拓扑A已嵌入（图2），并且我们在3.1.1节中指定了关节的聚合方式。受到GraphMAE [22] 随机重构遮罩图节点的启发，我们的SkeletonMAE基于先前的骨架拓扑<strong>重构了遮罩骨架特征X</strong>，<strong>而不是重构图结构A</strong> [71, 17] 或<strong>同时重构结构A和特征X</strong> [60, 56]。</p>
<p>**为了遮罩骨架关节特征，我们从V = {V0, …, V5}中随机选择一个或多个关节集，其中包括一个子集V ⊆ V用于遮罩。**对于人体骨架序列，每个关节与其相邻的一些关节进行通信，以表示特定的动作类。因此，<strong>不可行的是对所有动作类的所有关节集进行遮罩。然后，它们的每个特征都用可学习的掩码令牌向量[MASK] = x[M] ∈ RD 进行遮罩</strong>。因此，对于遮罩特征矩阵X中的Vi ∈ V中的关节特征xi，可以定义为xi = x[M]如果vi ∈ V，否则xi = xi。<strong>我们将X ∈ RN×D 设置为SkeletonMAE的输入关节特征矩阵</strong>，其中X中的每个关节特征可以定义为xi = x[M]，xi，i = 1, 2, · · · ,N。因此，遮罩骨架序列可以被形式化为G = (V,A,X)，**SkeletonMAE的目标是在给定局部观察到的关节特征X和输入邻接矩阵A的情况下重构遮罩骨架特征。**SkeletonMAE的重构过程被形式化为：</p>
<p>�=��(�,�),�∈��×�ℎ<em>H</em>=<em>GE</em>(<em>A</em>,<em>X</em>),<em>H</em>∈<em>RN</em>×<em>D**h</em></p>
<p>�=��(�,�),�∈��×�<em>Y</em>=<em>G**D</em>(<em>A</em>,<em>H</em>),<em>Y</em>∈<em>RN</em>×<em>D</em></p>
<p>其中H和Y分别表示编码器输出和解码器输出。SkeletonMAE的目标可以被形式化为最小化X和Y之间的差异。</p>
<h4 id="3-1-3-Reconstruction-Criterion">3.1.3 Reconstruction Criterion</h4>
<p><strong>重构准则章节</strong></p>
<p>对于遮罩自编码器的常见重构<strong>准则在图像和视频任务中是均方误差</strong>（MSE）。然而，对于骨架序列，关节特征的多维连续性使得MSE很难实现有希望的特征重构，因为<strong>MSE对于特征的维度和向量范数很敏感</strong>。受到观察到的l2-归一化在余弦误差中将向量映射到单位超球面并且显著提高训练稳定性的启发，我们使用余弦误差作为重构准则。</p>
<p>为了<strong>使重构准则集中在</strong>  <strong>不平衡的易和难样本中更难的样本上</strong>，我们引入了SkeletonMAE的Re-weighted Cosine Error（RCE）。**RCE的基本思想是通过将余弦误差与β ≥ 1的幂进行缩放，****可以在训练中降低易样本的贡献。对于高置信度的预测，它们对应的余弦误差通常小于1，并且当缩放因子β &gt; 1时迅速衰减为零。**形式上，给定原始特征X ∈ RN×D和重构输出Y ∈ RN×D，RCE定义为：</p>
<p>其中表示所<strong>有遮罩关节上重构特征和输入特征之间相似性差距的平均值</strong>。在我们的工作中，β设置为2。</p>
<p>通过训练SkeletonMAE来重构骨架序列，预训练的SkeletonMAE可以全面感知人体骨架结构并获取具有区分性的动作表示。在预训练之后，SkeletonMAE可以优雅地嵌入到Skeleton Sequence Learning（SSL）框架中进行微调。</p>
<h3 id="3-2-Fine-tuning-for-Skeleton-Action-Recognition">3.2. Fine-tuning for Skeleton Action Recognition</h3>
<p>为了评估骨架动作识别的泛化能力，我们在预训练的骨架动作识别模型的基础上，构建了一个完整的骨架动作识别模型——骨架序列学习(skeleton Sequence Learning, SSL)。为了捕捉多人交互，我们整合了两个预训练的骨骼mae编码器来构建时空表征学习(STRL)模块，如图3(b)-©所示。整个SSL由一个Mlayer STRL模型和一个分类器组成。SSL模型最终在具有交叉熵损失的骨架动作识别数据集上进行了微调，以识别动作。</p>
<h4 id="3-2-1-Spatial-Temporal-Representation-Learning">3.2.1 Spatial-Temporal Representation Learning</h4>
<p><strong>Spatial-Temporal Representation Learning章节</strong></p>
<p>STRL包含两个预训练的SkeletonMAE编码器，用于空间建模（SM）。SM的输入是骨架序列S。SM的输出通过1×1卷积与输入连接，以实现残差连接（图3（b））。</p>
<p>如图3（c）所示，骨架序列S ∈ RN×T×D首先与可学习的时间位置嵌入PE相加，得到骨架序列特征H(l)t ∈ RP×N×D(l)。为了建模多个人类骨架的相互作用，我们从H(l)t获取两个个体特征（P = 2），即两个人的H(l)t,0 ∈ RN×D(l)和H(l)t,1 ∈ RN×D(l)。在这里，我们以第0个人的关节特征为例，第1个人的操作类似。我们将关节表示H(l)t,0和关节的先验知识eA发送到SM模块：</p>
<p>SM(�(�)�,0)=Repeat(SP(GE � �,�(�)�,0);�)⊕�(�)�,0SM(<em>H</em>(<em>l</em>)<em>t</em>,0)=Repeat(SP(GE<em>A**e</em>,<em>H</em>(<em>l</em>)<em>t</em>,0);<em>N</em>)⊕<em>H</em>(<em>l</em>)<em>t</em>,0</p>
<p>其中GE是SkeletonMAE编码器，SP(·)表示汇总，Repeat(·;N)表示在求和池化之后将单个关节重复成N个关节表示，然后与H(l)t,0残差相连接以获取全局关节表示SM(H(l)t,0)。通过这种方式，SM模块可以通过单个关节表示获取全局信息，并通过所有关节表示对一些关节特征进行约束。类似地，以相同的方式获得SM(H(l)t,1)。如图3©所示，我们得到包含第0个人和第1个人之间的动作交互的关节特征SM(H(l)t)。根据图卷积的更新规则，可以从H(l)t中获得H(l+1)t在多层GCN中。有关详细信息，请参阅第D节的附录。最终的骨架序列表示定义如下：</p>
<p>�(�+1)�=�(SM(�(�)�)�(�))<em>H</em>(<em>l</em>+1)<em>t</em>=<em>σ</em>(SM(<em>H</em>(<em>l</em>)<em>t</em>)<em>W</em>(<em>l</em>))</p>
<p>其中W(l)表示第l层的可训练权重矩阵，σ(·)表示ReLU激活函数。然后，我们采用广泛使用的多尺度时间池化[5, 39]来获取最终输出。最后，由MLP和softmax组成的分类器预测动作类别。</p>
<p><strong>总结：</strong> Spatial-Temporal Representation Learning（STRL）通过两个预训练的SkeletonMAE编码器进行空间建模，通过图卷积网络（GCN）学习多层表示，最终采用多尺度时间池化获取最终输出，用于预测动作类别。该过程中利用了骨架序列的时空信息，以获得更丰富的表示。</p>
<h1>Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition基于<strong>小动作集</strong>的对比学习在无监督骨架动作识别中的应用</h1>
<h2 id="abstract-2">abstract</h2>
<p>自监督的预训练范式在基于骨架的动作识别中取得了巨大成功。然而，这些方法<strong>平等地对待动态部分和静态部分，并缺乏针对不同部分的自适应设计，这对动作识别的准确性产生了负面影响</strong>。为了实现对两个部分的自适应动作建模，我们提出了一种称为Actionlet-Dependent Contrastive Learning (ActCLR)的方法。动作元（actionlet）被定义为<strong>人体骨架的区分性子集</strong>，有效地<strong>将动作区域分解为更好的动作建模</strong>。具体来说，通过与没有动作的静态锚点进行对比，我们以无监督的方式提取了骨架数据的动作区域，这充当了<strong>动作元</strong>。然后，以动作元为中心，构建了一种动作自适应的数据转换方法。 ==不同的数据转换==<strong>被应用于动作元和非动作元区域，引入更多的多样性</strong>，同时保持它们各自的特征。同时，我们提出了一种<strong>语义感知的特征汇聚方法</strong>，以<strong>区分的方式</strong>构建了<strong>动态和静态区域之间的特征表示</strong>。在NTU RGB+D和PKUMMD上进行的广泛实验表明，所提出的方法实现了显著的动作识别性能。更多的可视化和定量实验证明了我们方法的有效性。</p>
<p>现在让我为您解释摘要的要点</p>
<ol>
<li>自监督的预训练方法在基于骨架的动作识别中取得了巨大成功。</li>
<li>现有方法在处理骨架数据时<strong>平等地对待动态和静态部分，而缺乏自适应设计，这降低了动作识别的准确性。</strong></li>
<li>为了实现自适应的动作建模，作者提出了Actionlet-Dependent Contrastive Learning（ActCLR）方法。</li>
<li><strong>动作元（actionlet）是人体骨架的区分性子集，用于更好地捕捉动作区域。</strong></li>
<li>通过对比动态和没有动态的静态部分，作者以无监督的方式提取了动作区域。</li>
<li>作者构建了一种动作自适应的数据转换方法，以区分地应用于动作元和非动作元区域，引入更多的多样性，同时保持它们各自的特征。</li>
<li>作者还提出了一种语义感知的特征汇聚方法，用于以区分的方式构建动态和静态区域之间的特征表示。</li>
<li>在NTU RGB+D和PKUMMD数据集上进行的实验表明，所提出的方法在动作识别方面表现出色。</li>
<li>进一步的可视化和定量实验进一步证明了该方法的有效性。</li>
</ol>
<p>总的来说，这项研究旨在提高基于骨架的动作识别的准确性，通过引入动作元和自适应设计，以更好地捕捉不同部分的动作特征。该方法在多个数据集上进行了验证，取得了显著的性能提升。</p>
<blockquote>
<p>提出了动作元子集的概念，设定动态域和静态域，【将数据转换成动态数据和静态数据】</p>
<p>提出了一种语义感知的特征汇聚方式，区分的构建动态和静态数据的特征表示</p>
</blockquote>
<h2 id="introduction-4">introduction</h2>
<ol>
<li>引言 骨架使用3D坐标位置表示人体关节。与RGB视频和深度数据相比，骨架更轻便、有隐私保护并紧凑，用于表示人体运动。由于骨架在分析中更容易且更具区分度，因此已被广泛用于动作识别任务[19,23,31,32,46,48]。监督式的基于骨架的动作识别方法[3,27,28]已经取得了令人印象深刻的性能。然而，它们的成功<strong>高度依赖于大量的标记训练数据</strong>，这在获取时代价昂贵。为了摆脱对完全监督的依赖，引入了<strong>自监督学习[16, 32, 34, 49]到基于骨架的动作识别中</strong>。它采用了一个两阶段范式，即首先应用预文本任务进行无监督预训练，然后进行微调以用于下游任务。</li>
</ol>
<p>根据学习范式，所有方法可以分为两类：基于重建的[14, 32, 41]和基于对比学习的方法。基于<strong>重建的方法通过预测遮蔽的骨架数据来捕获时空相关性</strong>。Zheng等人[49]<strong>首次提出了为基于重建的方法通过预测遮蔽的骨架数据来捕获时空相关性</strong>。此外，基于<strong>对比学习的方法近来显示出了显著的潜力</strong>。这些方法利用<strong>骨架转换生成正样本/负样本</strong>。Rao等人[24]应用了Shear和Crop作为数据增强。<strong>Guo等人[8]进一步提出使用更多的增强，例如旋转、遮罩和翻转，以提高对比学习的一致性</strong>。这些对比学习方法在前一方法中<strong>统一对待骨架序列的不同区域</strong>。然而，运动区域包含更丰富的动作信息，并对动作建模起更大的贡献。因此，在以前的工作中，直接对所有区域应用数据转换是次优的，这可能会对运动相关信息造成过多的干扰。<strong>例如，在举手动作中，如果对手部关节应用遮罩转换，则会完全破坏手部举高的运动信息。这将导致误报问题，即由于正样本之间的信息丢失而导致的语义不一致。因此，有必要为数据序列中的运动和静态区域采用可区分的设计。</strong></p>
<p>为了解决这些问题，我们提出了一种新的基于动作单元的对比学习方法（ActCLR），**通过不同对待运动和静态区域，<strong>如图1所示。动作单元[38]被定义为==骨架关节的连接结构==。它期望==对一个动作具有很高的代表性，并具有很高的区分度==，以区分该动作与其他动作</strong>。以前的工作中，动作单元是以监督方式定义的，这依赖于动作标签，并与自监督的预文本任务之间存在差距。<strong>为此，在无监督学习环境下，我们建议通过</strong>将动作序列与所有训练数据的平均运动进行比较来获取动作单元，以引导对比学习。**详细来说，<strong>平均运动被定义为数据集中所有系列的平均值。因此，平均运动被用作没有运动的静态锚点。我们将动作序列与平均运动进行对比，以获取差异最大的区域。认为该区域是发生运动的区域，即动作单元。</strong></p>
<p>基于这个动作单元，我们设计了一种<strong>运动自适应转换策略</strong>。==动作单元区域通过执行提出的语义保持数据转换而进行转换==。具体而言，我们**==仅对非动作单元区域应用更强的数据变换==在运动区域的干扰较小的情况下，这种运动自适应转换策略使模型学到更好的语义一致性并获得更强的泛化性能**。同样，我们利用了==**语义感知的特征汇聚方法==。通过提取动作单元区域的特征，这些特征可以更好地代表运动，而不受静态区域中语义的干扰。</p>
<p>我们在NTU RGB+D[17,26]和PKUMMD[18]数据集上进行了彻底的实验和详细的分析，以证明我们方法的优越性。与最先进的方法相比，我们的模型在自监督学习中取得了显著的结果。</p>
<p>总之，我们的贡献总结如下： • 我们提出了一种新颖的无监督动作单元对比学习方法。无监督动作单元被挖掘为与静态锚点，即所有训练数据的平均运动相比，最具区分性的骨架区域。 • 我们设计了一种<strong>运动自适应转换策略用于对比学习</strong>。在动作单元区域，我们采用了语义保持的数据转换。</p>
<p>以下是文章介绍部分的要点总结：</p>
<ul>
<li>骨架数据是一种轻量级、隐私保护且紧凑的人体运动表示方法，已广泛应用于动作识别任务。</li>
<li>以一种互利监督的方式</li>
<li>传统的监督式骨架动作识别方法需要大量标记训练数据，而获取这些数据既昂贵又耗时。</li>
<li>自监督学习已经引入到骨架动作识别中，以减少对完全监督的依赖。</li>
<li>==方法可以分为基于重建和基于对比学习两类，<strong>对比学习方法</strong>近来表现出了潜力==</li>
<li>以前的对比学习方法通常对待骨架序列的不同区域，但<strong>运动区域包含更多的动作信息，因此需要区分对待。</strong></li>
<li>以前的对比学习方法</li>
<li>作者提出了一种新的<strong>基于动作单元的对比学习方法</strong>（ActCLR），该方法通过<strong>区分对待运动和静态区域来改善自监督学习的性能</strong>。</li>
<li>动作单元是高度代表性且有区分度的骨架关节连接结构，可以用于区分不同的动作。</li>
<li>作者提出了一种<strong>运动自适应转换策略，该策略仅对非动作单元区域应用更强的数据转换，以提高模型的泛化性能</strong>。</li>
<li>==语义感知的特征汇聚方法有助于更好地代表运动单元数据==</li>
<li>在NTU RGB+D和PKUMMD数据集上的实验表明，该方法在自监督学习中取得了显著的结果。</li>
</ul>
<h2 id="related-work-2">related work</h2>
<h3 id="Skeleton-Based-Action-Recognition">Skeleton-Based Action Recognition</h3>
<p>Skeleton-Based Action Recognition（基于骨骼的动作识别）是计算机视觉研究中的一个基础且具有挑战性的领域。先前的基于骨骼的动作识别方法通常利用骨骼关节的几何关系来实现。最新的方法更加关注深度神经网络。Du等人应用分层RNN来处理身体关键点。还提出了基于注意力的方法，以自动选择重要的骨骼关节和视频帧，以更自适应地学习有关骨骼关节的同时出现情况。然而，循环神经网络经常受到梯度消失问题的困扰，这可能会导致优化问题。最近，图卷积网络引起了更多关于基于骨骼的动作识别的关注。为了从骨骼数据中提取空间和时间结构特征，Yan等人提出了时空图卷积网络。为了使图形表示更加灵活，注意机制被应用于[3, 27, 28]以自适应地捕获基于空间组成和时间动态的判别特征。</p>
<p><strong>Skeleton-Based Action Recognition章节解释</strong>：</p>
<p>Skeleton-Based Action Recognition指的是使用人体骨骼关节的空间坐标信息来识别人类动作的一种计算机视觉任务。在这个领域，研究人员试图通过分析骨骼关节之间的几何关系以及时间序列数据来实现对人类动作的自动识别。早期的方法主要依赖于几何关系和手工设计的特征提取器，但这些方法通常需要大量的人工标记数据。</p>
<p>随着深度学习方法的发展，基于骨骼的动作识别取得了显著的进展。现代方法采用深度神经网络来学习从骨骼数据中提取有关动作的特征。这些方法可以自动学习骨骼关节之间的关系以及动作的时空特征，从而在动作识别任务中取得更好的性能。</p>
<p>最近，图卷积网络成为基于骨骼的动作识别中的一个研究热点。这些网络可以更好地捕获骨骼关节之间的关系，从而提高了动作识别的准确性。此外，注意力机制也被引入以自适应地关注对动作识别最重要的信息。因此，Skeleton-Based Action Recognition是一个多领域交叉的研究领域，涉及计算机视觉、深度学习和模式识别等多个方面。</p>
<h3 id="Contrastive-Learning">Contrastive Learning</h3>
<p>对比表示学习可以追溯到[9]。以下方法[1, 13, 35, 39, 42]通过将正对比对与负对比对进行对比学习，使正对比对之间的表示比负对比对之间的表示更相似来学习表示。研究人员主要关注如何构建对以学习稳健的表示。由Chen等人提出的SimCLR [2] 使用一系列数据增强方法，如随机裁剪、高斯模糊和颜色扭曲来生成正样本。He等人 [10] 应用了一个采用队列存储负样本的内存模块，队列会随着训练不断更新。在自监督骨骼动作识别中，对比学习也吸引了众多研究人员的关注。Rao等人 [24] 应用了MoCo进行单流的对比学习。为了利用跨流知识，Li等人 [15] 提出了一种多视图对比学习方法，而Thoker等人 [34] 则采用多个模型来学习不同的骨骼表示。Guo等人 [8] 提出使用更极端的数据增强，大大提高了对比学习的效果。Su等人 [33] 通过感知运动一致性和连续性提出了新颖的表示学习方法。在MoCo v2 [10] 后继续，他们利用InfoNCE损失来优化对比学习：</p>
<p>其中，zi q = gq(fq(Xi q)) 和 zi k = gk(fk(Xi k))。K = PM j=1 exp(sim(zi q,mj)/τ) 和 τ 是温度超参数。fq(·) 是在线编码器，fk(·) 是离线编码器。gq(·) 是在线投影器，gk(·) 是离线投影器。离线编码器fk(·)通过在线编码器fq(·)的动量更新为fk ← αfk + (1 − α)fq，其中α是动量系数。mj是存储在内存库M中的负样本。sim(·, ·) 是余弦相似度。</p>
<p><strong>Contrastive Learning章节解释</strong>：</p>
<p>Contrastive Learning（对比学习）是一种自监督学习方法，其主要目标是学习数据表示，使来自相同类别的样本在表示空间中更加接近，而来自不同类别的样本更加分散。在骨骼动作识别中，对比学习被用来学习如何更好地表示骨骼数据以提高动作识别性能。</p>
<p>该章节提到了对比学习的发展历史和不同的方法，包括SimCLR和MoCo等。这些方法通常通过构建正对比对（相似样本）和负对比对（不相似样本）来进行训练，从而学习数据的有用表示。<strong>对比学习的一个关键优势是它可以在无监督的情况下进行训练，因此不需要大量的标记数据</strong>。</p>
<p>该章节还介绍了对比学习中的一些关键概念，如<strong>温度超参数、动量系数和余弦相似度</strong>。这些概念在对比学习中起着重要的作用，帮助优化学习过程。</p>
<p>总之，对比学习是一种有效的自监督学习方法，可用于提高骨骼动作识别等任务的性能。它通过将相似性信息嵌入数据表示中来帮助模型更好地理解和区分不同的数据样本。</p>
<h2 id="method">method</h2>
<h3 id="3-1Unsupervised-Actionlet-Selection">3.1Unsupervised Actionlet Selection</h3>
<blockquote>
<p>这一段 设定了怎样选择动作元区域</p>
<p>利用平均数据 X^和 X原数据进行相似度计算</p>
<p>对于动作相似性 有消极影响 表示 使得动作与平均数据 不相似 则就是运动单元，否则就是静止单元</p>
</blockquote>
<ol>
<li>Traditional actionlet mining methods rely on the action label to identify the motion region, which cannot be employed in the unsupervised learning context.
<ul>
<li>传统的动作部分挖掘方法依赖于动作标签来确定动作区域，这在无监督学习情境下无法应用。</li>
<li>解释：传统方法通常需要动作标签来确定哪些部分是与动作相关的，但这种方法在无监督学习中不适用，因为无监督学习不依赖于动作标签。</li>
</ul>
</li>
<li>Inspired by contrastive learning, we propose an unsupervised spatio-temporal actionlet selection method to mine the motion region as shown in Fig. 2.
<ul>
<li>受到对比学习的启发，我们提出了一种无监督的时空动作部分选择方法，用于挖掘动作部分，如图2所示。</li>
<li>解释：受到对比学习的启发，作者提出了一种无监督的方法来选择包含动作信息的部分，以便更好地进行动作识别。</li>
</ul>
</li>
<li>The actionlet is obtained by comparing the differences between an action sequence and the static sequence where we assume no motion takes place.
<ul>
<li>==动作部分是通过比较动作序列与静态序列之间的差异来获得的==我们假设在静态序列中没有动作发生。</li>
<li>解释：动作部分是通过将动作序列与没有动作发生的静态序列进行比较来确定的，这有助于找到包含动作的区域。</li>
</ul>
</li>
</ol>
<blockquote>
<p>使用静态数据作为数据锚点【静态数据视为没有发生动作的序列 也就是所有的数据的平均运动数据】</p>
</blockquote>
<h4 id="Average-Motion-as-Static-Anchor">Average Motion as Static Anchor</h4>
<ol>
<li>In the process of obtaining the sequence without action occurrence, we observe that most of the action sequences have no action in most of the regions.
<ul>
<li>在获取没有动作发生的序列的过程中，我们观察到大多数动作序列在大多数区域都没有动作。</li>
<li>解释：在寻找没有动作发生的序列时，作者观察到大多数动作序列的大部分区域都没有动作。</li>
</ul>
</li>
<li>The motion usually occurs in a small localized area, such as the hand or head.
<ul>
<li>运动通常发生在一个小的局部区域，例如手部或头部。</li>
<li>解释：动作通常发生在身体的特定区域，如手部或头部，而 不是整个身体都在运动。</li>
</ul>
</li>
<li>Therefore, as shown in Fig. 4, we can easily obtain the static anchor via averaging all the actions in the dataset, since most of the sequence has no motion in most of the regions and this average is a relatively static sequence.
<ul>
<li>因此，如图4所示，我们可以通过对数据集中的所有动作进行平均来轻松获得静态锚点，因为大多数序列在大多数区域都没有运动，而这个平均值是一个相对静态的序列。</li>
<li>解释：作者通过对数据集中的所有动作序列进行平均来获得静态锚点。由于大多数区域没有运动，所以这个平均值可以看作是相对静态的序列。</li>
</ul>
</li>
<li>It is formalized as:
<ul>
<li>公式化表示如下：</li>
<li>解释：这是对上述概念的正式数学表示，用于表示如何计算静态锚点的平均值。</li>
</ul>
</li>
</ol>
<p>在这段文字中，作者提到了如何获取静态锚点，即用于对比动作部分的相对静态的序列。作者观察到大多数动作序列的大部分区域都没有动作，因此可以通过对数据集中的所有动作进行平均来获得这个静态锚点。这个静态锚点在后续的动作部分挖掘中起到关键作用。</p>
<h4 id="Difference-Activation-Mapping-for-Actionlet-Localization">Difference Activation Mapping for Actionlet Localization</h4>
<ol>
<li></li>
</ol>
<p>这段文字描述了如何在骨骼序列中找到动作发生的区域。首先，他们将骨骼序列输入到离线编码器中，以获取密集特征。然后，他们计算这些特征之间的余弦相似度。接着，他们反向传播并反转相似度的梯度到密集特征，以获得神经元的重要性权重。这些权重通过全局平均池化计算。</p>
<p>总结：该段落介绍了一个方法，用于确定骨骼序列中动作发生的位置。该方法通过计算特征之间的余弦相似度来识别动作发生的位置，并使用反向传播和梯度反转来计算神经元的重要性权重。这有助于理解动作在骨骼序列中的位置,并减少相似的区域</p>
<ol>
<li>These importance weights capture the magnitude of the effect of each channel dimension on the final difference. 这些重要性权重捕捉了每个通道维度对最终差异的影响幅度。</li>
<li>Therefore, these weights αi 因此，这些权重 αi</li>
<li>c are considered difference activation mapping. c 被视为差异激活映射。</li>
<li>We perform a weighted combination of the difference activation mapping and dense features as follows: 我们执行以下方式的差异激活映射和密集特征的加权组合：</li>
<li>where σ(·) is the activation function and Gvv is the adjacency matrix of skeleton data for importance smoothing</li>
<li>The linear combination of maps selects features that have a negative influence on the similarity. 地图的线性组合选择对相似度产生负面影响的特征。</li>
<li><strong>The actionlet region is the area where the value of the generated actionlet Ai 动作区域是生成的动作区域 Ai 的值超过一定阈值的区域，</strong></li>
<li><strong>tv exceeds a certain threshold, while the non-actionlet region is the remaining part. 超过一定阈值，而非动作区域则是其余部分。</strong></li>
</ol>
<p>这段文字描述了如何使用重要性权重来捕捉每个通道维度对最终差异的影响幅度，并将这些权重视为<strong>差异激活映射</strong>。然后，他们执行了一种加权组合，将差异激活映射与密集特征相结合，**以选择对相似度产生负面影响的特征。**最终，==他们定义了动作区域和非动作区域，以区分特征值是否超过了一定阈值==</p>
<p>总结：这段文字描述了如何使用重要性权重来选择对相似度产生负面影响的特征，以定义动作区域和非动作区域。这有助于识别动作发生的位置。</p>
<h2 id="3-2-Actionlet-Guided-Contrastive-Learning">3.2. Actionlet-Guided Contrastive Learning</h2>
<h3 id="Actionlet-Transformation-Tact">Actionlet Transformation Tact:</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/image-20231023111545179.png" alt="image-20231023111545179"></p>
<blockquote>
<p>这段文字描述了在“Actionlet Transformation” 过程中应用的多种数据转换方法，包括空间、时间和时空变换，以及Skeleton AdaIn 方法，用于传输与动作无关的噪声信息，这些转换方法有主语丰富数据并保持动作信息</p>
</blockquote>
<h3 id="Non-Actionlet-Transformation-Tnon">Non-Actionlet Transformation Tnon:</h3>
<p>这段文字涉及了有关 “Non-Actionlet Transformation Tnon” 的数据变换方法。下面是详细解释：</p>
<ol>
<li>“Non-Actionlet Transformation Tnon” 意味着对非 “Actionlet” 区域应用额外的数据转换，以获得更强的泛化性能。</li>
<li>对于非 “Actionlet” 区域，作者应用了两种数据变换方法：
<ul>
<li>“Random Noise”：这是一种在单个数据实例内部应用的数据变换。这里的 “Noise” 指的是噪声。它有较大的方差，这意味着在非 “Actionlet” 区域引入了更多的随机性。</li>
<li>“Skeleton Mix”：这是一种跨数据实例之间的数据变换。它包括了多种数据混合方法，如 Mixup、CutMix 和 ResizeMix。这些方法是在元素级别混合数据。因为这些变换是在非 “Actionlet” 区域执行的，它们不会改变动作的语义。因此，变换后的数据与原始数据一起用作正样本。</li>
</ul>
</li>
</ol>
<p>总结：这段文字描述了在非 “Actionlet” 区域中应用的额外数据转换方法，包括在单个数据实例内部引入噪声以及跨不同数据实例的数据混合方法。这些方法旨在提高模型的泛化性能，而不改变动作的语义。</p>
<h3 id="Actionlet-Dependent-Combination">Actionlet-Dependent Combination:</h3>
<p>这段文字描述了两个关键的概念：Actionlet-Dependent Combination（基于动作子集的组合）和Semantic-Aware Feature Pooling（语义感知特征池化）。以下是详细解释：</p>
<ol>
<li>
<p><strong>Actionlet-Dependent Combination（基于动作子集的组合）</strong>：这是一种数据合并方法，用于合并两个区域（“Actionlet” 区域和 “Non-Actionlet” 区域）的数据变换结果。合并的方法涉及使用 “Actionlets” 来组合这些数据。公式中的描述表明，最终的合并数据 Xi_trans 是通过以下方式计算的：</p>
<p>Xi_trans = (Ai_tv * Xi_act) + ((1 - Ai_tv) * Xi_non)</p>
<p>其中，Xi_act 和 Xi_non 分别是使用 “Actionlet Transformations”（Tact）和 “Non-Actionlet Transformations”（Tnon）变换的结果，Ai_tv 表示 “Actionlet”。</p>
</li>
<li>
<p><strong>Semantic-Aware Feature Pooling（语义感知特征池化）</strong>：这是一种用于从数据中提取动作信息的方法。这个方法在空间和时间维度上聚焦于 “Actionlet” 区域内的特征表示，从而减少了来自其他静态区域对动作特征提取的干扰。这个方法可以用以下方式表达：</p>
<p>SAFP(h^i_ctv) = Σ (h^i_ctv * (Ai_tv / Σ Ai_tv))</p>
<p>这个方法旨在有效提取动作信息，使特征更具区分性。作者在线下流程中使用这种语义感知特征池化操作，以提供准确的基准特征。</p>
</li>
</ol>
<p>总结：这段文字描述了如何将两个区域的数据变换结果合并，以及如何使用语义感知特征池化方法从数据中提取动作信息，从而减少其他静态区域对动作信息提取的干扰。这些方法有助于提高模型的性能。</p>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>masked</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.fomal.cc/posts/cea7f2ca.html">https://www.fomal.cc/posts/cea7f2ca.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Fomalhaut🥝</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-10-16</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2024-04-28</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a"></audio><script defer="defer" src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/71f495ac.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">technology-π1</div></div></a></div><div class="next-post pull-right"><a href="/posts/71475a2e.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">对比学习</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:bold">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">【代码待更新】Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">【未开源】SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition用于自监督骨架动作识别的时空掩蔽自动编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-2"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-work"><span class="toc-text">Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Supervised-skeleton-based-action-recognition"><span class="toc-text">2.1. Supervised skeleton-based action recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Self-supervised-skeleton-based-action-recognition"><span class="toc-text">2.2. Self-supervised skeleton-based action recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Masked-Autoencoder"><span class="toc-text">2.3&quot;Masked Autoencoder&quot;</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Methodology"><span class="toc-text">3. Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Preliminaries"><span class="toc-text">3.1. Preliminaries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Spatial-temporal-masking-strategy"><span class="toc-text">3.2. Spatial-temporal masking strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-SkeletonMAE-architecture"><span class="toc-text">3.3. SkeletonMAE architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Fine-tuning-for-skeleton-action-recognition"><span class="toc-text">3.4. Fine-tuning for skeleton action recognition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D"><span class="toc-text">详细介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="toc-text">方法详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0"><span class="toc-text">参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%B6%E7%A9%BA%E9%81%AE%E8%94%BD%E7%AD%96%E7%95%A5"><span class="toc-text">时空遮蔽策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SkeletonMAE%E6%A1%86%E6%9E%B6%E7%BB%93%E6%9E%84"><span class="toc-text">SkeletonMAE框架结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E5%BE%AE%E8%B0%83"><span class="toc-text">动作识别微调</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Masked Motion Predictors are Strong 3D Action Representation Learners蒙面动作预测器是强大的3D动作表示学习者</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-work"><span class="toc-text">2.Related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-3D-Action-Recognition"><span class="toc-text">Supervised 3D Action Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-supervised-3D-Action-Recognition"><span class="toc-text">Self-supervised 3D Action Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Masked-Visual-Prediction"><span class="toc-text">Masked Visual Prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Our-Method"><span class="toc-text">Our Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Overview"><span class="toc-text">3.1. Overview</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Joint-Embedding"><span class="toc-text">3.2. Joint Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Motion-Extraction"><span class="toc-text">3.3. Motion Extraction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Motion-Aware-Masking"><span class="toc-text">3.4. Motion-Aware Masking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-Masked-Motion-Prediction"><span class="toc-text">3.5. Masked Motion Prediction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning    提示与masked运动建模的对比:  面向通用的3D动作表示学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1introduction"><span class="toc-text">1introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-RELATEDWORKS"><span class="toc-text">2 RELATEDWORKS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Skeleton-based-Action-Recognition"><span class="toc-text">2.1 Skeleton-based Action Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Contrastive-Learning-for-Skeleton"><span class="toc-text">2.2 Contrastive Learning for Skeleton</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Masked-Image-Skeleton-Modeling"><span class="toc-text">2.3 Masked Image&#x2F;Skeleton Modeling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-THE-PROPOSED-METHOD-PCM3"><span class="toc-text">3 THE PROPOSED METHOD: PCM3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Skeleton-Contrastive-Learning"><span class="toc-text">3.1 Skeleton Contrastive Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Intra-skeleton-transformation-learning"><span class="toc-text">1) Intra-skeleton transformation learning.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Inter-skeleton-transformation-learning"><span class="toc-text">2) Inter-skeleton transformation learning.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Relational-Knowledge-Distillation"><span class="toc-text">3) Relational Knowledge Distillation.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Masked-Skeleton-Prediction"><span class="toc-text">3.2 Masked Skeleton Prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Collaboration-between-Contrastive-Learning-and-Masked-Modeling"><span class="toc-text">3.3 Collaboration between Contrastive Learning and Masked Modeling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Novel-Positive-Pairs-as-Connection"><span class="toc-text">1) Novel Positive Pairs as Connection.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-High-Level-Semantic-Guidance-%E9%AB%98%E7%BA%A7%E8%AF%AD%E4%B9%89%E6%8C%87%E5%AF%BC"><span class="toc-text">2) High-Level Semantic Guidance.高级语义指导</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Dual-Prompted-Multi-Task-Pretraining-%E5%8F%8C%E6%8F%90%E7%A4%BA%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%8F%90%E7%A4%BA%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text">3.4 Dual-Prompted Multi-Task Pretraining 双提示多任务提示的预训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Domain-Specific-Prompt"><span class="toc-text">1) Domain-Specific Prompt.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Task-Specific-Prompt"><span class="toc-text">2) Task-Specific Prompt.</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training  基于图的骨架序列预训练的掩蔽自动编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-3"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text">related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-supervised-Learning-for-Skeleton-Sequence"><span class="toc-text">Self-supervised Learning for Skeleton Sequence.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Methodology-2"><span class="toc-text">3. Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1Pre-training-SkeletonMAE"><span class="toc-text">3.1Pre-training SkeletonMAE</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1SkeletonMAE-Structure"><span class="toc-text">3.1.1SkeletonMAE Structure</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-Skeleton-Joints-Masking-and-Reconstruction"><span class="toc-text">3.1.2 Skeleton Joints Masking and Reconstruction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-Reconstruction-Criterion"><span class="toc-text">3.1.3 Reconstruction Criterion</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Fine-tuning-for-Skeleton-Action-Recognition"><span class="toc-text">3.2. Fine-tuning for Skeleton Action Recognition</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Spatial-Temporal-Representation-Learning"><span class="toc-text">3.2.1 Spatial-Temporal Representation Learning</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition基于小动作集的对比学习在无监督骨架动作识别中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-2"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-4"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work-2"><span class="toc-text">related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Skeleton-Based-Action-Recognition"><span class="toc-text">Skeleton-Based Action Recognition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Learning"><span class="toc-text">Contrastive Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text">method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1Unsupervised-Actionlet-Selection"><span class="toc-text">3.1Unsupervised Actionlet Selection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Average-Motion-as-Static-Anchor"><span class="toc-text">Average Motion as Static Anchor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Difference-Activation-Mapping-for-Actionlet-Localization"><span class="toc-text">Difference Activation Mapping for Actionlet Localization</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Actionlet-Guided-Contrastive-Learning"><span class="toc-text">3.2. Actionlet-Guided Contrastive Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Actionlet-Transformation-Tact"><span class="toc-text">Actionlet Transformation Tact:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Non-Actionlet-Transformation-Tnon"><span class="toc-text">Non-Actionlet Transformation Tnon:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Actionlet-Dependent-Combination"><span class="toc-text">Actionlet-Dependent Combination:</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-color: transparent;"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</div><div class="btn-xz-box"><a class="btn-xz" target="_blank" rel="noopener" href="https://stellarium.org/">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/posts/eec9786.html">魔改指南</a><a href="/box/nav/">网址导航</a></li><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/personal/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/box/Gallery/">我的画廊</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/site/time/">建设进程</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a href="https://www.fomal.cc/" title="Fomalhaut🥝"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/60e5d4e39da7c077.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div></div></div></div><div class="copyright"><span><b>&copy;2022-2024</b></span><span><b>&nbsp;&nbsp;By Fomalhaut🥝</b></span></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v6.3.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Frame-Hexo-blue.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Theme-Butterfly-6513df.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" title="本站采用多线部署，主线路托管于Vercel"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Hosted-Vercel-brightgreen.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://user.51.la/" style="margin-inline:5px" title="本站数据分析得益于51la技术支持"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Analytics-51la-3db1eb.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/?keyword=20226665" style="margin-inline:5px" title="本站已加入萌ICP豪华套餐，萌ICP备20226665号"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/萌ICP备-20226665-fe1384.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://bitiful.dogecast.com/buckets" style="margin-inline:5px" title="本网站经Service Worker分流至缤纷云对象存储"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Bucket-缤纷云-9c62da.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://www.netdun.net/" style="margin-inline:5px" title="本站使用网盾星球提供CDN加速与防护"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/CDN-网盾星球-fff2cc.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Source-Github-d021d6.svg" alt=""/></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())
setTimeout(function(){preloader.endLoading();}, 5000);
document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: '',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: '',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.staticfile.org/twikoo/1.6.8/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script async src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script><script defer type="text/javascript" src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script defer src="https://cdn1.tianli0.top/gh/nextapps-de/winbox/dist/winbox.bundle.min.js"></script><script async src="//at.alicdn.com/t/c/font_3586335_hsivh70x0fm.js"></script><script async src="//at.alicdn.com/t/c/font_3636804_gr02jmjr3y9.js"></script><script async src="//at.alicdn.com/t/c/font_3612150_kfv55xn3u2g.js"></script><script async src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax","#bibi","body > title","#app","#tag-echarts","#posts-echart","#categories-echarts"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-10</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">HelloWorld</a><div class="blog-slider__text">这里是第一个马增龙写的文档，用来记录自己套用模板的全部过程</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 320px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/site/census/'|| '/site/census/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("/api?null",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'null')
    }
  </script><!-- hexo injector body_end end --></body></html>