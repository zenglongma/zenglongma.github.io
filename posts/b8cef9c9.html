<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>文献阅读5 | Fomalhaut🥝</title><meta name="keywords" content="Literature Reading"><meta name="author" content="Fomalhaut🥝"><meta name="copyright" content="Fomalhaut🥝"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="论文阅读总结">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读5">
<meta property="og:url" content="https://www.fomal.cc/posts/b8cef9c9.html">
<meta property="og:site_name" content="Fomalhaut🥝">
<meta property="og:description" content="论文阅读总结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://source.fomal.cc/img/default_cover_14.webp">
<meta property="article:published_time" content="2023-09-19T00:09:37.000Z">
<meta property="article:modified_time" content="2024-06-27T14:37:00.145Z">
<meta property="article:author" content="Fomalhaut🥝">
<meta property="article:tag" content="Literature Reading">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://source.fomal.cc/img/default_cover_14.webp"><link rel="shortcut icon" href="/"><link rel="canonical" href="https://www.fomal.cc/posts/b8cef9c9"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js',
      css: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读5',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-27 22:37:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="blurNum"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Fomalhaut🥝" type="application/atom+xml">
</head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/02/avatar.webp" onerror="onerror=null;src='/assets/r1.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Fomalhaut🥝</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xiangzi"></use></svg><span class="menu_word" style="font-size:17px"> 八宝箱</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/box/gallery/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-tubiaozhizuomoban">                   </use></svg><span class="menu_word" style="font-size:17px"> 画廊</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/animation/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-nvwumao">                   </use></svg><span class="menu_word" style="font-size:17px"> 动画</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/nav/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-zhifengche">                   </use></svg><span class="menu_word" style="font-size:17px"> 网址导航</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/social/link/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 友人帐</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-wangye"></use></svg><span class="menu_word" style="font-size:17px"> 网站</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/site/census/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--tongjibiao">                   </use></svg><span class="menu_word" style="font-size:17px"> 网站统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/echarts/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shujutongji1">                   </use></svg><span class="menu_word" style="font-size:17px"> 文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/site/time/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-xianxingshalou">                   </use></svg><span class="menu_word" style="font-size:17px"> 旧时光</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 唠叨</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg><span> 搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-tupian1"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon">       </use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读5</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于 </span><time class="post-meta-date-created" datetime="2023-09-19T00:09:37.000Z" title="发表于 2023-09-19 08:09:37">2023-09-19</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-27T14:37:00.145Z" title="更新于 2024-06-27 22:37:00">2024-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-biaoqian"></use></svg><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">3.8w</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>131分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读5"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>Self-Adaptive Graph With Nonlocal Attention Network for Skeleton-Based Action Recognition基于非局部注意力网络的自适应图骨架动作识别</h1>
<h2 id="abstract">abstract</h2>
<p>这篇文章介绍了一个名为SAGGAN的创新性的空间-时间模型，旨在解决图卷积网络（GCNs）在建模人体骨骼时存在的问题。传统方法存在两个固有缺陷：首先，这些模型基于人体的物理结构处理输入数据，导致一些潜在的关节之间的相关性被忽略。其次，它们忽视了非相邻帧之间的关键时间关系，无法充分学习骨骼关节沿时间维度的变化。为了解决这些问题，文章提出了SAGGAN模型，包括自适应GCN（SAGCN）和全局注意力网络。具体来说，SAGCN模块构建了两个额外的动态拓扑图，分别用于学习所有数据的共同特征和为每个样本表示唯一模式。同时，全局注意力模块（空间注意力和时间注意力模块）用于提取单帧中不同关节之间的全局连接，并建模相邻和非相邻帧之间的时间关系。通过这种方式，网络能够捕捉更丰富的动作特征，实现准确的动作识别，克服了标准图卷积的缺陷。在三个基准数据集上的广泛实验证明了我们提出的方法的卓越性能。</p>
<p>总结：</p>
<ul>
<li>
<p>传统的GCN方法在建模人体骨骼时存在问题，忽略了关节之间的潜在关联和时间关系。</p>
</li>
<li>
<p>SAGGAN引入了SAGCN模块和全局注意力网络，以解决这些问题。</p>
</li>
<li>
<p>SAGCN构建了动态拓扑图，学习了所有数据的共同特征和每个样本的唯一模式。</p>
</li>
<li>
<p>全局注意力模块用于提取单帧中不同关节之间的全局连接和建模时间关系。</p>
</li>
<li>
<p>实验证明，SAGGAN在动作识别方面具有卓越性能。</p>
</li>
<li>
<p>传统图卷积网络（GCNs）在建模人体骨骼时存在问题，因为它们忽略了骨骼关节之间的潜在相关性和<strong>非相邻帧之间的时间关系</strong>。</p>
</li>
<li>
<p>为了解决这些问题，文章提出了SAGGAN，一种创新的空间-时间模型。</p>
</li>
<li>
<p>SAGGAN包括两个主要组成部分：SAGCN模块和全局注意力网络。</p>
</li>
<li>
<p>SAGCN模块用于构建两个额外的动态拓扑图，一个用于学习所有数据的共同特征，另一个用于为每个样本表示唯一的模式。</p>
</li>
<li>
<p>全局注意力网络包括空间注意力和时间注意力模块，用于提取单帧中不同关节之间的全局连接以及建模相邻和非相邻帧之间的时间关系。</p>
</li>
<li>
<p>SAGGAN的设计旨在捕捉更丰富的动作特征，以实现准确的动作识别，并克服标准图卷积的缺陷。</p>
</li>
<li>
<p>文章通过在三个基准数据集上进行广泛的实验，展示了SAGGAN方法的卓越性能。</p>
</li>
</ul>
<h2 id="Introduction">Introduction</h2>
<h3 id="翻译">翻译</h3>
<ol>
<li><strong>背景介绍</strong>：人类动作识别经过多年的深入研究，因其在智能安全、医疗辅助和人机交互等广泛应用领域而备受关注。准确的动作识别的主要挑战在于如何有效地提取独特且丰富的特征，以描述动作的空间和时间动态。</li>
<li><strong>骨骼数据的优势</strong>：相比之下，以往基于RGB的方法侧重于从整个图像帧中提取基于像素的外观和运动特征。然而，这些方法容易受到图像背景和视角的影响，导致动作信息不够精确。相反，骨骼数据作为对人体动作的简洁且高层次的抽象，在视角或外观上是不变的。此外，人类骨骼具有图形结构，因此在骨骼数据的动作识别任务中应用图卷积网络（GCNs）是自然而然的选择。</li>
<li><strong>ST-GCN的不足</strong>：自然-时空GCN（ST-GCN）[6]是一种先驱网络，用于根据关节之间的自然连接和相邻帧之间的时间边来建模骨骼数据。随后，一些ST-GCN的变种也采用了这一策略。通过这种方式，人体始终被表示为一个针对所有动作固定的预定义图形，忽略了非邻接关节之间的某些隐含关系。例如，在“鼓掌”和“摸鼻子”动作中，两只手之间存在连接，但对于“摸鼻子”动作，两只手之间的连接不存在。通过这一观察，有人认为固定的图形结构可能不适用于所有动作。此外，现有方法主要关注如何捕获相邻关节之间的局部关系，而忽略了非相邻关节之间的全局关系，这在动作识别中也起着关键作用。以“鼓掌”动作为例，动作“鼓掌”中的两只手不是直接连接的，但它们之间的互动实际上对于识别动作非常有用。</li>
<li><strong>时间动态特征的挑战</strong>：为了解决上述挑战，我们提出了一种创新框架，称为自适应图卷积与全局关注网络（SAGGAN）。我们的网络在时间和空间维度上均有操作。在时间维度上，我们利用时间卷积网络（TCN）模块提取低级时间特征，并引入了时间关注（TA）模块，以自动构建跨不同帧的远程关系。在空间维度上，它包含自适应GCN（SAGCN）模块和空间关注（SA）模块。具体而言，SAGCN模块用于基于三种图形捕获每一帧中的关系。一种是表示人体物理结构通用模式的固定图，另外两种是应用于学习所有数据的共同特征和分别表示每个样本的动态图。虽然动态图包含丰富的信息，但它仍然面临捕获远程关系的挑战和由于层叠过多导致的平滑过度问题。因此，我们引入了SA模块以增强局部关节之间的连接，并独立评估每一帧中任意关节对之间的关系，这可以使模型对各种数据样本更具通用性。</li>
<li><strong>主要贡献</strong>：我们工作的主要贡献可总结如下。
<ul>
<li>我们提出了一种名为SAGGAN的创新框架，用于描述包含时间和空间维度中一些附加的非物理连接的时空特征。</li>
<li>我们引入了SAGCN模块，以自适应地捕获所有数据的共同特征和每个样本的独特模式，这可以为动作识别提供更丰富和有用的信息。</li>
<li>我们设计了全局关注模块，以捕获任意关节对之间的连接，包括SA模块和TA模块。SA模块可以有效避免平滑过度，并在单帧中捕获远程连接。TA模块可以获取时间序列中的判别性特征。</li>
<li>我们在三个基准数据集上进行了广泛的实验，以评估我们提出的动作识别任务技术。</li>
</ul>
</li>
</ol>
<h3 id="总结">总结</h3>
<ol>
<li><strong>背景介绍</strong>：动作识别在智能安全、医疗辅助和人机交互等领域有广泛的应用。关键挑战是如何有效提取具有差异性和丰富性的特征，以描述动作的时空动态。</li>
<li><strong>骨骼数据的优势</strong>：与基于RGB的方法相比，骨骼数据更适合动作识别，因为它不受视角或外观的影响，同时以图形形式表示，适合应用图卷积网络（GCNs）。</li>
<li><strong>ST-GCN的不足</strong>：ST-GCN等现有方法采用固定的图结构表示人体，忽略了一些非邻接关节之间的潜在关系。同时，它们主要关注如何捕捉邻接关节之间的局部关系，而忽略了非邻接关节之间的全局关系。</li>
<li><strong>时间动态特征的挑战</strong>：以前的方法使用2D卷积来获取骨骼序列中相邻帧之间的时间关联，但2D卷积受卷积核大小的限制，难以捕捉全局时间信息。此外，长序列可能导致梯度消失或梯度爆炸问题。</li>
<li><strong>SAGGAN框架</strong>：为了解决上述挑战，提出了自适应图卷积与全局关注网络（SAGGAN）框架。该框架在时空维度上进行操作，包括使用时间卷积网络（TCN）模块提取低级时间特征，引入时间关注（TA）模块自动构建跨不同帧的远程关系，以及使用自适应GCN（SAGCN）模块和空间关注（SA）模块捕获关节之间的关系。</li>
<li><strong>主要贡献</strong>：SAGGAN的主要贡献包括引入了SAGCN模块来自适应捕获所有数据的共同特征和每个样本的独特模式，设计了全局关注模块以捕获任意关节对之间的关系，并在三个基准数据集上进行了广泛的实验验证。</li>
</ol>
<p>这些要点概括了介绍部分的主要内容，突出了作者的研究动机、挑战、方法和贡献。</p>
<h2 id="related-work">related work</h2>
<h3 id="图神经网络">图神经网络</h3>
<ol>
<li><strong>神经网络在图上的应用</strong>：
<ul>
<li>卷积神经网络（CNNs）在欧几里得数据，如图像和视频上取得了巨大成功。</li>
<li>但是，它们无法直接处理图数据，需要将图数据转换为伪图像数据，这可能会丢失图数据中隐藏的有用信息。</li>
<li>递归神经网络（RNNs）在处理时间序列方面具有更多优势。它们将图数据转换为包含一个帧中所有关节位置信息的向量序列，但不能感知人体骨骼的空间相关性。因此，保留图数据结构可以探索更多隐藏信息。</li>
</ul>
</li>
<li><strong>图神经网络（GNNs）</strong>：
<ul>
<li>图神经网络在从图结构数据中提取特征方面取得了巨大进展。</li>
<li>提出的GNNs可以分为两大类：频谱GNNs和空间GNNs。</li>
<li>频谱GNNs将输入图形转换为图傅里叶域，并使用一系列滤波器。但频谱GNNs需要将整个图形加载到内存中，处理大型图形时效率低下。</li>
<li>相反，空间GNNs通常使用一些函数为每个节点完成逐层更新，如邻域函数、聚合函数和激活变换。</li>
<li>邻域函数用于选择邻居，如相邻节点。聚合函数用于合并所选邻居和自身的特征，如均值池化。</li>
<li>激活变换将非线性激活函数（如ReLU或softmax）应用于聚合特征向量。</li>
</ul>
</li>
<li><strong>基于GCN的方法</strong>：
<ul>
<li>在不同的GNN变种中，处理图数据的最有效方法是基于GCN的方法。这些方法将CNNs推广为处理具有任意结构和形状的图形。</li>
<li>为了捕获来自非局部邻居的特征，一些多尺度GCNs被提出。</li>
<li>在一些研究中，他们使用高阶邻接矩阵来获取远距离节点之间的关系。</li>
<li>为了更好地表示GCN，引入了图注意力网络（GAT）的注意力机制。GAT从所有输入中选择相对关键的信息。</li>
<li>在此启发下，[30]在空间和时间维度上都使用了自注意力机制。我们的工作基于空间视角方法进行了一些改进。</li>
</ul>
</li>
</ol>
<p>这段文字介绍了神经网络在处理图数据时所面临的挑战，以及图神经网络（GNNs）在处理图结构数据中的进展，特别是基于GCN的方法的有效性。此外，它提到了一些关于多尺度GCNs和图注意力网络（GAT）的方法，以及自注意力机制在空间和时间维度上的应用。作者指出他们的工作是在空间角度上对这些方法进行改进的。</p>
<h2 id="method">method</h2>
<h3 id="GNN机制">GNN机制</h3>
<ol>
<li><strong>GCN建立空间图的过程</strong>：使用GCN建立空间图的过程是通过使用边信息来聚合节点信息，以生成新的节点表示。人体关节被表示为由编码在邻接矩阵A ∈ Rn×n中的边连接的图节点。输入的骨骼序列被表示为Xin ∈ RCin×T×N，其中Cin是输入骨骼序列的通道数，T是序列的长度，N是每帧中的节点数。每个向量Xt in = {vt1, vt2, . . . , vtn}表示在时间t时的n个人体关节的坐标。</li>
<li><strong>基本的空间图卷积操作</strong>：在第l层上对骨骼序列进行的基本空间图卷积操作可以表示为Xl+1 = W * Xl * A（式1），其中W和A分别是权重矩阵和邻接矩阵。</li>
<li><strong>连接强度的差异</strong>：尽管人们在执行动作时多个关节一起移动，但每个关节与其他关节之间的连接强度是不同的。为了探索这些连接强度的差异，ST-GCN实施了可学习的重要性加权。具体来说，每个邻接矩阵对应于一个可学习的权重矩阵M，它是一个N × N矩阵，表示每个顶点的重要性。学习过程可以表示如下（式2）：
<ul>
<li>fout表示输出特征映射，fin表示输入特征映射。</li>
<li>Kv表示应用于ST-GCN的分区策略。</li>
<li>Ak是用于提取特定子集中连接节点的邻接矩阵。</li>
<li>Mk确定连接的强度。</li>
<li>⊗表示两个矩阵之间的逐元素乘法运算。</li>
<li>Ak中的每个元素的值为0或1，Mk的所有元素都初始化为1。</li>
<li>Wk表示权重矩阵，起到与卷积核相同的作用。</li>
</ul>
</li>
</ol>
<p>在这里，GCN（图卷积网络）被用于处理骨骼序列数据，其中邻接矩阵和权重矩阵用于建模关节之间的连接和权重。这有助于捕获动作中不同关节之间的关系和强度差异。这种方法允许模型自动学习关节之间的连接强度，以更好地理解动作。</p>
<h3 id="Transformer机制">Transformer机制</h3>
<p>以下是段落的逐句翻译和Transformer机制的解释：</p>
<ol>
<li><strong>Transformer：</strong> Transformer自注意力机制的主要思想是基于短距离和长距离的相关性来丰富单词的嵌入，最初设计用于自然语言处理（NLP）任务。具体来说，输入序列通过三个可训练的线性变换（Wq，Wk和Wv）编码为三个矩阵，即查询矩阵q ∈ Rdq，键矩阵k ∈ Rdk和值矩阵v ∈ Rdv。然后，通过查询矩阵和键矩阵之间的点积来计算分数，以表示单词j对单词i的注意力。
<ul>
<li>公式：αi j = qi · kTj，其中qi表示单词i的查询矩阵，kj表示单词j的键矩阵，n是单词的总数。</li>
</ul>
</li>
<li><strong>缩放点积注意力：</strong> 单词i的最终嵌入是通过缩放点积注意力生成的。
<ul>
<li>公式：Attention(qi, ki, vi) = ∑j softmax(αi j) / √dk * vi，其中qi表示单词i的查询矩阵，ki表示单词i的键矩阵，vi表示单词i的值矩阵，n是单词的总数，dk是键向量的通道维度。除以√dk的目的是增加梯度的稳定性。</li>
</ul>
</li>
<li><strong>多头注意力：</strong> 通常采用多头注意力方式，多次应用具有不同可学习参数的注意力。每个注意力头的输出被串联并投影以获得最终值。
<ul>
<li>公式：Vfinal = Concat(V1, . . . , Vh)WO，其中Vh表示第h个注意力头的输出，WO是投影参数矩阵，dq = dk = dv = dmodel/h。</li>
</ul>
</li>
</ol>
<p><strong>Transformer机制解释：</strong> Transformer是一种用于处理序列数据的深度学习模型，最初设计用于自然语言处理。其中的关键部分是自注意力机制，它允许模型根据输入序列中不同位置的相关性来动态调整每个元素的权重。这可以用于丰富嵌入表示，从而更好地捕获序列中的信息。多头注意力允许模型以不同的方式关注序列中的不同部分，以提高性能。 Transformer已经在各种领域取得了卓越的成果，不仅限于自然语言处理，还在计算机视觉等领域广泛应用。</p>
<h3 id="SAGCN-Module机制">SAGCN Module机制</h3>
<p>以下是段落的逐句翻译和SAGCN Module机制的解释：</p>
<ol>
<li><strong>当处理骨骼数据时，大多数时空图卷积网络始终在预定义的图上操作，使用邻接矩阵A(i, j)表示拓扑关系，其中A(i, j) = 0表示关节vi和v j不相邻（否则相邻）。然而，这种策略只能描述人体骨架的物理结构，可能忽略了对动作识别有用的某些信息。这有两个主要原因。首先，由于每一层的图的拓扑结构相同，它缺乏对包含在所有GCN层中的多重语义信息的建模能力。其次，它未能捕捉到额外的非物理连接，因为如果两个关节之间没有连接，则经过多次乘法运算后A(i, j)仍然为0。此外，每个动作都具有在识别任务中起关键作用的多个独特特征。因此，需要构建不同的图来描述每个样本的独特特征。</strong></li>
<li><strong>为了解决上述问题，我们提出了一种新颖的GCN模块，采用自适应策略构建骨架图。我们引入了一个相似性矩阵，通过计算样本之间的差异来确定两个关节之间的连接强度。与原始的ST-GCN不同，我们的SAGCN通过逐元素相加邻接矩阵、掩码矩阵和相似性矩阵来定义。</strong></li>
<li><strong>公式：fout = ∑k (Ak + Mk + Sk) * fin * W，其中k表示卷积核的数量，Ak表示表示人体物理结构的邻接矩阵，Mk是大小为N×N的掩码矩阵，Sk表示两个节点的相似性。</strong></li>
<li><strong>与原始ST-GCN一样，我们提出的SAGCN中的掩码矩阵Mk用于确定连接的强度。与Ak不同，这些元素的值在模型训练过程中与其他参数一起更新。元素的值越大，连接关系就越强。</strong></li>
<li><strong>相似性矩阵Sk表示特征向量之间的相似性，每个元素Si j k表示节点vi和节点v j的相似性。这些元素的值在0到1之间。</strong></li>
<li><strong>最终的SAGCN通过将三个矩阵Ak、Mk和Sk相加来更新骨架图的结构，这与元素级的相乘不同。</strong></li>
<li><strong>总的来说，SAGCN相对于常规的时空图卷积可以更好地更新和优化骨架图的结构。此外，每个样本对应一个不同的图，使模型更加灵活。</strong></li>
</ol>
<p><strong>SAGCN Module机制解释：</strong> 这一段落描述了SAGCN（Self-Adaptive Graph Convolution Network）模块的机制。SAGCN的目标是更好地捕获骨骼数据中的信息，并构建不同的骨架图来适应不同样本的特征。</p>
<ul>
<li><strong>邻接矩阵Ak</strong>：表示人体骨架的物理结构，通常是一个固定的预定义矩阵。</li>
<li><strong>掩码矩阵Mk</strong>：用于确定连接的强度，其值在训练过程中会更新，以适应不同的层次和语义特征。</li>
<li><strong>相似性矩阵Sk</strong>：表示节点之间的相似性，根据节点的特征向量计算得出，用于捕获不同节点之间的关系。</li>
</ul>
<p>SAGCN通过将这三个矩阵相加而不是相乘来构建图，以确保在不存在连接的情况下不会消失。这允许模型根据数据动态构建图，以更好地适应不同样本的特征。此外，SAGCN采用了一种自适应策略，允许模型根据训练数据的不同层次和特征来更新掩码矩阵Mk的值，从而提高了模型的灵活性。这种方法有助于更好地捕获骨架数据中的关系，并改善动作识别性能。</p>
<h3 id="SA-Module机制">SA Module机制</h3>
<p>虽然SAGCN模块可以弥补无法在乘法操作中替代A(i, j) = 0的缺陷，但远距离的连接仍然容易被低估。此外，随着层叠的增加，图卷积的过度平滑风险会增加。因此，我们引入SA模块，以自动发现每个帧内的关节关系，舍弃任何预定义的骨架结构。对于帧t中骨架的每个节点nt i，可以通过应用具有三个参数矩阵Wq ∈ RCin×dq、Wk ∈ RCin×dk和Wv ∈ RCin×dv的可训练线性变换来计算查询向量qt i、键向量kt i和值向量vt i，这些参数矩阵是所有节点共享的。使用这些向量，计算每对节点nt i和nt j之间的相关分数αt i j，公式如下：</p>
<p>αt i j = qt i · ktT j (9)</p>
<p>其中，qt i表示帧t中节点i的查询向量，kt j表示同一帧t中节点j的键向量。然后，相关分数αt i j被用作对应于查询向量的节点vt i的权重。节点nt i的最终嵌入通过加权求和计算如下：</p>
<p>zt i = Σ j softmax(αt i j) * √dk * vt i (10)</p>
<p>其中，zt i表示节点nt i的新嵌入，dk表示查询向量和键向量的维度。</p>
<p>与原始的Transformer类似，我们还使用多头注意力，通过多次重复嵌入过程Nh次来捕获多重关注信息。每个头使用不同组的可学习参数来获取节点嵌入(zt i1, zt i2, . . . , zt i H)。最终的表示通过将它们连接在一起获得。</p>
<p>此外，我们选择了五个关键关节点来展示SA模块在空间维度上对骨架关节的学习过程，如图3所示。首先，我们为每个关节计算查询向量q、键向量k和值向量v。然后，通过查询向量与所有其他关节点的键向量的点积来获取每对关节的连接强度。根据连接强度，每个关节将与查询关节进行缩放。请注意，如果与查询关节存在强连接，那么关键关节将被赋予更高的权重。最后，计算查询关节的新表示，通过加权关节的求和来完成。</p>
<h3 id="TA-Module机制">TA Module机制</h3>
<p>这段文字讲述了在视频中获得动作的判别性特征，这些特征主要包括短期时间上下文和长期依赖关系，这些特征主要来自时间序列。大多数现有的基于GCN的方法使用2D时间卷积来关注短期时间上下文，以建模骨架序列中的时间动态。然而，2D卷积的操作范围受到卷积核的限制，忽略了一些长期依赖关系。</p>
<p>为了探索两个远距离帧中非相邻关节之间的关系，引入了TA模块，以获取相邻和非相邻帧之间的短程和长程相关性。更具体地说，我们使用不同帧中相同节点的嵌入在时间维度上的变化来获取不同帧之间的相关性。</p>
<p>首先，对每个时间t处的帧内节点nti应用线性变换，使用参数Wq、Wk和Wv，这类似于SA模块，以获得查询向量qit、关键向量kit和值向量vit。然后，我们计算用于确定一个节点向其他节点支付多少关注的分数si tu，这是通过查询-关键点乘积得出的：</p>
<p>si tu = qi t · ki u T (11)</p>
<p>其中qi t表示t帧中第i个节点，kiu表示u帧中第i个节点。</p>
<p>最后，获取节点nti的嵌入zi t，通过对所有分数进行求和获得：</p>
<p>zi t = Σ j softmax(si tu) * √dk * vi t (12)</p>
<p>其中，dk表示关键向量的嵌入维度，除以（dk）1/2的函数是为了防止梯度消失。</p>
<p>为了直观展示TA模块在时间维度上的操作，图4中提供了TA模块的示例。具体来说，计算了每个关节的查询向量q、关键向量k和值向量v。然后计算了每对关节之间的相似性分数，如q1kT3、q2kT4和q4kT1。这些分数可以衡量每个身体关节nti的影响，表明节点之间的相关性。</p>
<p>总之，TA模块提取了不同帧中节点之间的关系，以学习帧之间的相关性。通过这种方式，我们可以捕获比使用2D卷积获得的更丰富的特征，这对于动作识别可能至关重要。</p>
<h1>Hypergraph Transformer for Skeleton-based Action Recognition基于骨架的动作识别超图Transformer</h1>
<h2 id="abstract-2">abstract</h2>
<p>翻译： 基于骨骼的动作识别旨在通过具有骨骼相互连接的人体关节坐标来识别人类动作。通过将关节作为顶点，它们的<strong>自然连接作为边来定义图形</strong>，先前的研究成功地采用了图卷积网络（GCNs）来模拟关节共现情况，并取得了卓越的性能。最近，GCNs 的一个局限性被识别出来，即拓扑结构在训练后是固定的。为了放宽这种限制，引入了<strong>自注意力机制（SA）</strong>，使GCNs 的拓扑结构能够适应输入，从而产生了最先进的混合模型。同时，也尝试使用<strong>普通的Transformer 模型</strong>，但由于缺乏结构先验知识，它们仍然落后于最先进的基于GCN 的方法。与混合模型不同，我们提出了一种更加优雅的解决方案，通过<strong>图距离嵌入将骨骼连接性融入到Transformer 中</strong>。我们的嵌入在训练过程中保留了骨骼结构的信息，而GCNs 仅在初始化阶段使用它。更重要的是，我们揭示了==图形模型的一个潜在问题，即成对聚合本质上忽略了身体关节之间的高阶运动依赖关系==为了填补这一差距，我们提出了一种新的<strong>超图自注意力（SA）机制</strong>，称为超图自注意力（HyperSA），以<strong>将内在的高阶关系融入模型</strong>中。我们将结果模型命名为Hyperformer，它在NTURGB+D、NTURGB+D 120 和Northwestern-UCLA 数据集上的准确性和效率方面击败了最先进的图形模型。我们的代码可在Hyperformer Github 存储库中找到。</p>
<p>总结： 这篇论文关注骨骼基础的动作识别，通过使用<strong>人体关节坐标以及它们之间的连接关系</strong>来识别人类动作。以前的研究使用图卷积网络（GCNs）来处理这一问题，并取得了出色的成绩。然而，GCNs 存在一个限制，即它们在训练后不能调整拓扑结构。为了解决这个问题，研究引入了自注意力（SA）机制，以使GCNs 的拓扑结构能够自适应输入数据，这导致了最先进的混合模型。与此同时，一些研究尝试使用普通的Transformer 模型，但由于缺乏结构信息，它们的性能落后于GCN 方法。</p>
<p>本文提出了一种更优雅的解决方案，将<strong>骨骼连接性融入到Transformer 模型</strong>中，通过<strong>图距离嵌入</strong>在训练期间保留了骨骼结构的信息，而不仅仅在初始化阶段使用。此外，文章揭示了图形模型的一个潜在问题，即现有方法在处理身体关节之间的高阶运动依赖关系时存在不足。为了解决这个问题，研究提出了一种新的自注意力机制，称为Hypergraph Self-Attention（HyperSA），以引入更高阶的关系。最终的模型被命名为Hyperformer，它在多个数据集上的性能超越了最先进的图形模型。这项研究的代码可在Hyperformer Github 存储库中找到。</p>
<h3 id="总结-2">总结</h3>
<ul>
<li>本文关注骨骼基础的动作识别，使用人体关节坐标和连接关系来进行动作识别。</li>
<li>以前的方法主要采用图卷积网络（GCNs）来模拟关节共现情况，取得了卓越的性能。</li>
<li>GCNs 存在的一个问题是，在训练后不能调整拓扑结构，限制了其灵活性。</li>
<li>为了解决这个问题，引入了自注意力（SA）机制，使GCNs 的拓扑结构能够自适应输入数据。</li>
<li>最近还尝试使用普通的Transformer 模型，但由于缺乏结构信息，性能较差。</li>
<li>本文提出了一种更优雅的解决方案，通过图距离嵌入将骨骼连接性融入到Transformer 模型中，保留了结构信息。</li>
<li>文章还揭示了图形模型的一个潜在问题，即忽略了高阶运动依赖关系。</li>
<li>为了解决这个问题，提出了一种新的自注意力（SA）机制，称为Hypergraph Self-Attention（HyperSA），以引入更高阶的关系。</li>
<li>最终的模型被命名为Hyperformer，在多个数据集上的性能超越了最先进的图形模型。</li>
</ul>
<h2 id="introduction">introduction</h2>
<ol>
<li>Introduction 骨架基础的人体动作识别由于其计算效率和对环境变化以及摄像机视角的鲁棒性而受到越来越多的关注。骨架基础动作识别的关键优势之一是可以使用传感器（如Kinect [53]）或可靠的姿势估计算法[2]轻松获取身体关键点。这为传统的基于RGB或深度的方法提供了更可靠的替代方案，使其成为各种现实世界应用的有前途的解决方案。</li>
<li>Graph Convolution Networks (GCNs) have been widely used for modeling off-grid data. To our knowledge, Yan et al. [46] were the ﬁrst to treat joints and their natural connec- tions as nodes and edges of a graph, and employ a GCN [20] on such a predeﬁned graph to learn joint interactions. Since then, GCNs have become the de facto standard of choice for skeleton-based action recognition. To further capture the in- teractions between physically unconnected joints, state-of- the-art GCNs [7, 3, 47, 31, 35, 36] adopt a learnable topol- ogy which only uses the physical connections for initializa- tion. Even so, they still need to rely on attention mecha- nisms to relax the restriction of the ﬁxed topology, which is the key to the improved performances, as shown in their ablation studies. 图卷积网络（GCNs）已广泛用于建模离散数据。据我们所知，Yan等人[46]是第一个将关节及其自然连接视为图的节点和边，并在这样的预定义图上使用GCN [20]来学习关节交互的人。自那以后，GCNs已成为骨架基础动作识别的事实上的首选标准。为了进一步捕捉物理上不相连的关节之间的相互作用，最先进的GCNs [7, 3, 47, 31, 35, 36]采用了可学习的拓扑结构，仅使用物理连接进行初始化。即便如此，它们仍需要依赖注意力机制来放宽固定拓扑结构的限制，这是提高性能的关键，正如在它们的消融研究中所示。</li>
<li>Given these facts, it is natural to question whether a purely attention-based Transformer model would be a better candidate for skeleton-based action recognition. However, current research [28, 32] has shown that the performance of such models is far from satisfactory. This can be at- tributed to the fact that the formulation of the vanilla Trans- former ignores the unique characteristics of skeleton data, i.e., the permutation invariant attention operation is agnostic to the bone connectivity between human body joints. To ad- dress this issue, absolute positional embeddings have been used [39, 11, 38] , but they still lack the necessary struc- tural information. In contrast, relative positional embed- dings have been shown to be more effective for Transform- ers in various tasks, involving language [30, 9, 15], vision [25, 55, 42, 55], and graph data [48]. To incorporate the information of bone connectivity of human body as well, we also introduce a powerful relative positional embedding based on graph distance, inspired by Spatial Encoding in [48]. Our embedding retains the information of skeletal structure during the entire training process, whereas GCNs merely use it for initialization. 鉴于这些事实，自然会有疑问，纯粹基于注意力的Transformer模型是否更适合用于骨架基础的动作识别。然而，当前的研究[28, 32]表明，这种模型的性能远未令人满意。这可以归因于普通Transformer的构建忽略了骨架数据的独特特性，即排列不变的注意力操作不考虑人体关节之间的骨连接。为了解决这个问题，已经使用了绝对位置嵌入[39, 11, 38]，但它们仍然缺乏必要的结构信息。相反，相对位置嵌入已经被证明对于Transformer在各种任务中更加有效，包括语言[30, 9, 15]、视觉[25, 55, 42, 55]和图数据[48]。为了同时整合人体骨骼的连接信息，我们还引入了一种基于图距离的强大相对位置嵌入，灵感来自[48]的空间编码。我们的嵌入在整个训练过程中保留了骨骼结构的信息，而GCNs仅在初始化阶段使用它。</li>
<li>More importantly, we reveal an underlying issue of graph models for this task in general. For human actions, each type of body joint has its own unique physical functional- ity. As a result, certain re-occurring groups of body joints are often involved in specific actions, such as the subcon- scious hand movement for maintaining balance. Vanilla at- tention is not capable of capturing these underlying rela- tionships that are independent from joint coordinates and go beyond pair-wise interactions. To compensate for this, we employ the concept of hypergraph [54, 13, 1] to ac- commodate the higher-order relations of body joints. With the hypergraph representation, we propose a novel variant of Self-Attention (SA) called Hypergraph Self-Attention (HyperSA), which considers both pair-wise and high-order relations. As shown in Fig. 1, given a partition of the human body joints into different groups, a representation of each group is derived based on its assigned joints. The group representation is then linearly transformed and multiplied with joint queries, allowing joint-to-group interactions in addition to the vanilla joint-to-joint SA. Though HyperSA works well with empirical partitions, we additionally pro- pose an approach to search the optimal partition strategy automatically, further improving its performance. 更重要的是，我们揭示了一般情况下用于这项任务的图模型的一个潜在问题。对于人体动作，每一种类型的身体关节都有其独特的物理功能。因此，某些反复出现的身体关节组通常参与特定的动作，例如保持平衡的下意识手部运动。普通的注意力无法捕捉到这些与关节坐标无关且超越成对交互的基础关系。为了弥补这一点，我们采用了超图的概念[54, 13, 1]来容纳身体关节的高阶关系。通过超图表示，我们提出了一种称为超图自注意力（HyperSA）的新型自注意力（SA）变体，考虑了成对和高阶关系。如图1所示，给定人体关节划分为不同组，根据其分配的关节派生出每个组的表示。然后对该组表示进行线性变换并与关节查询相乘，除了基本的关节对关节SA外，还允许关节对组的相互作用。虽然HyperSA在经验划分中表现良好，但我们另外提出了一种自动搜索最佳划分策略的方法，进一步提高了其性能。</li>
<li>At the same time, Transformers spend a large portion of their capacity on intra-token modeling in their feed-forward layer. While this is important for complex tokens such as image patches or word embeddings, we analyze that such an expensive step is not necessary for joint coordi- nates which are merely three-dimensional. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key to successful action recognition. We thus suggest removing MLP layers for computation and memory reduction, and show in Sec. 5.3.1 that MLP lay- ers are indeed negligible. This leads to a light-weighted Transformer which is comparable to GCNs in model size and computation cost. 与此同时，Transformer在其前向传递层中将其大部分容量用于对令牌的内部建模。虽然这对于复杂的令牌（如图像块或单词嵌入）很重要，但我们分析发现，对于仅为三维的关节坐标来说，这样的昂贵步骤是不必要的。这意味着建模令牌间关系，或者所谓的关节共现，是成功的动作识别的关键。因此，我们建议去除MLP层以进行计算和内存减少，并在第5.3.1节中显示MLP层确实可以忽略不计。这导致了一个轻量级的Transformer，其模型大小和计算成本与GCNs相当。</li>
<li>Our main contributions can be summarized as follows: 我们的主要贡献可以总结如下：</li>
</ol>
<p>• We propose to incorporate the structural information of human skeleton into Transformer via a relative po- sitional embedding based on graph distance, leverag- ing the gap between Transformer and state-of-the-art hybrid models. • 我们提出通过基于图距离的相对位置嵌入，将人体骨骼的结构信息整合到Transformer中，弥合了Transformer和最先进的混合模型之间的差距。</p>
<p>• We devise a novel variant of Self-Attention (SA) called Hypergraph Self-Attention (HyperSA), based on the hypergraph representation. To our best knowledge, our work is the first to apply the hypergraph representation for skeleton-based action recognition, which considers the pair-wise as well as high-order joint relations, go- ing beyond current state-of-the-art methods. • 我们设计了一种称为Hypergraph Self-Attention（HyperSA）的新型自注意力（SA）变体，基于超图表示。据我们所知，我们的工作是第一个将超图表示应用于基于骨架的动作识别的工作，它考虑了成对和高阶关节关系，超越了当前最先进的方法。</p>
<p>• We construct a light-weighted Transformer based on our proposed relative positional embedding and HyperSA. It beats state-of-the-art graph models on skeleton-based action recognition benchmarks w.r.t. both efficiency and accuracy. • 我们构建了一个基于我们提出的相对位置嵌入和HyperSA的轻量级Transformer。在骨架基础的动作识别基准测试中，它在效率和准确性方面超越了最先进的图模型。</p>
<h2 id="Related-work">Related work</h2>
<ol>
<li><strong>Graph Representation（图表示）</strong>：图是用于表示非欧几里得数据的最常见选择，而人类骨骼可以自然地表示为图。与其他图模型相比，Kipf等人提出的图卷积网络（Graph Convolutional Network，GCN）因其简单性和更高的防过拟合性而被广泛用于动作识别。最近，Graphformer在通用图数据上实现了最先进的性能。它揭示了有效地将结构信息编码到Transformer中的必要性。他们的发现与我们关于拓扑结构方面的分析一致，因此我们提出了适用于我们的Hyperformer的强大的k-Hop RPE（相对位置嵌入）。</li>
<li><strong>Hypergraph Representation（超图表示）</strong>：在现实世界的情景中，关系可能超出了成对的关联。超图进一步考虑了数据中的高阶关联。虽然可以通过诸如团扩展等技术将超图近似建模为图，但这种近似无法捕获数据中的高阶关系，导致性能不可靠。这激发了对超图上的学习的研究。已经提出了基于注意力的超图模型，用于多模态学习和归纳文本分类。我们提出的HyperSA是专为基于骨骼的动作识别而设计的第一个超图注意力模型。</li>
</ol>
<p><strong>Representation of skeleton data（骨骼数据的表示）</strong>：这段文字介绍了两种方法来表示骨骼数据，即图表示和超图表示。图表示使用简单的图模型（如GCN）来表示骨骼数据，这种方法在动作识别中广泛使用。超图表示考虑了数据中更高阶的关联关系，这对于某些复杂的关联关系可能更加有用，特别是在超出成对关联的情况下。作者提出了一种新的超图注意力模型（HyperSA），专门用于骨骼数据的动作识别，以更好地捕获这些高阶关系。这是一种创新性的方法，有望提高动作识别性能。</p>
<h2 id="Preliminaries">Preliminaries</h2>
<p>自注意力（Self-Attention）机制是一种用于处理序列数据的方法，通常在Transformer等深度学习模型中使用。在这个机制中，首先，输入序列中的每个标记都会被映射成**Key（键），Query（查询）和Value（值）**三元组。然后，两个标记之间的所谓注意力分数由它们的Query和Key的点积经过softmax函数计算而得。最终，<strong>每个位置的输出是所有值的加权和，权重由这些注意力分数决定</strong>。多头自注意力（Multi-Head Self-Attention，MHSA）是Transformer等模型中常用的扩展版本，<strong>它将通道维度划分为子组，并在每个子组上并行应用自注意力，以学习不同类型的依赖关系</strong>。</p>
<p>超图表示（Hypergraph representation）与标准图的边（edges）不同，它的超边（hyperedge）可以连接两个或多个顶点。一个==无权超图==（unweighted hypergraph）可以表示为H = (V, E)，其中包括<strong>一个顶点集合V和一个超边集合E</strong>。超图H可以用一个|V|×|E|的关联矩阵H来表示，其元素定义如下：如果顶点v在超边e中，则hv,e = 1，否则为0。节点v的度数（degree）定义为d(v) = ∑e∈E hv,e，超边e的度数定义为d(e) = ∑v∈V hv,e。度数矩阵De和Dv是通过将所有边的度数和所有顶点的度数设置为它们的对角线条目而构建的。</p>
<p>在这个工作中，我们考虑了一种特殊情况，即对于所有顶点，d(v) = 1，也就是说，身体关节被划分为|E|个不相交的子集，这在实践中是高效的。值得注意的是，在这种情况下，关联矩阵H等效于一个分区矩阵，其中每一行都是一个独热向量，表示每个关节所属的组。</p>
<p>简单来说，自注意力机制用于处理输入序列中的关系，而超图表示用于捕捉高阶关系，这对于骨骼数据等复杂关系数据的建模非常重要。</p>
<h3 id="总结-3">总结</h3>
<p><strong>3.1. Self-Attention</strong></p>
<p><strong>自注意力（Self-Attention）机制是一种用于处理序列数据的方法，通常在Transformer等深度学习模型中使用。</strong></p>
<p>Given an input sequence in the form of X = (x1, …, xn), each token xi is first projected into Key ki , Query qi and Value vi triplets.</p>
<p><strong>对于以X = (x1, …, xn)形式的输入序列，首先将每个标记xi投影为Key ki，Query qi和Value vi三元组。</strong></p>
<p>Then the so-called attention score Aij between two tokens is obtained by applying a softmax function to the dot product of qi and kj [39]:</p>
<p><strong>然后，两个标记之间的所谓注意力分数Aij通过将qi和kj [39]的点积应用softmax函数来获得：</strong></p>
<p>Aij = qi · kj,</p>
<p>the final output at each position is computed as the weighted sum of all Values:</p>
<p><strong>每个位置的最终输出是所有Value的加权和：</strong></p>
<p>yi = ∑j=1 to n Aijvj.</p>
<p>An extension called Multi-Head Self-Attention (MHSA) is often adopted by Transformers in practice. It divides the channel dimension into subgroups and apply Self-Attention to each subgroup in parallel to learn different kinds of inter-dependencies.</p>
<p><strong>实际上，Transformer等模型通常采用名为Multi-Head Self-Attention (MHSA)的扩展版本。它将通道维度划分为子组，并在每个子组上并行应用自注意力，以学习不同类型的依赖关系。</strong></p>
<p>For simplicity, we omit the notation of MHSA in this paper.</p>
<p><strong>为简单起见，在本文中，我们省略了MHSA的符号表示。</strong></p>
<p><strong>3.2. Hypergraph representation</strong></p>
<p><strong>3.2. 超图表示</strong></p>
<p>Unlike standard graph edges, a hyperedge in a hypergraph connects two or more vertices. An unweighted hypergraph is defined as H = (V, E), which consists of a vertex set V and a hyperedge set E.</p>
<p><strong>不同于标准图的边，超图中的超边连接两个或多个顶点。无权超图被定义为H = (V, E)，包括一个顶点集合V和一个超边集合E。</strong></p>
<p>The hypergraph H can be denoted by a |V| × |E| incidence matrix H, with entries defined as follows:</p>
<p><strong>超图H可以用一个|V|×|E|的关联矩阵H来表示，其元素定义如下：</strong></p>
<p>hv,e = 1, if v ∈ e 0, if v /∈ e</p>
<p><strong>如果顶点v在超边e中，则hv,e = 1，否则为0。</strong></p>
<p>The degree of a node v ∈ V is defined as d(v) = ∑e∈E hv,e, and the degree of a hyperedge e ∈ E is defined as d(e) = ∑v∈V hv,e.</p>
<p><strong>节点v ∈ V的度数定义为d(v) = ∑e∈E hv,e，超边e ∈ E的度数定义为d(e) = ∑v∈V hv,e。</strong></p>
<p>The degree matrices De and Dv are constructed by setting all the edge degrees and all the vertex degrees as their diagonal entries, respectively.</p>
<p><strong>度数矩阵De和Dv是通过将所有边的度数和所有顶点的度数分别设置为它们的对角线条目而构建的。</strong></p>
<p>In this work, we consider the special case of d(v) = 1 for all vertices, i.e., body joints are divided into |E| disjoint subsets, which is efficient in practice.</p>
<p><strong>==在这项工作中，我们考虑了一种特殊情况，即对于所有顶点，d(v) = 1，也就是说，身体关节被划分为|E|个不相交的子集，在实践中是高效的==。</strong></p>
<p>Notably, the incidence matrix H is equivalent to a partition matrix in this case. Each row is a one-hot vector denoting the group to which each joint belongs.</p>
<p><strong>值得注意的是，在这种情况下，关联矩阵H等效于一个分区矩阵。每一行都是一个独热向量，表示每个关节所属的组。</strong></p>
<p>简单来说，自注意力机制用于处理输入序列中的关系，而超图表示用于捕捉高阶关系，这对于骨骼数据等复杂关系数据的建模非常重要。</p>
<h3 id="超图-超边和节点点集合，以及边度和节点度矩阵">超图 超边和节点点集合，以及边度和节点度矩阵</h3>
<h3 id="Relax-the-binary-partition-matrix">Relax the binary partition matrix</h3>
<p>“Relax the binary partition matrix” 是一个用于聚类分析的术语，通常用于描述在聚类算法中，将传统的二进制（0或1）的分区矩阵（partition matrix）进行一种松弛（relax）或放宽，以允许每个数据点同时属于多个簇或类别的情况。</p>
<p>在传统的硬聚类（Hard Clustering）中，每个数据点仅属于一个簇，因此分区矩阵中的每个元素要么是0，表示数据点不属于某个簇，要么是1，表示数据点属于某个簇。这种二进制分区矩阵通常用于K均值聚类等算法。</p>
<p>然而，在某些情况下，数据点可能不仅仅属于一个簇，而是模糊地属于多个簇。为了处理这种情况，可以使用&quot;relax the binary partition matrix&quot; 的方法，将分区矩阵中的元素变成介于0和1之间的小数值，表示数据点对于不同簇的隶属度（membership degree）。这种方法通常用于软聚类（Soft Clustering）或模糊聚类（Fuzzy Clustering）中。</p>
<p>举例来说，考虑一个具有三个簇的数据集，一个硬聚类的分区矩阵可能如下所示：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">csharpCopy</span> code[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]<span class="meta"></span></span><br><span class="line"><span class="meta">[0, 1, 0]</span><span class="meta"></span></span><br><span class="line"><span class="meta">[0, 0, 1]</span></span><br></pre></td></tr></table></figure>
<p>在这个分区矩阵中，每个数据点被分配给一个唯一的簇。</p>
<p>然而，如果我们采用&quot;relax the binary partition matrix&quot; 的方法，我们可以得到一个软聚类的分区矩阵，如下所示：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">csharpCopy</span> code[<span class="number">0</span>.<span class="number">8</span>, <span class="number">0</span>.<span class="number">2</span>, <span class="number">0</span>.<span class="number">0</span>]<span class="meta"></span></span><br><span class="line"><span class="meta">[0.1, 0.9, 0.0]</span><span class="meta"></span></span><br><span class="line"><span class="meta">[0.0, 0.1, 0.9]</span></span><br></pre></td></tr></table></figure>
<p>在这个分区矩阵中，每个数据点对于不同簇都有一定的隶属度。例如，第一个数据点在簇1中的隶属度为0.8，在簇2中的隶属度为0.2，而不再是严格的0或1。</p>
<p>这种松弛的方法允许更灵活地处理数据，特别是在存在数据点模糊隶属度的情况下。它在模糊聚类、概率图模型等领域中有广泛应用。</p>
<blockquote>
<p>Speciﬁcally, a novel Hypergraph Self-Attention (HyperSA) layer is in- troduced, which makes Transformers aware of extra higher- order relations shared by a subset of joints connected to each hyperedge.</p>
</blockquote>
<p>这句话是在描述一种新的层次结构，称为&quot;Hypergraph Self-Attention (HyperSA)&quot;层。这个层次结构被引入到模型中，以使Transformer模型能够意识到一组关节连接到每个&quot;hyperedge&quot;的额外高阶关系。</p>
<p>在这里，&quot;hyperedge&quot;表示超图中的一种特殊边，连接着两个或更多的顶点（关节）。所谓的&quot;higher-order relations&quot;是指连接到这个超边的关节之间的更复杂的关系，这些关系不仅仅是直接连接的关系，而是涉及到多个关节的互动。</p>
<p>因此，这个新的HyperSA层的作用是在处理输入数据时，允许Transformer模型考虑到与超边连接的关节之间的更复杂的关系，这样模型可以更好地捕获数据中的高阶结构和特征。这有助于提高模型对数据的理解和性能。</p>
<h3 id="4-1-Deriving-the-hyperedge-feature-导出超边缘特征">4.1 Deriving the hyperedge feature 导出超边缘特征</h3>
<ol>
<li>“Given an incidence matrix H, we propose an effective approach to obtain the feature representation for each subset of joints connected to a hyperedge.”
<ul>
<li>给定一个关联矩阵 H，我们提出了一种有效的方法，用于获取与超边连接的每个关节子集的特征表示。</li>
</ul>
</li>
<li>“Let C denote the number of feature dimensions, individual joint features X ∈ R|V|×C are first aggregated into subset representations E ∈ R|E|×C by the following rule:”
<ul>
<li>令 C 表示特征维度的数量，首先根据以下规则将各个关节的特征 X ∈ R|V|×C 聚合成子集表示 E ∈ R|E|×C：</li>
</ul>
</li>
<li>“E = D−1 e HXWe,”
<ul>
<li>E = D^(-1)_e H * (X * We)，</li>
<li>这一步骤是将每个关节特征与相应的超边连接进行汇总。这里使用了度矩阵的逆来进行归一化，以及投影矩阵 We 来最终转换超边的特征。</li>
</ul>
</li>
<li>“Then we construct an augmented hyperedge representation Eaug ∈ R|V|×C by assigning hyperedge representations to the position of each associated joint:”
<ul>
<li>然后，我们通过将超边的表示分配给每个关联关节的位置来构建增强的超边表示 Eaug ∈ R|V|×C：</li>
</ul>
</li>
<li>“Eaug = HD−1 e HXWe.”
<ul>
<li>Eaug = HD^(-1)_e H * (X * We)。</li>
<li>这一步骤将超边的表示重新分配到与之关联的每个关节的位置上，以构建一个更丰富的超边表示。</li>
</ul>
</li>
</ol>
<p>在这个章节中，作者讨论了如何根据超边连接的关节子集来获取特征表示。通过对关联矩阵和关节特征的处理，他们构建了超边的表示，并将其分配给每个相关联的关节，以创建一个更丰富的超边表示。这有助于模型更好地理解超边上的高阶关系，从而提高了超图注意力机制的性能。</p>
<h3 id="4-2-Encoding-human-skeleton-structure">4.2 Encoding human skeleton structure</h3>
<ol>
<li>“Human body joints are naturally connected with bones and form, together with the latter, a bio-mechanical model.”
<ul>
<li>人体关节自然与骨骼相连接，并与后者一起形成生物力学模型。</li>
</ul>
</li>
<li>“In such a mechanical system, the movement of each joint in an action is strongly influenced by their connectivities.”
<ul>
<li>在这种机械系统中，动作中每个关节的运动都受到它们的连接性的强烈影响。</li>
</ul>
</li>
<li>“Therefore, it is beneficial to take the structural information of human skeleton into account.”
<ul>
<li>因此，考虑人体骨骼的结构信息是有益的。</li>
</ul>
</li>
<li>“Analogous to the established Relative Positional Embedding (RPE) for image [42] and language [15, 30] Transformer, we propose a powerful k-Hop Relative Positional Embedding Rij ∈ RC, which is indexed from a learnable parameter table by the Shortest Path Distance (SPD) between the ith and jth joints.”
<ul>
<li>类似于已建立的用于图像和语言Transformer的相对位置嵌入（RPE），我们提出了一种强大的k-Hop相对位置嵌入 Rij ∈ RC，它是根据第i个和第j个关节之间的最短路径距离（SPD）从可学习参数表中索引的。</li>
</ul>
</li>
<li>“In comparison to the learnable scalar spatial encoding in [48], it has larger capacity and interacts with the query additionally.”
<ul>
<li>与[48]中可学习的标量空间编码相比，它具有更大的容量，并额外与查询进行交互。</li>
</ul>
</li>
</ol>
<p>在这个章节中，作者讨论了如何将人体骨骼的结构信息编码到模型中。他们提出了一种相对位置嵌入方法，类似于用于图像和语言Transformer的方法，但针对骨骼数据的特点进行了定制。这种嵌入方法有助于模型更好地理解骨骼结构，并与查询进行交互，从而提高了模型性能。这有助于改进动作识别的准确性和效率。</p>
<h3 id="4-3-Hypergraph-Self-Attention">4.3 Hypergraph Self-Attention</h3>
<ol>
<li>“With the obtained hyperedge representation and skeleton topology encoding, we now define our Hypergraph Self-Attention as follows:”
<ul>
<li>通过获取超边表示和骨骼拓扑编码，我们现在定义我们的Hypergraph Self-Attention如下：</li>
</ul>
</li>
<li>“Aij = qi · kj (a) + qi · Eaug,j (b) + qi · Rφ(i,j) © + u · Eaug,j (d), (6) where u ∈ RC is a learnable static key regardless of the query position.”
<ul>
<li>Aij = qi · kj（a） + qi · Eaug,j（b） + qi · Rφ(i,j)（c） + u · Eaug,j（d），其中u ∈ RC 是一个可学习的静态密钥，不受查询位置影响。</li>
</ul>
</li>
<li>“Term (a) alone is the vanilla SA, which represents joint-to-joint attention.”
<ul>
<li>术语（a）单独是普通的SA，表示关节对关节的注意力。</li>
</ul>
</li>
<li>“Term (b) computes the joint-to-hyperedge attention between the ith query and the corresponding hyperedge of the jth key.”
<ul>
<li>术语（b）计算第i个查询与第j个密钥的相应超边之间的关节对超边的注意力。</li>
</ul>
</li>
<li>“Term © is the term for injecting the structural information of human skeleton with k-Hop Relative Positional Embedding.”
<ul>
<li>术语（c）用于通过k-Hop相对位置嵌入注入人体骨骼的结构信息。</li>
</ul>
</li>
<li>“Term (d) is intended for calculating the attentive bias of different hyperedges independent of the query position. It assigns the same amount of attention to each joint connected to a certain hyperedge.”
<ul>
<li>术语（d）旨在计算不同超边的注意偏差，与查询位置无关。它为连接到某个超边的每个关节分配相同数量的注意力。</li>
</ul>
</li>
<li>“Note that terms (a) and (b) can be combined by distributive law and require merely an extra step of matrix addition.”
<ul>
<li>请注意，术语（a）和（b）可以通过分配法则组合，只需要额外的矩阵加法步骤。</li>
</ul>
</li>
<li>“Moreover, term (d) has O(|V|C2) complexity and thus requires negligible computation in comparison to term (a).”
<ul>
<li>此外，术语（d）具有O（|V|C2）复杂性，因此与术语（a）相比需要极少的计算。</li>
</ul>
</li>
<li>“Relational Bias Transformers assume the input tokens to be homogeneous, whereas human body joints are inherently heterogeneous, e.g. each physical joint plays a unique role and thus has different relations to others.”
<ul>
<li>关系偏置变换器假定输入标记是同质的，而人体关节本质上是异质的，例如，每个物理关节都有独特的角色，因此与其他关节的关系不同。</li>
</ul>
</li>
<li>“Taking the heterogeneity of the skeleton data into account, we propose to represent the inherent relation of each joint pair as a scalar trainable parameter Bij, called Relational Bias (RB).”
<ul>
<li>考虑到骨骼数据的异质性，我们提出将每对关节的固有关系表示为标量可训练参数Bij，称为关系偏置（RB）。</li>
</ul>
</li>
<li>“It is added to the attention scores before aggregating the global information:”
<ul>
<li>它在聚合全局信息之前添加到注意分数中：</li>
</ul>
</li>
<li>“yi = n j=1 (Aij +Bij)vj”
<ul>
<li>yi = n j=1 (Aij +Bij)vj</li>
</ul>
</li>
</ol>
<p>在这个章节中，作者介绍了Hypergraph Self-Attention的定义和计算过程。他们<strong>将注意力分为不同的项，包括传统的自注意力项以及考虑超边关系和关节之间固有关系的项</strong>。这种设计有助于模型更好地理解骨骼数据的复杂性，并提高了动作识别的性能。此外，作者还提到了<strong>关系偏置（Relational Bias），它有助于处理人体关节之间的异质性。Hypergraph Self-Attention的设计和关系偏置的引入有助于改进模型的表现</strong>。</p>
<h3 id="4-4-Partition-strategy分区策略">4.4 Partition strategy分区策略</h3>
<ol>
<li>“Empirically, human skeletons could be divided into a number of body parts, which have been well studied in previous work [37, 17, 36].”
<ul>
<li>从经验上看，人类骨架可以分为多个身体部分，这在以前的研究中已经得到了很好的研究。</li>
</ul>
</li>
<li>“We experimentally show that our Hyperformer with empirical partitions yields excellent performance.”
<ul>
<li>我们通过实验证明，采用经验性分区的Hyperformer表现出色。</li>
</ul>
</li>
<li>“However, finding an optimal empirical partition strategy is laborious and the optimal partition strategy is restricted to a certain skeleton with a fixed number of recorded joints.”
<ul>
<li>然而，找到最佳的经验分区策略是费时的，最佳的分区策略限制在具有固定数量记录关节的特定骨架上。</li>
</ul>
</li>
<li>“In this work, we also provide an approach to automate the search process for an effective partition strategy.”
<ul>
<li>在这项工作中，我们还提供了一种自动化搜索有效分区策略的方法。</li>
</ul>
</li>
<li>“To make the partition matrix learnable, we parameterize and relax the binary partition matrix to its continuous version by applying a softmax along its column axis:”
<ul>
<li>为了使分区矩阵可学习，我们通过在其列轴上应用softmax将二进制分区矩阵参数化和松弛为其连续版本：</li>
</ul>
</li>
<li>“The problem of finding an optimal discrete partition matrix H is thus reduced to learning an optimal continuous partition matrix H˜, which can be optimized jointly with Transformer parameters.”
<ul>
<li>因此，寻找最佳离散分区矩阵H的问题被简化为学习最佳连续分区矩阵H˜，可以与Transformer参数一起进行优化。</li>
</ul>
</li>
<li>“At the end of the optimization, a discrete partition matrix can be obtained by applying an argmax operation along each row of ˜H:”
<ul>
<li>在优化结束时，可以通过在˜H的每一行上应用argmax操作来获得离散分区矩阵：</li>
</ul>
</li>
<li>“Note that a number of different proposals can be easily acquired by varying the initialization of H˜.”
<ul>
<li>请注意，通过改变H˜的初始化，可以轻松获得许多不同的提议。</li>
</ul>
</li>
<li>“We experimentally show that all the proposals prove to be reasonable.”
<ul>
<li>我们通过实验证明，所有的提议都是合理的。</li>
</ul>
</li>
<li>“Interestingly, all the learned proposals are symmetric as shown in Fig. 3, indicating that symmetry is an important aspect of inherent joint relations.”
<ul>
<li>有趣的是，所有学到的提议都是对称的，如图3所示，这表明对称性是固有关节关系的重要方面。</li>
</ul>
</li>
</ol>
<p>在这个章节中，作者讨论了关于如何分割人体骨骼数据以提高模型性能的问题。他们提到，虽然经验性的分区方法可以取得良好的效果，但寻找最佳的经验分区策略是困难且具有限制性的。因此，他们提出了一种自动化的方法，通过<strong>将分区矩阵参数化为连续版本</strong>，<strong>并将其与Transformer参数一起进行优化来找到最佳的分区</strong>策略。最后，他们指出所有学到的提议都具有对称性，这表明对<strong>称性在关节关系中是一个重要的方面。</strong></p>
<h3 id="4-5-Model-architecture">4.5 Model architecture</h3>
<ol>
<li>“We first revisit the architectural design of Transformers for skeleton data.”
<ul>
<li>我们首先重新审视了用于骨骼数据的Transformer的架构设计。</li>
</ul>
</li>
<li>“Then we built our Hyperformer based on our analysis.”
<ul>
<li>然后，我们根据我们的分析构建了我们的Hyperformer。</li>
</ul>
</li>
<li>“<strong>HyperSA</strong> is employed for <strong>spatial modeling</strong> of each frame and a <strong>lightweight convolutional module</strong> is adopted for <strong>temporal modeling</strong>, following the design of state-of-the-art models [7, 3] in this field.”
<ul>
<li>我们采用HyperSA来对每个帧进行空间建模，采用轻量级卷积模块来进行时间建模，遵循了这一领域现有先进模型的设计。</li>
</ul>
</li>
<li>“Spatial Modeling We apply Layer Normalization (LN) before the multi-head HyperSA and add a residual connection to the output, following the standard Transformer architecture [39].”
<ul>
<li>空间建模：我们在多头HyperSA之前应用层归一化（LN），并在输出中添加了残差连接，遵循了标准的Transformer架构。</li>
</ul>
</li>
<li>“Based on our analysis in Sec. 1, we further remove the Multi-Layer-Perceptron (MLP) layers. To introduce non-linearity, a ReLU layer is added after each block of spatial and temporal modeling modules instead.”
<ul>
<li>根据我们在第1节中的分析，我们进一步<strong>移除了多层感知机（MLP）层</strong>。为了引入非线性，我们在每个空间和时间建模模块块之后添加了ReLU层。</li>
</ul>
</li>
<li>“Temporal Modeling To model the temporal correlation of the human pose, we adopt the Multi-Scale Temporal Convolution (MS-TC) module [3, 26, 7] for our final model. This module contains three convolution branches with a 1 × 1 convolution to reduce channel dimension, followed by different combinations of kernel sizes and dilations. The outputs of convolution branches are concatenated.”
<ul>
<li>时间建模：为了建模人体姿势的时间相关性，我们采用了<strong>多尺度时间卷积（MS-TC）模块作为我们的最终模型</strong>。该模块包含三个卷积分支，其中包括1×1卷积以减小通道维度，然后是不同组合的核尺寸和膨胀。卷积分支的输出被级联在一起。</li>
</ul>
</li>
<li>“Hyperformer is constructed by stacking HyperSA and Temporal Convolution layers alternately as follows:”
<ul>
<li>==Hyperformer是通过交替堆叠HyperSA和时间卷积层来构建的==，如下所示：</li>
</ul>
</li>
<li>“z(l) = HyperSA(LN(z(l−1))) + z(l−1)”
<ul>
<li>这里定义了Hyperformer的一个层次结构，其中z(l)表示第l层的输出。在每一层，首先应用HyperSA，然后进行层归一化（LN），最后将前一层的输出添加到当前层的输出中。</li>
</ul>
</li>
<li>“z(l) = TemporalConv(LN(z(l))) + z(l−1)”
<ul>
<li>这是Hyperformer中的另一层次结构，其中z(l)表示第l层的输出。在每一层，首先应用TemporalConv，然后进行层归一化（LN），最后将前一层的输出添加到当前层的输出中。</li>
</ul>
</li>
<li>“z(l) = ReLU(z(l))”
<ul>
<li>这一步引入了非线性，通过在每一层的输出上应用ReLU激活函数来增加模型的表达能力。</li>
</ul>
</li>
</ol>
<p>在&quot;Model architecture&quot;章节中，作者描述了Hyperformer模型的架构设计。他们使用了HyperSA来进行空间建模，并采用轻量级卷积模块进行时间建模。模型的构建基于Transformer架构，但根据他们的分析，他们移除了MLP层并添加了ReLU层来引入非线性。这个章节还详细描述了模型中的空间建模和时间建模层次结构，以及它们之间的堆叠方式。这些设计和结构对于理解Hyperformer的工作原理和性能至关重要。</p>
<h1>GRAPH CONTRASTIVE LEARNING FOR SKELETON BASED ACTION RECOGNITION</h1>
<h2 id="abstract-3">abstract</h2>
<p>这篇文章提出了一种用于骨骼动作识别的图对比学习框架（SkeletonGCL），旨在<strong>探索跨所有序列的全局上下文</strong>。当前表现出色的图卷积网络（GCN）在构建自适应图进行特征聚合时利用了序列内部的上下文信息，但作者认为这种上下文仍然是局部的，因为丰富的跨序列关系尚未明确研究。SkeletonGCL的主要思想是<strong>通过强制要求图具有类别判别性，即类内紧凑而类间分散，来实现跨序列的图学习</strong>。这提高了<strong>GCN在区分不同动作模式方面的能力</strong>。此外，SkeletonGCL还设计了两个内存库，<strong>用于从两个互补的层面（即实例级别和语义级别）丰富跨序列上下文</strong>，从而在多个上下文尺度上进行图对比学习。因此，SkeletonGCL建立了一种新的训练范式，并可以无缝地与当前的GCNs结合使用。在不失一般性的情况下，作者将SkeletonGCL与三个GCNs（2S-ACGN、CTR-GCN和InfoGCN）相结合，取得了NTU60、NTU120和NW-UCLA基准测试中持续的改进。</p>
<p>总结要点：</p>
<ul>
<li>该论文提出了SkeletonGCL框架，用于骨骼动作识别，旨在探索跨多个序列的全局上下文。</li>
<li>SkeletonGCL通过强化图的类别判别性来改善GCN的性能，使其能够更好地区分不同的动作模式。</li>
<li>框架包括两个内存库，用于在多个上下文尺度上进行图对比学习。</li>
<li>SkeletonGCL的方法可以与当前的GCNs无缝结合，且在NTU60、NTU120和NW-UCLA基准测试中都取得了一致的改进。</li>
</ul>
<h2 id="introduction-2">introduction</h2>
<ol>
<li><strong>Graph convolutional networks (GCNs) have been widely applied in skeleton-based action recognition since they can naturally process non-grid skeleton sequences.</strong>
<ul>
<li>GCNs广泛应用于基于骨骼的动作识别领域，因为它们可以自然地处理非网格骨骼序列。</li>
</ul>
</li>
<li><strong>For GCN-based methods, how to effectively learn the graphs remains a core and challenging problem.</strong>
<ul>
<li>对于基于GCN的方法，如何有效地学习图仍然是一个核心且具有挑战性的问题。</li>
</ul>
</li>
<li><strong>In particular, ST-GCN (Yan et al., 2018) is a milestone work, using pre-defined graphs to extract the action patterns. However, the pre-defined graphs only enable each joint to access the fixed neighboring joints but are hard to capture long-range dependency adaptively.</strong>
<ul>
<li>具体来说，ST-GCN（Yan等人，2018年）是一项具有里程碑意义的工作，使用预定义的图来提取动作模式。然而，预定义的图仅允许每个关节访问固定的相邻关节，但难以自适应地捕捉长距离依赖性。</li>
</ul>
</li>
<li><strong>Therefore, a mainstream of subsequent works (Li et al., 2019; Shi et al., 2019; Zhang et al., 2020b;a; Ye et al., 2020; Chen et al., 2021b; Chi et al., 2022) take efforts to solve this issue by generating adaptive graphs. The adaptive graphs can dynamically aggregate the features within each sequence and thus show significant advantages in performance comparison.</strong>
<ul>
<li>因此，随后的大部分研究（Li等人，2019年；Shi等人，2019年；Zhang等人，2020b；Ye等人，2020年；Chen等人，2021b；Chi等人，2022年）努力通过生成自适应图来解决这个问题。自适应图可以动态地聚合每个序列中的特征，因此在性能比较中显示出明显的优势。</li>
</ul>
</li>
<li><strong>Generally, adaptive graphs are constructed by using intra-sequence context. However, such context will still be “local” when viewing the cross-sequence information as an available context. Therefore, we wonder: Is it possible to involve the cross-sequence context in graph learning?</strong>
<ul>
<li>通常，自适应图是通过使用序列内部的上下文构建的。然而，当将跨序列信息视为可用的上下文时，这种上下文仍然是“局部”的。因此，我们想知道：是否可能将跨序列的上下文引入到图学习中？</li>
</ul>
</li>
<li><strong>To find out the answer, in Fig. 1, we visualize the adaptive graphs learned from sequences of two easily confused classes (“point to something” and “take a selfie”). The graphs are learned by a strong GCN, i.e., CTR-GCN (Chen et al., 2021b).</strong>
<ul>
<li>为了找到答案，在图1中，我们可视化了从两个容易混淆的类别（“指向某物”和“自拍”）的序列中学到的自适应图。这些图是由强大的GCN，即CTR-GCN（Chen等人，2021b）学习的。</li>
</ul>
</li>
<li><strong>These observations remind us that graph learning in current adaptive GCNs can implicitly learn class-specific graph representations to some extent. But without explicit constraints, it leads to class-ambiguous representations in some cases, thereby affecting the GCN capacity to discriminate classes (in Tab. 9 of Sec. 4.4, we provide quantitative results to further support our hypothesis).</strong>
<ul>
<li>这些观察结果提醒我们，当前自适应GCNs中的图学习在一定程度上可以隐式地学习特定类别的图表示。但是，在没有明确约束的情况下，这在某些情况下会导致类别不明确的表示，从而影响了GCN区分类别的能力（在第4.4节的表格9中，我们提供了定量结果以进一步支持我们的假设）。</li>
</ul>
</li>
<li><strong>Therefore, we speculate that if the cross-sequence semantic relations are incorporated as guidance in graph learning, the class-ambiguity issue will be alleviated and the graph representations will better express individual characteristics of actions.</strong>
<ul>
<li>因此，我们推测，如果将跨序列的语义关系作为图学习的指导因素，将会缓解类别不明确的问题，图表示将更好地表达动作的个体特征。</li>
</ul>
</li>
<li><strong>In recent years, contrastive learning has achieved great success in self-supervised representation learning (He et al., 2020; Chen et al., 2020; 2021a). It studies cross-sample relations in the dataset. The essence of contrastive learning is “comparing”, which pulls together the feature embedding from positive pairs and pushes away the feature embedding from negative pairs.</strong>
<ul>
<li>最近几年，对比学习在自监督表示学习（He等人，2020年；Chen等人，2020年；2021a年）中取得了巨大的成功。它研究数据集中的样本间关系。对比学习的本质是“比较”，它将正样本的特征嵌入拉拢在一起，将负样本的特征嵌入推开。</li>
</ul>
</li>
<li><strong>Based on the analysis above and the advances in contrastive learning, we propose a graph contrastive learning framework for skeleton-based action recognition in the fully-supervised setting, dubbed SkeletonGCL.</strong></li>
</ol>
<ul>
<li>基于上述分析和对比学习的进展，我们<strong>提出了一个用于完全监督的基于骨骼的动作识别的图对比学习框架，称为SkeletonGCL</strong>。</li>
</ul>
<ol>
<li><strong>Instead of just using the local information within each sequence, SkeletonGCL explores the cross-sequence global context to improve graph learning.</strong></li>
</ol>
<ul>
<li>SkeletonGCL==不仅仅使用每个序列内的局部信息，还探索跨序列的全局上下文以改进图学习==</li>
</ul>
<ol>
<li><strong>SkeletonGCL achieves consistent improvements combined with three GCNs (2S-AGCN (Shi et al., 2019), CTR-GCN (Chen et al., 2021b), and InfoGCN (Chi et al., 2022)), on three popular benchmarks (NTU60 (Shahroudy et al., 2016), NTU120 (Liu et al., 2019) and NW-UCLA (Wang et al., 2014)).</strong></li>
</ol>
<ul>
<li>SkeletonGCL与三个GCNs（2S-AGCN（Shi等人，2019年），CTR-GCN（Chen等人，2021b年）和InfoGCN（Chi等人，2022年）结合使用，在三个流行的基准测试（NTU60（Shahroudy等人，2016年），NTU120（Liu等人，2019年）和NW-UCLA（Wang等人，2014年）上取得了一致的改进。</li>
</ul>
<ol>
<li><strong>SkeletonGCL only introduces a small amount of training consumption but has no impact at the test stage.</strong></li>
</ol>
<ul>
<li>SkeletonGCL仅引入了少量的训练消耗，但对测试阶段没有影响。</li>
</ul>
<ol>
<li><strong>Though there exist some works that apply contrastive learning in skeleton-based action recognition (Li et al., 2021; Guo et al., 2022; Mao et al., 2022), our method differs from them as follows:</strong></li>
</ol>
<ul>
<li>尽管存在一些将对比学习应用于基于骨骼的动作识别的方法（Li等人，2021年；Guo等人，2022年；Mao等人，2022年），但我们的方法与它们有以下不同：</li>
</ul>
<ol>
<li><strong>Summarily, the contribution of this paper can be concluded as follows:</strong></li>
</ol>
<ul>
<li>总之，本文的贡献可以总结如下：</li>
</ul>
<p>以上是引言部分的逐句翻译和内容解释。这个引言部分介绍了论文的研究背景、问题陈述、动机和贡献，以及提出的SkeletonGCL框架的核心思想。它强调了如何利用对比学习来改进图学习，以及如何将跨序列语义关系引入图学习中。最后，它提到了SkeletonGCL在基准测试中的一致改进和其与以往方法的不同之处。</p>
<blockquote>
<p>逐字翻译************************************************************************************************************************************************************************************************************************************************************************</p>
</blockquote>
<ol>
<li><strong>The core idea is to pull together the learned graphs from the same class while pushing away the learned graphs from different classes.</strong>
<ul>
<li>核心思想是将来自相同类别的学习图聚集在一起，同时将来自不同类别的学习图推开。</li>
<li>解释：这句话描述了SkeletonGCL的核心概念。它的目标是通过拉近相同类别的图，同时将不同类别的图分开，以帮助模型更好地理解不同的动作模式。</li>
</ul>
</li>
<li><strong>Since graphs can reveal the action patterns of sequences, enforcing graph consistency in the same class and inconsistency among different classes helps the model understand various motion modes.</strong>
<ul>
<li>由于图可以揭示序列的动作模式，==<strong>强化相同类别中的图一致性以及不同类别之间的不一致性有助于模型理解各种运动模式</strong>==。</li>
<li>解释：这句话强调了在相同类别内保持图的一致性，同时在不同类别之间引入不一致性，以帮助模型更好地理解不同的动作模式。</li>
</ul>
</li>
<li><strong>In addition, to enrich the cross-sequence context, we build memory banks to store the graphs from historical sequences.</strong>
<ul>
<li>此外，为了丰富跨序列的上下文，我们建立了存储历史序列图的内存库。</li>
<li>解释：为了增强模型对跨序列上下文的理解，作者引入了内存库，用于存储历史序列的图信息。</li>
</ul>
</li>
<li><strong>In specific, an instance-level memory bank stores the sequence-wise graphs, which hold the individual properties of each sequence. In contrast, a semantic-level memory bank stores the aggregation of graphs from each class, which contains the class-level representation.</strong>
<ul>
<li>具体而言，<strong>==一个实例级内存库存储了按序列划分的图，其中包含了每个序列的个体属性。相比之下，一个语义级内存库存储了来自每个类别的图的汇总，其中包含了类别级别的表示==</strong></li>
<li>解释：作者详细解释了内存库的构建方式。实例级内存库存储了按序列组织的图，这些图包含了每个序列的特定属性。语义级内存库存储了来自每个类别的图的汇总信息，其中包含了类别级别的信息。</li>
</ul>
</li>
<li><strong>The two banks are complementary to each other, enabling us to leverage more samples.</strong>
<ul>
<li>这两个内存库相辅相成，使我们能够利用更多的样本。</li>
<li>解释：这句话强调了两个内存库的互补性，它们一起提供了更多的样本信息，有助于模型的训练。</li>
</ul>
</li>
<li><strong>SkeletonGCL can be seamlessly combined with existing GCNs.</strong>
<ul>
<li>SkeletonGCL可以无缝地与现有的GCNs结合使用。</li>
<li>解释：==SkeletonGCL可以轻松集成到现有的GCN模型中，为这些模型增加了图对比学习的功能==。</li>
</ul>
</li>
<li><strong>Eventually, we combine SkeletonGCL with three GCNs (2S-AGCN (Shi et al., 2019), CTR-GCN (Chen et al., 2021b), and InfoGCN (Chi et al., 2022)), and conduct experiments on three popular datasets (NTU60 (Shahroudy et al., 2016), NTU120 (Liu et al., 2019) and NW-UCLA (Wang et al., 2014)).</strong>
<ul>
<li>最后，我们将SkeletonGCL与三个GCNs（2S-AGCN（Shi等人，2019），CTR-GCN（Chen等人，2021b）和InfoGCN（Chi等人，2022））相结合，并在三个流行的数据集上进行实验（NTU60（Shahroudy等人，2016年），NTU120（Liu等人，2019年）和NW-UCLA（Wang等人，2014年））。</li>
<li>解释：这句话描述了作者的实验设计。他们将SkeletonGCL与三个不同的GCN模型结合，并在三个常用的数据集上进行了实验以评估其性能。</li>
</ul>
</li>
<li><strong>SkeletonGCL achieves consistent improvements with these models using different testing protocols (single-modal or multi-modal) on all three datasets, which widely demonstrates the effectiveness of our design.</strong>
<ul>
<li>SkeletonGCL在这些模型上使用不同的测试协议（单模态或多模态）在所有三个数据集上都取得了一致的改进，这充分证明了我们设计的有效性。</li>
<li>解释：这句话总结了实验结果，指出SkeletonGCL在不同的测试协议和数据集上都取得了一致的性能改进，证明了其有效性。</li>
</ul>
</li>
<li><strong>Notably, SkeletonGCL only introduces a small amount of training consumption but has no impact at the test stage.</strong>
<ul>
<li>值得注意的是，SkeletonGCL仅引入了少量的训练开销，但在测试阶段没有影响。</li>
<li>解释：这句话强调了SkeletonGCL的训练效率以及其在测试阶段的轻量性。</li>
</ul>
</li>
<li><strong>Though there exist some works that apply contrastive learning in skeleton-based action recognition (Li et al., 2021; Guo et al., 2022; Mao et al., 2022), our method differs from them as follows:</strong>
<ul>
<li>尽管存在一些将对比学习应用于基于骨骼的动作识别的工作（Li等人，2021年；Guo等人，2022年；Mao等人，2022年），但我们的方法与它们有以下不同：</li>
<li>解释：这句话强调了SkeletonGCL与之前的工作的不同之处，特别是与应用对比学习的工作的区别。</li>
</ul>
</li>
<li><strong>Summarily, the contribution of this paper can be concluded as follows:</strong>
<ul>
<li>总之，本文的贡献可以总结如下：</li>
<li>解释：这句话总结了论文的主要贡献，包括提出了一种新的图学习方法，结合了对比学习和跨序列图学习的思想，实现了一致的性能改进，并且具有高效的训练性能</li>
</ul>
</li>
</ol>
<h2 id="method-2">method</h2>
<h3 id="3-1-preliminary">3.1 preliminary</h3>
<ol>
<li><strong>We denote a human skeleton as a vertex set V = {v1, v2, …, vN}, where N denotes the number of vertices. For each vertex vi, the feature dimension is set as C. Hence, a skeleton sequence with T frames can be denoted as X ∈ RT×N×C.</strong>
<ul>
<li>我们将人类骨架表示为一个顶点集合 V = {v1, v2, …, vN}，其中 N 表示顶点的数量。对于每个顶点 vi，特征维度设置为 C。因此，具有 T 帧的骨架序列可以表示为 X ∈ RT×N×C。</li>
<li>解释：这部分介绍了作者如何表示人类骨架数据，其中 V 是顶点集合，vi 表示单个顶点，N 表示顶点数量，C 表示特征维度，X 是表示骨架序列的张量，包括时间帧数 T、顶点数量 N 和特征维度 C。</li>
</ul>
</li>
<li><strong>Graph topology is used to represent the correlations between joints, formulated as g.</strong>
<ul>
<li>图拓扑结构用于表示关节之间的相关性，表示为 g。</li>
<li>解释：这句话说明了作者将图拓扑结构用于表示关节之间的关系，将其表示为符号 g。</li>
</ul>
</li>
</ol>
<h4 id="GCNs-in-Skeleton-Based-Action-Recognition">GCNs in Skeleton-Based Action Recognition.</h4>
<ol>
<li><strong>GCNs in Skeleton-Based Action Recognition. Generally, GCN models alternatively apply graph convolutions and temporal convolutions to extract the spatial configuration and motion pattern of skeletons, respectively.</strong>
<ul>
<li>骨骼动作识别中的GCN。通常情况下，<strong>GCN模型交替应用图卷积和时间卷积来分别提取骨架的空间配置和运动模式</strong>。</li>
<li>解释：这部分介绍了骨骼动作识别中的GCN模型的工作原理，它们交替使用图卷积和时间卷积来提取骨架的空间和时间特征。</li>
</ul>
</li>
<li><strong>The graph g is vital for graph convolutions since it determines the message passing among joints. In current adaptive GCNs, g is learned within each sequence and has different sizes…</strong>
<ul>
<li>图 g 在图卷积中至关重要，因为它决定了关节之间的消息传递。<strong>在当前的自适应GCNs中，g 在每个序列中学习，而且具有不同的大小</strong>…</li>
<li>解释：这句话指出了图 g 在图卷积中的关键作用，它决定了关节之间的信息传递。当前的自适应GCNs中，每个序列都学习自己的图 g，而且这些图的大小可以不同。</li>
</ul>
</li>
<li><strong>The graph convolution is defined as: XS = KS∑k=1 gkXWkS, (1) where XS ∈ RT×N×C denotes the spatial extracted feature with C channels, and WS ∈ RKS×C×C denotes the spatial feature transformation filters.</strong>
<ul>
<li>图卷积的定义如下：XS = KS∑k=1 gkXWkS，其中 XS ∈ RT×N×C 表示带有 C 通道的空间提取特征，而 WS ∈ RKS×C×C 表示空间特征变换滤波器。</li>
</ul>
</li>
<li>解释：这句话给出了图卷积的数学定义，其中 XS 表示空间特征，KS 表示子图的数量，gk 表示子图，X 表示输入特征，WkS 表示空间特征变换滤波器。</li>
</ol>
<h4 id="Self-Supervised-Contrastive-Learning">Self Supervised Contrastive Learning</h4>
<ol>
<li><strong>Self-Supervised Contrastive Learning. In the context of self-supervised contrastive learning, each input sample is processed by data augmentations to produce a positive pair: I and I+. Through a feature extraction network, I and I+ are transformed into feature vectors f ∈ RD and f+ ∈ RD.</strong>
<ul>
<li>自监督对比学习。在自监督对比学习的背景下，每个输入样本都经过数据增强处理，以产生正样本对：I 和 I+。通过特征提取网络，I 和 I+ 被转化为特征向量 f ∈ RD 和 f+ ∈ RD。</li>
<li>解释：这句话描述了自监督对比学习的背景，其中输入样本经过数据增强处理，产生正样本对，然后通过特征提取网络将它们转化为特征向量。</li>
</ul>
</li>
<li><strong>As for the negative samples, they are selected from the dataset excluding I and I+, represented as a negative set N−. Each negative in N− is denoted as f− ∈ RD. The similarity between two feature vectors is calculated as sim(f+, f−) =f+f− .</strong>
<ul>
<li>至于负样本，它们是从除了 I 和 I+ 之外的数据集中选择的，表示为负样本集合 N−。N− 中的每个负样本用 f− ∈ RD 表示。两个特征向量之间的相似度计算为 sim(f+, f−) =f+f− 。</li>
<li>解释：这部分描述了如何选择负样本，并计算特征向量之间的相似度，其中正样本是 I 和 I+，负样本从数据集中排除了这两个样本。</li>
</ul>
</li>
<li><strong>InfoNCE (Gutmann &amp; Hyv¨arinen, 2010; Oordet al., 2018) is widely adopted for contrastive learning, which is formulated as: LNCE = −log sim(f, f+)/τ sim(f, f+)/τ + f−∈N− sim(f, f−)/τ , (3) where temperature τ &gt; 0 is a hyper-parameter.</strong>
<ul>
<li>InfoNCE（Gutmann＆Hyv¨arinen，2010; Oord等人，2018）被广泛用于对比学习，其公式为：LNCE = −log sim(f, f+)/τ sim(f, f+)/τ + f−∈N− sim(f, f−)/τ ，其中温度 τ &gt; 0 是一个超参数。</li>
<li>解释：这句话介绍了InfoNCE对比学习方法的公式，它用于计算对比损失，其中 τ 是一个超参数。对比学习的目标是使正样本之间的相似度高，而正样本与负样本之间的相似度低。</li>
</ul>
</li>
</ol>
<h3 id="3-2-GRAPH-CONTRASTIVE-LEARNING">3.2 GRAPH CONTRASTIVE LEARNING</h3>
<ol>
<li><strong>The proposed SkeletonGCL is illustrated in Fig. 2. The framework consists of two branches, where the classification branch is parallel to the graph contrast branch.</strong>
<ul>
<li>所提出的SkeletonGCL在图2中进行了说明。该框架由两个分支组成，其中分类分支与图对比分支平行。</li>
<li>解释：这句话介绍了SkeletonGCL方法，它包含两个分支，一个用于分类，另一个用于图对比。</li>
</ul>
</li>
<li><strong>Graph Projection Head. In order to contrast the graphs in a common feature space, we embed the graphs into vectors by a graph projection head.</strong>
<ul>
<li>图投影头。为了在公共特征空间中对比图，我们通过图投影头将图嵌入为向量。</li>
<li>解释：这里提到了一个“图投影头”，它用于将图形式的数据嵌入到向量空间中，以便在共同的特征空间中进行对比学习。</li>
</ul>
</li>
<li><strong>Memory Bank. To enrich the cross-sequence context, we build memory banks to store the cross-batch graphs.</strong>
<ul>
<li>内存银行。为了丰富跨序列的上下文信息，我们构建了内存银行来存储跨批次的图形数据。</li>
<li>解释：内存银行用于存储来自不同批次的图形数据，以增强模型对跨序列上下文的理解。</li>
</ul>
</li>
<li><strong>Hard Sampling. As the training continues, most samples become too easy, which contribute less to the training. Therefore, methods in… are proposed to use hard mining strategies to focus on informative samples.</strong>
<ul>
<li>硬采样。随着训练的进行，大多数样本变得太容易，对训练的贡献较小。因此，提出了…中的方法，用于使用硬采样策略，以便集中关注信息丰富的样本。</li>
<li>解释：在训练过程中，一些样本可能会变得太容易，无法提供足够的信息。因此，硬采样策略被用来选择更具挑战性的样本，以改善训练效果。作者提到了一种方法来选择硬样本。</li>
</ul>
</li>
</ol>
<p>这一章节主要描述了SkeletonGCL方法的关键部分，包括图对比和内存库的构建，以及硬采样策略的应用。该方法旨在通过对比学习来提取骨架数据中的特征，并借助内存库来丰富跨序列的上下文信息，以改善动作识别性能。硬采样策略用于选择更具挑战性的样本，以更有效地进行训练。</p>
<h3 id="3-2-1Graph-Projection-Head">3.2.1Graph Projection Head</h3>
<p>为了在一个共同的特征空间中对比这些图形，我们通过一个图形投影头将这些图形嵌入到向量中。不同GCNs的投影头是相似的（详细信息请参见附录6.1）。在图2中，以CTR-GCN（Chen等人，2021b）中学到的图形g∈RKS×C×N×N为例，我们首先通过平均池化层将g在通道维度上挤压成g∈RKS×N×N。然后，我们将图g展平为1D向量，表示为g∈RKSN2，并通过FC层WG∈RKSN2×Cg将g投影到向量v∈RCg中。WG中的不同通道对应于图中的不同顶点，因此图形投影是顶点感知的，因此可以编码骨架的结构。随后，我们使用v更新了两个内存库，这些内存库在图2中有详细说明。</p>
<p>这段话描述了如何将学习到的图形嵌入到向量中以进行对比学习。首先，图形被平均池化以适应相同维度，然后被展平并通过FC层进行投影。这些投影的目的是将图形编码为向量，以便后续进行对比学习。投影的权重是由不同的顶点和通道组成的，以确保对图形的不同部分进行了考虑。这些向量将用于更新两个内存库，这些库将在后文中进一步详细解释。这个过程有助于提高图形学习的效果，使模型能够更好地理解动作序列的结构和特征。</p>
<h3 id="3-2-2-Memory-bank">3.2.2 Memory bank</h3>
<p>为了丰富跨序列的上下文，我们建立了内存库来存储跨批次的图形数据。具体而言，我们构建了两个内存库，即一个是实例级别的内存库MIns，维度为RCk×P×Cg，另一个是语义级别的内存库MSem，维度为RCk×Cg。其中，P表示每个类别在MIns中存储的实例数。具体而言，MIns中的每个元素代表来自一个类别的图形实例。相比之下，MSem中的每个元素代表一个类别的图形汇总。因此，这两个内存库在互补的层次上工作，实例级别的内存库强调每个序列的实例区分，而语义级别的内存库涵盖了跨序列的类别属性。</p>
<p>我们以先进先出的方式更新MIns，以保持每个类别的实例数量为P。至于MSem，我们使用动量更新策略，该策略整合了来自当前时间戳和所有先前时间戳的相同类别的图形，被视为长期表示。动量更新定义如下：</p>
<p>mc∗ sem ← αmc∗ sem + (1 − α)v， 其中mc∗ sem是类别c∗的表示，c∗是输入I的类别标签，0&lt;α&lt;1是一个超参数。</p>
<p>这段文本描述了为了增强跨序列的上下文信息，作者建立了两个内存库：实例级别的内存库（MIns）和语义级别的内存库（MSem）。这些内存库用于存储不同类别的图形数据，从而帮助模型更好地理解不同动作模式。MIns按照先进先出的原则更新，而MSem使用动量更新策略，整合了来自多个时间戳的图形数据，以获得更全面的类别属性信息。这些内存库允许模型在训练过程中利用不同类别和实例的上下文信息，有助于提高动作识别的性能。</p>
<h3 id="3-2-4Hard-Sampling">3.2.4Hard Sampling.</h3>
<p>随着训练的进行，大多数样本变得太容易，对训练的贡献减少。因此，提出了一些方法（如Tabassum等人在2022年，Robinson等人在2020年，Kalantidis等人在2020年，Wang等人在2021年）来使用硬采样策略，集中关注信息丰富的样本。在本文中，考虑到MIns中包含大量实例，与所有这些实例进行对比自然会导致冗余并妨碍训练。为了缓解这个问题，我们提出在MIns中进行硬样本挖掘。具体来说，我们将相似度计算sim(v, v)作为评估样本难度的标准。更难的正样本具有较低的相似度，而更难的负样本具有较高的相似度。总而言之，对于MIns，我们选择K+ H个最难的正样本、K− H个最难的负样本和K− R个随机负样本。</p>
<p>这段话讨论了训练过程中的样本难度问题。随着训练的进行，许多样本变得太容易，它们对进一步的训练没有太大帮助。因此，一些研究提出了使用“硬采样策略”的方法，专注于选择更具挑战性和信息丰富的样本来进行训练。然而，在本文中，由于MIns中包含了大量的实例，与所有这些实例进行对比会导致重复的信息，而且会妨碍训练。因此，作者提出了在MIns中进行“硬样本挖掘”的方法，以选择更具挑战性的正样本和负样本。为了确定难度，他们使用样本之间的相似度计算，较低相似度的样本被认为更难，而较高相似度的样本被认为更容易。最终，他们选择了一定数量的最难的正样本、最难的负样本和随机负样本来进行训练，以更好地应对样本难度的问题。</p>
<h1>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations用于基于骨架的动作识别和不断增强的分层一致对比学习</h1>
<h2 id="abstract-4">abstract</h2>
<p>这段文字描述了一个名为&quot;HiCLR&quot;的骨骼动作识别方法，它基于对比学习技术。<strong>对比学习</strong>已被证明在自监督骨骼动作识别中具有益处。大多数对比学习方法利用<strong>精心设计的增强技术</strong>，以生成相同语义的<strong>骨骼不同运动模式的样本对</strong>。然而，目前<strong>尚未解决的问题是如何应用强增强技术</strong>，因为它们会扭曲图像/骨骼的结构并导致语义丢失，从而使训练不稳定。</p>
<p>因此，在本文中，研究人员探讨了<strong>采用强增强技术的潜力</strong>，并提出了一种<strong>通用的分层一致对比学习框架（HiCLR）<strong>用于骨骼动作识别。具体而言，首先</strong>设计了一个逐渐增强的增强策略</strong>，以<strong>生成多个有序的正样本对</strong>，这些<strong>对</strong>  <strong>有助于实现从不同视角学到的表示的一致性</strong>。然后，提出了一种<strong>不对称损失</strong>，通过特征空间中的方向性聚类操作来强制执行分层一致性，将来自强增强视角的表示拉近到来自弱增强视角的表示，以获得更好的泛化性能。</p>
<p>同时，研究人员还提出并评估了三种用于3D骨骼的强增强技术，以证明他们方法的有效性。大量实验证明，HiCLR在三个大规模数据集（NTU60、NTU120和PKUMMD）上明显优于最先进的方法。</p>
<p><strong>简而言之，这个方法研究了如何应用强增强技术来提高骨骼动作识别的性能，并提出了一种分层一致对比学习框架（HiCLR），该框架通过生成有序的正样本对和引入不对称损失来实现一致性学习。它在多个大规模数据集上表现出色，超越了其他方法。</strong></p>
<h2 id="introduction-3">introduction</h2>
<p><strong>1 Introduction</strong></p>
<p>人体动作识别对于连接人工系统和人类在现实世界中的互动至关重要。它在视频理解、人机交互、娱乐等领域广泛应用（Tang等，2020年；Rodomagoulakis等，2016年；Shotton等，2011年）。由于轻量化、稳健性和隐私保护等优势，基于3D骨架数据的动作识别近来引起了广泛关注。有许多作品针对基于骨架的动作识别（Shi等，2019年；Cheng等，2020年；Liu等，2020年b；Chen等，2021年），但大多数是以全监督方式设计的，需要大量标记的数据。考虑到大规模数据集的注释成本高昂且耗时，最近越来越多的研究人员开始关注从无标签数据中学习表示（Lin等，2020年；Li等，2021年；Kim等，2022年）。</p>
<p>在各种自监督学习方法中，对比学习是一种有效的方法，并且已经被证明对于基于骨架的动作识别（Rao等，2021年；Thoker，Doughty和Snoek，2021年；Li等，2021年）非常成功。对于对比学习，数据增强被证明非常关键，它为相同语义引入了各种运动模式，直接影响了模型学习的特征表示的质量（Tian等，2020年；Guo等，2022年）。然而，对于骨架数据，仍然没有完全研究到应该使用什么增强以及如何使用它。与人体动作的RGB表示相比，3D骨架数据是一种更高级的模态表示，这加剧了对增强的敏感性，使得在设计更高级的对比学习方法中谨慎选择增强成为瓶颈。一方面，如表3右侧所示，一些增强（例如随机遮挡）会导致基线算法性能下降。根据（Bai等，2022年），这些增强被称为强增强（也称为重增强），它们会扭曲图像/骨架结构并引起语义损失，从而导致不稳定的训练。一些研究揭示了使用强数据增强的潜力（Cubuk等，2020年；Wang和Qi，2021年）。但是，<strong>如何直接准确地测量和约束对比学习的基础——一致性，从强增强的视角，仍然具有困难。</strong></p>
<p>另一方面，基于对比学习的先前作品通常平等对待所有增强，<strong>忽视了不同增强的重要性差异</strong>。最近的研究（Tian等，2020年；Zhang和Ma，2022年）已经表明，每种增强对下游任务有不同的影响，因此<strong>在不区分应用增强的重要性的情况下学习增强后的不变性，会不可避免地导致下游任务的非最优表示</strong>。</p>
<p>为了解决上述问题，我们受到启发，探索了一种应用逐渐增强的一般性对比框架。在本文中，我们提出了一种新颖的层次一致对比学习框架（HiCLR），它从<strong>层次逐渐增强的不变性中学习，并以不同的方式处理不同的增强</strong>。与以往的作品（Li等，2021年；Guo等，2022年）不同，我们<strong>关注如何更好地利用和从包括强增强在内的多个增强中受益</strong>。具体地，我们<strong>提出了一个逐渐增强的层次增强策略，用于构建多个相关的正样本对</strong>。这些正样本中的每一个都是通过比前一个更多的增强生成的，并扩展了特征分布。然后，为了更好地利用强增强带来的新模式，我们提出了一种不对称的层次学习策略。与直接同时学习所有增强的方法不同，我们的目标建议了一种不同增强的层次一致学习方式，如图1所示。与此同时，这种不对称设计鼓励强增强视图与弱增强视图相似，有助于模型更好地泛化。基于我们的新框架，我们进一步分析和评估了不同的强增强策略。在图形卷积网络（GCNs）和变换器上进行了大量实验，以验证我们方法的有效性。</p>
<p>我们的贡献可以总结如下：</p>
<p>• 我们提出了一个层次一致的对比学习框架HiCLR，成功地将强增强引入传统的骨架对比学习流程中。层次设计集成了不同的增强，减轻了从强增强视图学习一致性的困难，这些视图伴随着严重的语义信息丢失。</p>
<p>• 我们引入了逐渐增强，以及不对称的层次学习，用于约束构建的正样本对的表示一致性。借助这些，模型通过利用强增强带来的丰富信息进一步提高了表示能力。</p>
<p>• 基于我们提出的框架，我们进一步设计和分析了三种强增强策略：随机遮罩、删除/添加边缘和SkeleAdaIN。尽管在直接应用它们时观察到了不利影响，但它们在我们的HiCLR下变得明显有效，并超过了最先进的性能。</p>
<p>以下是对要点的总结：</p>
<ol>
<li>引言介绍了人体动作识别的重要性，特别是基于3D骨架数据的动作识别，以及当前存在的问题，包括大规模数据集的标注成本和对数据表示的需求。</li>
<li>对比学习作为一种自监督学习方法，被广泛用于骨架数据的动作识别，但如何选择和使用增强对骨架数据进行了初步讨论。</li>
<li>3D骨架数据的高级表示增加了对比学习对增强的敏感性，这导致了对增强选择的谨慎。某些增强被称为强增强，它们可能导致性能下降和训练不稳定。</li>
<li>先前的对比学习方法通常平等对待所有增强，没有区分应用增强的重要性。</li>
<li>为了解决这些问题，引入了逐渐增强的概念，提出了HiCLR框架，该框架利用层次结构的增强策略，有针对性地处理不同的增强。</li>
<li>使用不对称的层次学习策略，鼓励强增强视图与弱增强视图相似，以改善模型的泛化能力。</li>
<li>实验结果表明，HiCLR在骨架动作识别中的性能显著优于先前方法，并且引入了强增强后表现出色。</li>
</ol>
<p>这些要点突出了HiCLR框架的创新性和有效性，以及强增强在骨架动作识别中的潜力。</p>
<h3 id="Contrastive-Representation-Learning-strong-augmentations">Contrastive Representation Learning  &amp;&amp; strong augmentations</h3>
<blockquote>
<p>列举了对比学习在无监督方向的应用以及影响</p>
<p>提及了一些主要的强增强的来源 以及本文的基础</p>
</blockquote>
<blockquote>
<p>这些作品仍然缺乏使用强增强的有效设计，并且强增强的潜力没有得到充分利用。</p>
</blockquote>
<h2 id="3-Proposed-Method-HiCLR">3 Proposed Method: HiCLR</h2>
<h4 id="Contrastive-Learning-for-Skeleton">Contrastive Learning for Skeleton</h4>
<p>这里首先我们要给出对于骨架的对比学习（Bai等人，2022年）的统一公式，遵循最近的研究，包括以下要素：</p>
<ul>
<li>数据增强模块，包含增强策略集T，用于生成原始数据的不同视图，这些视图被视为正样本对。</li>
<li>查询/关键编码器f(·)，用于将输入映射到潜在特征空间。</li>
<li>嵌入式投影仪h(·)，用于将潜在特征映射到嵌入空间，自监督损失将应用于该空间。</li>
<li>自监督损失，在嵌入空间中执行特征聚类操作。</li>
</ul>
<p>总结： 这一章介绍了骨架的对比学习方法的基本构成要素，包括数据增强模块、查询/关键编码器、嵌入式投影仪和自监督损失。<strong>这些要素共同构成了骨架对比学习的框架，用于在嵌入空间中执行特征聚类操作</strong>。</p>
<p>SkeletonCLR (Li et al. 2021)采用了最近的<strong>对比学习框架MoCov2 (Chen et al. 2020b) 作为我们方法的基准算法</strong>。具体来说，给定一个骨架序列s，通过T构建了正样本对（x，x）。随后，我们可以通过查询/关键编码器f(·)和嵌入式投影仪h(·)分别获得<strong>相应的特征表示（z，z</strong>）。维护了一个<strong>存储许多负样本</strong>用于对比学习的<strong>内存队列M</strong>。整个网络通过<strong>InfoNCE损失</strong>（Oord，Li和Vinyals 2018）进行优化： LInfo = −log exp(z · z/τ) exp(z · z/τ) + M i=1 exp(z · mi/τ) , (1) 其中mi是与第i个负样本对应的M中的特征，M是负特征的数量，τ是超参数。在每个训练步骤之后，批处理中的所有样本将按照先进先出的原则更新为M中的负样本。Key编码器是Query编码器的动量更新版本，通过梯度更新。具体来说，将Query编码器和Key编码器的参数分别表示为θq和θk，关键编码器的更新如下：θk ← mθk + (1 −m)θq，其中m ∈ [0, 1) 是动量系数。</p>
<p>总结： 这一部分<strong>介绍了SkeletonCLR方法</strong>，它<strong>采用了对比学习框架，并通过构建正样本对和维护内存队列来执行对比学习。方法的优化是通过InfoNCE损失进行的，该损失用于衡量正样本和负样本之间的相似性。关键编码器是通过动量更新来不断更新的，以提高对比学习的性能。</strong></p>
<h4 id="Hierarchical-Consistent-Contrastive-Learning-分层次的一致性对比学习">Hierarchical Consistent Contrastive Learning[分层次的一致性对比学习]</h4>
<p>传统的对比学习方法<strong>直接一次性应用增强集合以生成正样本对</strong>。当应用强烈的增强时，这些正样本会严重受到语义信息的丢失影响，共享较少的相关性。然而，从这些退化样本的一致性约束中学习到有用的信息非常困难。为了解决这个问题，我们提出了一个分层一致性对比学习框架。我们通过<strong>逐渐增加的增强逐渐生成一系列高度相关的正样本对</strong>。因此，这些样本对提供了<strong>特征相似性的分层指导，并有助于模型从不同视角的强增强中学习一致性的知识</strong>。</p>
<p>首先，我们概述一下我们的方法。如图2所示，HiCLR具有多个分支来提取特征，主要由两个组件组成：（1）<strong>逐渐增长的增强策略</strong>，构建与不同增强对应的多个正样本对。（2）来自<strong>强增强视图的表示一致性的不对称分层学习约束</strong>。接下来，我们将详细介绍每个组件。</p>
<p>总结： 本节介绍了分层一致性对比学习（HiCLR）方法，旨在解决传统对比学习方法在应用强增强时可能出现的语义信息丢失问题。HiCLR通过逐渐增加的增强逐渐生成高度相关的正样本对，从而提供了分层的特征相似性指导，并有助于模型从不同视角的强增强中学习一致性的知识。方法主要包括逐渐增长的增强策略和不对称分层学习约束两个组件。</p>
<h5 id="1-Gradual-growing-augmentation-逐渐增长增强">1)Gradual growing augmentation.逐渐增长增强</h5>
<p>为了促进学习过程，以实现更好的增强不变性，<strong>引入了逐渐增长的增强策略</strong>。增强策略包括多个增强集合，每个集合是现有集合的扩展版本。借助这一策略，生成了<strong>多个有序的正样本对</strong>，其扭曲逐渐增加。</p>
<p>下面我们正式描述一下我们的增长增强流程。所提出的增长分层增强策略构建了以下增强集合：T0，T1，…，Tk-1，其中k是不同增强集合的总数。T0包含基本的增强策略，并且<strong>每个集合比前一个集合采用更多的增强实例</strong>。这些集合可以表示为：</p>
<p>T0 = {a0,0}, T1 = {a0,1, a1,1}, … Tk-1 = {a0,k-1, a1,k-1, … , ak-1,k-1},</p>
<p>其中ai,j表示<strong>从属于第i个增强策略的第j个增强集合中采样的实例</strong>。请注意，==我们<strong>在每个增强集合中重新采样每个增强策略的实例</strong>，这意味着ai,j ≠ ai,j’，j ≠ j’。重新采样策略进一步扩展了特征分布，使模型能够为下游任务学习到更可区分的特征空间==。</p>
<p>借助该模块，我们构建了k-1个有序正样本对(v0, v1)，…，(vk-2, vk-1)，其中vi = Ti (s)。与以前的工作不同，应用于一个正样本对的增强是不同的，这==使得模型在特征聚类方面具有方向性==。同时，我们还可以通过T0（用于查询和关键）获得基本正样本对，如第3.1节所述，(v0, v0)。逐渐增长的增强策略使模型<strong>能够通过调整其应用的分支并解耦不同增强的学习</strong>来以不同方式处理增强。</p>
<p>总结： 本节介绍了逐渐增长的增强策略，旨在提高模型对增强的不变性。该策略包括多个逐渐扩展的增强集合，每个集合包含多个增强实例，逐渐增加扭曲程度。这种方法允许模型根据应用的增强和不同的学习来处理增强，生成有序的正样本对，从而有助于特征聚类和下游任务的学习。</p>
<h5 id="2-Asymmetric-hierarchical-learning-非对称分层学习">2) Asymmetric hierarchical learning.非对称分层学习</h5>
<p>先前的对比学习工作使用了等式（1）中的InfoNCE损失进行表示学习。然而，当应用强增强时，这通常会导致性能下降，这是因为不同的增强视图之间缺乏足够的互信息。因此，提出了一种<strong>分层自监督学习目标，以学习多个增强视图的表示一致性</strong>。</p>
<p>如图2所示，首先对正样本对进行特征嵌入编码。形式上，对于骨架序列s，我们构建了<strong>正样本对</strong>（v0，v1），…，（vk-2，vk-1）和（v0，v0），如上所述。然后，依次应<strong>用查询编码器fθq和MLP头部hθq来提取特征表示</strong>： zi = hθq (fθq (vi)) ，i = 0, 1, …，k-1。 同样，我们可以通过关键编码器fθk和MLP hθk获得特征表示z0： z0 = hθk (fθk (v0))。</p>
<p>模型通过不同的增强视图（vi-1，vi）的特征相似性来<strong>优化学习相邻分支之间的表示一致性</strong>。由于==这些相邻视图共享更多的增强策略，它允许目标特征更顺利地收敛到潜在聚类的中心==。然而，根据以前的工作（Bai等，2022），由于严重的扭曲，当强度增强的视图用作模拟目标时，可能会导致性能下降。因此，我们设计了一个不对称损失来单方面地将特征拉近。</p>
<p>==分层自监督学习目标通过相邻分支的特征相似性计算，可以表述为： Lh = k-1∑i=1 sim (zi, stopgrad (zi-1))。 在这里，我们使用<strong>stop-gradient（stopgrad）操作</strong>来获取<strong>更自信的相似性学习目标</strong>。<strong>强度增强的视图zi受限于减小与弱增强的视图zi-1的特征距离，但反之则不然。sim（·）可以是任何度量两个特征嵌入之间相似性的函数，例如余弦相似性和Kullback-Leibler（KL）散度（Kullback和Leibler，1951）。这可以看作是一种不对称的表示一致性学习设计，类似于Simsiam（Chen和He，2021），BYOL（Grill等，2020）和CO2（Wei等，2020）。通过从多个正样本对中进行不对称分层学习，模型利用了强增强带来的丰富信息，进一步提高了对下游任务的泛化能力。</strong>==</p>
<p>总结： 这一部分介绍了不对称分层学习的方法，旨在提高对强增强的不变性，并通过从多个正样本对中学习来自丰富信息，进一步提高了对下游任务的泛化能力。</p>
<h4 id="3-Instantiation">3) Instantiation.</h4>
<p>我们接下来给出了我们方法的一个具体实例。对于不对称分层学习，我们使用KL散度作为sim（·）函数。**一个问题是很难计算特征zi的理想准确分布。**受到（Wang and Qi 2021）的启发，我们获取了特征zi的条件分布，其中包括由关键编码器输出的正特征和M中维护的大量负特征。具体来说，zi的条件分布如下： p（z | zi）= exp（z · zi / τ） exp（z0 · zi / τ）+ M i = 1 exp（mi · zi / τ） 。==等式（6）描述了由正特征和负特征测量的特征zi的相似性分布。根据Wang和Qi的发现（Wang和Qi 2021），通过随机初始化的网络，p（z | zi）和p（z | zi-1）的分布是相似的==。这启发我们优化p（z | zi）和p（z | zi-1）之间的分布距离，即DKL（stopgrad（p（z | zi-1）），p（z | zi））作为sim（·），以学习不同增强视图之间的一致性。此外，我们将LInfo应用于基本正样本对（z0，z0）并联合优化模型。总体损失如下所示： L = LInfo + λhLh， 其中λh是分层自监督损失的权重。对于增强，该模型被实例化为k=3，包括基本增强集（对于a0，<em>）、正常增强集（对于a1，</em>）和随机遮罩（对于a2，*）。我们将在下一节更多讨论包括随机遮罩在内的强增强。</p>
<p>总结： 该部分描述了该方法的一个具体实例，详细介绍了不对称分层学习的方法，以及用于计算特征分布的KL散度。还介绍了损失函数的组成部分，包括信息最大化自监督损失（LInfo）和分层自监督损失（Lh），它们的组合构成了总体损失L。</p>
<blockquote></blockquote>
<h1><strong>InfoGCN: Representation Learning for Human Skeleton-based Action Recognition—InfoGCN:基于骨骼的动作识别的表征学习</strong></h1>
<h2 id="abstract-5">abstract</h2>
<ol>
<li>“Human skeleton-based action recognition offers a valuable means to understand the <strong>intricacies of human behavior</strong> because it can handle the complex relationships between physical constraints and intention.”
<ul>
<li>翻译：基于人体骨骼的动作识别为理解人类行为的复杂性提供了有价值的手段，因为它能够处<strong>理物理约束和意图之间的复杂关系</strong>。</li>
<li>解释：这句话介绍了人体骨骼的动作识别的重要性，因为它可以帮助我们理解人类行为中涉及到的物理约束和意图之间的复杂关系。</li>
</ul>
</li>
<li>“Although several studies have focused on encoding a skeleton, less attention has been paid to <strong>embed this information into the latent representations of human action</strong>.”
<ul>
<li>翻译：尽管有几项研究专注于对骨骼进行编码，但较少关注将这些信息嵌入到人类动作的潜在表示中。</li>
<li>解释：这句话指出，过去的研究主要关注了如何对人体骨骼数据进行编码，但较少关注如何将这些编码后的信息有效地融入到人类动作的潜在表示中，这是本研究的关注点。</li>
</ul>
</li>
<li>“InfoGCN proposes a learning framework for action recognition <strong>combining a novel learning objective and an encoding method</strong>.”
<ul>
<li>翻译：InfoGCN提出了一个用于动作识别的学习框架，结合了一种新颖的学习目标和编码方法。</li>
<li>解释：这句话提到了一个名为InfoGCN的方法，它提出了一个学习框架，用于动作识别。这个框架结合了一种新颖的学习目标和编码方法，旨在提高动作识别的性能。</li>
</ul>
</li>
<li>“First, we design an <strong>information bottleneck-based learning objective</strong> to guide the model to learn informative but <strong>compact latent representations</strong>.”
<ul>
<li>翻译：首先，我们设计了一个<strong>基于信息瓶颈的学习目标</strong>，以指导模型学习具有信息丰富但紧凑的潜在表示。</li>
<li>解释：这句话介绍了InfoGCN方法的第一部分。他们设计了<strong>一种基于信息瓶颈的学习目标</strong>，旨在引导模型学习具有信息丰富但紧凑的潜在表示，这有助于提高动作识别的效果。</li>
</ul>
</li>
<li>“To provide discriminative information for classifying action, we introduce attention-based graph convolution that captures the context-dependent intrinsic topology of human action.”
<ul>
<li>翻译：为了提供用于分类动作的区分信息，我们引入了基于注意力的图卷积，用于<strong>捕获与上下文相关的人体动作的内在拓扑结构</strong>。</li>
<li>解释：这句话说明了InfoGCN方法的另一方面。他们引入了基于注意力的图卷积，以捕获人体动作的内在拓扑结构，这有助于提供区分不同动作的信息。</li>
</ul>
</li>
<li>“In addition, we present a multi-modal representation of the skeleton using the relative position of joints, designed to provide complementary spatial information for joints.”
<ul>
<li>翻译：此外，我们提出了一种<strong>使用关节的相对位置的多模态骨骼表示，旨在为关节提供互补的空间信息。</strong></li>
<li>解释：这句话介绍了InfoGCN方法的另一个方面，他们提出了一种多模态的骨骼表示方法，利用关节的相对位置信息，以提供对关节的互补空间信息。</li>
</ul>
</li>
<li>“InfoGCN1 surpasses the known state-of-the-art on multiple skeleton-based action recognition benchmarks with the accuracy of 93.0% on NTU RGB+D 60 cross-subject split, 89.8% on NTU RGB+D 120 cross-subject split, and 97.0% on NW-UCLA.”
<ul>
<li>翻译：InfoGCN1在多个基于骨骼的动作识别基准上取得了突破，其准确率分别为NTU RGB+D 60跨主体分割上的93.0％，NTU RGB+D 120跨主体分割上的89.8％，以及NW-UCLA上的97.0％。</li>
<li>解释：这句话总结了InfoGCN1方法的性能，指出它在多个骨骼动作识别基准测试上取得了比已知最先进方法更高的准确率。</li>
</ul>
</li>
</ol>
<p>总之，这个摘要介绍了一个名为InfoGCN的方法，它旨在提高基于人体骨骼的动作识别的性能。它通过<strong>引入新颖的学习目标、编码方法和基于注意力的图卷积</strong>，以<strong>及多模态表示来实现这一目标</strong>，并且在多个基准测试中取得了显著的性能提升。</p>
<h2 id="introduction-4">introduction</h2>
<p>Introduction: 人体动作识别是计算机视觉中的一个基本问题，具有广泛的应用，包括紧急检测、手语识别和虚拟/增强现实中的手势识别等。特别是基于骨架的人体动作识别因其对杂乱背景的鲁棒性而引起了计算机视觉领域的极大兴趣。骨架表示（skeleton-based）人体动作识别的关键成就之一是基于图卷积网络（GCN）的方法。</p>
<p>本文介绍了一种<strong>新颖的基于骨架的动作识别预测框架</strong>。我们的方法在三个关键方面推动了技术发展。首先，是<strong>表示学习的算法</strong>。大量研究已经证明，表示学习极大地影响了机器学习任务的性能。我们的方法<strong>受信息瓶颈（IB）理论启发</strong>。我们提出了新的IB目标和相应的损失函数，以便<strong>在条件和边际上压缩输入信息的同时，学习使潜在表示对目标变量最具信息性的表示</strong>，如图1顶部所示。使用提出的目标训练的模型通过<strong>编码隐式和通用的潜在表示来执行识别，将输入级别的物理信息与动作语义相连接</strong>。</p>
<p>其次，是<strong>骨架的编码方法</strong>。使用<strong>骨连通性（外部拓扑结构）的骨架图表示在固有上有一定局限性</strong>，它可能<strong>忽略了可能的关节关系，称为固有拓扑结构</strong>。例如，当我们自拍时，手拿手机与上半身之间可能存在内在关系，因为我们共同移动它们以将上半身定位在手机屏幕上（如图1中我们模型推断的内在拓扑结构所示）。关节的固有拓扑结构为人体动作识别提供了上下文信息。在这个背景下，我们<strong>开发了一种新的基于自注意力的图卷积（SA-GC）模块</strong>，用于在编码骨架序列时提取内在图结构。如图1底部所示，<strong>对于出现在不同动作中的相似姿势，根据它们的行为背景，推断出的拓扑结构可以是不同的</strong>。</p>
<p>最后，我们通过利用关节的相对位置提出了多模态骨架表示。<strong>它提供了关节的补充空间信息</strong>。使用表示的模型集合极大地提高了识别性能。</p>
<p>通过结合上述三个提案，我们引入了一个名为InfoGCN的基于骨架的动作识别新学习框架。为验证我们方法的有效性，我们在骨架基础的动作识别中进行了实证评估，并将我们的结果与三个常用的基准数据集上的竞争性基线进行了比较：NTU RGB+D 60和120以及NW-UCLA。实验结果表明，我们的模型在准确性方面在这三个数据集上均达到了最先进的性能水平。分析表明，所学到的动作的潜在表示遵循了所提出的信息瓶颈约束，并且上下文相关的固有拓扑结构是根据行为背景自适应地推断出来的。</p>
<p>我们的贡献如下：</p>
<ul>
<li>信息瓶颈目标：我们引入了一种基于信息瓶颈的新型学习目标，旨在学习动作的高效压缩潜在表示。</li>
<li>基于自注意力的图卷积：我们提出了一个SA-GC模块，用于在骨架的空间建模中推断上下文相关的内在拓扑结构。</li>
<li>多模态表示：我们提出了一种多模态骨架表示，用于模型集合，极大地提高了动作识别性能。</li>
<li>实验证明：大量实验证明了我们工作的优势。InfoGCN在基于骨架的动作识别中在这三个数据集上均取得了最先进的性能。</li>
</ul>
<h2 id="method-3">method</h2>
<h3 id="3-1Information-Bottleneck-Objectives信息瓶颈目标和损失">3.1Information-Bottleneck Objectives信息瓶颈目标和损失</h3>
<h4 id="3-1-1Learning-Objective">3.1.1Learning Objective</h4>
<p>学习目标 我们的目标是设计一个包含与输入变量X（一系列骨架）相关的压缩信息的随机潜在变量Z，同时为了目标变量Y（动作标签）保留最大的信息。这种受限制的优化问题可以通过拉格朗日乘数转化为无约束问题：maxZ I(Z; Y) - β1I(Z; X)，其中I(·; ·)是互信息，β1是拉格朗日乘数。与以前的工作一样，我们假设变量之间的关系遵循图模型Z ← X ↔ Y，<strong>唯一可访问的内容是随机编码器p(z|x)</strong>。在infoGCN中，我们提出了以下与最先进的信息瓶颈(IB)目标等效的目标（请参阅附录）：</p>
<p>R(Z) = I(Z; Y) - λ1I(Z; X) - λ2I(Z; X|Y)，其中λ1和λ2是控制参数。第一项I(Z; Y)<strong>强制Z对于预测Y具有足够的信息</strong>。第二项确保Z是简洁的。第三项允许在给定类别时，将潜在变量Z相对于输入变量X进行压缩。我们的<strong>目标采用了VIB和CEB中的压缩正则化项的组合</strong>，同时保持了信息瓶颈的哲学。我们得到的目标比[2, 13]的目标更加通用，同时将先前的目标作为特殊情况包括在内（当λ1 = 0时是VIB，当λ2 = 0时是CEB）。</p>
<h4 id="3-1-2Variational-Bound">3.1.2Variational Bound</h4>
<p>这里我们推导出了我们IB目标（方程1）的变分界限。R(Z)的每一项的变分界限都是根据最近的研究[2,4,37]进行推导的，这些研究使用可计算的变分界限和深度学习技术来估计互信息。我们使用变分分类器q(y|z)获得了I(Z;Y)的变分下界：</p>
<p>I(Z;Y) ≥ E_{p(x, y)p(z|x)}[log q(y|z)] + H(Y) (方程2)</p>
<p>其中RHS的第一项对应于对数似然，RHS的第二项是常数，当底层数据生成分布固定时，它不会影响优化。类似于[13, 17]，我们将r(z)定义为变分边际，r(z|y)定义为变分类条件边际。我们按照[13, 17]的方式获得了I(Z;X)和I(Z;X|Y)的变分上界：</p>
<p>I(Z;X) ≤ E_{p(x)p(z|x)}[log(p(z|x)/r(z))] (方程3) I(Z;X|Y) ≤ E_{p(x)p(z|x)p(y|x)}[log(p(z|x)/r(z|y))] (方程4)</p>
<p>将方程（2）至（4）代入方程（1），我们得到了R(Z)的下界：</p>
<p>R(Z) ≥ E_{p(x, y)p(z|x)}[log q(y|z)] - λ1 E_{p(x)p(z|x)}[log(p(z|x)/r(z))] - λ2 E_{p(x)p(z|x)p(y|x)}[log(p(z|x)/r(z|y))] (方程5)</p>
<p>方程（2）至（4）的推导在附录中提供。</p>
<h4 id="3-1-3Training-Loss">3.1.3Training Loss</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="../assets/image-20230925220717504.png" alt="image-20230925220717504"></p>
<h3 id="3-2-Neural-Architecture">3.2. Neural Architecture</h3>
<h4 id="3-2-1The-Importance-of-Learning-Intrinsic-Topology">3.2.1The Importance of Learning Intrinsic Topology</h4>
<p>我们通过展示<strong>仅使用外在拓扑的GC可能导致消息传递方面的严重低效性和信息损失</strong>来==强调内在拓扑的重要性==。假设由于身体结构的双侧对称性，两只手的关节都存在内在关系。这两个叶子节点必须通过物理连接路径传递信息以相互传递信息。当按照GC的机制传递信息时，需要根据消息传递的最短路径长度增加网络的深度，这意味着节点之间的信息交换效率非常低。</p>
<p>此外，可能会发生信息丢失。GC在对邻居节点的特征进行平均后<strong>执行非线性变换</strong>。如果表示节点信息的特征向量不是线性独立的，那么在平均后很难重构每个邻居的信息。假设α是节点因向量组合而引起的信息稀释的最小部分。如果两个节点之间的距离具有内在关系l，则信息可以以(1 − α)l的最大比率传输。当α &gt; 0时，距离l越长，信息稀释越多。</p>
<p>一个直接的方法是通过像[19, 26]中那样将邻接矩阵的幂次作为卷积核大小，但这不是理想的，因为它不能动态建模可能的内在拓扑。<strong>更好的解决方案是自适应地推断需要改变信息的关节关系。因此，我们提出了一种利用自注意机制来捕捉内在拓扑的架构。这可以更好地适应复杂的关节关系，从而提高信息传递的效率。</strong></p>
<h4 id="3-2-2Architecture-Overview">3.2.2Architecture Overview</h4>
<p>该编码器由一个嵌入块和一堆L = 9个编码块组成，后面跟着一个全局平均池化层。嵌入块将骨架的序列转换为初始关节表示。然后，编码块从初始关节表示中提取时空特征。我们利用了<strong>VAE中的再参数化技巧</strong>[20]。通过一个辅助独立的随机噪声 ϵ ∼ N(0, I)，z 被采样为 z = µ + Σϵ，其中多元高斯分布的均值 µ 和对角协方差矩阵 Σ 是从编码器的输出推断出来的。这个<strong>技巧通过使用基于梯度的优化以端到端的方式估计无偏梯度来使模型可训练</strong>。</p>
<p>分类器由单一的线性层和 softmax 函数组成，将潜在向量 z 转换为分类分布的模型参数。</p>
<p>总结：这一部分描述了模型的编码器和分类器。编码器将输入的骨架序列转换成潜在表示，使用了VAE中的再参数化技巧。然后，分类器将这个潜在表示映射到分类分布的参数。这个架构用于将骨架动作映射到动作类别。</p>
<h4 id="3-2-3Embedding-Block">3.2.3Embedding Block</h4>
<p>这段文字描述了人体骨架的表示方式以及嵌入块的功能。</p>
<ol>
<li>人体骨架可以被表示为一个图 G(V, E)，其中关节作为一组 N 个顶点 V，骨骼作为边 E。边可以表示为一个邻接矩阵 A ∈ RN×N，其中如果关节 i 和 j 在物理上连接，则 Ai,j = 1，否则为 0。一系列<strong>骨架图的序列表示为关节特征张量 X ∈ RT×N×C</strong>，其中 <strong>T 是骨架的总帧数，C 是特征维度。</strong></li>
<li>嵌入块将关节特征线性转换为具有<strong>可学习参数的 D(0) 维向量</strong>，然后<strong>添加位置嵌入（PE）以注入关节的位置信息</strong>。这里采用了可学习的 PE，它在时间上是共享的。</li>
</ol>
<p>总结：嵌入块的作用是<strong>将关节特征转换为低维向量，并通过可学习的参数和位置嵌入来编码关节的位置信息</strong>。这有助于模型理解骨架序列中的时空关系。</p>
<h4 id="3-2-4Encoding-Block">3.2.4Encoding Block</h4>
<ol>
<li>编码块的核心包括两个子模块：基于自注意力的图卷积（SA-GC）模块用于空间建模，以及多尺度时间卷积（MS-TC）模块用于时间建模。关节的输入和隐藏表示通过SA-GC、MS-TC、残差连接和层归一化依次编码。</li>
<li>空间建模：SA-GC模块用于推断上下文相关的内部拓扑结构。SA-GC<strong>利用关节特征的自注意力来推断内部拓扑</strong>，<strong>并将拓扑作为图卷积的邻域顶点信息</strong>。SA-GC使用自注意力映射关节表示为查询和键，以获得自注意力图。此外，SA-GC还学习了一个共享的拓扑结构，用于不同时间和实例，以及多头自注意力机制来同时关注不同的表示子空间。</li>
<li>时间建模：MS-TC模块用于建模人体骨架的时间特征。该模块由三个卷积分支组成，具有不同的卷积核尺寸和膨胀率组合。卷积分支的输出被串联在一起，周围有一个带有1×1卷积的残差连接。</li>
</ol>
<p>总结：编码块包括SA-GC模块和MS-TC模块，用于分别进行空间建模和时间建模。SA-GC模块利用自注意力来推断内部拓扑结构，而MS-TC模块用于捕捉时间特征。这些模块协同工作，有助于模型理解人体骨架序列的复杂时空关系。</p>
<h5 id="空间建模">空间建模</h5>
<p><strong>The core of our encoding block consists of two sub-modules: a Self-Attention based Graph Convolution (SA-GC) module for spatial modeling and a Multi-Scale Temporal Convolution (MS-TC) module for temporal modeling. The input and hidden representation of joints are encoded sequentially with an SA-GC, an MS-TC, a residual connection, and a layer normalization [3] (See Fig. 2).</strong></p>
<p>编码块的核心包括两个子模块：一个用于空间建模的基于自注意力的图卷积（SA-GC）模块，以及一个用于时间建模的多尺度时间卷积（MS-TC）模块。关节的输入和隐藏表示通过一系列步骤进行编码，包括SA-GC、MS-TC、残差连接以及层归一化。</p>
<p><strong>Spatial Modeling. We propose a novel module SA-GC to infer context-dependent intrinsic topology. Before describing SA-GC, we revisit vanilla GC [21], which is composed of two processes; 1) average neighborhood vertex features and 2) linearly transform aggregated features. The update rule of hidden representation for GC is as follows</strong></p>
<p>空间建模：我们提出了一个新的模块SA-GC，用于推断依赖上下文的内部拓扑结构。在描述SA-GC之前，我们回顾了传统的图卷积（GC）[21]，它由两个步骤组成：<strong>1）对邻域顶点特征的平均值和2）线性变换聚合特征</strong>。GC的隐藏表示更新规则如下：</p>
<p><strong>SA-GC utilizes the self-attention [51] of joint features to infer intrinsic topology and uses the topology as a neighborhood vertex information for the GC. A self-attention is an attention mechanism that relates different joints of the body. Considering all possible joint relations, SA-GC infers positive and bounded weight, called self-attention map, to represent the strength of the relation. We linearly project joint representation Ht to queries and keys of D′ dimensions with learned matrices WQ, WK ∈ RD×D′ to get a self-attention map.</strong></p>
<p>SA-GC<strong>利用关节特征的自注意力[51]来推断内部拓扑结构，并将该拓扑结构用于GC的邻域顶点信息</strong>。自注意力是一种关联人体不同关节的关注机制。考虑所有可能的关节关系，**SA-GC推断正值且有界的权重，称为自注意力图，以表示关系的强度。**我们将关节表示Ht线性投影到D′维度的查询和键上，使用学习到的矩阵WQ、WK∈RD×D′来获取自注意力图。</p>
<p><strong>In addition to the self-attention map, we let SA-GC learn a topology ˜A shared over time and instance as in [6, 44]. The shared-topology and self-attention map have M multi-head to make the model jointly attend from different representation subspaces. For a head in 1 ≤ m ≤ M, we combine the shared topology ˜Am ∈ RN×N with the self-attention map SAm(Ht) ∈ RT×N×N to obtain the intrinsic topology.</strong></p>
<p>除了自注意力图外，我们允许SA-GC学习一个类似于[6, 44]中的<strong>共享拓扑结构˜A</strong>，该结构随时间和实例而共享。<strong>共享的拓扑结构和自注意力图都有M个多头</strong>，以使模型可以从不同的表示子空间共同关注。对于1 ≤ m ≤ M中的一个多头，我们将共享的拓扑结构˜Am ∈ RN×N与自注意力图SAm(Ht) ∈ RT×N×N相结合，以获取内部拓扑结构。</p>
<p><strong>SA-GC utilizes ˜Am ⊙ SAm(Ht) as neighborhood information for GC. The overall update rule of joint representation is formulated as</strong></p>
<p>SA-GC利用˜Am ⊙ SAm(Ht)作为GC的邻域信息。关节表示的整体更新规则如下：</p>
<p><strong>We employ a residual connection [15] with 1 × 1 convolution around the SA-GC module.</strong></p>
<p>我们在SA-GC模块周围使用1×1卷积进行残差连接。</p>
<h5 id="时间建模">时间建模</h5>
<p><strong>Temporal Modeling. To model the temporal feature of the human skeleton, we adopt the MS-TC module [6, 33] as shown in Fig. 2. This module consists of three convolution branches with different combinations of kernel sizes and dilation rates. The outputs of convolution branches are concatenated. A residual connection with 1×1 convolution is around this module.</strong></p>
<p>时间建模：为了对人体骨架的时间特征进行建模，我们采用如图2所示的MS-TC模块[6, 33]。该模块包括三个卷积分支，具有不同的卷积核尺寸和膨胀率的组合。卷积分支的输出被串联在一起。在该模块周围有一个带有1×1卷积的残差连接。</p>
<p>总结：编码块包括SA-GC模块和MS-TC模块，用于分别进行空间建模和时间建模。SA-GC模块利用自注意力来推断内部拓扑结构，MS-TC模块用于捕捉时间特征。这些模块协同工作，有助于模型理解人体骨架序列的复杂时空关系。</p>
<h2 id="3-3Ensemble-with-Multi-Modal-Representation">3.3Ensemble with Multi-Modal Representation</h2>
<p><strong>In this section, we introduce a generalized form of well-known skeleton representation such as bone and joint, which we call multi-modal representation. We train our model with each modal representation and ensemble upon inference. The representation provides complementary features using the relative position of joints. See Fig. 3 for illustration.</strong></p>
<p>在这一部分，我们介绍了一种<strong>通用形式的骨骼表示</strong>，如骨骼和关节等，我们称之为多模态表示。我们使用每种模态表示训练我们的模型，并在推理时进行集成。这种<strong>表示利用关节的相对位置提供互补的特征</strong>。请参见图3以进行说明。</p>
<p><strong>Shi et al. [44] introduce bone information, which is defined as a vector pointing toward its target joint from its source joint that are physically connected, as shown at k = 1 in Fig. 3. Previous works [6, 33, 44] show that the ensemble of models trained with bone and joint information drastically improves action recognition performance, implying that these different representations of skeleton are complementary. We propose multi-modal skeleton representation to define additional representations, based on the fact that bone information is a linear transformation of joint.</strong></p>
<p>Shi等人[44]介绍了骨骼信息，它被定义为从其物理连接的源关节指向目标关节的向量，如图3中k = 1所示。先前的研究[6, 33, 44]表明**，使用骨骼和关节信息训练的模型集合显著提高了动作识别性能，这意味着骨骼的不同表示与关节的表示相互补充。我们提出了多模态骨骼表示，以定义基于骨骼信息是关节的线性变换的附加表示。**</p>
<p><strong>In detail, we generalize joint-bone relation at time t as</strong></p>
<p>具体而言，我们将时间t的关节-骨骼关系进行了泛化：</p>
<p><strong>\tilde {\textbf {\text {X}}}^{(k)}_t=(\textbf {\text {I}}-\textbf {\text {P}}^k) \textbf {\text {X}}_t, (16)</strong></p>
<p><strong>where P ∈ RN×N denotes a binary matrix that contains source-target relations of the skeleton graph, Pij = 1 if the i-th joint is the source of the j-th joint, otherwise 0. We set the row corresponding to the center of mass in P as a zero vector so that it does not have a source joint. We refer to ˜X (k) t as the k-th mode representation of a skeleton. The representations with different k values provide distinct spatial features for a joint. We define K = maxv d(v) + 1 for v ∈ V, where d(v) gives the shortest distance in the number of hops between the vertex v and the center of mass. Then, if k = 1, the k-th mode representation ˜X (k) t corresponds to the bone as defined in [44] and if k = K, joint since PK = 0. For instance, at k = 1 in Fig. 3, a joint of the center of mass is represented as a blue dot, so K is equal to 5 in this case.</strong></p>
<p>其中，P ∈ RN×N表示一个<strong>包含骨骼图源-目标关系的二进制矩阵</strong>，如果第i个关节是第j个关节的源，则Pij = 1，否则为0。我们将与质心对应的行在P中设为零向量，以便它不具有源关节。我们<strong>将˜X (k) t 称为骨骼的第k种模态表示</strong>。具有不同k值的表示为关节提供了不同的空间特征。我们为v ∈ V定义K = maxv d(v) + 1，其中d(v)表示<strong>顶点v与质心之间的最短距离</strong>（通过跳数计算）。然后，如果k = 1，第k种模态表示˜X (k) t 对应于[44]中定义的骨骼，如果k = K，则对应于关节，因为PK = 0。例如，在图3中的k = 1处，质心的关节表示为蓝点，因此在这种情况下K等于5。</p>
<p><strong>总结：</strong> 本节介绍了一种<strong>多模态表示，它基于骨骼的不同表征，如骨骼和关节。模型使用每种模态</strong></p>
<h2 id="3-4Learning-Framework">3.4Learning Framework</h2>
<p><strong>This section describes the overall training regime of InfoGCN. Sequences of the skeletons are batched together after being resized to 64 frames as in [6].</strong></p>
<p>本节描述了InfoGCN的总体训练过程。骨骼的序列在被调整为64帧后被分批处理，就像[6]中一样。</p>
<p><strong>The model is updated to minimize the total loss (Eq. (10)) using SGD optimizer with a momentum coefficient 0.9.</strong></p>
<p>使用动量系数为0.9的SGD优化器更新模型，以最小化总损失（Eq.（10））。</p>
<p><strong>We set the µr(z) to be 0 so that LmMMD behaves as a regularizer of the norm of µˆp(z). We set the µr(z|y) of each action class as random orthogonal vectors [41] with a scale of 3.</strong></p>
<p>我们将µr(z)设置为0，以便LmMMD表现为µˆp(z)范数的正则化器。我们将每个动作类别的µr(z|y)设置为随机正交向量[41]，尺度为3。</p>
<p><strong>During the training, we estimate ˆµp(z) and ˆµp(z|y) by averaging marginal and class conditional marginal latent vectors of a mini-batch, respectively.</strong></p>
<p>在训练过程中，我们分别通过对小批次的边际和类别条件边际潜在向量进行平均来估计ˆµp(z)和ˆµp(z|y)。</p>
<p><strong>Also, we employ label smoothing [48] of value 0.1. During inference, we ensemble models that trained with different k-mode representations as the multi-stream ensemble in [6, 33, 44].</strong></p>
<p>此外，我们采用标签平滑[48]，其值为0.1。在推理过程中，我们将使用不同k模态表示训练的模型进行集成，就像[6, 33, 44]中的多流集成一样。</p>
<p><strong>总结：</strong> 本节概述了InfoGCN的整体训练框架，包括骨骼序列的处理、损失函数的优化、超参数的设置以及训练和推理过程中的策略。这个框架的目标是训练一个能够准确识别骨骼动作的模型，并且在推理时集成不同的k模态表示以提高性能。</p>
<h1>Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</h1>
<p><strong>引言</strong></p>
<p>人类动作识别已成为许多实际应用的重要组成部分，包括但不限于安全领域和人机交互。由于姿势估计[13]和传感器[45]的发展，获取骨架数据并不困难。因此，由于骨架数据对外观和背景变化具有稳健性，以及提供了个体的无偏表示能力，人类骨架数据已经成为传统RGB视频数据的有希望的替代选择。</p>
<p>许多研究人员探索了这一任务的全面监督方法，这需要大量已标记的训练样本。然而，在现实场景中，由于许多动作的样本收集耗时且昂贵，处理众多的动作类别并不经济。因此，如果没有训练样本，而只有一些语义信息，如新类别的名称、属性或描述可用，那么零样本学习[18, 29]用于识别新类别。由于3D骨架动作数据的注释和标记因包含深度信息和人类动作语义的复杂性而变得更加困难，因此，在实际应用中，零样本基于骨架的动作识别非常有吸引力，因为它可以显著减少收集和标记新动作的需求。</p>
<p>在图1中，鉴于已提取的视觉和语义特征，零样本学习的核心是建立在已知类别之间的视觉和语义空间之间的连接模型。在测试阶段，学到的模型用于促进从已知类别到未知类别的知识传输。为解决迁移学习问题，零样本动作识别依赖于外部知识库，即来自预训练的大规模语言模型（如Sentence-Bert [28]或CLIP [27]）的每个类别标签的语义嵌入。有效利用语义信息对于弥合两个不同模态之间的差距至关重要。有一些关于零样本基于骨架的动作识别的研究。现有方法[9, 14]将动作序列嵌入到视觉特征中。为了建立视觉和语义空间的连接模型，根据训练阶段中已知类别的数据学习了一个兼容的投影函数[14]或深度度量[6]。然后在测试阶段，测试动作序列的视觉特征与未知类别的句子嵌入[14]或词性标记的单词[41]之间的相似性要么在投影的公共空间中要么通过已学习的度量来衡量。然而，投影操作仅将视觉或语义特征映射到嵌入空间中的公共锚点，忽略了视觉和语义特征的分布之间的全局对齐。此外，已学习的投影或度量未充分利用语义信息来捕获两种模态之间的关联。由于视觉和语义空间之间的显著差异，尝试执行跨模态重建而不对齐分布是具有挑战性的，最终导致难以推广到具有不同分布的新类别。</p>
<p>其次，在零样本动作识别任务场景中，由于一些语义类别需要动态信息来区分彼此，信息丧失情况变得严重。例如，“行走&quot;和&quot;跳跃&quot;之间的区别仅在局部部分，因为这两个动作序列的初始帧相似。要观察到&quot;跳跃”，必须观察到人体起立过程。因此，对于人类动作来说，利用固有的时间动态信息也在零样本连接模型的泛化能力中发挥作用。</p>
<p>在本文中，我们提出了一种基于骨架的零样本动作识别的互信息估计和最大化框架（SMIE）。为了更好地捕获视觉和语义空间之间的依赖关系，我们的方法避免了直接映射，而是使用全局对齐模块来对齐这两个空间的分布。该模块利用互信息作为相似性度量，并应用基于Jensen-Shannon散度（JSD）的估计器来最大化成对的视觉和语义特征之间的互信息，同时最小化未成对的视觉和语义特征之间的互信息。在测试阶段，将神经网络作为连接模型来估计JSD估计器中的相似性得分，该得分在未知类别上使用。然后，考虑到动作的固有时间信息，SMIE提出了一个时间约束模块，以鼓励视觉和语义特征之间的互信息在执行动作的更多部分时增加。具体而言，JSD估计器应用对比学习来估计全局互信息。成对的视觉和语义特征形成正样本，而未成对的特征形成负样本。为了识别包含动作序列中更多差异信息的关键帧，时间约束模块计算每个序列的运动关注度，并遮盖具有更高关注度的关键帧，以生成包含部分时间信息丧失的额外正样本。在训练期间，与全局互信息相比，计算了具有相同负样本的时间约束互信息，并且保持较小。</p>
<p>本文的主要贡献有三个方面： • 我们提出了基于互信息最大化的基于骨架的零样本动作识别的互信息估计和最大化框架（SMIE），这是一种新的零样本方法，用于基于互信息最大化的骨架动作识别，可以捕获视觉空间和文本语义空间分布的复杂统计相关性。 • 提出了一种新颖的时间约束模块，用于计算时间约束互信息，并应用了时间排名损失来帮助连接模型捕获动作的固有时间信息。 • 大量的实验和分析证明了所提方法的有效性，该方法在性能上明显优于基线方法。</p>
<h1>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</h1>
<p><strong>1. 引言</strong></p>
<p>人体动作识别（HAR）是通过接收视频数据作为输入来对动作类别进行分类的任务。HAR用于许多应用，如人机交互和虚拟现实。随着深度学习技术的发展，最近提出了几种基于RGB和骨架的HAR方法。然而，基于RGB的方法[31, 29]不能稳健地识别人的动作，因为它们受到背景颜色、光亮度和服装等环境噪声的强烈影响。因此，使用骨架模态的方法[35, 24, 36, 26, 5, 4, 18, 2, 15]引起了注意，因为它们不受这些噪声的影响。这些方法通过接收主要人体关节的2D或3D坐标作为时间序列输入来识别动作。</p>
<p>最近的方法[24, 18, 4, 2]采用了图卷积网络（GCNs）将人体骨架图应用于卷积层。然而，现有的基于GCN的方法[35, 24, 25, 4, 2]存在以下局限性。 （1）使用广泛使用的手工制作图形，无法识别远处关节节点之间的关系，因为它们仅使用人体骨架中的PC边缘的关系。虽然具有PC边缘的图形具有语义意义，但仅具有PC边缘的图形由于是经验性固定的，因此存在长程依赖性问题。然而，要使人类识别动作，远处关节之间以及相邻关节之间的关系强相关。尽管一些方法[24, 2]已经尝试通过训练引导注意力的可学习图形来解决此类局限，但它们仍然使用[35]的手工制作图形以及它们的可学习图形。此外，由于[35]图形的元素值比可学习图形的元素值更主导，它们不能充分突出远程节点之间的关系。 （2）最近的一些方法[35, 24, 4, 18]冒着通过简单聚合边缘特征并忽略每个边缘的贡献而陷入亚优性的风险，从而不完全识别哪些边缘对每个骨架样本是重要的。例如，在“蹲下”动作的情况下，腿和手之间的交互应该受到强调。</p>
<p>受到这些局限性的启发，我们提出了一种分层分解的图卷积网络（HD-GCN），其中包括分层分解的图形（HD-Graph）和引导注意的分层聚合（A-HA）模块。此外，我们提出了一种有效利用我们的HD-Graph的六种合奏方法。我们提出的方法的框架如图1所示，用于“蹲下”动作。HD-GCN将GCN与我们的HD-Graph相结合，该图形在相同的语义空间（例如右手和左手、右脚和左脚）中识别远程关节节点之间的关系。通过从图的质心（CoM）节点逐步向外移动来形成相同的语义空间。例如，如果腹部是CoM节点，则第一个语义空间包括腹部节点，下一个空间包括胸部和臀部节点，随后的空间包括左肩和右肩以及左臀和右臀节点。相同语义空间中的节点被定义为层次节点集。要检测远程关节节点之间的关系，网络应具有较大的感知场。所提出的HD-Graph通过连接相邻层次节点集中的所有节点，并识别用于大感知场的那些节点之间的连接来包含具有意义的相邻和远程关节节点。我们采用类似于根树状结构的结构以有效地表示每个边缘。我们应用空间边缘卷积（S-EdgeConv）层来考虑不能被HD-Graph捕获的在每个样本中不能被捕获的语义接近边缘。要创建S-EdgeConv层，我们借用了广泛用于3D点云的[33]的结构。</p>
<p>为了考虑每个边缘集的贡献，选择主导层次信息的过程应该依赖于动作数据样本，以适当关注最主导的边缘集。例如，为了识别“拍手”动作，必须强调包括双手的层次边缘集。为了解决这个问题，我们提出了一个引导注意的分层聚合（A-HA）模块，它包括两个子模块：代表性空间平均池化（RSAP）和分层边缘卷积（H-EdgeConv）。如果我们在没有任何节点提取过程的情况下使用空间平均池化模块，将出现缩放偏差问题，因为每个节点具有不同数量的相邻节点。为了防止这种情况，我们应用了RSAP，其中包括一个代表性节点提取过程，触发池层后的特征来表示每个节点。为了有效管理由RSAP获得的分层特征，我们应用了分层边缘卷积（H-EdgeConv）层。H-EdgeConv将每个分层特征视为图节点，并通过特征空间中的欧几里德距离识别应该强调哪些分层特征。通过RSAP和H-EdgeConv，我们的模型成功确定了哪些分层边缘集和关节应该在输入特征中受到强调。</p>
<p>现有的合奏方法使用由联合、骨骼坐标、关节运动和骨骼运动流数据组成的四个流数据，它们分别是原始骨架坐标、关节坐标之间的空间差异和关节的时间差异以及骨骼的时间差异。大多数现有的合奏方法[25, 2]使用额外的运动数据，但仅利用运动数据的模型表现相对较差。为了解决这个问题，我们提出了一种新的方法，六向合奏。我们通过使用带有关节和骨流数据的三个HD-Graph来应用此合奏方法。每个图都有不同的质心节点，以提取不同语义空间的特征（参见附录）。</p>
<p>我们在四个基准动作识别数据集上进行了广泛的实验：NTU-RGB+D 60 [22]，NTU-RGB+D 120 [16]，Kinetics-Skeleton [11]和Northwestern-UCLA [30]。</p>
<p>我们的主要贡献总结如下：</p>
<p>• 我们提出了一种分层分解的图形（HD- 图），以全面识别相同层次节点集之间重要的远程边缘。 • 我们提出了一个引导注意的分层聚合 (A-HA) 模块，以代表性的空间平均池化 (RSAP) 和分层边缘卷积 (H-EdgeConv) 强调关键边缘集。 • 我们使用一种新的六向合奏方法来进行基于骨架的动作识别，使用具有不同重心（CoM）的HD-Graph，优于没有任何运动数据的常规合奏。 • 我们的HD-GCN在基于骨架的动作识别的四个基准数据集上优于最先进的方法。</p>
<h1>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</h1>
<p><strong>引言</strong> 动作识别是应用于各种应用程序的最重要的视频理解任务之一，如虚拟现实和人机交互。最近关于动作识别的研究分为两种方法，基于RGB的方法[33, 31]和基于骨架的方法[36, 26, 27, 2, 4, 16]。使用骨架模态的动作识别接收具有主要人体关节的三维坐标的视频序列作为输入。骨架模态的动作识别具有以下优点，通过紧凑地压缩人体结构可以创建计算复杂性低的轻量级模型。此外，与基于RGB的方法不同，它具有鲁棒性，不受背景噪声、天气和光照条件的影响。</p>
<p>早期的方法[7, 8, 38, 14, 23]通过独立处理每个关节来提取特征，这意味着它们不考虑结构相关的人体关节之间的信息。然而，人体关节之间的连接被识别为Yan等人提出骨架模态的时空图卷积网络（GCNs）后的图形结构。最近的方法[27, 4, 2, 6]将GCNs作为基线，并尝试在空间领域上扩大感知场。然而，基于Yan等人的GCNs的方法存在一些局限性。 (1) 当一个人执行动作时，他们的身体部分的运动同时发生在空间和时间上，这两个方面的运动本质上是相互关联的。尽管同时包含空间和时间组件可以提供更全面和准确的人体动作表示，但直接利用这种时空互连性是不可行的，因为时空模块是相互独立的。 (2) 由于它们使用的图仅包括物理上相邻的关节的连接，因此具有此类图的网络具有较小的空间感知场。尽管提出了几种自我注意方法[27, 2]来增加空间感知场，但它们仍然依赖于使用物理上相邻的图，这可能导致对那些物理上相邻的图产生偏见，并突出显示了它们有效性的潜在局限性。为了处理这个问题，Liu等人提出了一种多尺度图，用于识别结构上远离的节点之间的关系。然而，正如Yan等人所述，虽然将人体动作区分为同心和离心模式至关重要，但Liu等人的方法并不考虑这些模式。此外，Liu等人的模型存在模型复杂性高的限制，因为在单个层中存在太多并行操作。</p>
<p>为了解决限制（1），我们提出了一个时空曲线（STC）模块，以反映骨架序列中直接的时空依赖性。除了应用时间卷积来聚合节点级别的时序特征，我们构建曲线，考虑每个节点的时序时空特征，并将它们与输入特征图聚合在一起。为了创建这些曲线，我们在特征空间中选择了在所有相邻帧之间具有高度相关性的节点，并将它们连接起来。因此，通过赋予节点之间的时间连接以自主性，可以自适应生成更具语义效果的图形结构。图1比较了现有方法[36, 27, 2]和我们提出的方法的时空GCN的时间流。图1 (a)显示，该模型仅在每一帧中反映相同节点的特征，而图1 (b)显示，该模型通过生成的曲线考虑了不同帧中不同节点的时空相关性。受[35]启发，我们使用一个聚合模块来有效地组合所有曲线特征并将其应用于输入特征图。</p>
<p>为了解决限制（2），我们提出了图卷积的扩展内核（DK-GC），以使骨架模态具有大的空间感知场，而无需任何额外参数。用于人体骨架的GCNs聚合朝内（向心）、标识和朝外（离心）特征，与卷积神经网络（CNNs）不同，后者聚合了左侧、标识和右侧像素特征。为了将扩张内核应用于这种GCNs，我们创建相邻矩阵来通过修改向心和离心矩阵来识别结构上远离的关系。为了从低级到高级整合空间感知场，我们将空间模块分成具有不同扩张窗口的几个分支操作。同时，Li等人[17]已经为3D点云分析任务引入了扩张图卷积。然而，Li等人的扩张图卷积与我们提出的方法完全不同，并且不适用于人体骨架模态。首先，这种方法不利用给定的相邻矩阵，而是使用k最近邻（k-NN）算法通过动态图来。未能利用给定的相邻矩阵会降低动作识别模型的鲁棒性，因为Shi等人[27]通过实验证明，不使用这些矩阵会导致性能不佳。此外，单独使用k-NN无法识别所有物理相邻的节点。第二个原因是Li等人的方法需要大量的GPU资源。由于动态图是通过计算每个GCN层的所有节点之间的两两距离来构建的，因此它会导致GPU内存消耗非常高，推理速度较慢。</p>
<p>为了验证我们的STC-Net的优越性，在四个基于骨架的动作识别基准数据集上进行了广泛的实验：NTU-RGB+D 60 [25]，NTU-RGB+D 120 [18]，Kinetics-Skeleton [12]和Northwestern-UCLA [32]。</p>
<p>我们的主要贡献总结如下： • 我们提出了时空曲线（STC）模块，以利用不同帧不同节点之间的直接时空相关性。 • 我们提出了图卷积的扩展内核（DK-GC），通过修改给定的骨骼相邻矩阵，使模型具有大的空间感知场，而无需任何额外参数。 • 我们提出的STC-Net在四个基于骨架的动作识别基准数据集上优于现有的最先进方法。</p>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>文献阅读5</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://www.fomal.cc/posts/b8cef9c9.html">https://www.fomal.cc/posts/b8cef9c9.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Fomalhaut🥝</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-09-19</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2024-06-27</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Literature-Reading/"><div class="tags-punctuation"><svg class="faa-tada icon" style="height:1.1em;width:1.1em;fill:currentColor;position:relative;top:2px;margin-right:3px" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg></div>Literature Reading</a></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/2.webp" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://tuchuang.voooe.cn/images/2023/01/04/20f8e49805975b8f8.webp" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a"></audio><script defer="defer" src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/ca7eff4.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献略读6</div></div></a></div><div class="next-post pull-right"><a href="/posts/cfc9c95f.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读4</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/cfc9c95f.html" title="文献阅读4"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2024-01-27</div><div class="title">文献阅读4</div></div></a></div><div><a href="/posts/7c36b624.html" title="文献阅读"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-09-12</div><div class="title">文献阅读</div></div></a></div><div><a href="/posts/26aa6c6a.html" title="文献阅读2"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-09-16</div><div class="title">文献阅读2</div></div></a></div><div><a href="/posts/bfa33dd0.html" title="文献阅读1"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-09-12</div><div class="title">文献阅读1</div></div></a></div><div><a href="/posts/51ad5cfc.html" title="文献阅读3"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-12-07</div><div class="title">文献阅读3</div></div></a></div><div><a href="/posts/ca7eff4.html" title="文献略读6"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2023-10-30</div><div class="title">文献略读6</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:bold">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Self-Adaptive Graph With Nonlocal Attention Network for Skeleton-Based Action Recognition基于非局部注意力网络的自适应图骨架动作识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BF%BB%E8%AF%91"><span class="toc-text">翻译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#related-work"><span class="toc-text">related work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">图神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method"><span class="toc-text">method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GNN%E6%9C%BA%E5%88%B6"><span class="toc-text">GNN机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%9C%BA%E5%88%B6"><span class="toc-text">Transformer机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SAGCN-Module%E6%9C%BA%E5%88%B6"><span class="toc-text">SAGCN Module机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SA-Module%E6%9C%BA%E5%88%B6"><span class="toc-text">SA Module机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TA-Module%E6%9C%BA%E5%88%B6"><span class="toc-text">TA Module机制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Hypergraph Transformer for Skeleton-based Action Recognition基于骨架的动作识别超图Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-2"><span class="toc-text">abstract</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-work"><span class="toc-text">Related work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Preliminaries"><span class="toc-text">Preliminaries</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%9B%BE-%E8%B6%85%E8%BE%B9%E5%92%8C%E8%8A%82%E7%82%B9%E7%82%B9%E9%9B%86%E5%90%88%EF%BC%8C%E4%BB%A5%E5%8F%8A%E8%BE%B9%E5%BA%A6%E5%92%8C%E8%8A%82%E7%82%B9%E5%BA%A6%E7%9F%A9%E9%98%B5"><span class="toc-text">超图 超边和节点点集合，以及边度和节点度矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relax-the-binary-partition-matrix"><span class="toc-text">Relax the binary partition matrix</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Deriving-the-hyperedge-feature-%E5%AF%BC%E5%87%BA%E8%B6%85%E8%BE%B9%E7%BC%98%E7%89%B9%E5%BE%81"><span class="toc-text">4.1 Deriving the hyperedge feature 导出超边缘特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Encoding-human-skeleton-structure"><span class="toc-text">4.2 Encoding human skeleton structure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Hypergraph-Self-Attention"><span class="toc-text">4.3 Hypergraph Self-Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Partition-strategy%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-text">4.4 Partition strategy分区策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-Model-architecture"><span class="toc-text">4.5 Model architecture</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">GRAPH CONTRASTIVE LEARNING FOR SKELETON BASED ACTION RECOGNITION</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-3"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-2"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method-2"><span class="toc-text">method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-preliminary"><span class="toc-text">3.1 preliminary</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GCNs-in-Skeleton-Based-Action-Recognition"><span class="toc-text">GCNs in Skeleton-Based Action Recognition.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Self-Supervised-Contrastive-Learning"><span class="toc-text">Self Supervised Contrastive Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-GRAPH-CONTRASTIVE-LEARNING"><span class="toc-text">3.2 GRAPH CONTRASTIVE LEARNING</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1Graph-Projection-Head"><span class="toc-text">3.2.1Graph Projection Head</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Memory-bank"><span class="toc-text">3.2.2 Memory bank</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4Hard-Sampling"><span class="toc-text">3.2.4Hard Sampling.</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations用于基于骨架的动作识别和不断增强的分层一致对比学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-4"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-3"><span class="toc-text">introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Representation-Learning-strong-augmentations"><span class="toc-text">Contrastive Representation Learning  &amp;&amp; strong augmentations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Proposed-Method-HiCLR"><span class="toc-text">3 Proposed Method: HiCLR</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Contrastive-Learning-for-Skeleton"><span class="toc-text">Contrastive Learning for Skeleton</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-Consistent-Contrastive-Learning-%E5%88%86%E5%B1%82%E6%AC%A1%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="toc-text">Hierarchical Consistent Contrastive Learning[分层次的一致性对比学习]</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Gradual-growing-augmentation-%E9%80%90%E6%B8%90%E5%A2%9E%E9%95%BF%E5%A2%9E%E5%BC%BA"><span class="toc-text">1)Gradual growing augmentation.逐渐增长增强</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Asymmetric-hierarchical-learning-%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%88%86%E5%B1%82%E5%AD%A6%E4%B9%A0"><span class="toc-text">2) Asymmetric hierarchical learning.非对称分层学习</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Instantiation"><span class="toc-text">3) Instantiation.</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">InfoGCN: Representation Learning for Human Skeleton-based Action Recognition—InfoGCN:基于骨骼的动作识别的表征学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#abstract-5"><span class="toc-text">abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-4"><span class="toc-text">introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method-3"><span class="toc-text">method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1Information-Bottleneck-Objectives%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88%E7%9B%AE%E6%A0%87%E5%92%8C%E6%8D%9F%E5%A4%B1"><span class="toc-text">3.1Information-Bottleneck Objectives信息瓶颈目标和损失</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1Learning-Objective"><span class="toc-text">3.1.1Learning Objective</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2Variational-Bound"><span class="toc-text">3.1.2Variational Bound</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3Training-Loss"><span class="toc-text">3.1.3Training Loss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Neural-Architecture"><span class="toc-text">3.2. Neural Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1The-Importance-of-Learning-Intrinsic-Topology"><span class="toc-text">3.2.1The Importance of Learning Intrinsic Topology</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2Architecture-Overview"><span class="toc-text">3.2.2Architecture Overview</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3Embedding-Block"><span class="toc-text">3.2.3Embedding Block</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4Encoding-Block"><span class="toc-text">3.2.4Encoding Block</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E5%BB%BA%E6%A8%A1"><span class="toc-text">空间建模</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E5%BB%BA%E6%A8%A1"><span class="toc-text">时间建模</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3Ensemble-with-Multi-Modal-Representation"><span class="toc-text">3.3Ensemble with Multi-Modal Representation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4Learning-Framework"><span class="toc-text">3.4Learning Framework</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Zero-shot Skeleton-based Action Recognition via Mutual Information Estimation and Maximization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-color: transparent;"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</div><div class="btn-xz-box"><a class="btn-xz" target="_blank" rel="noopener" href="https://stellarium.org/">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/posts/eec9786.html">魔改指南</a><a href="/box/nav/">网址导航</a></li><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/personal/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/box/Gallery/">我的画廊</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/site/time/">建设进程</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"><div class="img-group-item"><a href="https://www.fomal.cc/" title="Fomalhaut🥝"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/60e5d4e39da7c077.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div><div class="img-group-item"><a href="javascript:void(0)" title="广告位招租"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://lskypro.acozycotage.net/LightPicture/2022/12/65307a5828af6790.webp" alt=""/></a></div></div></div></div><div class="copyright"><span><b>&copy;2022-2024</b></span><span><b>&nbsp;&nbsp;By Fomalhaut🥝</b></span></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v6.3.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Frame-Hexo-blue.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Theme-Butterfly-6513df.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" title="本站采用多线部署，主线路托管于Vercel"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Hosted-Vercel-brightgreen.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://user.51.la/" style="margin-inline:5px" title="本站数据分析得益于51la技术支持"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/Analytics-51la-3db1eb.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/?keyword=20226665" style="margin-inline:5px" title="本站已加入萌ICP豪华套餐，萌ICP备20226665号"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/萌ICP备-20226665-fe1384.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://bitiful.dogecast.com/buckets" style="margin-inline:5px" title="本网站经Service Worker分流至缤纷云对象存储"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Bucket-缤纷云-9c62da.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://www.netdun.net/" style="margin-inline:5px" title="本站使用网盾星球提供CDN加速与防护"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://sourcebucket.s3.ladydaily.com/badge/CDN-网盾星球-fff2cc.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src=" https://sourcebucket.s3.ladydaily.com/badge/Source-Github-d021d6.svg" alt=""/></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/vanilla-lazyload/17.3.1/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())
setTimeout(function(){preloader.endLoading();}, 5000);
document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: '',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: '',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.staticfile.org/twikoo/1.6.8/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script async src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script><script defer type="text/javascript" src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script defer src="https://cdn1.tianli0.top/gh/nextapps-de/winbox/dist/winbox.bundle.min.js"></script><script async src="//at.alicdn.com/t/c/font_3586335_hsivh70x0fm.js"></script><script async src="//at.alicdn.com/t/c/font_3636804_gr02jmjr3y9.js"></script><script async src="//at.alicdn.com/t/c/font_3612150_kfv55xn3u2g.js"></script><script async src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script src="https://lib.baomitu.com/pjax/0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax","#bibi","body > title","#app","#tag-echarts","#posts-echart","#categories-echarts"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.fomal.cc/categories/计算机基础/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍼 小Fの计算机基础笔记 (2)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.fomal.cc/categories/演示/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🍥 小Fの案例演示笔记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.fomal.cc/categories/" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #e9e9e9;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: var(--text-bg-hover)}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-08-14</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt="">LeetCode1</a><div class="blog-slider__text">帅地训练营day1</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/75055f1f.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_14.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-10</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">HelloWorld</a><div class="blog-slider__text">这里是第一个马增龙写的文档，用来记录自己套用模板的全部过程</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/77770c79.html&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 320px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/site/census/'|| '/site/census/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("/api?null",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'null')
    }
  </script><!-- hexo injector body_end end --></body></html>