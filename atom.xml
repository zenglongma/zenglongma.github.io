<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fomalhaut🥝</title>
  
  
  <link href="https://www.fomal.cc/atom.xml" rel="self"/>
  
  <link href="https://www.fomal.cc/"/>
  <updated>2024-09-28T00:26:27.273Z</updated>
  <id>https://www.fomal.cc/</id>
  
  <author>
    <name>Fomalhaut🥝</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Un_technology</title>
    <link href="https://www.fomal.cc/posts/261d6e2f.html"/>
    <id>https://www.fomal.cc/posts/261d6e2f.html</id>
    <published>2024-09-27T23:40:11.000Z</published>
    <updated>2024-09-28T00:26:27.273Z</updated>
    
    <content type="html"><![CDATA[<h1>非技术岗</h1><blockquote><p>产品经理</p></blockquote><blockquote><p>产品与解决方案工程师</p></blockquote><blockquote><p>技术支持工程师</p></blockquote><h1>非技术岗面试复盘</h1><p>第一个点是是自己展示的自己的技术特点过于强或者想走技术的路线过去强烈，</p><p>尤其是面试官给出了两个方向，一个是研发，另一个是对于产品相关，但自己仍然在表述是，表示自己想要走技术路线</p><p>应该在表述时，表明：</p><p>自己想要走技术，但这里并不是指纯技术路线，而是向产品方向靠拢。我对产品的兴趣以及跟人打交道的乐趣要更大于自己在技术突破上的欢乐。</p><blockquote><p>自己在开发过程中碰到问题</p></blockquote>]]></content>
    
    
    <summary type="html">非技术岗面试复盘</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>ACM-model</title>
    <link href="https://www.fomal.cc/posts/7ab4171.html"/>
    <id>https://www.fomal.cc/posts/7ab4171.html</id>
    <published>2024-09-24T00:30:02.000Z</published>
    <updated>2024-09-24T10:22:20.307Z</updated>
    
    <content type="html"><![CDATA[<h1>ACM 模式</h1><blockquote><p>参与了几次笔试，58同城、4399、中通快递的笔试，有几个题目是考察的用acm模式完成代码编写，所以单独拿出一个章节来练习强化一下，防止后边出现类似的题目再次被绊倒。</p></blockquote><p>在ACM模式下，输入输出指的是程序竞赛或算法竞赛中，<strong>程序与外部环境进行交互的方式，通常要求标准化、高效处理</strong>。ACM（Association for Computing Machinery）程序竞赛通常遵循严格的输入输出规范，</p><p>ACM 模式更加关注标准输入输出+效率上的关注</p><blockquote><p>时间和空间复杂度上掌控要求更高一点</p></blockquote><p>与公司测评的区别</p><blockquote><p>公司测试的题目更加关注对于准确性和程序的稳定性要求，比如 测试用例的目标不仅是验证<strong>程序的正确性</strong>，还包括<strong>业务逻辑的完整性</strong>和对特殊情况的处理能力。公司更看重你的代码是否能在真实业务环境中稳定工作，因此要求不仅限于通过测试，可能还包括代码的可读性、可维护性和扩展性。</p></blockquote><blockquote><p><strong>公司笔试</strong>：</p><ul><li><strong>自己设计测试用例</strong>：你可能需要自己设计输入数据和期望的输出结果，特别是一些边界情况、极端数据、以及可能导致错误的特殊输入。测试用例设计不仅仅关注常规输入，还会考虑异常输入、性能瓶颈等问题。</li><li><strong>输入输出数据的灵活性</strong>：不仅仅是数字、字符串等简单数据，可能还包括复杂的业务逻辑和异常处理。比如你可能要处理空指针、数据格式不对等情况，确保程序健壮性。</li></ul></blockquote><h2 id="实例">实例</h2><p>假设公司笔试要求你写一个函数，计算两个数字的和，编写测试用例验证正确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试常规情况</span></span><br><span class="line"><span class="keyword">assert</span> add(<span class="number">1</span>, <span class="number">2</span>) == <span class="number">3</span></span><br><span class="line"><span class="keyword">assert</span> add(<span class="number">0</span>, <span class="number">0</span>) == <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试边界情况</span></span><br><span class="line"><span class="keyword">assert</span> add(-<span class="number">1</span>, -<span class="number">1</span>) == -<span class="number">2</span></span><br><span class="line"><span class="keyword">assert</span> add(<span class="number">1000000</span>, <span class="number">2000000</span>) == <span class="number">3000000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试异常情况（如果需要处理非整数输入）</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    add(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> TypeError:</span><br><span class="line">    <span class="keyword">pass</span>  <span class="comment"># 通过</span></span><br></pre></td></tr></table></figure><h3 id="非核心代码模式的具体含义">非核心代码模式的具体含义</h3><ol><li><strong>核心代码 vs. 非核心代码</strong><ul><li><strong>核心代码</strong>：指的是实现题目要求的主要功能或算法部分，比如解决一个算法问题的主要逻辑。</li><li><strong>非核心代码</strong>：指的是围绕核心功能的辅助代码，比如如何处理输入、格式化输出，或如何搭建一个测试框架来验证核心代码的正确性。这类代码可能与题目的业务逻辑或算法无关，但对整体程序的运行至关重要。</li></ul></li><li><strong>非核心代码的常见任务</strong><ul><li><strong>输入输出处理</strong>：从文件、数据库、或者API中获取输入数据，并将结果输出到指定的格式或终端。例如，将输入数据转换成函数参数的形式，或者对输出结果进行格式化。</li><li><strong>数据准备</strong>：在核心逻辑运行之前，预先生成或组织好数据，比如处理输入数据、初始化对象、构建数据结构等。</li><li><strong>测试用例编写</strong>：编写单元测试或集成测试，测试核心代码是否按预期工作。这种情况下，你可能需要设计各种输入条件并检查输出。</li><li><strong>异常处理</strong>：确保系统在异常情况下的健壮性，比如处理输入数据中的错误格式、空数据、或者系统崩溃等场景。</li></ul></li><li><strong>非核心代码模式的应用场景</strong><ul><li><strong>公司笔试或面试</strong>：公司笔试题目中，可能会提供一部分核心代码（如核心算法的实现），要求你为其编写相应的测试用例，或者通过不同的输入输出形式来验证其正确性。这种模式下，你的任务并不是优化算法或修改核心逻辑，而是确保输入输出过程和系统的健壮性。</li><li><strong>编程竞赛</strong>：在某些竞赛中，题目可能会要求选手处理数据输入输出，算法本身可能并不复杂，但需要在特定的格式下完成任务。</li></ul></li></ol><h3 id="举例说明">举例说明</h3><p>假设你参加一个公司笔试，题目给你一个已经写好的核心函数，要求你编写代码来进行测试和验证输入输出。核心函数如下：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心代码：实现了两个数相加的功能</span></span><br><span class="line">def <span class="title function_ invoke__">add</span>(<span class="attr">a</span>: <span class="keyword">int</span>, <span class="attr">b</span>: <span class="keyword">int</span>) -&gt; <span class="keyword">int</span>:</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>你的任务是在非核心代码模式下编写以下部分：</p><ul><li>处理输入，调用<code>add</code>函数。</li><li>输出结果。</li><li>编写测试用例来验证这个<code>add</code>函数。</li></ul><h3 id="非核心代码示例">非核心代码示例</h3><h4 id="1-输入输出处理">1. 输入输出处理</h4><figure class="highlight plaintext"><figcaption><span>main():</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    # 从标准输入读取两个整数</span><br><span class="line">    a, b = map(int, input().split())</span><br><span class="line">    </span><br><span class="line">    # 调用核心函数 add</span><br><span class="line">    result = add(a, b)</span><br><span class="line">    </span><br><span class="line">    # 输出结果</span><br><span class="line">    print(f&quot;The sum of &#123;a&#125; and &#123;b&#125; is: &#123;result&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 运行程序</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="2-编写测试用例">2.编写测试用例</h4><figure class="highlight plaintext"><figcaption><span>test_add():</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    assert add(1, 2) == 3</span><br><span class="line">    assert add(-1, 1) == 0</span><br><span class="line">    assert add(0, 0) == 0</span><br><span class="line">    assert add(100, 200) == 300</span><br><span class="line"></span><br><span class="line"># 运行测试</span><br><span class="line">test_add()</span><br><span class="line">print(&quot;All test cases passed!&quot;)</span><br></pre></td></tr></table></figure><h3 id="总结">==总结==</h3><p>==<strong>非核心代码模式</strong>要求你关注围绕核心功能的辅助代码，而非核心逻辑本身。这通常意味着你需要处理输入输出、测试用例、数据准备或异常处理等方面的工作。==</p><h4 id="重点1">重点1</h4><p>当你从控制台或其他输入源读取数字（比如 <code>int</code> 或 <code>double</code> 类型）后，如果你接下来要读取字符串，通常需要调用 <code>in.nextLine()</code> 进行换行操作。这是因为读取数字的 <code>in.nextInt()</code> 或 <code>in.nextDouble()</code> 之类的方法不会读取行结束符（换行符），导致后续的 <code>in.nextLine()</code> 直接读取到空行。如果你不手动调用 <code>in.nextLine()</code> 进行换行，可能会导致无法正确读取你期望的字符串内容。</p><h4 id="重点2">重点2</h4><p>最后给出了对应的模板解析：</p><p><a href="https://blog.csdn.net/qq_39295220/article/details/116785551">【ACM模式】牛客网ACM机试模式Python&amp;Java&amp;C++主流语言OJ输入输出案例代码总结-CSDN博客</a></p><p>上边的参考了下边的链接</p><p><a href="https://blog.csdn.net/jl6667/article/details/115439428">ACM模式自己写输入输出_acm模型写输入输出-CSDN博客</a></p><p><img src="../assets/image-20240924145402072.png" alt="image-20240924145402072"></p><blockquote><p>剑指offer 42连续子数组的最大和——ACM模式</p></blockquote><p>详细展示出了ACM与核心代码模式之间的区别：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.<span class="property">util</span>.*;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution1</span> &#123;</span><br><span class="line">    <span class="comment">//自己写输入输出。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span>(<span class="built_in">String</span>[] args) &#123;</span><br><span class="line">        Scanner sc=<span class="keyword">new </span><span class="class title_">Scanner</span>(System.<span class="property">in</span>);</span><br><span class="line">        System.<span class="property">out</span>.<span class="property">println</span>(<span class="string">&quot;在下面一行中以空格为间隙输入元素:&quot;</span>);</span><br><span class="line">        <span class="comment">//得出的是string类型</span></span><br><span class="line">        <span class="built_in">String</span>[] <span class="built_in">str</span> = sc.<span class="property">nextLine</span>().<span class="property">split</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="comment">//将String类型数组转成Integer类型数组。</span></span><br><span class="line">        <span class="type">int</span>[] nums=<span class="keyword">new </span><span class="class title_">int</span>[<span class="built_in">str</span>.<span class="property">length</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nums.<span class="property">length</span>; i++) &#123;</span><br><span class="line">            nums[i]=Integer.<span class="property">valueOf</span>(<span class="built_in">str</span>[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        System.<span class="property">out</span>.<span class="property">println</span>(<span class="string">&quot;结果是：&quot;</span>+<span class="title function_">maxSubArray</span>(nums));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">maxSubArray</span>(<span class="type">int</span>[] nums) &#123;</span><br><span class="line">        <span class="type">int</span> n=nums.<span class="property">length</span>;</span><br><span class="line">        <span class="comment">//1.定义dp数组</span></span><br><span class="line">        <span class="type">int</span>[] dp=<span class="keyword">new </span><span class="class title_">int</span>[n];</span><br><span class="line">        <span class="comment">//3.base case</span></span><br><span class="line">        dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        <span class="comment">//2.状态转移方程</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="comment">//可以通过具体的数字列明观察各个数之间的关系</span></span><br><span class="line">            <span class="comment">//得出状态转移方程，这个状态转移方程一定要能够体现出来i项和i-1项之间的区别。</span></span><br><span class="line">            dp[i]=Math.<span class="property">max</span>(   (Math.<span class="property">max</span>(dp[i<span class="number">-1</span>],nums[i<span class="number">-1</span>]) +nums[i]),  nums[i] );</span><br><span class="line">            <span class="comment">//这里还要再添加和它本身的一个对比，比如[-2,1],最大子序列和为它本身即1.</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//遍历dp数组找出max元素</span></span><br><span class="line">        <span class="type">int</span> <span class="built_in">max</span>=dp[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(dp[i]&gt;<span class="built_in">max</span>)&#123;</span><br><span class="line">                <span class="built_in">max</span>=dp[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="重点3">重点3</h4><p>对于不同的数据结构之间的转换</p><p><a href="https://segmentfault.com/a/1190000042310202#comment-area">Java] ACM 模式刷题模板——面试时手动构建链表、二叉树 - 个人文章 - SegmentFault 思否</a></p><p>个人看完代码之后，感觉数组的转换有些不对的地方</p><p>==str.length() -2应该改成 str.length() - 1==</p><p>==只看了数组和链表，二叉树没看==</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span>[] <span class="title function_">StringToIntArray</span>(<span class="built_in">String</span> <span class="built_in">str</span>) &#123;</span><br><span class="line">      <span class="built_in">String</span>[] parts = <span class="built_in">str</span>                <span class="comment">//根据“,”分割成字符串数组</span></span><br><span class="line">          .<span class="property">substring</span>(<span class="number">1</span>, <span class="built_in">str</span>.<span class="property">length</span>() - <span class="number">1</span>)</span><br><span class="line">          .<span class="property">split</span>(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">      <span class="type">int</span>[] nums = <span class="keyword">new </span><span class="class title_">int</span>[parts.<span class="property">length</span>];</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; parts.<span class="property">length</span>; i ++) </span><br><span class="line">          nums[i] = Integer.<span class="property">parseInt</span>(parts[i]);    <span class="comment">//转为整型数组</span></span><br><span class="line">      </span><br><span class="line">      <span class="keyword">return</span> nums;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="重点4">重点4</h4><p>对于不同的数据结构和转换详细讲解【==待学习==】</p><p><a href="https://blog.csdn.net/KingGue/article/details/123335283">ACM模式下输入输出总结（Java）_java acm输入输出-CSDN博客</a></p><p><a href="https://openatomworkshop.csdn.net/664ee41eb12a9d168eb70d6a.html?spm=1001.2101.3001.6650.9&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~activity-9-135202188-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~activity-9-135202188-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4#devmenu1">ACM模式Java输入输出模板_java_Miraclo_acc-开放原子开发者工作坊 (csdn.net)</a></p><p><a href="https://zhuanlan.zhihu.com/p/438909352">ACM模式之输入输出（Java/Python例题） - 知乎 (zhihu.com)</a></p><p>==<a href="https://zhuanlan.zhihu.com/p/392440391">Java刷ACM模式的各类输入输出构造 - 知乎 (zhihu.com)</a>==</p><blockquote><p>题解总结</p><p><a href="https://blog.csdn.net/qq_41855453/article/details/131542412?spm=1001.2101.3001.6650.15&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-15-131542412-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-15-131542412-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4">ACM模式常见输入输出专题（Java版）_java acm输入输出-CSDN博客</a></p></blockquote><p><strong>==卡码网==</strong></p><p><a href="https://kamacoder.com/">卡码网KamaCoder</a></p><p>专门用来练习输入输出的网站</p><p><a href="https://github.com/youngyangyang04/kamacoder-solutions/blob/main/problems/0002.A%2BB%E9%97%AE%E9%A2%98II.md">kamacoder-solutions/problems/0002.A+B问题II.md at main · youngyangyang04/kamacoder-solutions · GitHub</a></p><p><a href="https://programmercarl.com/qita/acm.html">如何练习ACM模式输入输出模式 | 如何准备笔试 | 卡码网 | 代码随想录 (programmercarl.com)</a></p><blockquote><p>下边的链接讲解的蛮清晰的，利用牛客的a+b 题目和相应的题解</p><p><a href="https://blog.csdn.net/weixin_44052055/article/details/120117628?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522FBB8C9DC-3A91-4B7F-8265-EFC65A75C082%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=FBB8C9DC-3A91-4B7F-8265-EFC65A75C082&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-120117628-null-null.nonecase&amp;utm_term=acm&amp;spm=1018.2226.3001.4450">https://blog.csdn.net/weixin_44052055/article/details/120117628?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522FBB8C9DC-3A91-4B7F-8265-EFC65A75C082%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=FBB8C9DC-3A91-4B7F-8265-EFC65A75C082&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-120117628-null-null.nonecase&amp;utm_term=acm&amp;spm=1018.2226.3001.4450</a></p></blockquote><p>详细解析了 hasNext（） nextInt()   hasNextLine()区别</p><p><strong>采用has xxxx的话，后面也要用next xxxx。比如前面用hasNextLine，那么后面要用 nextLine 来处理输入。</strong></p><blockquote><p><a href="https://fmy1024.cn/?cdk=3643fOvKdReDFDDa&amp;url=https%3A%2F%2Fblog.csdn.net%2Fweixin_44052055%2Farticle%2Fdetails%2F120054907">csdn (fmy1024.cn)</a>]</p><p>在线解析csdn会员文章</p></blockquote><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.Scanner;</span><br><span class="line"></span><br><span class="line">public class InOutTest &#123;</span><br><span class="line">    public static void main(String[] args)&#123;</span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span>. 多组空格分割的两个整数</span><br><span class="line">        <span class="regexp">//</span> eg：每行输入两个数求和</span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">10</span> <span class="number">20</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNext())&#123; <span class="regexp">//</span> <span class="keyword">while</span>处理多个case</span><br><span class="line"><span class="regexp">//</span>            int a = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            int b = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            System.out.println(a + b);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">2</span>. 第一行组数接空格分割的两个正整数 （行数限制）</span><br><span class="line">        <span class="regexp">//</span> eg: 输出a+b的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">2</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">10</span> <span class="number">20</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        int num = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">for</span> (int i =<span class="number">0</span>; i &lt; num; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>            int a = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            int b = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            System.out.println(a + b);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">3</span>. 空行分割的两个正整数，<span class="number">0</span> <span class="number">0</span> 结束 （结束符限制）</span><br><span class="line">        <span class="regexp">//</span> eg: 输出a+b的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">10</span> <span class="number">20</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNext())&#123;</span><br><span class="line"><span class="regexp">//</span>            int a = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            int b = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">if</span>(a == <span class="number">0</span> &amp;&amp; b == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(a + b);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span>. 每行第一个为个数后带空格分割整数，<span class="number">0</span>结束 （结束符限制,每行有个数限制）</span><br><span class="line">        <span class="regexp">//</span> 输入数据包括多组。</span><br><span class="line">        <span class="regexp">//</span> 每组数据一行,每行的第一个整数为整数的个数n(<span class="number">1</span> &lt;= n &lt;= <span class="number">100</span>), n为<span class="number">0</span>的时候结束输入。</span><br><span class="line">        <span class="regexp">//</span> 接下来n个正整数,即需要求和的每个正整数。</span><br><span class="line">        <span class="regexp">//</span> eg: 每组数据输出求和的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">5</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">0</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNext())&#123;</span><br><span class="line"><span class="regexp">//</span>            int n = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">if</span> (n == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line"><span class="regexp">//</span>            int sum = <span class="number">0</span>;</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>                sum += <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(sum);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">5</span>. 第一行组数接第一个个数接空格分开的整数 （行数限制,每行有个数限制）</span><br><span class="line">        <span class="regexp">//</span> 输入的第一行包括一个正整数t(<span class="number">1</span> &lt;= t &lt;= <span class="number">100</span>), 表示数据组数。</span><br><span class="line">        <span class="regexp">//</span> 接下来t行, 每行一组数据。</span><br><span class="line">        <span class="regexp">//</span> 每行的第一个整数为整数的个数n(<span class="number">1</span> &lt;= n &lt;= <span class="number">100</span>)。</span><br><span class="line">        <span class="regexp">//</span> 接下来n个正整数, 即需要求和的每个正整数。</span><br><span class="line">        <span class="regexp">//</span> eg: 每组数据输出求和的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">2</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">5</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        int num = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; num; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>            int n = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            int sum = <span class="number">0</span>;</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span>(int j = <span class="number">0</span>; j &lt; n; j++)&#123;</span><br><span class="line"><span class="regexp">//</span>                sum += <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(sum);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line">        <span class="regexp">//</span> <span class="number">6</span>. 每行第一个为个数后带空格分割整数 (无结束限制,每行有个数限制)</span><br><span class="line">        <span class="regexp">//</span> 输入数据有多组, 每行表示一组输入数据。</span><br><span class="line">        <span class="regexp">//</span> 每行的第一个整数为整数的个数n(<span class="number">1</span> &lt;= n &lt;= <span class="number">100</span>)。</span><br><span class="line">        <span class="regexp">//</span> 接下来n个正整数, 即需要求和的每个正整数。</span><br><span class="line">        <span class="regexp">//</span> eg: 每组数据输出求和的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">5</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNextInt())&#123;</span><br><span class="line"><span class="regexp">//</span>            int num = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            int sum = <span class="number">0</span>;</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; num; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>                sum += <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(sum);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">7</span>. 多组空格分隔的正整数 (无结束限制,每行无个数限制,需要当作字符串处理)</span><br><span class="line">        <span class="regexp">//</span> 输入数据有多组, 每行表示一组输入数据。</span><br><span class="line">        <span class="regexp">//</span> 每行不定有n个整数，空格隔开。(<span class="number">1</span> &lt;= n &lt;= <span class="number">100</span>)。</span><br><span class="line">        <span class="regexp">//</span> eg: 每组数据输出求和的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNextLine())&#123;   <span class="regexp">//</span> 直接读一行字符串</span><br><span class="line"><span class="regexp">//</span>            String[] s = <span class="keyword">in</span>.nextLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            int sum = <span class="number">0</span>;</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span>(int i = <span class="number">0</span>; i &lt; s.length; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>                sum += Integer.parseInt(s[i]);</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(sum);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> 扩展：第一行组数接空格分开的多个整数 (有行数限制,每行无个数限制,需要当作字符串处理)</span><br><span class="line">        <span class="regexp">//</span> eg: 每组数据输出求和的结果</span><br><span class="line">        <span class="regexp">//</span> <span class="number">3</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">4</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line">        int num = <span class="keyword">in</span>.nextInt();</span><br><span class="line">        <span class="keyword">in</span>.nextLine();  <span class="regexp">//</span> 跳过行数所在行</span><br><span class="line">        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; num; i++ )&#123;</span><br><span class="line">            String[] s = <span class="keyword">in</span>.nextLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            int sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(int j = <span class="number">0</span>; j &lt; s.length; j++)&#123;</span><br><span class="line">                sum += Integer.parseInt(s[j]);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(sum);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">8</span>. 第一行个数第二行字符串</span><br><span class="line">        <span class="regexp">//</span> 输入有两行，第一行n</span><br><span class="line">        <span class="regexp">//</span> 第二行是n个空格隔开的字符串</span><br><span class="line">        <span class="regexp">//</span> eg: 输出一行排序后的字符串，空格隔开，无结尾空格</span><br><span class="line">        <span class="regexp">//</span> <span class="number">5</span></span><br><span class="line">        <span class="regexp">//</span> c d a bb e</span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        int num = <span class="keyword">in</span>.nextInt();</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">in</span>.nextLine();</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNext())&#123;</span><br><span class="line"><span class="regexp">//</span>            String[] s = <span class="keyword">in</span>.nextLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            Arrays.sort(s);</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; s.length; i++)&#123;</span><br><span class="line"><span class="regexp">//</span>                System.out.print(s[i] + <span class="string">&quot; &quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">9</span>. 多行空格分开的字符串</span><br><span class="line">        <span class="regexp">//</span> 多个测试用例，每个测试用例一行。</span><br><span class="line">        <span class="regexp">//</span> 每行通过空格隔开，有n个字符，n＜<span class="number">100</span></span><br><span class="line">        <span class="regexp">//</span> eg: 对于每组测试用例，输出一行排序过的字符串，每个字符串通过空格隔开</span><br><span class="line">        <span class="regexp">//</span> a c bb</span><br><span class="line">        <span class="regexp">//</span> f dddd</span><br><span class="line">        <span class="regexp">//</span> nowcoder</span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNextLine())&#123;</span><br><span class="line"><span class="regexp">//</span>            String[] s = <span class="keyword">in</span>.nextLine().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            Arrays.sort(s);</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span> (String c:s)&#123;</span><br><span class="line"><span class="regexp">//</span>                System.out.print(c + <span class="string">&quot; &quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println();</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="regexp">//</span> <span class="number">10</span>. 多行逗号分开的字符串 （逗号和空格的区别就是最后一个字符输完的时候<span class="string">&quot;,&quot;</span>还是<span class="string">&quot; &quot;</span> ）</span><br><span class="line">        <span class="regexp">//</span> 多个测试用例，每个测试用例一行。</span><br><span class="line">        <span class="regexp">//</span> 每行通过,隔开，有n个字符，n＜<span class="number">100</span></span><br><span class="line">        <span class="regexp">//</span> eg: 对于每组用例输出一行排序后的字符串，用<span class="string">&#x27;,&#x27;</span>隔开，无结尾空格</span><br><span class="line">        <span class="regexp">//</span> a,c,bb</span><br><span class="line">        <span class="regexp">//</span> f,dddd</span><br><span class="line">        <span class="regexp">//</span> nowcoder</span><br><span class="line"><span class="regexp">//</span>        Scanner <span class="keyword">in</span> = new Scanner(System.<span class="keyword">in</span>);</span><br><span class="line"><span class="regexp">//</span>        <span class="keyword">while</span> (<span class="keyword">in</span>.hasNextLine()) &#123;</span><br><span class="line"><span class="regexp">//</span>            String[] s = <span class="keyword">in</span>.nextLine().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            Arrays.sort(s);</span><br><span class="line"><span class="regexp">//</span>            int l = s.length;</span><br><span class="line"><span class="regexp">//</span>            <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; l - <span class="number">1</span>; i++) &#123;</span><br><span class="line"><span class="regexp">//</span>                System.out.print(s[i] + <span class="string">&quot;,&quot;</span>);</span><br><span class="line"><span class="regexp">//</span>            &#125;</span><br><span class="line"><span class="regexp">//</span>            System.out.println(s[l-<span class="number">1</span>]);</span><br><span class="line"><span class="regexp">//</span>        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="ACM-模板">ACM 模板</h3><blockquote><p><a href="https://blog.csdn.net/qq_40222433/article/details/138311329?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Ctr-2-138311329-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Ctr-2-138311329-blog-123821153.235%5Ev43%5Epc_blog_bottom_relevance_base4">Java语言中ACM模式输入输出- 1（大厂 笔试题必备知识）_java acm模式-CSDN博客</a></p><p>==这个是对与模板的补充，待看==</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> java.lang.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//1.数据输入</span></span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="comment">//读数字</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">numLen</span> <span class="operator">=</span> in.nextInt();</span><br><span class="line">        <span class="type">int</span>[] numArr = <span class="keyword">new</span> <span class="title class_">int</span>[numLen];</span><br><span class="line">        <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(in.hasNextInt() &amp;&amp; i &lt; numLen)&#123;</span><br><span class="line">            numArr[i] = in.nextInt();</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//读字符串</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">strLen</span> <span class="operator">=</span> in.nextInt();</span><br><span class="line">        in.nextLine(); <span class="comment">//数字到字符串要换行</span></span><br><span class="line">        String[] strArr = <span class="keyword">new</span> <span class="title class_">String</span>[strLen];</span><br><span class="line">        <span class="comment">//或者 strArr[] = in.nextLine().split(&quot; &quot;);</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(in.hasNextLine() &amp;&amp; j &lt; strLen)&#123;</span><br><span class="line">            strArr[j] = in.nextLine();</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//2. 处理</span></span><br><span class="line">        <span class="type">Solution</span> <span class="variable">solution</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Solution</span>();</span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> solution.process(numArr, strArr);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//3. 输出</span></span><br><span class="line">        System.out.println(result);</span><br><span class="line">        <span class="comment">//四舍五入输出小数</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> String.format(<span class="string">&quot;%.2f&quot;</span>,<span class="number">3.555</span>);</span><br><span class="line">        System.out.println(str);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//下面类似 LeetCode 的核心代码模式</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">process</span><span class="params">(<span class="type">int</span>[] nums, String[] strs)</span> &#123;</span><br><span class="line">        <span class="type">StringBuilder</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuilder</span>();</span><br><span class="line">        sb.append(Arrays.toString(nums));</span><br><span class="line">        sb.append(<span class="string">&quot; &amp;&amp; &quot;</span>);</span><br><span class="line">        sb.append(Arrays.toString(strs));</span><br><span class="line">        <span class="keyword">return</span> sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">https:<span class="comment">//blog.csdn.net/dlhjw1412/article/details/123821153</span></span><br></pre></td></tr></table></figure><h1>笔试准备</h1><p>笔试必刷题：</p><p><a href="https://www.nowcoder.com/exam/company?currentTab=recommand&amp;jobId=100&amp;selectStatus=0&amp;tagIds=175%2C768%2C11200%2C11203%2C142700">牛客网公司真题_免费模拟题库_企业面试|笔试真题 (nowcoder.com)</a></p><p><img src="../assets/image-20240924182218783.png" alt="image-20240924182218783"></p>]]></content>
    
    
    <summary type="html">练习acm输入输出模式，而非核心代码模式</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>job_selectivity</title>
    <link href="https://www.fomal.cc/posts/afaefb1a.html"/>
    <id>https://www.fomal.cc/posts/afaefb1a.html</id>
    <published>2024-09-22T12:50:48.000Z</published>
    <updated>2024-09-26T08:08:38.243Z</updated>
    
    <content type="html"><![CDATA[<h1>9.22_周天</h1><p>中通快递集团招聘  笔试筛选  一挂</p><p>小米   笔试评估中</p><p>4399  笔试被刷</p><p>奥飞娱乐 -测评  9.20查看</p><h2 id="9-23日">9.23日</h2><blockquote><p>完成海康威视测评</p></blockquote><h1>9.24日</h1><blockquote><p>19：00  完成数字马力的在线笔试</p></blockquote><p>选择（单选 19个 ）多选13个  编程题 两道</p><blockquote><p>都是核心代码模式，只需要完善对应的核心功能即可</p></blockquote><blockquote><p>第一道是 编程题目</p><p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为日标值 target的那 两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 输入：nums =[2,7,11,15] target =9 输出:[0,1]解释:因为 nums[0]+nums[1]==9，返回[0，1] 示例 2: 输入:nums=[3,2,4]，target =6 输出:[1,2] 示例 3: 输入:nums=[3,3]，target =6输出:[0,1]</p></blockquote><blockquote><p>第二道是sql语句题目</p><p>==被卡住，没有ac==</p></blockquote><p>数据库实现: S 表保存着学生关系，有两列，其中SNO 为学号，SNAME 为姓名C 表保存着课程关系，有三列，其中CNO 为课程号，CNAME 为课程名 CTEACHER 为老师 SC表保存着选课关系，有三列，其中SNO为学号，CNO为课程号，SCORE为成绩</p><p>找出没有选“小易”老师课程的所有学生姓名</p><p>列出有三门(包括三门)以上课程分数&gt;90的学生姓名及其平均成绩</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">输入为下边</span><br><span class="line"><span class="keyword">CREATE</span> TABLÉ <span class="string">&#x27;SSNO varchar(100) COLLATE utf8mb4 bin DEFAULT&quot; COMMENT&#x27;</span>学号”SNAME <span class="type">varchar</span>(<span class="number">255</span>)<span class="keyword">COLLATE</span> utf8mb4 bin <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span> COMMENT &quot;姓名)ENGINE=InnODB AUTO INCREMENT=146 DEFAULT CHARSET=utf8mb4COLLATE=utf8mb4 bin ROW FORMAT=DYNAMIC COMMENT=&#x27;学生关系&#x27;</span><br><span class="line">CREATE TABLE &#x27;C(CNO varchar(100) COLLATE utf8mb4 bin DEFAULT&quot; COMMENT <span class="string">&#x27;课程号&#x27;</span>CNAME&quot; varchar(255) COLLATE utf8mb4 bin DEFAULT NULL COMMENT &#x27;课程名&#x27;CTEACHER varchar(255)COLLATE utf8mb4 bin DEFAULT NULL COMMENT &#x27;老师)ENGINE=InnODB AUTO INCREMENT=146 DEFAULT CHARSET=utf8mb4COLLATE=utf8mb4 bin ROW FORMAT=DYNAMIC COMMENT=&#x27;课程关系&quot;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">&#x27;SC&#x27;</span>(SNO <span class="type">varchar</span>(<span class="number">100</span>) <span class="keyword">COLLATE</span> utf8mb4_bin <span class="keyword">DEFAULT</span>&quot; COMMENT&#x27;学号&#x27;CNO varchar(100) COLLATE utf8mb4 bin DEFAULT&quot; COMMENT <span class="string">&#x27;课程号”SCORE bigint(100) NOT NULL DEFAULT &#x27;</span>O<span class="string">&#x27; COMMENT &#x27;</span>成绩”)ENGINE<span class="operator">=</span>InNODB AUTO INCREMENT<span class="operator">=</span><span class="number">146</span> <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8mb4COLLATE<span class="operator">=</span>utf8mb4 bin <span class="type">ROW</span> FORMAT<span class="operator">=</span><span class="keyword">DYNAMIC</span> COMMENT<span class="operator">=</span><span class="string">&#x27;选课关系&quot;</span></span><br><span class="line"><span class="string">INSERT INTO &#x27;</span>SC<span class="string">&#x27;(&#x27;</span>SNO<span class="string">&#x27;, &#x27;</span>CNO<span class="string">&#x27;, SCORE&quot;) VALUES ( &#x27;</span><span class="number">2</span><span class="string">&#x27;, &#x27;</span><span class="number">11</span><span class="string">&#x27;, 120);INSERT INTO &#x27;</span>S(SNO,<span class="string">&#x27;SNAME&quot;) VALUES (&quot;2&#x27;</span>,<span class="string">&#x27;易&#x27;</span>);<span class="keyword">INSERT</span> <span class="keyword">INTO</span> C(CNO,CNAME<span class="string">&#x27;,CTEACHER’) VALUES (&#x27;</span><span class="number">11</span><span class="string">&#x27;,&#x27;</span>小易<span class="string">&#x27;,&quot;物理老师”);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">输出 </span></span><br><span class="line"><span class="string">SMAME</span></span><br><span class="line"><span class="string">小易</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 找出没有选“小易”老师课程的学生姓名</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> S.SNAME</span><br><span class="line"><span class="keyword">FROM</span> S S</span><br><span class="line"><span class="keyword">WHERE</span> S.SNO <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> SC.SNO</span><br><span class="line">    <span class="keyword">FROM</span> SC SC</span><br><span class="line">    <span class="keyword">JOIN</span> C C <span class="keyword">ON</span> SC.CNO = C.CNO</span><br><span class="line">    <span class="keyword">WHERE</span> C.CTEACHER = <span class="string">&#x27;小易&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 找出有三门以上课程分数 &gt; 90 的学生姓名及其平均成绩</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    S.SNAME,</span><br><span class="line">    AVG(SC.SCORE) <span class="keyword">AS</span> AverageScore</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">    S S</span><br><span class="line"><span class="keyword">JOIN</span> </span><br><span class="line">    SC SC <span class="keyword">ON</span> S.SNO = SC.SNO</span><br><span class="line"><span class="keyword">WHERE</span> </span><br><span class="line">    SC.SCORE &gt; <span class="number">90</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">    S.SNAME</span><br><span class="line"><span class="keyword">HAVING</span> </span><br><span class="line">    COUNT(<span class="keyword">DISTINCT</span> SC.CNO) &gt;= <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="9-25">9.25</h3><blockquote><p>科技股份有限公司</p></blockquote>]]></content>
    
    
    <summary type="html">投递/完成对应要求后的结果查询记录</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>my_resume</title>
    <link href="https://www.fomal.cc/posts/fca410bb.html"/>
    <id>https://www.fomal.cc/posts/fca410bb.html</id>
    <published>2024-09-21T01:36:24.000Z</published>
    <updated>2024-09-27T06:40:56.123Z</updated>
    
    <content type="html"><![CDATA[<h1>简历项目与问答准备</h1><h2 id="测试开发工程师-整体性提问">测试开发工程师(整体性提问)</h2><blockquote><p>下边是一些对应的整体的概述提问</p></blockquote><h3 id="1-项目经验相关的问题">1. <strong>项目经验相关的问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>你在“室内空气环境数据监测与控制系统”项目中的具体贡献是什么？<ul><li>先简要介绍一下项目背景、在介绍一下我做的主要贡献<ul><li>背景：毕业设计项目，linux嵌入式软件——开发板（硬件公司合作）+师哥的软件系统</li><li>STM32开发板（数据控制，串口处理，命令转发）+WiFi模块+各类传感器</li><li></li></ul></li></ul></li><li>你在项目中如何实现前后端分离，以及如何进行云端数据处理？</li><li>硬件集成方面，你使用了哪些技术和协议？你如何处理数据采集与传输？</li></ul></li><li><strong>应对策略</strong>：<ul><li>描述项目时尽量具体，提到技术栈时要结合你使用的SpringBoot、MyBatis、MySQL和Vue等技术，强调如何解决了项目中的具体问题。</li><li>针对硬件部分，重点说明你如何配置STM32开发板、使用esp8266wifi模块进行数据传输，特别是在传输协议（如MQTT）的使用上。</li></ul></li></ul><h3 id="2-实习经验相关的问题">2. <strong>实习经验相关的问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>在红笔部门的管理工作中，你是如何统筹和协调300+实习生的？<ul><li>飞书工具</li><li>技术文档说明书【培训过程改进】</li><li>技术管理——系统管理,利用教师批改系统完成工作</li><li>分组，小组长管理培训，业绩考核与奖惩机制</li></ul></li><li>你在“军体拳视频人体行为识别系统”中负责了哪些算法研究与模型训练？如何参与STGCNs模型的训练？</li></ul></li><li><strong>应对策略</strong>：<ul><li>强调你在数据管理和开发方面的贡献，特别是在项目管理和团队合作中的经验。对于算法部分，具体说明你如何应用深度学习技术，提到如何使用STGCN进行人体行为识别，并描述你的数据采集与处理过程。</li></ul></li></ul><h3 id="3-技术技能相关的问题">3. <strong>技术技能相关的问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>你熟悉SpringBoot、MyBatis等技术栈，能否详细讲解一下你在实际项目中的应用场景？</li><li>你提到熟悉Java集合和常见的排序算法（如快速排序、堆排序），请你讲解一下这些算法的复杂度以及如何选择合适的算法？</li></ul></li><li><strong>应对策略</strong>：<ul><li>通过具体项目中的例子展示你如何应用这些技术，比如讲解SpringBoot和MyBatis如何用于开发和数据持久化。</li><li>针对排序算法，除了复杂度分析，还可以提到实际应用中的选择依据（如根据数据集规模、数据分布选择合适的算法）。</li></ul></li></ul><h3 id="4-数据库与优化相关的问题">4. <strong>数据库与优化相关的问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>你在数据库优化方面有什么实际经验？如何处理数据库性能问题？</li><li>请解释MySQL中的事务隔离级别以及如何避免死锁？</li></ul></li><li><strong>应对策略</strong>：<ul><li>针对数据库优化，可以提到你在项目中如何通过索引优化查询性能、如何设计合理的表结构。</li><li>对于事务和锁机制，结合实际项目中的场景讲解你如何运用事务隔离级别来保证数据一致性，避免并发问题。</li></ul></li></ul><h3 id="5-测试相关的问题">5. <strong>测试相关的问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>你如何进行系统测试？如何保证测试覆盖率和有效性？</li><li>你在项目中如何进行单元测试、集成测试以及系统测试？</li></ul></li><li><strong>应对策略</strong>：<ul><li>重点描述你在项目中的测试工作，提到具体的测试框架和工具（如JUnit、TestNG），并结合项目中遇到的问题如何通过自动化测试、手动测试等方法确保系统稳定性。</li></ul></li></ul><h3 id="6-深度学习相关问题">6. <strong>深度学习相关问题</strong></h3><ul><li><strong>提问点</strong>：<ul><li>你提到使用了PyTorch，请你详细讲解一下在“人体行为识别系统”项目中如何利用深度学习库进行模型训练和测试？</li><li>STGCNs模型的优点和挑战是什么？你在训练过程中遇到了哪些困难，如何解决的？</li></ul></li><li><strong>应对策略</strong>：<ul><li>结合PyTorch的使用，说明你如何搭建模型、处理数据并进行训练。可以提到使用的模型架构、训练方法和优化策略。</li><li>针对STGCNs的挑战，可以谈谈模型复杂性、训练时间和数据标注的困难，并说明如何通过优化模型结构和数据处理来应对这些问题。</li></ul></li></ul><h3 id="快速准备面试的策略："><strong>快速准备面试的策略</strong>：</h3><ol><li><strong>技术概念复习</strong>：重点复习简历中提到的技术栈，如SpringBoot、MyBatis、Java集合与算法、数据库优化等知识点，确保在面试中能够快速准确地回答。</li><li><strong>项目经历梳理</strong>：针对每个项目，准备一个简洁的项目概述，重点描述你在项目中的贡献和所使用的技术，特别是与测试相关的部分。</li><li><strong>常见问题模拟</strong>：提前准备好项目管理、测试策略、算法优化等问题的回答，确保回答时逻辑清晰、具体、有条理。</li></ol><h2 id="回答">回答</h2><h3 id="1-项目经验相关问题">1. <strong>项目经验相关问题</strong></h3><p>**问题：**你在“室内空气环境数据监测与控制系统”项目中的具体贡献是什么？<br><strong>回答：</strong> “在这个项目中，我负责了整个系统的搭建和业务流程的实现。具体来说，我主要承担了软硬件系统的开发，包括基于SpringBoot、MyBatis和MySQL的后端开发，以及Vue的前端搭建。在硬件集成方面，我使用了STM32开发板和esp8266wifi模块进行空气数据的采集与传输，采用了自定义的协议和MQTT通信，实现了数据的实时采集和用户设备管理功能。通过这个项目，我积累了丰富的全栈开发经验，特别是软硬件结合项目中的挑战。”</p><h3 id="2-实习经验相关问题">2. <strong>实习经验相关问题</strong></h3><p>**问题：**在红笔部门的管理工作中，你是如何统筹和协调300+实习生的？<br><strong>回答：</strong> “作为实习组长，我主要负责统筹和协调300多位实习生的工作安排与任务分配。通过搭建有效的项目管理系统，我制定了清晰的工作流程，确保每位实习生都能高效完成任务。我还负责与其他部门进行协调，定期评估实习生的工作表现，并通过优化工作安排提升整体的效率。这个过程中，我学会了如何管理大型团队，并通过团队协作完成复杂的任务。”</p><p>**问题：**你在“军体拳视频人体行为识别系统”中负责了哪些算法研究与模型训练？<br><strong>回答：</strong> “在这个项目中，我参与了行为识别系统的算法设计与模型训练工作。主要负责的是基于STGCNs（时空图卷积网络）的模型搭建与本地训练，模型用于识别并分析军体拳训练中的动作。我还参与了数据处理和标注，编写了数据采集与处理方案，确保数据的准确性和模型训练的有效性。通过这个项目，我深入了解了深度学习在行为识别中的应用，特别是在时空图卷积网络的设计和优化方面。”</p><p>教师批改系统已有，自己的学生的作业，同时给自己的老师</p><p>【涉及到教师的工作分配、作业任务量的管理、】需要完成批改的作业处理系统:</p><p>在线处理——上传作业——教师批改——反馈交流</p><p>【作业池】————【红笔老师池】</p><blockquote><p>老师批改管理系统</p></blockquote><h3 id="3-技术技能相关问题">3. <strong>技术技能相关问题</strong></h3><p>**问题：**你熟悉SpringBoot、MyBatis等技术栈，能否详细讲解一下你在实际项目中的应用场景？<br><strong>回答：</strong> “在实际项目中，我使用SpringBoot作为后台框架，结合MyBatis实现了数据的持久化处理。比如在‘室内空气监测系统’项目中，SpringBoot处理了数据请求与设备控制的逻辑，而MyBatis则负责对MySQL数据库的访问与操作。我还设计了用户管理、权限分配等功能，通过SpringBoot的AOP机制增强了业务的安全性与扩展性。”</p><p>**问题：**你提到熟悉Java集合和常见的排序算法，请你讲解一下这些算法的复杂度以及如何选择合适的算法？<br><strong>回答：</strong> “我熟悉Java集合框架中的常用数据结构以及相应的排序算法。比如快速排序的平均时间复杂度为O(n log n)，适合处理大量无序数据；而堆排序也是O(n log n)，但由于堆的结构，它的空间复杂度较低，更适合在内存有限的情况下使用。在实际应用中，我会根据数据规模和应用场景来选择合适的排序算法。例如，在处理小规模数据时，插入排序的实现可能会更加高效，而对于大规模数据，快速排序或堆排序会更合适。”</p><h3 id="4-数据库与优化相关问题">4. <strong>数据库与优化相关问题</strong></h3><p>**问题：**你在数据库优化方面有什么实际经验？如何处理数据库性能问题？<br><strong>回答：</strong> “在项目中，我通过以下几种方式进行数据库优化：首先是通过合理设计索引来加快查询速度，尤其是在查询频繁的字段上建立合适的复合索引。其次是优化SQL语句，避免使用不必要的嵌套查询或过多的联表操作。在MySQL中，我还使用了缓存机制来减少数据库的压力，利用事务隔离级别来防止数据的脏读与幻读。通过这些优化措施，我有效提高了系统的响应速度和稳定性。”</p><p>**问题：**请解释MySQL中的事务隔离级别以及如何避免死锁？<br><strong>回答：</strong> “MySQL中的事务隔离级别分为四种：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。其中，‘可重复读’是MySQL的默认级别，它可以防止脏读和不可重复读。为了避免死锁，我会尽量减少事务的持有时间，保证在同一事务中按照固定顺序访问表，避免长时间占用资源。另外，我会通过监控和调整锁机制来降低死锁的风险。”</p><h3 id="5-测试相关问题">5. <strong>测试相关问题</strong></h3><p>**问题：**你如何进行系统测试？如何保证测试覆盖率和有效性？<br><strong>回答：</strong> “在项目开发过程中，我会根据功能模块进行单元测试、集成测试和系统测试。在单元测试方面，我使用JUnit和TestNG框架来验证每个模块的功能是否符合预期，确保关键功能点没有问题。为了保证测试覆盖率，我会通过覆盖率工具来统计代码覆盖率，确保核心功能模块的覆盖率达到80%以上。在系统测试阶段，我会进行性能测试、压力测试和兼容性测试，以确保系统在不同负载下的稳定性。”</p><p>**问题：**你在项目中如何进行单元测试、集成测试以及系统测试？<br><strong>回答：</strong> “我通常会先进行单元测试，确保每个模块的逻辑正确，测试工具主要使用JUnit。在集成测试中，关注各模块之间的接口调用和数据传递，确保模块协作正常。最后的系统测试则包括整体功能的验证，以及性能、负载和安全性方面的测试，特别是涉及到数据库和多线程的场景，我会使用模拟多用户并发的场景来确保系统的可靠性。”</p><h3 id="6-深度学习相关问题-2">6. <strong>深度学习相关问题</strong></h3><p>**问题：**请你详细讲解一下在“人体行为识别系统”项目中如何利用深度学习库进行模型训练和测试？<br><strong>回答：</strong> “在这个项目中，我使用了PyTorch库来搭建行为识别的深度学习模型。数据的预处理步骤包括对原始视频数据进行关键点提取，然后使用STGCNs模型进行时空特征的捕捉。在模型训练时，我设置了多种优化器和学习率调度策略，以提高模型的精度。对于模型的测试，我会使用交叉验证的方法，确保模型在不同数据集上的泛化能力，并且通过不断调整模型参数，提升了模型的识别准确率。”</p><p>**问题：**STGCNs模型的优点和挑战是什么？你在训练过程中遇到了哪些困难，如何解决的？<br><strong>回答：</strong> “STGCNs模型的优点在于它能够有效地捕捉时空特征，特别是在行为识别场景中，可以同时考虑时间维度和空间维度的依赖关系。挑战在于模型较为复杂，训练时间较长，而且需要大量的数据标注。在训练过程中，我遇到的主要困难是模型容易过拟合。为了解决这一问题，我通过正则化和数据增强技术（如随机旋转、缩放）来增加数据的多样性，进而提高模型的泛化能力。”</p><p>这些回答示范可以帮助你更好地应对面试中的问题，建议你提前熟悉内容，并根据实际项目经验稍作调整，确保面试时回答自信流畅。祝你面试成功！</p><h1>深入测试开发技术</h1><p>请你结合我的简历，请你从java测试开发工程师的角度，从开始面试到问答和追问环节，全程针对测试技术来展开，完成提问-回答-追问再回答的过程模拟，给出问题分析和详细回答吧~</p><h2 id="个人独立开发者项目">个人独立开发者项目</h2><p>可以先简单介绍一下你之前的项目经验和你在测试方面的贡献吗？</p><p><strong>你</strong>： “您好，感谢您的时间。我之前的项目经验主要集中在Java开发和测试上，例如‘室内空气环境数据监测与控制系统’的项目中，我负责了从后端开发到集成测试的全流程。在项目中，我使用SpringBoot和MyBatis进行开发，同时我负责系统的测试，确保功能模块的稳定性和性能表现。在测试方面，我主要使用JUnit进行单元测试，同时也使用了模拟工具进行集成测试和系统测试，保证系统各模块之间的正常交互和数据流通。”</p><h3 id="提问-1："><strong>提问 1：</strong></h3><p><strong>面试官</strong>：你提到了单元测试和集成测试，可以详细解释一下你在这两方面是如何进行的？特别是你是如何确保测试==覆盖率==的？</p><p><strong>你</strong>： “在==单元测试==方面，我主要使用JUnit框架，针对每个功能模块进行详细的测试。例如，在空气监测系统中，每个传感器数据采集模块都有相应的单元测试，确保在各种输入情况下都能返回预期结果。为保证覆盖率，我使用了代码覆盖率工具（例如JaCoCo）来分析哪些代码未被测试，确保关键的业务逻辑和边界情况都被覆盖。</p><p>在==集成测试==中，我主要关注模块之间的数据流和接口调用。例如，前后端数据传输是否正常、数据库访问是否按预期进行。在集成测试中，我使用了Mock工具模拟数据库或第三方API的行为，确保每个模块在独立运行和相互依赖时都能正常工作。”</p><h3 id="追问-1：-Mock">**追问 1：**Mock</h3><p><strong>面试官</strong>：在==集成测试==中，你提到了使用Mock工具。你能具体讲一下使用什么Mock工具，以及你如何决定什么时候该使用Mock，什么时候需要进行真实环境下的测试？</p><p><strong>你</strong>： “在项目中，我主要使用了Mockito作为Mock工具。Mock的使用场景主要是在测试时需要隔离外部依赖或者第三方服务的时候。例如，当我们测试空气监测系统的API层时，我会Mock掉数据库层，以确保不必连接真实数据库，同时又能测试API的业务逻辑和错误处理机制。</p><p>决定何时使用Mock取决于测试的目的。如果目的是测试某个模块的功能而不依赖其他模块（例如，测试某个API是否正确处理传入数据），那么Mock是合适的。如果需要测试系统在真实环境下的表现，比如性能测试或者与第三方系统的集成性测试，我会使用真实的数据库和API进行全面测试。”</p><h3 id="提问-2："><strong>==提问 2：==</strong></h3><p><strong>面试官</strong>：对于系统性能测试，你有过哪些实际经验？你是如何进行性能测试的，特别是在Java Web应用中如何进行负载测试？</p><p><strong>你</strong>： “在性能测试方面，我曾使用过JMeter进行Java Web应用的负载测试。具体来说，JMeter可以模拟多个用户并发访问系统，帮助我们评估系统的响应时间和吞吐量。在‘室内空气监测系统’项目中，我使用JMeter模拟了大量用户同时访问系统的数据展示功能，并记录了系统的响应时间。</p><p>为了提高性能，我通过分析瓶颈，优化了数据库查询和API调用。特别是在并发高峰期，我调整了数据库的索引结构，使用了连接池优化了数据库访问速度，从而提升了系统的响应能力。”</p><h3 id="追问-2：-sql-并发情况处理"><strong>追问 2：</strong>==sql  并发情况处理==</h3><p><strong>面试官</strong>：你刚才提到你优化了数据库查询和API调用，可以具体说一下是通过什么手段优化的？在面对慢查询时，你是如何进行排查和改进的？</p><p><strong>你</strong>： “首先，我通过分析查询日志和性能监控工具（如MySQL的慢查询日志）发现了性能瓶颈。在一些高频查询的情况下，我发现数据库没有合适的索引，导致查询速度变慢。为了解决这个问题，我在涉及大量查询的字段上创建了复合索引，同时优化了SQL语句，减少了不必要的联表操作。</p><p>另外，我还通过在应用层引入缓存机制，减少了频繁的数据库访问。通过引入Redis缓存，在短时间内存储热门数据，大大减轻了数据库的负载压力。此外，我还优化了API的调用逻辑，通过异步处理和批量操作减少了网络请求的次数和延迟。”</p><h3 id="提问-4：sql"><strong>提问 4：sql</strong></h3><p><strong>面试官</strong>：在简历中你提到你使用过MyBatis进行项目的开发工作。在与数据库交互的过程中，你是如何进行SQL注入测试的？能否介绍一下你的防护措施？</p><p><strong>你</strong>： “在使用MyBatis进行项目开发时，我非常关注SQL注入的风险。为了预防SQL注入，我严格遵循参数化查询的最佳实践，使用MyBatis中的<code>#&#123;&#125;占位符</code>来处理SQL语句中的用户输入，避免使用<code>$&#123;&#125;</code>这种直接拼接SQL的方式。此外，我还确保输入的数据进行了严格的校验和过滤，不允许直接执行包含用户输入的动态SQL。</p><p>在测试SQL注入时，我会设计一些恶意输入，比如插入<code>' OR 1=1 --</code>这样的测试数据，通过JUnit进行单元测试，确保应用能够正确处理并返回预期的错误响应。通过这种方式，我验证了系统的安全性。”</p><h3 id="追问-4：">==<strong>追问 4：</strong>==</h3><p>==<strong>面试官</strong>：除了防护SQL注入，你有没有使用过其他安全测试工具或者方法来确保系统的安全性？==</p><p>==<strong>你</strong>： “除了防护SQL注入，我还使用了OWASP ZAP这样的开源安全工具来进行自动化安全扫描，检测系统中是否存在常见的Web应用漏洞。通过ZAP，我能够模拟XSS攻击、CSRF攻击等常见威胁，并通过修复漏洞来提高系统的安全性。==</p><p>==此外，我还进行了手动渗透测试，特别是对认证和授权模块进行了重点测试，确保不同用户的权限边界清晰，避免未授权访问。在项目的开发过程中，我会定期进行这些安全测试，特别是在上线前进行全面的安全评估。”==</p><h3 id="提问-3：多线程测试"><strong>==提问 3：多线程测试==</strong></h3><p><strong>面试官</strong>：在单元测试中，如何处理一些比较难以测试的场景，例如多线程操作或异步调用？</p><p><strong>你</strong>： “在处理多线程操作和异步调用时，单元测试确实会更加复杂。对于多线程操作，我通常会确保线程安全性，例如在操作共享资源时，我会使用Java中的同步机制，或者使用线程安全的数据结构。在测试时，我会通过JUnit中的<code>CountDownLatch</code>等工具来确保多个线程都已经执行完毕，然后检查最终的结果是否符合预期。</p><p>对于异步调用，我会使用JUnit的<code>CompletableFuture</code>来处理异步结果的测试，确保测试程序不会因为异步调用的延迟而直接返回。在某些情况下，我还会使用Mockito的<code>doAnswer</code>方法来模拟异步行为并测试其结果。”</p><hr><h3 id="追问-3："><strong>追问 3：</strong></h3><p><strong>面试官</strong>：那在多线程测试中，你有没有遇到过死锁问题？你是如何排查和解决的？</p><p><strong>你</strong>： “在之前的项目中，我确实遇到过一次死锁问题。通过使用线程监控工具（如JVisualVM），我发现某些线程在竞争同一资源时陷入了死锁。解决的办法是分析代码中锁的顺序，确保所有线程按相同的顺序获取锁，避免循环等待。</p><p>另外，我重构了部分代码，减少了锁的粒度，避免不必要的长时间锁定资源。在测试时，我会使用较高的并发量模拟这种场景，确保经过优化后，死锁问题不再发生。”</p><h2 id="个人介绍">个人介绍</h2><p>“面试官您好，我叫马增龙，我求职的岗位是XXXXX。我将于2025年毕业于山东师范大学，目前正在攻读电子信息专业的硕士学位，方向是计算机技术。我一直对技术驱动的项目充满热情，并且在校期间有多次实习和相关的项目经验，这些经历使我积累了丰富的实践经验。</p><p>对于实习和项目经历，主要分为技术与管理、算法实习和个人软硬件系统项目三个方面：</p><p>在学而思实习期间，以实习生进入研发部门，然后调入到新项目中，参与并完成了红笔部门的构建与扩张，同时作为技术组长与研发部门对接，完成整个作业批改系统的上线与搭建。</p><p>第二段远程实习，我参与了军体拳行为识别与评价系统的实现，主要参与数据和模型训练的工作。</p><p>最后自己以独立开发者的身份完成了一个个人项目，室内空气环境数据监测与控制系统，同时包含软件端、硬件端和云端的系统实现。</p><p>在完成实习和项目学习的基础上，我的校内课程成绩也名列前茅。本科毕业时我GPA进入专业前10%，也获取到了保研名额，毕业时获取到优秀毕业生、优秀学生等奖项。读研期间，我扎根于自己的方向，热爱技术和科研，扩展自己的可能性。我热爱写博客，读研两年左右，我搭建个人博客写下将近39w字的个人技术博客，有很强的学习和动手能力；喜欢看书和运动，有良好的身体素质和抗压能力，热情开朗，善于与人沟通。</p><p>好，以上便是我的个人介绍，感谢各位的聆听和时间，敬请大家的批评指正。</p><h4 id="自我介绍需要注意的点："><strong>自我介绍需要注意的点：</strong></h4><ol><li><strong>突出核心技能</strong>：强调了你的Java开发和测试经验，提到具体工具（如JUnit、Mockito、JMeter、Docker、Jenkins等）来展示你的技术深度。</li><li><strong>结合项目经验</strong>：重点提到两个实习项目，具体描述你在其中的角色和贡献，展示你在实际项目中的能力。</li><li><strong>展示团队合作和管理能力</strong>：通过提到你在学而思的管理经验，展现了你的领导力和协调能力。</li><li><strong>引导面试官</strong>：你在自我介绍中提到了测试、性能优化、数据库管理等多个方向，面试官可能会根据这些内容提出进一步的问题，从而引导面试的重点话题。</li></ol><p>这份自我介绍既突出了你的技术背景，又能够引导面试官朝着你的优势领域展开提问，使你更容易展示自己的实力。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在我之前的实习经历中，我参与了多个关键项目的开发和测试工作。比如，在北京学而思的实习期间，我作为技术组长，负责后台系统的开发和测试，管理了团队的代码质量，制定了详细的测试策略，使用JUnit和Mockito等工具进行了单元测试和集成测试，确保了系统的稳定性和安全性。此外，我还参与了权限分配和数据库优化的工作，并通过性能测试工具如JMeter进行了压力测试，有效提升了系统的性能表现。</span><br><span class="line"></span><br><span class="line">同时，我还在‘军体拳视频人体行为识别系统’项目中负责了数据采集、算法研究和模型测试。通过对模型的测试和优化，我对深度学习在实际应用中的测试流程有了更深刻的理解。我还善于通过自动化工具管理测试环境，使用Docker和Jenkins简化了部署流程。</span><br><span class="line"></span><br><span class="line">我擅长快速学习和解决问题，尤其是在面对复杂系统时，能够迅速找到问题的根源并提出解决方案。在团队合作方面，我具有良好的沟通和组织能力，能够有效统筹团队工作，并推动项目顺利进行。</span><br><span class="line"></span><br><span class="line">总的来说，我相信我的开发和测试能力，加上对技术的深入理解和团队管理经验，能够为贵公司带来价值。我也很期待能在这次面试中展示更多我的技能和经验。”</span><br><span class="line"></span><br><span class="line">技术范围比较广泛：</span><br><span class="line"><span class="bullet">1.</span> 软硬件系统项目</span><br><span class="line"><span class="bullet">2.</span> 技术与管理项目</span><br><span class="line"><span class="bullet">3.</span> 算法项目</span><br></pre></td></tr></table></figure><h2 id="为什么转到测试上来"><strong>为什么转到测试上来</strong></h2><p>了解你为什么从事测试岗位而不是继续走Java开发的方向。你可以结合自己在开发过程中对<strong>软件质量</strong>的关注，以及<strong>测试对于整个开发流程的重要性</strong>来回答。以下是一个有针对性的回答模板：</p><p><strong>回答模板：</strong></p><p>“感谢您的问题。我在简历中的确参与了很多Java开发的项目，这些经历让我对软件开发有了深入的理解。然而，随着我在项目中越来越多地接触到软件测试相关的工作，我意识到<strong>测试对于整个开发流程的重要性</strong>。一个好的软件不仅需要出色的开发，还需要严密的测试来保证它的稳定性和可扩展性。</p><p>在实际项目中，我深刻体会到<strong>开发和测试是相辅相成的</strong>。在开发过程中，我会主动编写单元测试和集成测试，使用JUnit和Mockito等工具确保代码质量。在这些过程中，我逐渐发现自己对测试的严谨性和系统化的设计产生了浓厚的兴趣。特别是在做性能优化和安全性测试时，我意识到测试不仅仅是为了找出问题，更是为了<strong>提升系统的整体质量</strong>，这让我非常有成就感。</p><p>因此，我选择了软件测试这个方向，因为我希望能够更加专注于<strong>提升软件质量</strong>，帮助团队及早发现潜在问题，并通过测试保障产品的稳定性和用户体验。我相信我在开发中的经验将帮助我更好地理解系统架构，从而在测试中能更有针对性地设计测试用例，识别潜在问题。”</p><hr><h4 id="回答的核心要点："><strong>回答的核心要点：</strong></h4><ol><li><strong>从开发到测试的过渡</strong>：解释你从开发转向测试的原因，表明你对质量保障的重视。</li><li><strong>开发和测试的互补性</strong>：强调你在开发过程中已经积累了丰富的测试经验，并且认识到测试的重要性。</li><li><strong>个人兴趣</strong>：表明你对测试工作的兴趣和热情，尤其是你发现测试能够带来系统性的改进。</li><li><strong>结合开发经验的优势</strong>：指出你的开发背景能够帮助你更好地理解系统，从而设计出更有效的测试策略。</li></ol><p>通过这种方式，面试官会明白你选择测试岗位是基于你对质量和测试工作的认同，而不是因为开发方面的不足。</p><blockquote><p>加入保证用户质量的回答</p></blockquote><p><strong>回答模板：</strong></p><p>“感谢您的问题。我在简历中的确有很多Java开发相关的项目经验，这让我对软件的设计和实现有了深入的理解。然而，随着我逐渐参与更多的开发工作，我发现自己对软件质量、特别是<strong>用户体验</strong>的提升越来越关注。在实际项目中，我意识到软件不仅仅是要代码正确、性能优秀，还必须要确保它在最终用户手中能够无缝使用，这让我对<strong>测试岗位</strong>产生了浓厚的兴趣。</p><p>在开发过程中，我负责过用户管理和权限分配等功能模块的开发和测试，这些模块的准确性直接关系到用户的使用感受。在执行功能测试时，我不只是关注代码的正确性，还特别留意<strong>用户的使用路径和潜在痛点</strong>。我常常通过设计不同场景的测试用例，模拟用户的操作，确保无论是正常使用还是极端情况下，系统都能给用户提供良好的反馈和体验。</p><p>测试不仅是为了找出技术问题，更是为了从用户的角度出发，保证软件的<strong>稳定性</strong>和<strong>可用性</strong>。所以我选择了转向软件测试这个方向，因为我希望通过我的工作，帮助开发团队及早发现那些可能会影响用户体验的细节问题，并通过全面的测试来提升产品的用户满意度。结合我在开发中的背景，我可以更好地理解用户需求和系统逻辑，从而设计出<strong>更贴近用户真实场景</strong>的测试用例。”</p><hr><h4 id="回答的核心要点：-2"><strong>回答的核心要点：</strong></h4><ol><li><strong>用户体验的关注</strong>：强调测试岗位的重要性不仅在于技术问题的解决，还在于确保最终用户的使用体验。</li><li><strong>实际项目中的用户视角</strong>：通过具体项目展示你在测试过程中如何考虑用户的操作路径和感受。</li><li><strong>结合用户视角与开发背景的优势</strong>：指出你不仅能从开发角度进行测试设计，还能站在用户的立场来审视系统。</li><li><strong>提升用户满意度</strong>：强调你希望通过测试工作帮助提升用户的满意度，这也是你选择转向测试岗位的重要原因。</li></ol><p>通过这种方式，面试官能够看到你对软件测试有更全面的理解，尤其是在<strong>用户体验</strong>方面的重视。这种回答能够很好地展示你不仅关注技术实现，更希望从用户角度出发，提升产品的整体质量和满意度。</p><h2 id="关于测试开发的未来规划">关于测试开发的未来规划</h2><p>未来规划和工作方向时，你可以利用这个机会展示你的<strong>学习能力</strong>和<strong>对测试开发岗位的兴趣与信心</strong>。结合你过往经验和未来规划，展现你对持续学习和发展的态度。</p><h3 id="回答模板">回答模板</h3><p>“关于我的未来规划，尽管我之前的项目主要以Java开发为主，但正是这些项目经验让我认识到<strong>软件质量保障</strong>和<strong>用户体验</strong>的重要性，这也是我选择测试开发作为未来工作方向的原因。我希望未来能在测试开发领域深入发展，并将自己在开发中的优势带入测试工作中，帮助团队不仅实现功能，还能确保产品的稳定性和用户体验。</p><p>我非常清楚测试开发岗位需要不断学习新的技术和工具，特别是针对自动化测试、性能测试以及安全性测试等方面，我会通过自学和团队合作来逐步掌握这些技能。我也非常适应<strong>快速学习</strong>，比如在过去的项目中，我通过自学掌握了SpringBoot、MyBatis等框架，并能独立完成项目的开发和测试工作。</p><p>我的规划是，在未来的工作中，<strong>通过持续学习和项目实践</strong>，快速弥补我在测试领域的知识空白。我已经开始深入学习与测试开发相关的知识和工具，如Selenium、Jenkins、JMeter等，确保能够快速上手并独立负责测试任务。我相信我的开发背景能让我在理解系统架构和设计时具有一定的优势，这将帮助我设计出更加有效的测试用例。</p><p>在工作中，我希望通过不断优化测试流程，帮助团队提升软件质量，并推动开发和测试的紧密结合。我相信测试开发不仅仅是发现问题，更是为了预防问题，提升整个产品的用户满意度。”</p><hr><h3 id="回答的关键点："><strong>回答的关键点：</strong></h3><ol><li><strong>学习能力和适应性</strong>：展现你强大的学习能力，强调你在未来可以快速掌握与测试开发相关的知识。</li><li><strong>明确的工作方向</strong>：表明你对测试开发岗位的坚定选择，并结合你现有的开发经验，展示你对测试工作有长远的规划。</li><li><strong>开发背景的优势</strong>：突出你的Java开发经验如何帮助你更好地理解测试工作，尤其是系统架构的理解和测试用例的设计。</li><li><strong>未来的提升和贡献</strong>：表达你愿意在测试流程优化和质量提升方面贡献力量，明确你对岗位的长远价值。</li></ol><h2 id="个人反问">个人反问</h2><p>您对这个岗位的求职者或者胜任者在展开具体工作时，有什么具体的要求吗？主要涉及到业务上和技术上的~</p><p>您最希望在这个岗位上工作的人是一个什么样的人？具备什么样的能力或者特性</p><p>您觉得我的简历或者刚刚面试过充过程中最大的问题是什么，应该怎样去改进呢~</p><h2 id="软测准备链接">软测准备链接</h2><p><a href="https://www.yuque.com/tuobaaxiu/inyebp/ngxaas">https://www.yuque.com/tuobaaxiu/inyebp/ngxaas</a></p><p><a href="https://www.yuque.com/tuobaaxiu/inyebp/nugrfc">https://www.yuque.com/tuobaaxiu/inyebp/nugrfc</a></p><p>一面一分钟的自我介绍 一定要卡好时间喔</p><p>终面大家要去上海，但是会报销来回车费和住宿</p><p>二面是情景化面试朋友</p><p>薪资的话是没有绩效这一说的</p><blockquote><p>一分钟自我介绍准备</p></blockquote><p>1.2w的薪资构成是怎样的</p><p>你好，每进入一个流程前发邮件更新简历的</p><blockquote><p>销售简历 重置一下</p></blockquote><p>建议穿正装</p><p>一面三次机会呀，没开始一面的朋友好好准备一下吧</p><p>1.2w-1.5w/月，详情可以查看岗位JD朋友</p>]]></content>
    
    
    <summary type="html">个人简历侧开模拟问答</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>boss_and_job</title>
    <link href="https://www.fomal.cc/posts/c0bca80f.html"/>
    <id>https://www.fomal.cc/posts/c0bca80f.html</id>
    <published>2024-09-17T02:42:34.000Z</published>
    <updated>2024-09-24T10:21:43.104Z</updated>
    
    <content type="html"><![CDATA[<h1>面试准备</h1><p>第一场面试——与爱为舞的用户增长实习生面试</p><blockquote><p>公司个人了解： 主要招聘的技术岗位基本为社招，且需要相关的工作经验(大部分为算法、大模型岗位)</p><p>主要以实习生转正为基础，感觉直接秋招的岗位比较少</p></blockquote><p>AI +教育科技有限公司——23年成立</p><p>AIGC 岗位 实现AI 教师生态系统</p><p>简单来说，就是把在线教育的各个场景分别用AI进行替代，例如AI流量，AI销售，AI老师等</p><p>==用户增长实习生招聘要求==</p><p><img src="../assets/9dbc8a8f997e1f612a2a1686e74c4e7e.png" alt="A1069449F9A03E7E7757F8FED51B5D09.webp"></p><blockquote><p>数据分析、实验设计、投放策略</p><p>智能投放平台，协助业务同学，构建平台</p><p>内容策略完善，迭代内容产品</p><p>策略创新，获取新的客户需求点</p></blockquote><h2 id="公司了解">公司了解</h2><p>爱度 ai度 ——非常喜欢</p><p>兴趣 阶段 效率 潜力 ——为什么要加上阶段 二字</p><p>怎样盈利/怎样落地呢？——为每一个用户生成一个个人的AI教育产品？？</p><blockquote><p>AIGC ——新东方，学而思，考虫网，飞驰学院</p><p>王一快——秘塔科技 写作猫</p><p>Chatgpt——搭建平台 搭载一个服务器</p><p>AI数据算法智能平台</p></blockquote><p>那么多才能怕</p><h2 id="用户增长java-招聘：">用户增长java 招聘：</h2><p>1)业务介绍：通过小红书运营获取自然流，做IP打造，从而获取流量，对接哆啦平台</p><p>2)岗位：Java（用户增长）</p><p>3)要求：做过小红书平台流量增长业务优先</p><p>Java（用户增长）</p><p>5.负责在线教育公域获客场景底层系统开发，构建低成本大规模获客技术能力；</p><p>6.负责将公司底层AI能力系统化/工程化，赋能流量获客端业务；</p><p>7.负责公司流量端团队业务系统开发，提效工具开发；</p><p>8.负责系统架构设计/开发/优化工作，保障获客系统的高可用性/高稳定性/高并发支撑</p><p>数据工程实习生</p><p>数据库工程实习生</p><h1>9.12-9.20 号总结</h1><h1>模拟面试</h1><p>我现在在求职找工作，针对自己简历做技术面准备。你作为一个面试官，完成技术面试中的提问-解答-追问-解答过程模拟。阅读完我下边简历中的一个实习部分，对我的该部分进行所涉及到的技术上的提问，然后给出分析过程和解答。注意重点针对技术内容提问，并重点关注技术细节，因为有些技术我可能已经忘记了，所以需要你来模拟场景帮我回忆并重新学习，可以设计到技术的八股文，多问技术问题，并给出解答，然后继续追问该技术问题，最后再次给出分析和回答并用实例来概述工作。</p><p>你作为一个面试官，完成技术面试中的提问-解答-追问-解答过程模拟。阅读完我下边简历中的一个实习部分，对我的该部分进行所涉及到的技术上和业务上的提问，然后给出分析过程和解答。注意在提问时一定要针对我下边简历描述的内容来提问，包括对于实习时项目背景、每一条所列举的承担工作以及最后实习收获上做的业务和技术上进行提问，你在给出回答之后继续进行追问，多问技术问题，然后再次给出分析和回答并用实例来概述工作</p><p>请继续对于我上边的简历内容完成提问-解答-追问-解答的面试过程模拟，问题不要与上边重复，仍然重点关注简历内容的技术和业务上的重点进行展开</p><p>你作为一个面试官，帮我详细分析一下下边我简历里边可能会被问到的一些问题，包括技术上和业务内容上的问题。 下边是我的简历详细内容</p><p>你作为一个资深的java测试开发工程师，来担任面试官，完成技术面试中的提问-解答-追问-解答过程模拟。我会把我的简历发给你，你对我的简历进行所涉及到的技术上和业务上的提问，然后给出分析过程和解答。注意在提问时一定要针对我下边简历描述的内容来提问，包括对于实习时项目背景、每一条所列举的承担工作以及最后实习收获上做的业务和技术上进行提问，你在给出回答之后继续进行追问，多问技术问题，然后再次给出分析和回答并用实例来概述工作</p><p>你作为一个资深的java测试开发工程师，来担任面试官，我会把我的简历发给你，你来帮我从面试官的角度分析一下对我的简历在测试开发面试中，会从哪些地方来提问？我应该如何作答呢？</p><h3 id="测试开发">测试开发</h3><ol><li>你作为一个资深的java测试开发工程师，来担任面试官，我会把我的简历发给你，你来帮我从面试官的角度分析一下对我的简历在测试开发面试中，会从哪些地方来提问？我应该如何作答呢？</li><li>这是我的个人简历，我下午有一个测试开发工程师面试，你来帮我从面试官的角度分析一下对我的简历在测试开发面试中，会从哪些地方来提问？我应该如何作答？我该怎样去快速准备来应对该面试？</li><li>针对你上边给出的这些提问，请你从java测试开发工程师的角度，结合我给你的我个人的简历，你帮我作答一下吧~</li><li>请你结合我的简历，请你从java测试开发工程师的角度，从开始面试到问答和追问环节，全程针对测试技术来展开，完成提问-回答-追问再回答的过程模拟，给出问题分析和详细回答吧~</li><li>请继续对于我上边的简历内容完成提问-解答-追问-解答的面试过程模拟，问题不要与上边重复，仍然重点关注简历的项目和实习经验的重点展开，主要针对测试开发技术</li><li>请继续对于我上边的简历内容完成提问-解答-追问-解答的面试过程模拟，问题不要与上边重复，这次重点针对测试开发技术，对我第一段实习经历中的业务和涉及到测试技术来展开问答</li></ol><blockquote><p>快速准备</p></blockquote><p>你现在是资深的测试开发工程师，作为面试官，你来看一下：这是我的个人测试开发的简历，但我现在基本没有学习过测试开发的知识点，也没有系统学习过测试开发的内容，如果我想要快速的用3天时间突击准备测试开发工程师的面试，我应该怎样准备呢？</p><p>请结合我的个人简历项目，从面试八股题、项目怎样加入侧开的技术引导面试官、如何快速准备等多个角度来帮我指导一下吧</p><p>请继续对于我上边的简历内容完成提问-解答-追问-解答的面试过程模拟，问题不要与上边重复，仍然重点关注简历的项目和实习经验的重点展开，主要针对测试开发技术</p><p>面试策略</p><p>熟识背诵mysql + 计算机网络的八股</p><blockquote><p>所有的问题都往数据层、mybatis层、网络层去讲，这样在那边详细解决</p></blockquote><h1>面试复盘&amp;&amp;准备</h1><h1>笔试复盘&amp;&amp;准备</h1><p>ACM输入输出VS 在LeetCode上的核心代码模式</p><h1>简历栏目</h1><blockquote><p>张文豪简历用的啥做的</p></blockquote><blockquote><p>秋招八股文: 常学习常更新，将新学的技术/背的八股文放到建立上去</p></blockquote><blockquote><p>版弄的⼲净简洁大方，</p></blockquote><blockquote><p>之前看过⼀个同学的简历，很丰富、优秀的经历，写了两⻚，但排版有点乱，⾏距有点窄，挤在⼀起，会给我<br>⼀种没有条理的感觉。</p></blockquote><blockquote><p>3.简历和岗位匹配程度(能扯测试就扯上测试)<br>我和测开的关系⽐较少，<strong>简历上能写的就是专业技能</strong>，写⼀下⽬前掌握的测试知识和⼯具等等。有时间的跟着<br>B站做个项⽬，可以添加到项⽬经验⾥。<br>也可以在你参加的团<strong>队⽐赛或者项⽬中，个⼈职责写上开发和测试，虽然我都没做过测试，主要是想增加简历</strong><br><strong>和岗位的匹配度</strong>，⾯试有可能会问你在<strong>这个项⽬中做过什么测试，你可以按照⽬前所学知识展开说，⽐如⽬前</strong><br><strong>只学到功能测试，就结合项⽬说⼀下从哪些⻆度设计了测试⽤例，如果你学了⾃动化测试⼯具和性能测试⼯</strong><br><strong>具，也可以先想想怎么对⾃⼰的项⽬做测试。</strong></p></blockquote><p><img src="../assets/image-20240922093025445.png" alt="image-20240922093025445"></p><blockquote><p>关于项目经历，可以按照项目技术栈、项目简介/描述、完成模块/自己职责/负责模块/核心技术点三个部门去写。 技术栈：主要介绍项目使用到的技术栈有哪些，挑重点核心的技术写即可，占据一行。 项目简介/描述：主要介绍项目的背景和相关的功能模块，语句一定要精简！ 完成模块/自己职责/负责模块/核心技术点：比如使用xx解决了xx问题，可以分点列几点比较核心的，语句一定要精简！ 注意⚠️：对于开发环境可写可不写。</p><p>作者：代码界的小白<br>链接：<a href="https://www.nowcoder.com/feed/main/detail/0b9e69155feb414597e4c5df678f2c9b?sourceSSR=search">https://www.nowcoder.com/feed/main/detail/0b9e69155feb414597e4c5df678f2c9b?sourceSSR=search</a><br>来源：牛客网</p></blockquote><h4 id="技术简历">技术简历</h4><blockquote><p>java软开、</p><p>软件测试开发、</p></blockquote><h4 id="非技术简历">非技术简历</h4><p>技术支持/运维/售前售后</p>]]></content>
    
    
    <summary type="html">个人成长&amp;&amp;公司深入了解</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>网站重建，梦想开启</title>
    <link href="https://www.fomal.cc/posts/c318edab.html"/>
    <id>https://www.fomal.cc/posts/c318edab.html</id>
    <published>2024-09-06T14:10:38.000Z</published>
    <updated>2024-09-07T14:14:37.406Z</updated>
    
    <content type="html"><![CDATA[<h1>网站重建，梦想开启</h1><h2 id="博客重启的巴拉巴拉（必备工具）">博客重启的巴拉巴拉（必备工具）</h2><p><a href="https://blog.cuijiacai.com/blog-building/">https://blog.cuijiacai.com/blog-building/</a></p><p><a href="https://www.bilibili.com/video/BV1qD4y1z783/?vd_source=3d5976f4f96ce120bf88891647d386cb">超详细的个人博客搭建教程（无需服务器）- 从原理到实践手把手带你搭建属于自己的个人博客_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb</a></p><p><a href="https://www.fomal.cc/posts/e593433d.html">🐖抓到你啦～ (fomal.cc)</a></p><h1>重启操作教程</h1><p>熟能生巧</p><p>github 用git访问+chatgpt的回答帮助</p><ol><li>nodejs + nmp  安装+版本匹配<ol><li>删除nmp</li><li>更改下载源</li></ol></li><li>hexo 下载 单独安装配置up项目</li><li></li><li></li></ol><p><img src="../assets/image-20240906221418850.png" alt="image-20240906221418850"></p><h1>心路历程</h1><blockquote><p>2024年5月3日 gitee的page服务 被关闭，不在对外访问，导致自己的网站无法访问，也不能再继续写笔记和完成自己的博客l</p></blockquote><p>在2024年9.6日 重新为了封装简历，想着哪怕拿出一天的时间，全部花进去，用来把自己的博客后台迁移，从不能访问的gitee 迁移到github</p><p>花了一天的时间，从一开始7:00左右的信心满满，动力十足，到快到10：20左右，【在这期间成功的把之前的GitHub不能用git访问的问题解决了这个问题困扰了好久】然后跟我家美女一起改简历，搬到小教室里边，用自己的手机开热点，直到11：40左右，成功的能够跟之前一样，用hexo d 和hexo -g 完成数据往远端传输，搜着网页上说问题是网络不好</p><p>而且在用hexo d 传输的时候，蹦出一个网页，用来验证github的账号登录情况，这时候就知道能够完成数据传输了</p><p>当时是开着热点在小教室里，就没去吃饭，一直开着热点，给手机充电，期望能够成功，幸好最后成功传输上去了！！！</p><h4 id="但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs-和hexo-版本问题，导致hexo下载主题失败">但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs 和hexo 版本问题，导致hexo下载主题失败</h4><p>npm  与 node 匹配</p><p>下载使用的node.js 是12.19.0版本</p><h6 id="nodejs和npm版本不匹配：ERROR-npm-is-known-not-to-run-on-Node-js-v10-16-2">nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2</h6><p><a href="https://blog.csdn.net/qq_41992943/article/details/124925758">nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_45056216/article/details/101163126">关于node js和 npm 版本不匹配的情况_linux中npm版本个nodejs版本不匹配解决方法-CSDN博客</a></p><p>删除node 重装</p><p>node 12.19.0 重装</p><p>更改 npd 源头</p><h6 id="npm报错：request-to-https-registry-npm-taobao-org-failed-reason-certificate-has-expired">npm报错：request to <a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a> failed, reason certificate has expired</h6><p><a href="https://blog.csdn.net/maoge_666/article/details/136038003">npm报错：request to https://registry.npm.taobao.org failed, reason certificate has expired-CSDN博客</a></p><p>hexo  重装——直接在新文件夹中安装</p><h6 id="win10系统使用npm安装hexo失败">win10系统使用npm安装hexo失败!</h6><p><a href="https://github.com/hexojs/hexo/issues/2741">win10系统使用npm安装hexo失败! · Issue #2741 · hexojs/hexo (github.com)</a></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="selector-tag">i</span> -<span class="selector-tag">g</span> hexo-cli    </span><br><span class="line">sudo npm install gulp-cli -<span class="selector-tag">g</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://github.com/hexojs/hexo-cli/issues/23">hexo-cli 安装后失败 ·问题 #23 ·hexojs/hexo-cli 命令 (github.com)</a></p><p>git上传文件和文件夹</p><h6 id="Github如何上传项目-超详细小白教程">Github如何上传项目(超详细小白教程)</h6><p><a href="https://blog.csdn.net/KevinRay_0854/article/details/140408003">Github如何上传项目(超详细小白教程)_github上传项目-CSDN博客</a></p><p>[git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤_github推送文件夹视图工具-CSDN博客](<a href="https://blog.csdn.net/qq_40413898/article/details/104287509#:~:text=git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%EF%BC%9A%E5%B0%86%E6%8C%87%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B9%E5%86%85%E5%AE%B9%E6%88%96%E4%BB%A3%E7%A0%81%E6%96%87%E4%BB%B6%E6%8E%A8%E9%80%81%E5%88%B0GitHub%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4">https://blog.csdn.net/qq_40413898/article/details/104287509#:~:text=git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤</a> 1 1、在GitHub中新建一个仓库 在创建仓库是，一些配置说明如下： Repository name%3A 仓库名称 Description,. … 5 5、最终结果 将本地仓库与远程仓库连接 再次要推送文件时，需要与远程仓库进行交互，使用git remote指令。 )</p><h4 id="验证登录框">验证登录框</h4><blockquote><p>为了方便下次再碰到这种问题，记录一下蹦出的网页情况</p></blockquote><p>网页蹦出来是个小的登录界面，可以直接打开链接，然后验证与github账号的关联，同时确定能够跟这个电脑设备关联上。</p><p>打开链接，是一个github页面，然后输入设备登录码！！！很重要的成功标志</p><blockquote><p>幸好chatgpt帮助 解决 git 访问github</p></blockquote><blockquote><ol><li>配置对应的github库，需要设置好远程库与用户名一致，<a href="http://xn--github-9o7i562adkda6123feszd.io">然后后边加上github.io</a></li><li>先将up的模板放到github库中，详见配置教程</li><li>再将自己的test文件上传到对应的目录中，在进行hexo d 上传</li></ol></blockquote><h1>秋招文件收集</h1><p>数据库项目</p><p><a href="https://articles.zsxq.com/id_gsuoxn7qnvip.html">轮子实战项目：手写数据库 MYDB (zsxq.com)</a></p><p><a href="https://blog.csdn.net/qq_40856284/article/details/121580249">一起写个数据库 —— 0. 项目结构和一些不得不说的话-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_40856284/category_11504274.html">一起写个数据库_何人听我楚狂声的博客-CSDN博客</a></p><p>技术派</p><p><a href="https://paicoding.com/article/detail/472">如何将技术派写到简历上？-技术派 (paicoding.com)</a></p><p>PmHub</p><p><a href="https://www.yuque.com/canghe-u0ocv/laigeoffer-pmhub/mdoff4twogexi4ri">✅人人都是产品经理，打造竞争优势（🌟新人必看） (yuque.com)</a>   cgd9</p><p>八股学习</p><p><a href="https://javabetter.cn/sidebar/sanfene/nixi.html">面渣逆袭必看，面试题八股文Java基础、Java 集合框架、Java 并发编程、JVM、Spring、Redis、MyBatis、MySQL、操作系统、计算机网络、RocketMQ、分布式、微服务、设计模式、Linux👍 | 二哥的Java进阶之路 (javabetter.cn)</a></p><p>面试题整理</p><p><a href="https://top.interviewguide.cn/">InterviewGuide大厂面试真题</a></p><h2 id="面试经验">面试经验</h2><blockquote><p>刷面经</p><p><a href="https://www.yuque.com/snailclimb/mf2z3k/guh0u9hb3pr70rtk">大厂四年，2024 阿里、字节、蚂蚁、小红书面试经历分享 (yuque.com)</a>      《面试指北》cnk4</p></blockquote><p><a href="https://interviewguide.cn/notes/05-xiustar/02-campus_prepare/04-01-%E4%BA%92%E8%81%94%E7%BD%91%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html">⭐阿秀三个多月、50余场面试经验浓缩为经验和总结 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>简历修改</p><p>优质简历样例——<a href="https://articles.zsxq.com/id_5p3fcdmwpw4h.html">精选简历：基本信息、教育背景、专业技能、工作经验、项目经历、实习经历、荣誉证书写的比较好的例子 (zsxq.com)</a></p><p><a href="https://wx.zsxq.com/dweb2/index/topic_detail/182882241124882">知识星球 | 深度连接铁杆粉丝，运营高品质社群，知识变现的工具 (zsxq.com)</a></p><p>如何写好简历——<a href="https://www.yuque.com/itwanger/gykdzg/grvrtx?singleDoc=#e9486e82">如何写好简历？（完结） (yuque.com)</a>——王二</p><p>毒液   <a href="https://erdengk.top/archives/jian-li--mian-shi">简历、面试 | 我的知识海洋 (erdengk.top)</a></p><h2 id="知识星球常用">知识星球常用</h2><p><a href="https://wx.zsxq.com/dweb2/index/favorites">收藏</a></p><p>秀哥资源——<a href="https://articles.zsxq.com/id_dmf00wjw9dtm.html">🏆资源沉淀 (zsxq.com)</a></p><p>秀哥问答——<a href="https://articles.zsxq.com/id_wm06pp9dnogj.html">📖知识图谱（星球问答文章精华） (zsxq.com)</a></p><p><a href="https://interviewguide.cn/notes/05-xiustar/03-resume/01-00-%E7%AE%80%E5%8E%86%E5%BC%80%E7%AF%87%E8%AF%8D.html">简历远远比你想要的重要的多 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>公司列表  <a href="https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u">https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u</a> 面试指南 gqya</p><p>优质主题——<a href="https://articles.zsxq.com/id_v6xfrmw5637h.html">JavaGuide 知识星球优质主题汇总（2024-04-01） (zsxq.com)</a></p><h2 id="AI">AI</h2><p><a href="https://www.coze.cn/">扣子 - AI 智能体开发平台 (coze.cn)</a></p><h1>准备</h1><blockquote><p>测试八股文/测开八股文</p></blockquote><p>算法题目</p><p>数据库基础知识+ sql题目</p><p>项目梳理</p><p>java八股文</p>]]></content>
    
    
    <summary type="html">网站迁移，重新开始博客之路</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>社科人文认知突破</title>
    <link href="https://www.fomal.cc/posts/cea7f2ca.html"/>
    <id>https://www.fomal.cc/posts/cea7f2ca.html</id>
    <published>2024-09-06T13:11:52.770Z</published>
    <updated>2024-09-17T06:10:07.016Z</updated>
    
    <content type="html"><![CDATA[<h2 id="社科人文认知突破">社科人文认知突破</h2><h2 id="社会科学入门">社会科学入门</h2><p>是这么回事，学习社会科学，学习文科，其实用处没你想得那么大。<br><strong>你如果想靠这个挣钱呢，那还是很难的。</strong></p><h2 id="学文科的好处">学文科的好处</h2><p><strong>但是呢，学这玩意用处很大的，一个是陶冶自身的情操。另一个能避免被政府欺骗。</strong></p><p>学文科的话，其实最重要的是多读书，多积累，然后能学会万事万物的联系。</p><p>这个万事万物的联系，说的比较笼统，就是<strong>你读一本书，能想到别的书之间和这本书的联系。</strong></p><p>比如你读唐代的历史书，能联系到北周的柱国，再联系到皇帝的更迭， 那恭喜你，你已经学会一点了。</p><p><strong>学文科的话，最高的水平是能做预测分析，其次是做事后分析。</strong></p><p>做预测分析这玩意属于有点是占卜，没有固定的套路，但是做事后分析，寻找历史的真相，其实如果多积累，多思考，总是能找到路径的。<strong>但是呢，这个时间可能需要很多年的积累。</strong><br>学文科另一个方面是<strong>有利于你炒股</strong>，如果你将来会看工信部的数据，懂得如何分析走向，不敢说带你走向暴富，但是穷应该是可以避免。</p><h2 id="文科的坏处">文科的坏处</h2><p>上面说了那么多学文科的好处，我也要说说学文科的坏处，一方面是要<strong>花费非常多的时间，</strong><br>另一方面是<strong>没有很明显的进步提示</strong>，他不像学数学，学了一本就有一本的知识，而是你要在庞大的积累后，<br>才能实现质的飞跃。</p><p>除此之外，另一个问题是<strong>容易走火入魔</strong>。关于走火入魔，比较明显的例子就是各种魔怔念经人，佐佐右右托派。</p><p>因为你读一些书，你就是和作者在产生交流和对话，作者会讲他的思考模式，然后把你说服，<br><strong>如果你能力不行，就很容易被他说服。</strong>      比较明显的例子就是哈耶克 和佛利德曼的书。</p><h2 id="怎么学习社科人文">怎么学习社科人文</h2><p>好了，我要正视开始讲学文科的事情了。<br>首先呢，<strong>你需要积累一定的历史，</strong>（方便在了解一系列的东西之后，能够分析出其发生的背景以及对应的因果关系，方便结合时势分析）（其实也可以不积累，但是不积累的话，在看一些东西的时候，不能结合时代背景去认知）。<br>第二是<strong>根据自己的爱好，去决定学什么。</strong><br>文科里面，我个人首推的是<strong>逻辑学导论</strong>。这本书会教你如何去正确的思考，会让你避免一些基础的逻辑推导错误。<br>第二 我是推荐看<strong>经济历史</strong>。经济历史对于人的收获是，你能理解说一些事，是如何演化过来的。<br>比如说我们曾经是社会主义，为啥会90年代破产， 为啥会cr ，为啥到现在又重新国进民退。<br>第三，我推荐是看<strong>历史类的书</strong>，历史类的书，其实我个人推荐是看法国人还有英国人写的书，<br>这两个国家的特点就是有丰富的国外交流经验，是认真的理解国外的，英国的博物学家是一群很不可思议的人，<br>19世纪的英国探险者能穿越喜马拉雅山跑到拉萨和当地的活佛交流，20世纪的劳伦斯一个人就能推翻伊拉克帝国，<br>瓦解德国在阿拉伯地区的努力，而法国在非洲的殖民，甚至一度让美国人都感到绝望。</p><p>​</p><h3 id="个人体验share">个人体验share</h3><p><strong>哲学类的书我不推荐看，因为这个很耗时间，容易学不到东西。</strong><br>关于道德类的讨论，还有国学这一块，旧文化毫无疑问是死掉了，现在什么复兴国学啊，<br>无非是对着一具僵尸祈祷。如果你们想学这个呢，可以<strong>大概扫扫孔子孟子荀子加王阳明的传习录，</strong><br>**不过心学讲的更多的是人的解放。**扫一扫就行，学深了没啥意义，这玩意本质上是一种道德教化，<br>但是实际上来说，真有用的还是看人能不能敢于斗争。。。。<br>学文科没啥固定的套路，<strong>有用就用，没用扔掉，别变成那只腐臭文人，指挥掉书袋。</strong><br>今天就暂时写那么多。欢迎大家一起互相讨论。</p><blockquote><p>天道有感</p></blockquote><ol><li>传统文化 强势文化 弱势文化 ==》演变成应该以政治文化为主，以道德和传统文化作为人文性情掌控的加锁</li><li>逐渐去学习现在很多事情的运行规律，多去尝试体会理解事物发展的本质规律，只有这样借势，慢慢去利用规律，才可能会帮助自己更好的认识本来的发展方向，更好的借力打力</li><li></li></ol><h3 id="政治cr是什么意思"><strong>政治cr是什么意思</strong></h3><p><a href="https://zh.wikipedia.org/wiki/CR">CR - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>极度濒危<br>极危<br>严重濒危<br>极度濒危物种</p><p><strong>政治cr是一种政治符号语言，CR是Community Relief（社区救济）的缩写，指政治上中左派团体对给弱势群体提供的各种救济措施的简称。这个术语是在网络上出现的，源自于社交媒体上用户对政治、文化、社会问题的讨论。这个术语主要出现在中国网络空间中，因为中国社交媒体的特殊环境，这个术语主要指代的是一种执政党在为人民办事的同时，也是在为自己、为党派、为官僚机构、为强势集团利益服务的政治现实。</strong><br><strong>政治CR的主要表现形式包括但不限于各种策划、活动、宣传、演讲、集会、网络言论、社区服务等。其中，社区服务在中左派政治圈子中被认为是最典型、最重要的CR方式。中左派倡导政治CR，不仅是为了弱势群体的合法权益，而且也是为了塑造中左派的良好形象和生存空间。当今中国的政治环境不容乐观，中左派圈子在应对复杂政治环境和外部压力时需要将社区服务和救济作为其政治理念和社区组织中的核心内容，以此来表明自己的政治态度和提高自己的公信力。</strong><br><strong>政治CR还有一层意义，它是在对传统CR概念的一个批判和拓展。传统CR是指在弱势群体受到自然灾害等不可抗因素影响时，社会团体或个人通过捐款、物资等方式向遭受不幸的群体提供救济的行为。这种救济行为受到了许多批评，人们认为它追求的仅仅是短期效应，而没有深入研究问题的根源。而政治CR则更注重于问题的根源研究，在提供救助的同时，也更注重于倡导相关的政治观点，从而更具有持续性和影响力。</strong><br><strong>总而言之，政治CR作为一种政治行动形式，在中国等一些国家有着广泛的意义。它是中左派的一种社会实践方式，在提供弱势群体救济的同时，也是在呼吁社会公正和推动社会进步。</strong></p><p>个人人生复盘</p><blockquote><p>个人角色</p><p>个人规划</p></blockquote><h1>润宇笔记</h1><p>拆解笔记，解读拆分   带货小时榜  咋没有知识付费或者教育类的呀  润宇老师<br>这个小时榜就代表了限时限刻的流行风<br>我今天也要调整我的带货顺序了[偷笑]<br>拆解他到底在买什么？  把每个直播间最撑着他直播销售的量的商品【100w中 有80w的商品】<br>10个商品 找出货盘，找出排名的前几个，哪些是主推的【人 货 场 】短视频的量？  投流来的量 还是转发来的</p><p>套路话术  看懂有什么过程  会逼单<br>如何卖？话术、时间循环、塑造价值、当下购买的理由<br>对哪些人   说那些话  卖什么货  产生了价值</p><p>先去看哪些有用  再去看他的逻辑【从消费者行为 去想为什么去买】<br>1、货盘、热卖的产品 2、引流品、信任品、利润品 3、流量是怎么来的：场观怎么来的<br>【ai 视频好 中华文化 疗愈  情绪价值 小红书【平台相关】  健康 】</p><p>普通人 ai 不是周鸿祎 则大可不必焦虑</p><blockquote><p>ai只是辅助创作的手段，从来不觉得ai是风口</p></blockquote><p>对于刚刚起步的商业体   找机会的话，一定冲着一个目标去：离现金流一定要近</p><blockquote><p>离现金流近，就一定要离交易近【交易场景明确】</p></blockquote><p>一定是需求和供给已经贼明确或者自己可以发现的时候: 人 货 场 形成干柴烈火的状态，一定最近且一定会发生</p><p>卖 ai 课的人，对ai感到焦虑的需求，视频直播平台提供课程 卖ai课是三者清晰的</p><blockquote><p>技术开发者  消费者  应用者</p></blockquote><p>AI写作，产生内容呢?</p><p>李一舟的课程一环套一环的感觉</p><p>写视频文案</p><blockquote><blockquote><p>技术开发者，需要有先机，抢开发机会</p></blockquote><p>技术抢先机，会形成商业壁垒</p></blockquote><p>哈哈，我现在都用ai来接文案商单了</p><blockquote><p>ai消费者</p></blockquote><p>新能源汽车</p><p>技术的应用者</p><blockquote><p>比亚迪 dmi的电池技术  宁德时代的玻璃 和 电池</p></blockquote><p>好不好用，好不好看，是否有高性价比</p><blockquote><p>伺候好消费者 是所有消费者的义务</p></blockquote><p>消费者好用便宜，不关注技术</p><blockquote><p>最终的技术最后都会不断降低壁垒，是不用学的</p><p>最后都会推动到消费者</p></blockquote><p>好的东西是不用学的</p><blockquote><p>但是你提前学会了，就有信息差，也能够提高你的生产效率。</p></blockquote><p>应用者 光有信息差，只能卖课，也需要产生出其他的价值帮助别人</p><p>大部分想要赚钱的 或者是利用好产品赚钱的，是应用者</p><blockquote><p>光写出文案 ，没有流量。光有数字人替代直播，但是卖不出去货</p><p>光有生成的视频，但是不能有浏览量，也没什么用</p></blockquote><blockquote><p>商业的底层逻辑</p><p>光帮助了生成量产，但是不能扩宽边界，仍然</p></blockquote><p>极其理解用户的交易场景，制造了一个想像空间，让用户感受到他也能，他也行的想法<br>价格由供求关系来决定 不是你的数量来决定[东西稀缺才值钱]</p><blockquote><p>热辣滚烫</p><p>用了一个很土的方法，100斤不稀缺，但是他在电影里真有的一个</p></blockquote><p>土是2024最大的稀缺<br>数字人没有情感与直播间的人不能产生共鸣。<br>手工写信，是因为你愿意花时间精力来干这样的事情，说明你很重要，被尊重，被重视，被需要<br>缺了真情实感<br>技术需要了解用户，靠应用+了解用户的需求才能产生money</p><p>核心竞争力，自己的优势到底是什么<br>应用者 利用信息差快速上位<br>了解客户，而不仅仅是了解技术；少去炫技，多去用感情+满足需求来挣钱<br>技术是应用者 利用信息差快速上位</p><p>应用者中赚到钱的一定是最理解用户的人，手里有客户 ，ai只是一个工具，所面对的用户仍然没有变过<br>铁打的用户，流水的技术</p><h3 id="孩子教育-是否要学ai">孩子教育 是否要学ai</h3><p>为什么要学扑克？ 象棋 围棋  人还要跑步？而不是打车呢</p><p>会让使用和学习的门槛回归0</p><p>提升审美和能力</p><p>从音乐 ，商业 到编程 都会一点的东西</p><p>培养孩子的内心力量，他们真的需求是什么</p><p>孩子的创造力和想象力很珍贵，AI影响不了</p><p>训练孩子的思维</p><p>情绪力 和创造力 ，人天然是一个大模型，</p><p>人要跟生活 跟很多元认知来产生很多碰撞</p><p>锻炼思考能力、逻辑能力、创造力</p><p>==遇到人生巨大困惑，需要使用创造力来进行利用解决，知识自己一只忽略了他的价值==</p><h1>中国基层的基础框架</h1><blockquote><p>政治经济学 组织经济学</p></blockquote><p>北上广深苏浙杭 VS 乡村 （90%人口）</p><p><img src="..%5Cassets%5Cimage-20240222101953924.png" alt="image-20240222101953924"></p><p>知之深才能爱之切</p><p><img src="..%5Cassets%5Cimage-20240222103121120.png" alt="image-20240222103121120"></p><blockquote><p>理解基层才能理解中国</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222103214019.png" alt="image-20240222103214019"></p><p>循序渐进，逐渐深入的关系</p><blockquote><p>主要研究政企关系，营商环境和制度经济学</p><p>三农问题和基层治理 并不强擅</p></blockquote><h3 id="怎么学？">怎么学？</h3><p><img src="..%5Cassets%5Cimage-20240222103441214.png" alt="image-20240222103441214"></p><blockquote><p>底层框架+冷冰冰的手术刀</p></blockquote><ol><li><p>构建思维框架，增强因果推断；</p></li><li><p>完成思维的培养和锻炼，在沟通中成长，在交流中进步</p></li></ol><p>==如切如磋如琢如磨==</p><h1>第一讲——理解中国的治理架构</h1><blockquote><p>治理架构也就是根本制度</p><p>掌握</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222103729013.png" alt="image-20240222103729013"></p><blockquote><p>元规则 支撑中国文明运行几千年的基本制度</p></blockquote><p>为什么要从制度或者底层治理框架上开始？</p><blockquote><p>只有先从制度角度理解制度背景，才能从宏观角度理解中国的基层治理，才能高屋建瓴的看透中国基层治理的现状、问题 和对策</p></blockquote><p>从制度角度看问题才能看的更透，更深 更远</p><p><img src="..%5Cassets%5Cimage-20240222104937019.png" alt="image-20240222104937019"></p><p>不是就事论事 形成头痛医头 脚痛医脚的毛病</p><p>主要研究三个问题：</p><ol><li>为什么要演研究中国的治理架构？</li><li>什么是中国治理的基本架构</li><li>如何在中央—地方关系下，理解基层与上层的互动<ol><li>中央和地方之间到底是兄弟关系还是父子关系呢？</li></ol></li></ol><p><strong>主要目的是形成理解基层治理的底层框架和系统思维</strong></p><p><img src="..%5Cassets%5Cimage-20240222105109455.png" alt="image-20240222105109455"></p><blockquote><p>万变不离其宗，掌握底层框架才算是掌握了知识的金钥匙，也就是武林秘籍中的 独孤九剑</p></blockquote><blockquote><p>条块结合，以块为主的中央集权体制</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222105215896.png" alt="image-20240222105215896"></p><blockquote><p>纪委为什么成为强势单位？ 县委书记为什么不好当?</p><p>怎么就形成了上边千条线，下边一根针的基层治理困境</p></blockquote><blockquote><p>大A  制度经济学</p></blockquote><p><img src="..%5Cassets%5Cimage-20240222110039821.png" alt="image-20240222110039821"></p><p><img src="..%5Cassets%5Cimage-20240222110129262.png" alt="image-20240222110129262"></p><blockquote><p>制度经济学 包括 政治制度 经济制度 法律 文化 以及政府政策</p></blockquote><h2 id="需求">需求</h2><ol><li>实时，需要有<strong>较快</strong>的推理速度（高优先级）</li><li>便捷，需要该模型便于部署，特别是部署到Unity平台</li><li>精确，至少要让用户觉得数字人的口型可以和实际口型对应</li><li>风格化，无需针对某个特定的音色/角色微调模型，即<strong>one model for all user</strong></li><li>口型、avatar面部表情、动作同步；其中口型、表情都是用blendershape驱动，动作暂时是循环播放固定动作</li></ol><h2 id="实验计划">实验计划</h2><p>优先参考现有的代码：</p><p><a href="https://github.com/yunik1004/SAiD.git">https://github.com/yunik1004/SAiD.git</a></p><p><a href="https://github.com/huailiang/LipSync">https://github.com/huailiang/LipSync</a></p><p><a href="https://github.com/zhongshaoyy/Audio2Face">https://github.com/zhongshaoyy/Audio2Face</a></p><p><a href="https://github.com/leventt/surat.git">https://github.com/leventt/surat.git</a></p><p><a href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p><p><a href="https://github.com/Rtyper/LipSync-Pro">https://github.com/Rtyper/LipSync-Pro</a></p><p><a href="https://github.com/guanjz20/StyleSync">https://github.com/guanjz20/StyleSync</a></p><h2 id="后续需要补的知识">后续需要补的知识</h2><p>@李润一</p><p>Unity开发、部署相关知识Sentis的使用：<a href="https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4">https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4</a></p><h2 id="Audio2Face-expression更新">Audio2Face+expression更新</h2><p>可以参考下列工作：</p><p><a href="https://github.com/FACEGOOD/FACEGOOD-Audio2Face/">https://github.com/FACEGOOD/FACEGOOD-Audio2Face/</a></p><p>更新：FaceGood无法在linux平台上测试，只能在windows上运行</p><p>（其余工作未使用blendershape的输出，可在下https://github.com/FACEGOOD/FACEGOOD-Audio2Face/列链接中查看：）</p><p><a href="https://xie.infoq.cn/article/2f414deaa57ca29783d6f873f">语音驱动嘴型与面部动画生成的现状和趋势_算法_行者AI_InfoQ写作社区</a></p><p><a href="https://www.gameres.com/895985.html">语音生成口型与表情技术的演进与未来 - GameRes游资网</a></p><p><a href="https://github.com/nowickam/facial-animation/tree/production">nowickam/facial-animation: Audio-driven facial animation generator with BiLSTM used for transcribing the speech and web interface displaying the avatar and the animation (github.com)</a></p><p>[<a href="https://www.jianshu.com/p/fbc5743152f0">Paper Reading] Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion - 简书 (jianshu.com)</a>（这篇工作就是nvidia开发的omnivetrse audio2face的基础）</p><h1>个人选择</h1><p>相对竞争优势 &amp;&amp; 个人的兴趣爱好</p><blockquote><p>根据对应的时代特征，来进行选择</p><p>有一个选择4象限</p></blockquote><p><img src="..%5Cassets%5Cimage-20240301094224506.png" alt="image-20240301094224506"></p><p><strong>政府部门进行招商引资</strong></p><p>PPT 造车  <a href="https://zhuanlan.zhihu.com/p/642907927">PPT造车鼻祖倒下？烧光174亿，九年也没憋出一辆车 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT%e9%80%a0%e8%bd%a6&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1">https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT造车&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1</a></p><h2 id="工具ChatGPT">工具ChatGPT</h2><p>ChatGPT4的归档功能</p><p><a href="https://www.landiannews.com/archives/101487.html">OPENAI宣布ChatGPT现在可以归档会话喽 不删除会话的同时保持简洁 - 蓝点网 (landiannews.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;社科人文认知突破&quot;&gt;社科人文认知突破&lt;/h2&gt;
&lt;h2 id=&quot;社会科学入门&quot;&gt;社会科学入门&lt;/h2&gt;
&lt;p&gt;是这么回事，学习社会科学，学习文科，其实用处没你想得那么大。&lt;br&gt;
&lt;strong&gt;你如果想靠这个挣钱呢，那还是很难的。&lt;/strong&gt;&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.fomal.cc/posts/4a17b156.html"/>
    <id>https://www.fomal.cc/posts/4a17b156.html</id>
    <published>2024-09-06T13:11:52.768Z</published>
    <updated>2024-09-06T05:01:06.893Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>思考的原点1.1</title>
    <link href="https://www.fomal.cc/posts/6c9e281a.html"/>
    <id>https://www.fomal.cc/posts/6c9e281a.html</id>
    <published>2024-03-19T11:59:12.000Z</published>
    <updated>2024-09-06T01:04:24.695Z</updated>
    
    <content type="html"><![CDATA[<p><img src="../assets/image-20240319200003990.png" alt="image-20240319200003990"></p><p><img src="../assets/image-20240319200138262.png" alt="image-20240319200138262"></p><p>具体影响:</p><p><img src="../assets/image-20240319200442568.png" alt="image-20240319200442568"></p><blockquote><p>民国时期的张爱玲 胡适  不超过10%</p><p>国民党的留学博士  20%  高智商的人 但是不一定明白</p></blockquote><p><img src="../assets/image-20240319200959489.png" alt="image-20240319200959489"></p><p>思考框架背后</p><p><img src="../assets/image-20240319222056735.png" alt="image-20240319222056735"></p><blockquote><p>哈耶克 说英美 两种经验主义？？</p><p><strong>14-15分钟左右 没听懂</strong></p></blockquote><p><img src="../assets/image-20240319223401990.png" alt="image-20240319223401990"></p><p><img src="../assets/image-20240319223752011.png" alt="image-20240319223752011"></p><p>人类积累了上万年的常识智慧 我们需要借鉴吗？</p><p>哈耶克认为 英国自由的保守主义 包括解决一战 和法国大革命 是以经验主义演变来的，马老师定义为经验的共识？</p><p><img src="../assets/image-20240319223950464.png" alt="image-20240319223950464"></p><blockquote><p>不言而喻 的东西 + 假设 形成自然科学</p></blockquote><p>社会科学 基于很多公理 在进行理性推理+假设</p><blockquote><p>先聊基本常识有哪些【多年积淀】</p><p>在聊到良知【希特勒 ，斯大林的大清洗 法国资产阶级大革命】</p></blockquote><p><img src="../assets/image-20240319225127785.png" alt="image-20240319225127785"></p><blockquote><p>人是目的还是工具？</p><p>与人相处的时候什么是平等和公平</p><p>个人权利和公众权力</p><p>平等和自由的时候哪个更重要 本质是左派和右派</p><p>权力和道德的关系 良知和正义的关系</p></blockquote><p>读写是最好的自我投资</p><p>经济持续放缓（5年之内）悲观的2%的商铺不会盈利，疫情导致大家消费欲望变低</p><blockquote><p>最难的反而是28-40 的中层危机，持续危机</p></blockquote><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240409221345478.png" alt="image-20240409221345478"></p><blockquote><p>读书是</p></blockquote><p>35岁 经验为导向的职业，努力不在撬动发展，需要持续做对选择来撬动个人发展</p><blockquote><p>生存问题变成发展问题</p></blockquote><p>π型人才，</p><blockquote><p>所有的变化和需求都需要持续的阅读，来保证个人的读写能力</p></blockquote><p>每个人都最容易触及的，都可以获得的，收益最大的都是读写能力，</p><p>读写只是自己的一种阅读输入输出的方式</p><blockquote><p>读写 在信息时代把数据吃进去再用更好的方式把数据吐出来，就永远都不会饿死</p></blockquote><p><img src="C:%5CUsers%5CLenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20240409221646881.png" alt="image-20240409221646881"></p><blockquote><p>消费型的阅读或者时间投入</p></blockquote><blockquote><p>仅仅是方便体验和知道了一些知道的</p></blockquote><p>投资型的阅读和输入</p><blockquote><p>咖啡豆的示例</p></blockquote><p><strong>自己的投资型和消费型的比例是怎样的呢？是否需要控制呢（包括视频和读书方面，甚至是任何各种输入自己眼中、头脑中的数据和信息）</strong></p><p>10分的话，消费占多少？投资占多少？</p><p><strong>思想变得浅薄</strong></p><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240409224042781.png" alt="image-20240409224042781"></p><p>底层思维的书  怎样应用呢？【】</p><blockquote><p>复用率 ？ 有短期收益或者长期收益？？？</p></blockquote><blockquote><p>遇到任何书的时候，都要去重新去思考，对症下药去分析</p></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240409224331895.png" alt="image-20240409224331895"></p><p>当你把自己价值能完整的输出的时候，就是一件非常有价值的东西</p><ol><li>你比别人想得深</li><li>你能够宣传，有品牌</li></ol><p>拍卖效应 + 手电思维？灯塔效应</p><blockquote><p>持续放光</p></blockquote><blockquote><p>德国人的灵魂？？丁元英  灵魂<br>开启一个一个的小镇</p></blockquote><p>穷人/普通人最好的投资、最好的职业规划、最好的学习方法 、是最便宜最舒适让内心能够更宁静的一种旅游小镇的方式</p><p><img src="..%5Cassets%5Cimage-20240409224953036.png" alt="image-20240409224953036"></p><h2 id="写卡感触">写卡感触</h2><p>我最有感悟的古典⽼师的⼀句话是：古典老师关于阅读读书时候的考量因素（如何选书或者评价书是否适合自己去阅读）+对于书复用率的不同分类（经典的底层逻辑的书  实现某种功能的书  畅销书 ）+古典老师如何论证阅读的效益的递进方式（先背景介绍-&gt;环境要求-&gt;个人发展需要(投资数据支持、职业规划)-&gt;心灵上读书是必要条件）+读写是一个持续的东西，很多公众号断更/没坚持到最后，只有持续的读+写，形成闭环才能够好的坚持走下去【读写是最好的个人投资，而不仅仅是只读/只写（写一篇没用，读一篇也没用）】</p><p>阅读不是要做的唯一一件事情，写作一样，并不是孤立的，而应该两者形成整体的一个系统，作为输入和输出，同时去构建他，维护他们，并作为长期主义，持续的做下去。</p><p>我过去踩的坑是：没坚持下来阅读+写卡（也有各种原因，也一直没有开始去做）;阅读读书没有将该原则应用，还是老套路选书；自己是第二次看这个视频，之前也看过这个视频好几遍，甚至都写过关于这个视频的卡（第一次参与共读会+选书开始写卡的时候，都是看的这个视频），但自己只是知道并没有做到，这两者中间真的差了一个巨大的鸿沟。</p><p>我以后可以更好的⽅式是：</p><ol><li><p>尽量减少完美主义的影响，有时候一想起要把事情做得很好会碰到一些苦难，所以就会害怕或者懒得去开启行动；尽可能快的开始，言出努力去行，用行动来进行反馈，尽量只要收获一点点东西就enough。</p></li><li><p>昨天听了师兄师姐的会议分享，感触颇多，要多跟大家去同频共振，减少自己对于现实世界中困难的想象，联机学习，一起进步</p></li></ol><h1>4.10</h1><ol><li>整理时间安排</li></ol><h2 id="读书读不懂？记不住？用不上？">读书读不懂？记不住？用不上？</h2><p>读不懂</p><ol><li>读深入 读透作为自己的读书指标</li><li>读书 缺少体验，<ol><li>听话vs阅读理解？ 人的大脑临时组件是不能对阅读理解来实现天生组件完成该功能的，所以需要将听觉、视觉、大脑打猎的部分临时组合来实现阅读理解</li><li>缺少人生经历和体验（任何的抽象概念都无法不通过具体的体验去理解）——没有见过圆的人永远想象不到圆有多完整（没见过趋近于直线的人无法想象到无限延长的直线）</li></ol></li></ol><p>记不住</p><p>存难取易（偷钱挨打这件事 一辈子记住（一辈子记住自己不要丢三落四））</p><p>大脑 easy come easy go 5</p><p>一天听4个小时 什么都记不住</p><blockquote><p>给别人讲一遍</p></blockquote><p>最好的记录员？？（跟他们做朋友）</p><p>利于记忆，但不利于提取</p><p>缺少必要的知识管理工具</p><blockquote><p>缺少实践【】</p><p>缺少系统【老板上课】</p></blockquote><h3 id="个人脑子里记录的东西">个人脑子里记录的东西</h3><blockquote><p>思维导图—— 笔记整理软件——写卡片</p></blockquote><blockquote><p>白天听课，晚上写</p><p>？？难度</p></blockquote><p>整体的系统框架？？</p><p>读写镜像，关于知识树的构建和拆分是利用读写来完成的</p><p>读的话 就是输入，将个人读的信息从一维提取主题后形成二维大纲，然后放入到大脑的知识网中</p><p>写的话，是输出，将大脑中的网状知识结构，先提取出一个主题形成大纲，然后大纲每一个主题往外写，形成一维输出</p><blockquote><p>既是一讲写作课，也是一讲阅读课</p></blockquote><p>写卡营第二讲体悟分享</p><p>【内容来源】写卡营第二讲课程古典老师分享（下）</p><p>【本卡主题】关于建立体系以及写卡的四个重要知识点</p><p>【原文摘录】<br>SS卡的实现体系是从①选书开始 ② 关注理解体验 ③深刻记忆内容④付出行动实践（各种训练营 打卡营 读书营） ⑤坚持30天不断积累卡片 ⑥形成卡片体系 ⑦卡片成文营训练 ⑧文章/作品 大纲⑨创造价值，不断发展<br>SS卡的四个重要知识点：①存难取易②最小单元（卡片） ③乐高式写作（搭建） ④读写镜像（读写同构）</p><p>【概念转述】<br>————————————————————<br>选书——选的是底层的，比较基础的且复用率比较高的书<br>理解——转述出来之后，用自己的话说一遍辅助加深理解<br>体验——记录那一瞬间的感触/体验/思考，并不断积累汇总<br>行动——行动与体验并行，才能帮助更好的融会贯通<br>其他的点是在上边的基础上不断的践行+积累，慢慢创造价值</p><p>————————————————————<br>存难取易——既能帮助深入理解，方便快速提取体验卡，也能实现系统搭建<br>最小单元——借用卡片的方式，每天复习，学习+联系<br>乐高式写作——反对模板化的方式，以阅读输入为起点，存下每块乐高，最终搭建出体系<br>读写镜像——读写不分离，读书—写作应该是同步进行，并且是一个相互可逆的过程<br>————————————————————<br>【个人体验/思考】</p><ol><li>形成整个读写体系应该是战略性的设定，然后再按照整个流程详细的展开，按照古典老师讲述的过程，不断积累，一点点去主动阅读并写下体验，深刻体会后并用行动去加深理解，想起了之前阅读《七个习惯》好像是一样的思路</li></ol><p>2.在自己一个月从158变成142的过程中，深刻体会到自己实践——体验——继续付出努力增强信心的整个闭环过程，现在想来应该也是得益于系统的搭建</p><p>3.读写同构这一点，让自己印象深刻；在读书的时候，是从1维度获取零散的知识，然后在笔记中进行罗列整理，形成2维度大纲，最后在脑中形成3维度网状知识体系；<br>书写的时候，是利用脑中的3维度网络知识图，将信息提取后，在纸面上形成2维度大纲，最后按照每个要点详细展开细节也即1维度的内容加工。</p><p>【行动指南】<br>写卡先，首先认识到，到知道，再到做到，感谢老师给了一个可行性比较强的抓手——写卡</p><p><a href="https://doc.weixin.qq.com/doc/w3_AdgAaQawAMceNWSNgcxRSWAS0TYZ0?scode=AFgAiwfiAGI7oggms0AdgAaQawAMc">软件打包 (qq.com)</a></p><p><a href="https://www.zhihu.com/column/c_1245860717607686144">Nuitka-Python打包exe - 知乎 (zhihu.com)</a></p><p><a href="https://github.com/erdengk">erdengk (尔等同学) · GitHub</a></p><p><a href="https://book.douban.com/subject/35050614/">我的二本学生 (豆瓣) (douban.com)</a></p><p><a href="https://avuucupcq6.feishu.cn/docx/EWYYdkYIeoL4XIxw97xcLqQnnRc">一册通.《锻炼》共读营 - Feishu Docs</a></p><p>请结合下边给出的5个Skeleton-based Action Recognition 介绍的内容，生成一个新的关于Skeleton-based Action Recognition的详细介绍，内容尽量新颖独特一点，用词合理且通顺，大概在500单词左右<br>1 Skeleton-based Action Recognition<br>With the huge advances of deep learning, recurrent neural net-<br>work (RNN)-based, convolutional neural network (CNN)-based,<br>graph convolutional network (GCN)-based, and transformer-based<br>methods are studied for skeleton-based action recognition. RNNs<br>have been widely used to model temporal dependencies and cap-<br>ture the motion features for skeleton-based action recognition. The<br>work in [9] uses RNN to tackle the skeleton as sequence data. Sub-<br>sequently, Song et al. [37, 38] proposed to utilize the attention<br>mechanism and multi-modal information to enhance the feature<br>representations. Some other works [15, 24] transform each skeleton<br>sequence into image-like representations and apply the CNN model<br>to extract spatial-temporal information. Recently, GCN-based meth-<br>ods have attracted more attention due to the natural topology struc-<br>ture of the human body. Many works [6, 35, 47] apply GCN to<br>the spatial and temporal dimension [47] and achieves remarkable<br>results in the supervised skeleton-based action recognition. Mean-<br>while, transformer models [29, 36] also show promising results,<br>owing to long-range temporal dependency learning by attention.</p><p>2 Skeleton-based action recognition<br>Generally, there are two ways to obtain skeleton data from<br>videos. Firstly, the position information of human joints is ob-<br>tained from RGB video frames by pose estimation methods, such<br>as Openpose [26]. Secondly, the 3D skeleton data of the human<br>body can be obtained directly through the depth sensor, e.g., Mi-<br>crosoft Kinect v.2. In this paper, the datasets we use includes two<br>forms, RGB videos and skeleton sequences. These two forms of<br>data are shown in Fig. 1.<br>Most early research on skeleton-based action recognition usu-<br>ally used handcrafted features to represent the human body.<br>Hussein et al. [10] used the covariance matrix for joint posi-<br>tions over time. In [11], rotations and translations were used to<br>model 3D geometric relationships between body parts. However,<br>these methods were complex in design processes and ignored the<br>correlations between specific human body parts.<br>With the further development of deep learning, CNN-based<br>and RNN-based methods were proposed to process the grid<br>data reconstructed from human skeleton data. CNN-based meth-<br>ods [27–29] convert the skeleton data into pseudo-images, then<br>attempt to learn action features. In [30], a skeleton-based trans-<br>former module was designed for action classification and recog-<br>nition. Kim et al. [31] proposed a 3D human action recognition<br>model named Temporal Convolutional Neural Networks. In [32],<br>an enhanced skeleton visualization method was presented to<br>deal with view variations and noisy data. Liu et al. [33] first<br>applied 3D convolution for the skeleton-based recognition task to<br>simultaneously capture motion correlations of spatial–temporal<br>dimensions. Huynh et al. [34] proposed a new encoding method,<br>Pose-Transition Feature to Image (PoT2I), which transforms high-<br>level features of skeleton data into color pixels. In recent works,<br>the skeleton-based data was formulated as sequences of grid-<br>shaped structures and then modeled with RNN-based methods [8,<br>35,36]. Shahroudy et al. [37] proposed an RNN-based structure<br>to model the long-term temporal correlation of motion features.<br>In [38], RNN was used to analyze the hidden sources of motion<br>relationships in both spatial and temporal dimensions. In [39],<br>an end-to-end spatial–temporal attention model was built with<br>Long Short-Term Memory (LSTM) to focus on discriminative<br>joints. In [40], a novel contrastive action learning paradigm called<br>AS-CAL with a momentum LSTM module was proposed to unsu-<br>pervised action recognition. However, the skeleton-based data are<br>embedded not in the form of vector sequences or 2D grids but<br>in the graph structure, so both RNNs and CNNs based methods<br>cannot represent the structure of skeleton-based data well.<br>GCN-based methods utilize graph convolution modules to ex-<br>tract motion dependencies from the graph structure of natural<br>human skeleton data, which has been demonstrated to be the<br>most expressive. Yan et al. [4] innovatively proposed the first<br>model with GCN, namely ST-GCN, to capture the balance between<br>spatial and temporal dependencies. In this model, the skeleton<br>is expanded into a graph structure, and the spatial–temporal<br>patterns are automatically extracted from skeleton-based graph<br>data. Subsequently, many studies [41–43] were proposed based<br>on ST-GCN. In [35], 2s-AGCN was presented with adaptive ad-<br>jacency matrices to capture the motion information from high-<br>order skeleton data. Liu et al. [44] proposed MS-G3D to conduct<br>multi-scale long-range modeling of spatial–temporal graphs. Sim-<br>ilarly, AS-GCN [21] inferred A-links from input data to capture<br>multi-scale actional relationships. Zhang et al. [45] investigated<br>high-level semantics information from skeleton-based data and<br>designed two modules to exploit the high-order correlation of<br>skeleton data.</p><p>3 Skeleton-based models: To find a more effective rep-<br>resentation of the dynamics of human actions, Johansson [24]<br>utilizes 3D skeleton sequences for action recognition, making<br>an obvious decrease of computational cost as well as a good<br>performance boost. Recently, with the rapid development of<br>deep learning techniques, skeleton-based action recognition<br>methods have attracted increasing attentions. Researchers have<br>proposed various models to improve the performance of action<br>recognition, which can be divided into three major categories.<br>The first category builds the models with convolutional net-<br>works. For example, Li et al. [16] propose a CNN-based<br>co-occurrence feature learning framework, which gradually<br>aggregates various levels of contextual information. Kim et<br>al. [15] build a temporal convolutional network to explicitly<br>learn readily interpretable spatio-temporal representations for<br>3D human action recognition.<br>Besides, for the second category, researchers concatenate all<br>joints in one frame into a single vector, then use sequential<br>models such as long short-term memory (LSTM) to explore<br>the temporal dynamics. Du et al. [25] design a hierarchical<br>bidirectional RNN to capture rich dependencies between dif-<br>ferent human body parts. The study in [9] employs a view<br>adaptive LSTM, which enables the network itself adaptive to<br>the most suitable observation viewpoints. Additionally, Song<br>et al. [26] firstly introduce attention modules into skeleton-<br>based action recognition.<br>Both CNN-based and RNN-based methods are still limited<br>to extract the spatial structure information among skeleton<br>joints, where the joints of different body parts are connected<br>as a skeleton graph. Instead, in the third category, graph-<br>based methods can be naturally utilized to deal with the<br>skeleton graph, which successfully captures the most infor-<br>mative features for various actions. Si et al. [13] use GNN to<br>model the relationships among five body parts. Yan et al. [10]<br>initially introduce GCN into skeleton-based action recognition,<br>and produce a baseline named ST-GCN for future research.<br>Based on the ST-GCN, many studies achieve continuous<br>improvements on skeleton-based action recognition [27], [28],<br>[29].<br>4Skeleton-based action recognition<br>The progress of deep learning [30–32] and the advancement of<br>human pose estimation algorithms have facilitated human action<br>research. Basically, skeleton-based action recognition deals with<br>temporal series, where the dynamics of human body poses over<br>time characterize human actions. Early methods generally used a<br>manual feature approach to capture human action [33–35] . How-<br>ever, this relies mainly on 3D rotations and translations between<br>joint points, so feature design is more complex and has average<br>performance.<br>Recently, deep learning methods have achieved favorable out-<br>comes in skeleton-based action recognition, which can be basi-<br>cally classified according to the network architecture: Convolu-<br>tional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),<br>and Graph Convolutional Networks (GCNs). CNN-based methods<br>convert human skeleton sequences into pseudo-images, which are<br>trained and then classified. Structurally, CNNs are constructed by<br>stacking convolutional layers, pooling layers, and activation func-<br>tions, etc. The CNN achieves network optimization by adjusting the<br>number of convolutional and pooling layers, the size of convolu-<br>tional kernel, and the step size of convolution. Li et al. [36] pro-<br>posed an end-to-end convolutional co-occurrence feature learn-<br>ing framework. The co-occurrence features are learned using a hi-<br>erarchical method with different levels of contextual information<br>gradually aggregated. Kim and Reiter [37] proposed a new model<br>called Temporal Convolutional Neural Network (TCN) for 3D hu-<br>man action recognition, it used one-dimensional residual CNNs and<br>based on directly connected joint coordinates to identify skeleton<br>sequences. Liu et al. [38] proposed an enhanced skeleton visual-<br>ization method, which visualizes the skeleton as a series of color<br>images and then implicitly encodes the spatio-temporal informa-<br>tion of the skeleton joints. Since CNN can only process regular grid<br>data in non-Euclidean space, it is not as good as other networks in<br>skeleton-based action recognition.<br>RNN-based approaches typically model skeleton sequences as<br>a series of coordinate vectors along spatial and temporal dimen-<br>sions, where the vectors represent the body’s joint points. Liu et al.<br>[39] proposed a tree-structure-based traversal method that uses<br>an RNN-based approach to model the spatio-temporal domain and<br>introduces a new gating mechanism in LSTM to learn the relia-<br>bility of sequential input data. Liu et al. [40] designed a Global<br>Context-Aware Attention L STM (GCA-L STM), which is able to se-<br>lectively focus on information joints in action sequences with the<br>assistance of global contextual information. Zhang et al. [41] de-<br>signed an attentional mechanism, it embeds a recurrent attentional<br>network that can explore the spatiotemporal relationships between<br>different local regions to focus on important regions. Zhu et al.<br>[42] proposed an end-to-end fully connected deep LSTM network<br>that takes the skeleton as input at each time point and introduces<br>a new regularization method to learn the co-occurrence features of<br>skeleton joints.<br>However, both CNNs and RNNs have difficulty in capturing the<br>natural graph structure of skeleton topological features. To better<br>capture human action features [16,43] , recent work has utilized<br>GCNs [44,45] for spatial and temporal modeling of actions</p><p>5 Skeleton-Based Action Recognition<br>Skeleton-based action recognition is a fundamental yet<br>challenging field in computer vision research. Previous<br>skeleton-based motion recognition methods are usually re-<br>alized with the geometric relationship of skeleton joints [7,<br>36, 37]. The latest methods pay more attention to deep net-<br>works. Du et al. [6] applied a hierarchical RNN to process<br>body keypoints. Attention-based methods are proposed to<br>automatically select important skeleton joints [28–30, 47]<br>and video frames [29, 30] to learn more adaptively about<br>the simultaneous appearance of skeleton joints. However,<br>recurrent neural networks often suffer from gradient van-<br>ishing [11], which may cause optimization problems. Re-<br>cently, graph convolution networks attract more attention<br>for skeleton-based action recognition. To extract both the<br>spatial and temporal structural features from skeleton data,<br>Yan et al. [40] proposed spatial-temporal graph convolution<br>networks.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;../assets/image-20240319200003990.png&quot; alt=&quot;image-20240319200003990&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/image-20240319200138262.png&quot; a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>今日一心</title>
    <link href="https://www.fomal.cc/posts/3ccff96b.html"/>
    <id>https://www.fomal.cc/posts/3ccff96b.html</id>
    <published>2024-03-13T14:09:23.000Z</published>
    <updated>2024-09-06T01:04:24.692Z</updated>
    
    <content type="html"><![CDATA[<h1>聂辉华番外课程1该不该走选调生</h1><h2 id="细节展开1">细节展开1.</h2><p><img src="../assets/image-20240315191357916.png" alt="image-20240315191357916"></p><p><img src="../assets/image-20240315191542319.png" alt="image-20240315191542319"></p><p><img src="../assets/image-20240315191701845.png" alt="image-20240315191701845"></p><p><img src="../assets/image-20240315191804417.png" alt="image-20240315191804417"></p><p><img src="../assets/image-20240315191829363.png" alt="image-20240315191829363"></p><p>区别</p><p><img src="../assets/image-20240315191931043.png" alt="image-20240315191931043"></p><p><img src="../assets/image-20240315192049444.png" alt="image-20240315192049444"></p><p><img src="../assets/image-20240315192057670.png" alt="image-20240315192057670"></p><p>选调生 vs 统考生</p><p><img src="../assets/image-20240315192337006.png" alt="image-20240315192337006"></p><p><img src="../assets/image-20240315192406696.png" alt="image-20240315192406696"></p><h2 id="选调生、国考、省考">选调生、国考、省考</h2><blockquote><p>资料汇总</p><p><a href="http://www.sdgwy.org/html/kszc/202403/2_81743.html">2025年山东公务员考试报名信息「提前收藏」 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/">山东公务员考试最新消息 -2024年山东公务员考试网上报名时间、考试大纲、历年真题 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/html/zkgg/xds/index.html">招考公告/选调生 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><img src="../assets/image-20240324125043891.png" alt="image-20240324125043891"></p><p>时政学习</p><p><a href="https://www.chinagwy.org/html/ggjczs/mszt/202401/52_615293.html">公务员考试备考必看：2024年时事政治合集「持续更新」 - 国家公务员考试最新消息 (chinagwy.org)</a></p></blockquote><h3 id="选调生">选调生</h3><p>23年毕业生有一个青岛的选调  但是24年毕业生没有，只面向了东南大学</p><p><strong>25年未知</strong></p><p><a href="http://www.sdgwy.org/html/gdzk/qd/202211/67_73792.html">2023年青岛市“青选计划”选调公告（山东师范大学） - 山东公务员考试最新消息 (sdgwy.org)</a></p><p>都是只能报考一个单位或者岗位。</p><blockquote><p>需要选调生的历年数据</p><p><a href="https://zhuanlan.zhihu.com/p/645424990">山东定向&amp;常规选调最全报录比数据（2023年最新） - 知乎 (zhihu.com)</a></p></blockquote><blockquote><p>详细政策和选择分析</p><p><a href="https://zhuanlan.zhihu.com/p/601525199">2023山东定向选调第一批——山东专额选调全面解读（附岗位指导） - 知乎 (zhihu.com)</a></p><p>专额选调 是指的第一批选调</p></blockquote><blockquote><p><a href="https://ah.huatu.com/zt/sdxdszwb/">2024年山东招录选调生考试职位表下载_岗位查询-山东公务员考试网 (huatu.com)</a> x</p><p>选调岗位  【下载下来的是22年的】</p><p>在济南或者说山东 是普通选调的县直 或者是乡镇</p></blockquote><h3 id="证监会-VS-金管局">证监会 VS 金管局</h3><p><a href="https://mp.weixin.qq.com/s/_XW4D0jRiy-g0Q6ldQWGVA">获取国考证监会历年真题资料方式 (qq.com)</a></p><h1>载脑体 爱情第一讲</h1><blockquote><p>系统性，本源性 去剖析男女关系的本质</p><p>从可实践的角度分析如何遇到一个对的人</p></blockquote><h2 id="摆正观念">摆正观念</h2><p>对于大多数人而言，人生中最重要的决定是跟什么样的人相伴一生</p><p><img src="../assets/image-20240320155836104.png" alt="image-20240320155836104"></p><p>其次才是选择行业，再差的行业都会给与自己一个安身立命之所，职业的周期性会贯穿个人的前半生，但在未来的AI时代，自己可能无法保证未来行业的连续性，且很多时候决定行业的趋势的往往是风口和周期率，而非自身所能左右</p><blockquote><p>选对一个好的伴侣不仅仅影响自己的金钱、时间，也会影响自己人生漫漫长路的质量，也是自己为数不多的能够抵抗世界周期率和波动的抉择之一</p></blockquote><p>好男儿不在乎儿女情长，但对于大多数人，尤其是男生，除了谋生赚钱的知识以及专业技能基础课之外，优先掌握如下知识：</p><p><img src="../assets/image-20240320160615680.png" alt="image-20240320160615680"></p><blockquote><p>每一条都事关个人一生真正的幸福</p></blockquote><p>其他事情可以感兴趣，但不能对于以上事情无知的情况下，去琢磨其他事情</p><h3 id="摆正对于婚姻的理解">摆正对于婚姻的理解</h3><blockquote><p>职场雇佣关系，朋友关系，商业上的价值互换关系</p></blockquote><p><img src="../assets/image-20240320160800430.png" alt="image-20240320160800430"></p><p>婚姻的本质 是找一个战场上的一个战友关系，是一个相互爱慕的战友</p><p>首先是同目标，共进退。不抛弃，不放弃；</p><p>其次才是对方的技能有多强</p><p>==慕强的本质关系==</p><h3 id="亲密关系的无知">亲密关系的无知</h3><p>大学教育里边却没有教育，自己家庭里边的空白，大多数事情里边都有一个专业，</p><p>而对于两性关系方面，大多数人是面对的确是自己没有自知之明的无知</p><h2 id="先进行自身建设">先进行自身建设</h2><blockquote><p>爱人之前，先爱自己</p></blockquote><p><img src="../assets/image-20240320161516105.png" alt="image-20240320161516105"></p><p>==沉默成本解释外，更深层的理解==</p><blockquote><p>自我认可度不高的人，很容易会把恋爱中的甜蜜时刻，理解为对方是命运给与自己的恩惠和赏赐，从而会给对方添加滤镜。即使往往对方对自己并不好，也很难结束一段有毒的关系。</p><blockquote><p>能做到这一步很难，很多人要用一生来去治愈童年</p></blockquote><p>自我认可度不高的人很容易为了人与人之间当下的短暂快乐，而自愿去做明知没有结果的事情，去维持明知没有结果的关系，甚至会让自己主动去付出巨额代价。</p></blockquote><blockquote><p>会遇到人渣并让自己在一段感情中自我献祭</p></blockquote><p><img src="../assets/image-20240320161612688.png" alt="image-20240320161612688"></p><blockquote><p>异地女朋友 单独和男生吃饭后才主动告知，下次买票钱我来出？？</p></blockquote><h3 id="如何把握严于律己，宽以待人的分寸？"><strong>如何把握严于律己，宽以待人的分寸？</strong></h3><p>严于律己，宽于待人 要当做适用于对待特殊情况下的策略，而不是适用于任何情况下的美德</p><p>对方和你没有任何的利益关系，且对方的行为在长远期看，不会损伤你的个人利益</p><p>朋友关系，朋友圈点赞关系，甚至某些情况下 要采用“宽于律己，宽以待人”的策略</p><p><img src="../assets/image-20240320163117338.png" alt="image-20240320163117338"></p><p>对于对方如果跟你有直接利益关系，并且它的行为会长远期看，会影响你自己的个人利益的情况下，==尤其是涉及到人品这个特殊变量的情况下==那么就要去采取“严于律己，同以待人”的策略</p><p><img src="../assets/image-20240320163603054.png" alt="image-20240320163603054"></p><p>学会善用反问句，要让那些让你自己不舒服的人同样难看</p><p><img src="../assets/image-20240321102350839.png" alt="image-20240321102350839"></p><blockquote><p>不管是亲密关系，还是社交关系，要清楚别人时刻是在筛选自己而自己也要时刻学会去主动筛选别人</p></blockquote><p><img src="../assets/image-20240320163636818.png" alt="image-20240320163636818"></p><blockquote><p>不是所有人都会与你同行，在一段亲密关系里边，在你为对方去付出并且对方很享受在你这里的获得时候，对方也应该为你而建立一份契约，你自己也有筛选别人的资格，而非被筛选的资格。</p><p>等你学会如何不惧直面冲突，再去学习如何高情商。</p></blockquote><p><img src="../assets/image-20240320164821898.png" alt="image-20240320164821898"></p><blockquote><p>宜人和温和是强者能量溢出的特权，再身弱之人那里，只会演变成懦弱和委曲求全而不自知，并最终反向筛选出一批人渣主动留在自己的身边。</p></blockquote><blockquote><p>要做到以上，内核是自尊自爱；而不管自己有没有做成一些事情，现在自己要做的就是在所有的关系中，去无条件的提高自我认可度。</p></blockquote><blockquote><p>重要的不是对自己高看或者低看，而重要的是在爱人之前，先让自己变得足够强</p></blockquote><h3 id="爱人之前先变强的两个心法">爱人之前先变强的两个心法</h3><ol><li><p>改变自己的关键是创造价值</p><ol><li>学会用价值规律看问题，让自己变成一个有价值的人</li><li>改变自己的关键是发掘自己并不断的创造价值（创造视频，写文案）<ol><li>帮助别人的同时不断的成就自己</li><li>创造的价值必须是长久的</li></ol></li></ol><blockquote><p>价值够长久，才能够价值够大。同时价值足够大时候，才能够在任何关系中得到对等的回馈</p><p>当把视角“我想要”变成“我能给”的时候，很多浮躁妄念，就会马上消失。</p></blockquote></li><li><p>建立自己的&quot;身份感&quot;,不是指的是社会世俗意义上的身份，而是从心底建设开始，<strong>弄清楚自己我要成为一个怎样的人</strong></p></li></ol><p>无论想要成为什么样的人，都要建立自己的身份感，是一种从上到下的全方面塑造，远比以结果为导向从下至上的塑造更具力量。</p><p>如果认为自己强大，那就要有自己发自内心的驱动力，让自己去承受相应的痛苦和困难，不断蜕变。</p><blockquote><p><strong>男性在两性关系中必须接受的事实</strong></p><p>男女而言，通用的是对一个人有没有情欲是爱情能不能发生的一个大前提，而体面感是情欲重要的催化剂。表面上是爱情，其实真正在乎的是体面感。</p><p>体面表面上的表现形式是外貌，财富，而==真正的体面感会在后边展现==</p><blockquote><p>生物进化领域的常识：</p><ol><li>长得太丑的男性，几乎不会获得女性来自情欲所产生的爱情；但当长得丑的男性，如果获得良好的供养能力的时候，比如有钱、有地位，女性会不介意去和其发展长期关系；<strong>但</strong>依旧很难因为情欲而产生爱情。同时这个阶段如果出现一个长得帅的男性来追求这个女生，即便是这个男生没有钱，也没有社会地位，多数女生也不会抗拒和其发展短期关系</li><li>男性在生物学上也并不是多高尚，让人舒服的话，如果说的太多就会让人丧失接收真相的能力</li><li>剔除掉男女两性中很善良的人，也剔除掉很坏的人，剩下的广大7成男女中就基本都是这样子</li><li>进化心理学的观点也就是对于大多数人的进行一个特征的描述</li></ol></blockquote><ol><li><p>财富在两性关系中很重要,但是不要过于高估</p><ol><li>有钱则低调，没钱尽量找一个没钱时爱你的女人，比如爱你的性格，爱你的才学等等、</li></ol></li><li><p>平时保持一个基本的整洁也算是一个必须要做的事情，保持一个基本的精气神（长得好看（生气时候看到就不会生气）vs精神干净，男性好看的包容性）</p></li><li><p>遇到人的“运气和概率”</p><p>烟花0.5s的幸运，会让20多年的努力显得渺小。</p><p>每个人的人生的关键节点，都充满着这0.5s幸运的随机性。如果你碰到的人足够好，不一定是你自己有多么的优秀，而是因为你自己遇到了这0.5妙的随机性，<strong>0.5妙的概率幸运，让自己的所有的人品，包括20多年的努力都会显得相形见绌</strong></p></li></ol></blockquote><h4 id="人唯一能做的">人唯一能做的</h4><p><img src="../assets/image-20240320170004561.png" alt="image-20240320170004561"></p><blockquote><p>而不是遇到对的人和爱自己的人时候，发现不了，识别不了，甚至将别人对自己的爱轻视甚至浪费掉，这样的人注定不会承接住好东西。</p></blockquote><p>提高自己在亲密关系里识别和抓住好东西能力重点在于<strong>祛魅</strong> ==第二节重点讲祛魅==</p><h2 id="总结">总结</h2><p><img src="../assets/image-20240321102137401.png" alt="image-20240321102137401"></p><p><img src="../assets/image-20240321102225633.png" alt="image-20240321102225633"></p><blockquote><p>唯有自知，方有自爱；唯有自爱，方有人爱。</p></blockquote><p><img src="../assets/image-20240321105014986.png" alt="image-20240321105014986"></p><p><strong>一切从认识自我开始</strong></p><blockquote><p>关于幸运的思考:  自己高考 524  差最后一名 ；大学里边成绩搞得挺好，但是能够让自己去兼职+成功大二；考研扩招+读研</p></blockquote><blockquote><p>中文互联网越来越趋向于封闭，获取有效信息的成本越来越大；想要看到真正有价值的思考，或者“隐学”正变得越来越难，很多真正的好回答将再次变成“家学”。</p></blockquote><blockquote><p>不再花费时间去回答很多热点内容，摒弃掉所有没有长期价值的内容，执着于长期价值的回答和做事（未来两三年或者长期十几年后的仍然有用的东西），力求每一件事情都能够解决实际问题或者改变一个观念，消除一切浮躁的动机。</p></blockquote><p>推荐电影《背靠背 脸对脸》，B站就有很多介绍</p><p><img src="../assets/image-20240322204210917.png" alt="image-20240322204210917"></p><p><img src="../assets/image-20240322211957586.png" alt="image-20240322211957586"></p><blockquote><p>纪律 服从 权威<br>1.恰当微笑。不得罪人，给人留下好印象，仪容仪表，把领导同事当客户，端庄微笑，和蔼可亲<br>2.夸别人。忌讳说别人坏话，夸领导表示服从，夸自己，夸同事打下群众基础<br>3.为领导分担。错事错办，但不能不办<br>4.忍。忍批评，受委屈，吃小亏<br>5.谨慎。慎之又慎，不背后说人坏话，不犯原则性错误；事无巨细，谨小慎微，反复检查<br>6.混。不鼓励个人英雄主义，强调集体主义，和光同尘。<br>7.熬。延迟满足，曲折前进，自制力强。熬资历，熬时间（人生低潮时期），熬领导。你年轻，时间站在你这边，终究会熬出头的</p></blockquote><p><a href="https://mp.weixin.qq.com/s/B2eBdYGzXEJkq0woTGWKAQ">冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究 (qq.com)</a></p><h1>冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究</h1><p>#善思会写更快乐219</p><p>节后复工第一天，感觉怎么样？</p><p>有一种说法我很喜欢，职业就像游戏打怪通关，倦怠意味着你在这一关地图都展开了，打到头了，而新一关的钥匙，就在某个你曾经视而不见的熟悉地方。</p><p>而古典老师说过的这4条职场认知，我觉得帮助很大，也分享给你。</p><p>1️⃣结果思维，过程不重要，结果才重要。</p><p>2️⃣贡献思维，怀才不遇是个伪命题。<br>现在的社会分工很细，就像一条流水线拆出来的内容。所以，专业人士有义务告诉上游和下游，你能干什么。换言之，有义务把自己的才华“卖出去”，变成“贡献”。</p><p>3️⃣得失思维，职场不谈对错，谈得失。</p><p>4️⃣灰度思维，所有的职业环境，都是多元视角妥协而成的产物。在职业世界中，不是非黑即白，任何事情都是灰色的、混色的。</p><p>以上这4点，你想清楚了，职业世界的事，无往而不胜。工作即道场，记得要升级哦！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;聂辉华番外课程1该不该走选调生&lt;/h1&gt;
&lt;h2 id=&quot;细节展开1&quot;&gt;细节展开1.&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../assets/image-20240315191357916.png&quot; alt=&quot;image-20240315191357916&quot;&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LLm_learn</title>
    <link href="https://www.fomal.cc/posts/1908cf59.html"/>
    <id>https://www.fomal.cc/posts/1908cf59.html</id>
    <published>2024-02-16T10:48:56.000Z</published>
    <updated>2024-03-01T09:58:27.040Z</updated>
    
    <content type="html"><![CDATA[<h1>Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs？</h1><blockquote><p>相比于封装好的相关的事实知识信息，利用外部数据能够扩展大模型的性能，并减少其对于训练数据依赖性；</p><blockquote><p>如何更好的使用外部数据集来合并新信息，或根据之前看到的信息改进llm的功能，是一个重大的挑战</p></blockquote></blockquote><p>比较了两种常见的扩展方法:<strong>无监督微调</strong>和<strong>检索增强生成(RAG)（retrieval-augmented generation (RAG)）</strong>。我们在不同主题的知识密集型任务上评估这两种方法。我们的发现表明，==尽管无监督的微调提供了一些改进，但RAG的性能始终优于它==，无论是在训练中遇到的现有知识还是全新的知识。此外，我们发现，llm很难通过无监督的微调来学习新的事实信息，而在训练期间让他们接触同一事实的众多变体可以缓解这个问题。</p><h2 id="1-Introduction">1. Introduction</h2><ol><li>大型语言模型（LLMs）能够捕获大量的事实信息（Petroni等，2019年；Cohen等，2023年；Hu等，2023年）。由于它们庞大的预训练数据集，LLMs在各个领域展示出了显著的知识水平。然而，这种知识存在两个显著的局限性。<strong>首先，它是静态的，不随时间更新。其次，它不具体，因此可能在特定领域缺乏细致的专业知识。虽然这是两个不同的问题，但它们深深相关，因为它们的解决方案是相同的：增强模型的知识。</strong></li><li>最近，将LLMs适应特定领域并更新其知识的想法变得越来越普遍（Yu等，2022年）。已经提出了各种模型来改进不同领域的事实知识和能力，例如医疗保健（Singhal等，2023a;b；Wu等，2023a）、金融（Wu等，2023b；Yang等，2023）和法律（Huang等，2023年；Nguyen，2023年）。</li><li>在这项工作中，我们**专注于评估模型的知识及其记忆、理解和检索事实数据的能力。**我们的目标是理解知识注入的概念（Wang等，2020年；Chen等，2022年；Liu等，2020年；Lauscher等，2020年）。<strong>在给定文本语料库形式相关某些知识库的情况下，找到如何教一个预训练模型这些知识的最佳方法</strong></li><li>向预训练模型添加知识的一种方法是通过微调。通过微调，我们继续模型的训练过程，并使用特定于任务的数据进行适应。通过让模型接触特定的知识库，我们期望模型的权重相应地适应。这个过程旨在优化模型以用于目标应用，提高其在专业领域的性能和上下文相关性。</li><li>另一种增强模型知识库的方法是通过使用上下文学习（ICL）（Chen等，2021年；Radford等，2019年；Min等，2021年；Lampinen等，2022年）。ICL背后的主要思想是通过修改模型的输入query而不直接改变模型的权重来提高预训练LLMs在新任务上的性能。<strong>ICL的一种形式是检索增强生成（RAG）</strong>（Lewis等，2020年；Neelakantan等，2022年）。<strong>RAG利用信息检索技术使LLMs能够从知识源中获取相关信息并将其合并到生成的文本中。</strong></li><li>本研究旨在通过比较微调和RAG评估LLMs的知识注入能力。为了说明原理，让我们使用一个类比。考虑三名大学生在特定主题的考试上。所有人都可以获取课堂材料，但事先不知道主题。第一名学生只在考试时有教材，第二名学生在考试之前有准备，并学习了，而第三名学生在考试通知发布后失去了准备时间。谁可能表现得更好？</li></ol><h2 id="2-Background">2. Background</h2><ol><li>背景：为了评估知识注入，我们必须首先了解对于LLMs而言知识意味着什么。知识和语言模型：定义知识是一个复杂的哲学任务，远远超出了本研究的范围。然而，我们可以考察在语言模型的背景下事实知识意味着什么。如果一个模型知道一个事实，它可以准确而一致地回答与之相关的问题。此外，它可以可靠地区分与该事实相关的真假陈述。然后，我们可以将这个定义扩展到整个知识库，而不仅仅是单个事实。数学上，设Q = {qn}N为N个多项选择事实问题的集合，其中每个问题有L个可能的答案和一个正确答案。设A = {(a1n，…，aLn)}N为相应的可能答案集合，C = {cn}N为正确的答案。设M是一个语言模型。我们用M(qn) ∈ {a1n, …，aLn}表示模型对第n个问题的预测答案。我们定义M关于Q的知识分数L为标准的准确度分数：LM,Q := #{qn| M(qn) = cn} / N。如果以下条件成立，我们说模型M在Q集合方面拥有任何知识：LM,Q &gt; 1 / L。简单地说，模型可以一致地给出正确答案，超过简单的随机猜测基线。自然地，如果模型M相对于另一个模型的知识得分LM,Q更高，那么我们断言前者相对于后者在Q方面更有知识。</li><li>先前见过的知识：一个重要的区分是模型在预训练期间曾接触过的知识与全新事实之间的区别。考虑到现代LLM训练集的规模，它们涵盖了通过网络文本获得的大量信息。因此，即使在利基领域，知识注入的目标不一定是教会模型全新的事实，而是通过向特定领域施加偏好来“刷新”其记忆。</li><li>知识和推理：我们强调，LLMs的这种知识评估框架是不完美的。重要的是，它并没有解决影响模型响应的其他质量指标。创建一个纯粹以知识为基础的数据集而不涉及某种程度的推理是具有挑战性的。因此，具有强大推理能力的模型可能通过在多项选择考试中进行“有根据的猜测”而在陌生的知识密集型任务上表现出色。因此，对LLMs中的知识的任何评估都应考虑到这一点，并将结果视为推理（Sakaguchi等，2021年）、阅读理解（Dua等，2019年）和一般语言能力（Srivastava等，2022年）的更广泛基准的一部分。然而，这个评估框架仍然强调事实信息高于其他一切。</li><li>事实错误的原因：模型无法准确回答事实问题有很多可能的原因。在Wang等人的研究中，Wang等人介绍了五个主要的模型级错误原因的分类：<ul><li>领域知识不足：语言模型可能在特定领域缺乏全面的专业知识，因为它没有接触过这个领域。例如，一个只在威廉·莎士比亚的文本上训练的模型在被问及马克·吐温的作品时表现会很差。</li><li>过时信息：LLMs的截止日期由它们的训练数据集确定。因此，任何在最后一次训练更新之后发生的事件、发现或变化，如果没有外部来源的访问，将不会在模型的知识范围内。</li><li>遗忘：有时，模型在训练过程中接触到知识，但没有保留下来。对于在训练数据集中很少出现的罕见事实，这一点尤为真实。</li><li>遗忘：语言模型在预训练阶段之后通常会进行额外的训练（微调）。在某些情况下，这可能会导致一种被称为灾难性遗忘的现象，其中模型会失去在微调过程之前具有的一些知识。</li><li>推理失败：在某些情况下，语言模型可能具有与事实相关的相关知识，但未能正确利用它。这在复杂的多步推理任务（Tan等，2023年）或当对同一事实提出不同问题时特别明显，导致不同的结果。</li></ul></li></ol><h2 id="3-Injecting-Knowledge-to-Language-Models">3. Injecting Knowledge to Language Models</h2><p>根据第2节中给出的背景，很明显，对于许多知识密集型任务来说，一般的预先培训是不够的。为了解决这个问题，额外的后处理步骤对于增强预训练模型的知识至关重要。这一步通常被称为知识注入（Wang et al.，2020年; Chen等人，2022; Liu等人，2020; Lauscher等人，2020年）。<strong>在本节中，我们将研究两种广泛使用的知识注入框架：微调（FT）和检索增强生成（RAG）。我们开始制定知识注入问题，旨在解释这两种方法使用一致的术语。</strong></p><p>现在使用相同的术语将这个公式扩展到知识注入的问题。给定一组事实问题，存在一些包含与这些问题相关的信息的文本语料库。**知识注入的核心假设是，如果能够完全访问该语料库，它可以作为辅助知识库，并提高模型在这组问题上的性能。**在数学上，假设M是一个预先训练好的模型，Q是一组事实问题。现在，假设我们有一个相关的辅助知识库BQ。我们的目标是发现一个变换，记为F，当应用时，将增强关于Q：M′：= F（M，BQ）s. t的知识。LM′，Q &gt; LM，Q.(3)</p><h3 id="3-2-微调">3.2. 微调</h3><p>3.2. 微调：微调是调整预训练模型以增强其在特定、通常更狭窄的数据集或任务上性能的过程。在这里，重要的是要区分不同类型的微调。FT技术通常被分类为监督、无监督和<strong>基于强化学习（RL）的方法</strong>。我们接下来简要回顾这些方法及其与知识注入问题的关系。</p><ul><li>监督微调：监督微调（SFT）需要标记的输入-输出对。其中一种最常见的SFT方法是指导微调，它已成为提高模型性能的最强大方法之一。使用指导微调，输入是自然语言任务描述，输出是所需行为的示例。许多当前最先进的LLMs在预训练阶段之后经历了指导微调。</li><li>强化学习：FT的另一种形式依赖于RL或<strong>RL启发的优化策略来更好地调整模型</strong>。其中一些突出的例子是来自人类反馈的强化学习（RLHF）、直接偏好优化（DPO）和近端策略优化（PPO）。这些技术已被证明在与指导微调结合使用时非常有用。然而，与指导微调类似，这些方法<strong>侧重于响应的整体质量和其预期行为，而不一定关注其知识的广度。</strong></li><li>无监督微调：我们讨论的最后一种FT策略是无监督的，这意味着模型没有可用的标签可供学习。一个常见的无监督FT技术通常被称为持续预训练或非结构化FT。在这种方法中，FT过程被视为预训练阶段的直接延续。</li></ul><blockquote><p>启发式离散数据微调【基于强化学习的方法】</p></blockquote><h3 id="3-3-检索增强生成：">3.3. 检索增强生成：</h3><p>3.3. 检索增强生成：检索增强生成（RAG）**是一种通过使用外部知识源扩展LLMs能力的技术，尤其是在知识密集型任务中。**原始的制定涉及每个任务的额外训练，但已经证明，**一个预训练的嵌入模型可以在没有额外训练的情况下获得改进的性能。**该想法是在给定一个辅助知识库和一个输入查询的情况下，使用RAG架构找到与输入查询相似的文档。这些文档随后被添加到输入查询中，从而使模型对查询的主题有进一步的上下文理解。实践中，实现建议的架构相当简单：<strong>给定一个辅助知识库BQ和一个预训练的嵌入模型Me，我们为每个文档b ∈ BQ创建一个密集向量表示（嵌入），并将这些存储在向量存储器中。收到新查询q后，我们使用其嵌入Me(q)来检索与q的前K个最近邻，bq = {bk}K1，根据点积排名。然后，我们更新q为˜q = bq∥q，其中∥表示字符串连接。最后，我们返回M(˜q)作为模型的输出。</strong></p><h2 id="4-Knowledge-Base-Creation">4. Knowledge Base Creation</h2><h3 id="4-1-任务选择和原理">4.1. 任务选择和原理</h3><p>MMLU基准：为了正确评估LLMs在知识密集型任务上的能力，我们从大规模多语言语言理解评估（MMLU）基准中选择了四个不同的任务，涵盖解剖学、天文学、大学生物学、大学化学和史前等主题。所选任务是基于它们对事实知识的重视以及对推理的最小依赖而选择的。作为<strong>一种启发式方法</strong>，我们<strong>选择了问题简短且不涉及上下文的任务</strong>。在实践中，我们选择了四个STEM学科以及一个人文学科，以确保评估不局限于某些领域。值得注意的是，史前涉及涵盖所有非现代历史的问题。这种方法旨在使我们能够测试LLM在理解和操纵信息方面的熟练程度，与其推理过程隔离开来。</p><p>时事任务：为了进一步孤立LLM学习新知识的能力，我们<strong>创建了一个包含关于当前事件的多项选择题的任务</strong>。该任务包括关于发生在各种模型训练数据截止日期之后的事件的多项选择题。具体来说，我们侧重于美国的“当前事件”，时间跨度为2023年8月至11月，这些事件包含在相关的维基百科索引中。这种方法使我们几乎可以保证模型没有接触过这些事实，因此能够直接测试知识注入的能力。</p><p>总结：在本节中，作者选择了四个来自MMLU基准的不同任务，涵盖了解剖学、天文学、大学生物学、大学化学和史前等主题，以评估LLMs在知识密集型任务上的能力。此外，为了测试LLM学习新知识的能力，还创建了一个关于当前事件的任务。这些任务的选择旨在确保测试能够涵盖各种领域，并在不同背景下评估LLM的能力。</p><h3 id="4-2-数据收集与预处理">4.2. 数据收集与预处理</h3><p>4.2. 数据收集与预处理 为了有效评估LLMs在这些知识密集型任务上的表现，通过从维基百科中针对每个主题抓取相关文章，收集了一个全面的辅助数据集。选择维基百科作为主要知识来源的原因是它对相关主题的广泛覆盖以及作为群众验证知识库的可靠性。通过识别每个主题的相关中心页面，通过官方维基百科API检索了所有与任务相关的文章。 随后，采用了严格的清理过程将数据从原始子部分转换为干净的块。此步骤使用了“wikiextractor”工具（Attardi，2015）。将数据分成小的、干净的块（例如，删除HTML、URL等）旨在增强LLMs在各种知识领域的理解，并帮助LLMs进行微调过程。</p><h3 id="4-3-当前事件任务创建">4.3. 当前事件任务创建</h3><p>4.3. 当前事件任务创建 在从维基百科收集到相关块之后，我们借助GPT-4（OpenAI，2023）创建了一个新的多项选择数据集。首先，我们移除了任何小块。对于语料库中的每个剩余块，要求GPT-4创建四个高度具体、高质量的只有一个正确答案的多项选择题。所谓具体，是指问题可以在不了解问题所指的上下文的情况下回答，并且具有最小的歧义性。接下来，要求GPT-4选择其中最具体的两个。然后进行手动评估和验证步骤。总共，这产生了910个新问题。</p><h3 id="4-4-释义生成">4.4. 释义生成</h3><p>4.4. 释义生成 在创建数据集之后，我们利用GPT-4生成数据集的增强版本。我们要求GPT-4提供输入数据的释义版本，这些版本完全保留信息但用不同措辞。为了确保多样性，每次释义迭代都使用不同的种子。我们随机选择了每个任务的240个块，并为每个块创建两个释义。这些被保留用于用于超参数调整的验证集。对于描述在第6节中的微调过程中使用的当前事件数据集，我们为每个块创建了十个释义。</p><h2 id="5-实验和结果">5.实验和结果</h2><p>实验框架 我们使用了流行的LM-Evaluation-Harness（Gao等人，2021）存储库来评估LLMs在所选知识密集型任务上的性能。LM-Evaluation-Harness是一个强大的基准测试工具，目前是模型评估的行业标准，并且是HuggingFace排行榜的基础。利用这个平台确保了标准化的评估框架，并允许跨模型、方法和数据集进行一致的比较。更重要的是，通过使用行业标准进行评估，我们可以避免由提示工程和格式问题引起的任何差异，并复制每个模型的报告基准结果。</p><p>模型选择 我们选择了三个模型进行推理评估：Llama2-7B（Touvron等人，2023）、Mistral-7B（Jiang等人，2023）和Orca2-7B（Mitra等人，2023）。选择这些模型旨在代表最流行的开源基础模型和跨各种基线能力的经过指令调整的模型。此外，我们选择了bge-large-en（Xiao等人，2023）作为RAG组件的嵌入模型，并使用FAISS（Johnson等人，2019）作为其向量存储。根据HuggingFace MTEB排行榜，这个嵌入模型目前是开源嵌入模型的SOTA。</p><p>配置变化 我们的评估包括多个配置，并对它们进行了网格搜索，以便进行更全面的基准测试。首先，我们比较了基线和微调模型以及它们与RAG组件的性能。其次，我们探索了在RAG中添加到上下文中的文本块的最佳数量。具体来说，采用了不同的K值 ∈ {0, . . . , 5}来分析对模型性能的影响。最后，我们探讨了5-shot性能与0-shot性能之间的比较。</p><p>训练设置 我们使用了第3.2节中描述的无监督训练过程对所有模型进行训练。对于每个数据集，我们将辅助知识库分成大小为256的相等块，通过连接或分割原始块来实现。我们还添加了两个特殊标记，<BOS>和<EOS>，来标记原始块的开头和结尾，以保留文档的结构。模型的训练使用了介于1 × 10^−6和5 × 10^−5之间的学习率，这是通过超参数搜索找到的。所有模型都在4个NVIDIA A-100 GPU上进行了最多5个时期的训练，并且批量大小为64。</p><p>评估方法 所有评估都是通过将每个多项选择选项附加到问题后，然后通过模型传递连接以获得每个选项的对数概率分数来完成的。最高分被解释为模型的选择，并用于准确性计算。更正式地说，在方程（1）中，我们说如果M(qn) = cn： cn = arg max l {M(qn∥a1 n), . . . ,M(qn∥aL n)}, 其中M(qn∥al n) = log PM(qn∥al n)。</p><p>MMLU结果 对于每个任务和模型，我们比较了四种方法：仅使用基本模型、RAG、FT以及通过使用微调模型作为生成器来合并FT和RAG。此外，我们使用了0-shot和5-shot场景来测试MMLU任务。完整的结果显示在表1中。相对准确性增益的聚合，即 (LM′,Q − LM,Q)/LM,Q， 其中M是基本模型，M′是注入知识的模型，显示在图2中。在所有情况下，RAG的表现明显优于基本模型。此外，将RAG与基本模型作为生成器一起使用一直比仅进行微调更好。在某些情况下，将微调模型而不是基本模型用作RAG管道中的生成器进一步提高了结果。然而，这并不一致，因此显示了微调的固有不稳定性。此外，我们发现5-shot方法在大多数情况下都略微提高了结果，在所有不同方法中观察到了类似的趋势。</p><p>当前事件结果 当前事件任务的评估显示在表2中。由于问题和辅助数据集之间的一对一对应（请参阅第4.3节），RAG证明了其特别有效性。微调与RAG相比不具竞争力。然而，微调与多个释义仍然比基线提供了显著的改进。我们注意到，与仅使用RAG相比，将RAG与微调相结合的性能较差。</p><p>值得注意的是，虽然问题是基于模型在训练过程中未接触到的信息，但基本模型的结果超过了1/L = 0.25。这部分可以通过模型在回答不独立于过去信息的问题时使用推理和/或现有知识来解释。一些这方面的例子可以在附录C中找到。</p><p>微调与RAG： **在MMLU和当前事件任务的结果中，RAG相对于微调的显着优势是明显的。**虽然微调在大多数情况下改善了结果，但与RAG方法相比并不具有竞争力。可能有几个因素会导致这种行为。首先，**RAG不仅向模型添加知识，还包含与问题相关的上下文，这是微调所缺乏的功能。**此外，**微调可能会影响模型的其他功能，因为微调程度可能导致灾难性遗忘。**最后，未监督微调模型可能会通过监督或基于RL的微调进一步对齐，正如Orca2相对于基本的Llama2的性能大大提高所示。</p><h2 id="6-重复的重要性">6.重复的重要性</h2><p>与其他任务不同，在其他任务中，模型已经在预训练期间暴露于与主题相关的方面，当前事件包括新信息。在这种情况下，标准的常规微调不仅没有提高Llama2的性能，而且还显着降低了它。为了改善微调结果，我们探索了使用释义的数据增强。</p><h2 id="7-结论和未来的工作">7.结论和未来的工作</h2><p>大型语言模型拥有各种主题的大量知识。在这项工作中，我们测试了他们适应新知识的能力：包括专业知识和完全看不见的知识。这是第一个研究比较两个突出的方法在这一领域，即微调和检索增强生成。虽然微调对许多用例都很有用，但我们发现RAG是知识注入的更可靠选择。这项工作的某些方面仍需要进一步研究。例如，我们专注于无监督训练作为我们的主要微调方法，而不是预防性调整或基于RL的方法。研究各种技术与各种辅助知识库的组合可能会产生更好的结果。这种方法，结合我们在第6节中的假设，可以进一步增强我们对通过FT进行知识注入的理解。虽然我们相信这项工作进一步提高了我们对LLM知识的理解，但在这一领域还有很多工作要做。具体而言，需要更多的研究，特别是从理论的角度来看，关于在LLM知识表示的问题。最后，需要进一步努力衡量法学硕士的知识。虽然我们采用了公式（2）中描述的经验方法，但探索其他关于知识的定义和观点并扩展这项工作也很重要。</p><h2 id="8-Limitations">8. Limitations</h2><p>在所有机器学习应用中，超参数的选择会显著影响结果。因此，我们强烈建议针对特定情况优化所有相关的超参数。我们通过在三个不同的模型上进行实验来支持我们的主张。然而，对其他LLM的推广应该彻底测试。例如，GPT-4对于某些MMLU任务实现了近乎完美的准确性（Nori等人，2023），因此不适用进一步的改进。最后，当我们为知识库选择各种主题时，我们所有的来源都来自维基百科。其他数据集可能会产生不同的结果，必须仔细评估。</p><blockquote><p>Knowledge Base Creation   创建启发式方法</p></blockquote><p>优化prompt</p><blockquote><p>基于公开的数据库 大家去做测试</p><p>能不能做两个 一个获取更好的数据，另一个去做</p></blockquote><blockquote><p>带着地理信息 图片信息 怎样放到prompt 中</p></blockquote><h1>RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning</h1><blockquote><p>强化学习+模型微调</p></blockquote><blockquote><p>自动为每个任务找到最佳prompt</p></blockquote><p>==<strong>现状</strong>==</p><p>现有的大多数工作都依赖于调优<strong>软提示</strong>(例如，嵌入式提示)，这些<strong>提示缺乏可解释性</strong>、跨模型之间的的可重用性以及当梯度不可访问时的适用性。另一方面，离散提示很难优化，通常是通过“枚举(例如，解释)-选择”启发式创建的，这种<strong>启发式不会系统地探索提示空间</strong>。</p><p>==提出了一种<strong>基于强化学习的离散提示优化方法RLPROMPT</strong>。RLPROMPT构建了一个参数高效的策略网络，在奖励训练后生成优化的离散提示。==  为了利用来自大LM环境的复杂和随机奖励信号，我们结合了有效的稳定奖励，大大提高了训练效率。RLPROMPT可以灵活地应用于不同类型的大模型，如mask(如BERT)和left-to-right模型(如GPTs)，用于分类和生成任务。在少镜头分类和无监督文本样式转换上的实验表明，在现有的各种微调或提示方法中，该方法具有优越的性能。有趣的是**，优化后的提示往往是不合语法的乱码;令人惊讶的是，这些胡言乱语提示在不同的LM之间是可转移的，以保持显著的性能，这表明LM提示可能不遵循人类的语言模式。**</p><h3 id="introduction">introduction</h3><p>Prompting已经成为使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，包括从左到右的模型（例如GPTs）（Radford等人，2019年；Brown等人，2020年）和掩码LMs（例如BERT）（Devlin等人，2019年）、RoBERTa（Liu等人，2019年）等。与昂贵地为每个下游任务更新大量LM参数的传统微调相比，<strong>Prompting在输入中连接附加文本，指导LM生成所需的输出，从而降低了成本</strong>。Prompting的一个关键问题是如何找到最佳提示来提高LM在各种任务上的性能，通常只有很少的训练样本。</p><ul><li>Prompting是使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，与传统的微调相比，它在输入中连接额外的文本以指导LM生成所需的输出，从而降低了成本。</li><li>关于Prompting的一个关键问题是如何找到最佳提示以提高LM在各种任务上的性能，通常只有很少的训练样本。</li></ul><p>其中，最流行的方案之一是调整软提示（即连续嵌入向量），因为它们易于梯度下降。然而，由于其本质上来说，由于人类难以理解且无法与其他LMs一起使用，生成的提示对人类来说很难理解，并且不兼容。此外，LM内部梯度通常难以计算，或者仅对仅具有推理API（例如GPT-3）的LMs不可用。因此，<strong>使用由词汇表中的具体标记组成的离散提示通常是可取的</strong>。然而，它们的离散性质使优化变得非常困难。以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择。AutoPrompt使用梯度信息编辑提示标记，但受到训练不稳定性和与基于梯度的软提示相同的适用性问题的影响，在实践中的效果有限。</p><ul><li>一个流行的方案是调整软提示，因为它们易于梯度下降，但生成的提示难以理解，而且不兼容其他LMs。</li><li>离散提示通常由具体标记组成，但它们的离散性质使得优化变得非常困难。</li><li>以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择，但这些方法在实践中的效果有限。</li></ul><p>本文提出了一种新的基于强化学习（RL）的离散提示优化方法RLPROMPT，该方法汇集了在不同任务和LMs上高效使用的广泛理想属性。与直接编辑离散标记不同，<strong>RLPROMPT训练一个生成所需提示的策略网络</strong>。离散提示优化实际上就是学习少量策略参数，我们将其设置为<strong>插入到冻结的紧凑模型中的MLP层</strong>，例如distilGPT-2。该公式还使我们能够<strong>使用现成的RL算法</strong>，这些算法<strong>可以使用任意奖励函数来学习策略，无论是使用可用数据（例如在少样本分类中）定义的还是在没有监督数据可用时（例如在可控文本生成中）使用的其他弱信号。</strong></p><ul><li>RLPROMPT是一种新的基于强化学习（RL）的离散提示优化方法，它训练一个生成所需提示的策略网络，并在不同任务和LMs上高效使用。</li><li>与直接编辑离散标记不同，RLPROMPT训练一个生成所需提示的策略网络，这使得离散提示的优化更加高效。</li></ul><p>然而，RL用于提示优化对学习效率提出了新的挑战：大型黑箱LM呈现出高度复杂的环境，给定提示（即操作）后，经过一系列复杂的转换（例如读取输入和推断输出）才能计算奖励。这使得奖励信号非常不稳定，很难学习。为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</p><ul><li>RL用于提示优化对学习效率提出了新的挑战，因为大型黑箱LM呈现出高度复杂的环境，奖励信号非常不稳定且难以学习。</li><li>为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</li></ul><p>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法（例如表1中描述的方法）上都有所改进，并且对不同的建模选择（例如分类中的verbalizers）都具有鲁棒性。所得到的离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。尤其是，尽管优化后的提示能够引起强大的任务性能，但倾向于是没有明确人类可理解含义的无意义文本，与最近的研究相呼应，即利用提示的LMs并不一定遵循人类语言模式。也许令人惊讶的是，学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</p><ul><li>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法上都有所改进，并且对不同的建模选择都具有鲁棒性。</li><li>离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。</li><li>学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</li></ul><p><strong>我们将给出离散prompt optimization的RL公式(§2.1-2.2)。然后我们讨论我们的策略网络的设计(§2.3)。最后，我们描述了我们的奖励工程技术来提高RL培训(§2.4)。</strong></p><blockquote><p>第一篇文章的knowledge base 搭建</p></blockquote><blockquote><p>学习了初步的在大模型上以及大模型之间的应用的微调策略以及增强式生成策略，并对比了相应方法的不同</p></blockquote><blockquote><p>第二篇文章 是对应与微调策略里边的强化学习提示网路模型，生成对应的网络提示</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs？&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;相比于封装好的相关的事实知识信息，利用外部数据能够扩展大模型的性能，并减少其对于训练数据依赖性；&lt;/p</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>参与开源社区</title>
    <link href="https://www.fomal.cc/posts/48379df9.html"/>
    <id>https://www.fomal.cc/posts/48379df9.html</id>
    <published>2024-01-24T10:27:14.000Z</published>
    <updated>2024-03-01T09:58:27.037Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>尝试参与开源活动，整理一下自己的github主页</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;尝试参与开源活动，整理一下自己的github主页&lt;/p&gt;
&lt;/blockquote&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AI_Agent调研</title>
    <link href="https://www.fomal.cc/posts/7641cfbd.html"/>
    <id>https://www.fomal.cc/posts/7641cfbd.html</id>
    <published>2024-01-09T10:50:43.000Z</published>
    <updated>2024-01-18T14:49:36.070Z</updated>
    
    <content type="html"><![CDATA[<h1>24.1.9日WorkShop活动准备</h1><blockquote><p>需要看的3D产品 链接是官网</p></blockquote><p><a href="https://www.tripo3d.ai/">Tripo AI (tripo3d.ai)</a></p><h2 id="活动前置须知">活动前置须知</h2><h3 id="AGI领域">AGI领域</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/607125649">知识科普：什么是AGI？ - 知乎 (zhihu.com)</a></p></blockquote><p>AGI最近经常被提到，主要是因为ChatGPT的开发公司OpenAI将其写在了自己的企业使命中了，只要在介绍OpenAI的场景都会介绍到企业使命，而且在此时此刻，AGI作为OpenAI的企业使命中重要的关键词，也显得那样伟大。</p><p>AGI是<strong>Artificial General Intelligence</strong>的缩写，中文应该是“通用人工智能”，是指一种能够像人类一样思考、学习和执行多种任务的人工智能系统。我觉得这个词就是为了有别于“特定领域人工智能”而创造出来的。</p><blockquote><p>个人感觉有点像使用了AI赋予能力之后的智能机器人，当然这种智能机器人是主要应用在企业领域</p></blockquote><p>AGI可以视为一种更高级别的人工智能，是当前人工智能技术发展的一个重要方向和目标。但由于其在技术和理论方面的挑战，它仍然是一个较为遥远的目标。Chatgpt也还不能被称为AGI，至少它还不会画画。</p><blockquote><p>几个AGI的实例：</p><ol><li>王一快的写作猫【帮助生成写作内容和文档】</li><li>AIGC的相关产品【AI 生成内容】</li><li>最近刚刚开始用的微信智能日程提醒</li></ol></blockquote><h3 id="AGI的发展方向">AGI的发展方向</h3><p><a href="https://m.huxiu.com/article/968593.html">AGI的大时代，我们可以做些什么？-虎嗅网 (huxiu.com)</a></p><ol><li><p><strong>通用的AGI平台</strong></p><ol><li>【商业公司】Google的Bard、百度的文心一言、Facebook的LLaMa、阿里、腾讯和字节的大语言模型</li><li>【个人或者社区可以自己搭建自己的AI平台】清华大学开源的GLM-130B 、Facebook开源出来的Alpaca<ol><li>可以从GitHub上pull代码，在自己的机器上快速地搭建起一个模型，提供服务</li><li><strong>大语言模型的平台已经白菜价</strong></li></ol></li><li>先从商业或者科研领域研发并推广，再到向企业和军方中广泛使用，最后在资金和风向的推导下为大众所熟知，再到个人领域的应用和不断完善【这其中估计要有3-5个月的延迟，类比于ChatGPT【自己12月份就已经开始用，但是到明年的3月底才推广】】</li><li><strong>现在的工程师投身通用AGI平台的建设，亲自去参与搭建平台，设计架构，做一位AI平台架构师和算法工程师，将毫无疑问成为最抢手的人才</strong></li></ol></li><li><p><strong>定制化专业垂直领域的AGI平台</strong></p><ol><li>金融行业、医疗行业、教育行业、汽车行业、制造行业等各种行业对应的，与专业知识结合的平台</li><li>赶紧在Github上pull GLM-130B，或者骆驼的代码，先学习起来</li><li>除了可以直接参与AGI平台的构建。其实也可以做平台之上的应用。<ol><li>**现有的软件通过AGI的能力进行提升，为用户或者客户提供增值的服务。**最典型的就是微软的Office 365 集成GPT的能力以后，可以极大提高办公人员的工作效率；Adobe的Firefly集成AGI的能力以后，可以帮助设计人员快速设计更多的创意产品。</li><li><strong>基于GPT的插件体系</strong>**，在平台上根据自己的需要进行插件开发，完成自己细分领域的创新型产品。**</li></ol></li></ol></li><li><p><strong>软件服务商升级AI的能力</strong></p><ol><li><strong>在传统软件行业的各个垂直领域，实际上都存在着用AI去提升软件生产力的机会</strong><ol><li>英伟达的CEO黄仁勋在GTC大会上，就介绍了在芯片制造领域，利用AI的能力提升芯片制程和生产效率的案例。所以，在未来十几年，很可能AI能够极大地加速人类的科技和生产能力</li></ol></li><li>微软的<strong>Office 365</strong>。最近微软也发布了接入GPT-4模型能力的Office 365 Copilot。有了AI能力的加持，我们通过简单的自然语言对话，告诉Office我们的文档需要什么，Office Copilot就会自动帮助我们生成结果，例如：微软的office365 自动生成需要的文档和对应文档内容；【本质是利用的ai自动生成office所识别的VBA代码】</li><li>Adobe也演示了Firefly，利用AI的能力进行自动绘画功能，增强了Adobe软件的设计效率</li></ol></li><li><p><strong>基于通用大平台，开发细分领域的创新型应用</strong></p><ol><li>基于小数量的用户创新或者开发面向有特殊需求的专业化产品</li><li><strong>基于类似GPT-4这样的通用AGI平台，一定可以在各个细分行业涌现无数的创新应用出来</strong><ol><li>过去的各种插件开发 例如谷歌或者edge 的插件开发</li><li>苹果 或者 安卓的应用市场中各种应用开发</li><li>微信平台中的某些特殊功能的产品开发</li></ol></li></ol></li></ol><h2 id="活动内容">活动内容</h2><blockquote><p>介绍产品</p><p>了解非3D从业者怎样上手玩转3D</p><p>Tripo产品和3D内容生成的未来发展趋势</p></blockquote><p><img src="../assets/image-20240109190152774.png" alt="image-20240109190152774"></p><h2 id="活动准则">活动准则</h2><blockquote><p>准备自己的问题，询问AGI领域内的超集个体怎样应对</p><p>学习或者了解现在大家都在做的AGI产品或者用的技术</p></blockquote><p><img src="../assets/image-20240109190322701.png" alt="image-20240109190322701"></p><h1>开始主题</h1><p>Founder Park创业者社区</p><ol><li>两个嘉宾分享</li><li>大家有问题 评论区提问</li></ol><p>索尼  gta5  到gta6 的跨越</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;24.1.9日WorkShop活动准备&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;需要看的3D产品 链接是官网&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.tripo3d.ai/&quot;&gt;Tripo AI (tripo3d.ai)&lt;/a&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>图卷积算法应用落地——骨架数据行为识别系统</title>
    <link href="https://www.fomal.cc/posts/99a90e7a.html"/>
    <id>https://www.fomal.cc/posts/99a90e7a.html</id>
    <published>2024-01-09T06:39:26.000Z</published>
    <updated>2024-09-06T01:06:35.588Z</updated>
    
    <content type="html"><![CDATA[<h1>图卷积算法应用——视频行为识别系统落地方案</h1><h2 id="1-识别系统介绍">1.识别系统介绍</h2><p>随着AI、web3、短视频的不断发展，现代生活对于图片的注意力开始转向视频数据，AI智能生成视频和各种主题短视频爆火等都是针对于视频数据，不断推动这视频内容的感知和创作。人作为视频中的最重要的一个主体，对视频内容的创作和生成有着至关重要的作用，所以对于视频中人体的动作进行识别变得越来越重要。近几年视频中人体动作识别领域已经成为热门领域，同时也有越来越多的产品开始应用该技术。</p><p>针对于视频中人体行为，最直观是将RGB视频直接作为视频数据或者间接裁剪为图片数据进行处理。利用CV中的算法将图片数据扩充上一个维度——时间帧维度，对于视频中人体的行为数据进行处理分析并最终输出一个类别概率数据。另外受限与视频数据处理量较大、视频数据噪音较多等因素，利用从视频中提取出的人体骨架信息能够更好的去除背景影响和其他噪音信息，重点关注与骨架动作和内容，提高识别准确率；因此针对于骨架数据的动作识别领域也不断兴起新技术，针对于从视频中提取出的骨架数据，不断提高对于骨架数据的模型性能和完善骨架数据动作识别技术生态有了至关重大的意义。</p><p>本篇文章针对于最近时间做的一项工作，针对于行为识别方面，尤其是视频和骨架数据方面，从视频采集到骨架数据库划定，再到算法设定和模型训练，完成整个动作视频实时处理和动作分类展示，完成了一项落地方案的实施，将内容进行输出和分享，希望跟大家讨论交流，不断改进方案的细节，精益求精。大家如果对于整套方案中所涉及到的某个技术或者实现过程感兴趣，或者能够提供相应的改进，欢迎在后台留言，与作者一起交流</p><h2 id="2-系统需求设定">2.系统需求设定</h2><h3 id="1-骨架数据标准库">1.骨架数据标准库</h3><p>对于骨架数据先进行动作识别，然后针对与完成的动作依据对应的动作标准进行评分，是非常常见的操作。根据日常的动作进行动作标准库选择，选择执行动作比较标准的数据，完成数据的采集、清洗、入库后，完成对应的数据标准库设定。</p><h3 id="2-数据存储格式指定">2.数据存储格式指定</h3><p>对应于所处理的骨架数据，在完成数据采集的时候，应该设定好按照什么样的格式进行存储，同时将数据标准进行设定，方便后边其他操作应用数据的时候，使用相应的标准进行数据解析，方便下游任务的实施。</p><h3 id="学习与评价算法">学习与评价算法</h3><p>针对于相应的视频或骨架数据，需要选择相应的学习算法和评价算法：</p><ol><li>学习算法完成对于数据集中视频的学习，并在推理阶段完成动作类别划定，完成动作分类</li><li>评价算法需要将测试的数据与标准库中的数据，进行动作比对，完成对于测试数据的分数划定</li></ol><h3 id="4-标准数据生产与模型训练流程">4.标准数据生产与模型训练流程</h3><p>下边图片是设定的整个项目的流程和操作过程，从数据源的采集到标准库的设定，在宝库模型训练系统的完善。</p><p><img src="../assets/image-20240109153021375.png" alt="image-20240109153021375"></p><h2 id="3-数据和算法调研">3.数据和算法调研</h2><h3 id="1-视频or骨架">1.视频or骨架</h3><p>针对于视频数据和骨架数据进行深入的分析，将两种数据进行详细的优劣对比，发现有如下对比：</p><p><img src="../assets/image-20240109160548890.png" alt="image-20240109160548890"></p><p>视频相对更加的方便采集，可以直接从摄像头拍摄后完成数据获取；但是因为视频数据中包含内容比较丰富，同时数据量比较大，所以导致数据中会包含一些对于动作识别分析过程中不可用的噪音数据，对于后续的识别过程会带来一些麻烦。</p><p>相对于视频数据，骨架数据的优势便是主要针对的就是动作进行获取的骨架数据，能够完整的表示动作表示的同时不受到噪音数据的影响，同时能够很好的减少像光照、视频背景等信息的干扰。但对于骨架数据的获取可能会需要一些相应的设备，并需要提前做好数据转换。</p><p><img src="../assets/image-20240109160022176.png" alt="image-20240109160022176"></p><blockquote><p>视频数据骨架数据转换图</p></blockquote><h3 id="2-算法分析">2.算法分析</h3><p>目前在基于视频动作识别和基于骨架数据动作识别方面主流方法准确率都能达到80%以上，主要基于深度学习方法包括但不限于（1）CNN，（2）GNN,（3）Transformer等主流方法</p><p><img src="../assets/image-20240109161059481.png" alt="image-20240109161059481"></p><p>​视频动作识别算法发展脉络图</p><p><img src="../assets/image-20240109161202559.png" alt="image-20240109161202559"></p><p>​骨架动作识别算法发展脉络图</p><h3 id="3-数据集选定">3.数据集选定</h3><p>针对于所要完成的整套方案，需要从视频中完成数据采集后，在处理成骨架数据。相较而言，骨骼数据相比于RGB视频数据不易受背景影响，避免遮挡、光照等噪音影响搜集到数据集包括太极拳数据、Ntu数据集及相关研究。这里我们直接选择了对应的NTU60骨架数据集来作为我们的预定义数据集，并在后续的实现方案中完成所需要的全部操作。</p><p>在这里我们也对于其他的一些数据集和开源算法以及可以直接进行实操的项目进行了实操和测试，下图是我们进行过测试的项目罗列，如果对图中兴趣感兴趣，欢迎在后台留言和交流</p><p><img src="../assets/image-20240109161839748.png" alt="image-20240109161839748"></p><p>​方案调研技术罗列</p><h2 id="4-NTU60标准库构建">4.NTU60标准库构建</h2><p>在这里，我们会对与我们使用到的标准库进行介绍，并完成我们方案所需要的标准库的数据格式解析：</p><p><img src="../assets/image-20240109162158671.png" alt="image-20240109162158671"></p><h3 id="1-骨架数据格式设定">1. 骨架数据格式设定</h3><p>NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p>文件命名方式如下：</p><p><img src="../assets/image-20240109162328756.png" alt="image-20240109162328756"></p><p>命名格式和相应实例</p><p><img src="../assets/image-20240109162353641.png" alt="image-20240109162353641"></p><p>这里为相机和不同的人物设定id 是为了下边在执行相同动作时候，能够划定不同的变量，保证数据集能够更加全面的展示相应的数据，并能够帮助模型学习同一个动作的不同表现格式。在模型完成动作识别后，会对于动作类别进行划定，输出的动作分类结果就是A后边相应的数字借以表示动作所属的类别</p><h3 id="2-骨架数据获取标准">2.骨架数据获取标准</h3><p>这里介绍一下根据不同的相机角度拍摄结果来完成动作跨视角的获取和解析，以及根据不同人物id完成相应动作来区别跨主题分类</p><p><img src="../assets/image-20240109162916550.png" alt="image-20240109162916550"></p><p>​<strong>原始数据图</strong></p><h4 id="1-跨视角">1.跨视角</h4><p>将所有的3个摄像头获取的数据，根据摄像头的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同视角的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162951421.png" alt="image-20240109162951421"></p><p>​处理后的可解析跨视角数据</p><h4 id="2-跨主题">2.跨主题</h4><p>将所有的不同id的人物执行的相同动作获取的数据，根据人物的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同主题的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162957586.png" alt="image-20240109162957586"></p><p>​<strong>处理后的可解析跨视角数据</strong></p><h3 id="3-动作类别介绍">3.动作类别介绍</h3><p>NTU-RGB+D60数据集包含60个种类的动作。数据集由Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括3D骨架信息、RGB视频等。</p><p><img src="../assets/image-20240109163500174.png" alt="image-20240109163500174"></p><h3 id="4-骨架数据内容标准">4.骨架数据内容标准</h3><p>这里解析一个.skeleton数据内容:一个动作包含104帧 ,25是关节个数,紧接着的25行12列表示,25个关节点的12类别信息,后边还有类似的其他103组表征103帧的关节特征数据</p><p><img src="../assets/image-20240109163607368.png" alt="image-20240109163607368"></p><p>​                                                                                                 <strong>.skeleton数据格式解析</strong></p><p>其中的‘x’, ‘y’, ‘z’, 构建成骨架关节点特征信息，用来表示动作发生过程中的空间坐标信息，构建X∈[25,3] 维特征向量标识一个动作的特征信息，点线数据结构表征空间信息。</p><p><img src="../assets/image-20240109163944894.png" alt="image-20240109163944894"></p><p>​                                                                                           <strong>X Y Z三维数据格式解析</strong></p><h3 id="5-骨架数据构图">5.骨架数据构图</h3><p>每个骨架都有25个骨骼点,使用自连接节点构建成数值为0/1 的[25，25]维邻接矩阵,数值为1表示节点之间有邻接关系,数值为0 表示节点间没有邻接关系</p><p>25个节点自身与自身之间具有自连接关系,利用25个节点选择不同的构图方式构建成为人体骨架模型</p><p><img src="../assets/image-20240109163803430.png" alt="image-20240109163803430"></p><h3 id="6-骨架数据可视化">6.骨架数据可视化</h3><p>下边是一些骨架执行动作的2d可视化</p><p><img src="../assets/image-20240109164258750.png" alt="image-20240109164258750"></p><p>​                                                                                                              <strong>抢劫</strong></p><p><img src="../assets/image-20240109164438223.png" alt="image-20240109164438223"></p><p>​                                                                                                              <strong>坐下</strong></p><p><img src="../assets/image-20240109164332785.png" alt="image-20240109164332785"></p><p>​                                                                                                             <strong>扔东西</strong></p><p>​</p><p><img src="../assets/image-20240109164455067.png" alt="image-20240109164455067"></p><p>​                                                                                                             <strong>单手捡东西</strong></p><h2 id="5-落地方案展示">5.落地方案展示</h2><h3 id="1-标准数据生产与模型训练流程">1.标准数据生产与模型训练流程</h3><p>下边图片展示的是我们本套方案所计划应用的全部技术，以及已经完成的相应展示，在后边视频中已经展示了相应的实现过程</p><p><img src="../assets/image-20240109165738475.png" alt="image-20240109165738475"></p><h3 id="2-动作学习方案展示">2.动作学习方案展示</h3><blockquote><p>视频插入</p></blockquote><p>视频中展示的是我们利用我们预训练后的模型学习完成整个NTU60动作后，对于骨架数据——扔东西这个动作先进行可视化，然后将测试数据输入给模型后，模型对于数据进行识别后将识别到的动作进行可视化并将测试结果写回到本地文件中并将模型的准确率进行计算并同步到数据文件中</p><h3 id="3-评价算法">3.评价算法</h3><h4 id="1评价算法的模型图和实现过程">1评价算法的模型图和实现过程</h4><ol><li><p>将两个样本分别送入到两个相同的深度神经网络中，两个特征提取网络的结构和参数相同;</p></li><li><p>使用损失函数计算两个输出结果的距离差，误差越小相似度越高;</p></li><li><p>损失函数根据输入数据和任务的不同可以使用对比损失、余弦损失、交叉熵损失等;</p></li></ol><p><img src="../assets/image-20240109171444116.png" alt="image-20240109171444116"></p><p><strong>评价算法的模型图</strong></p><h4 id="2-评价算法改进">2.评价算法改进</h4><p>我们对于评价算法重新划定评价等级，设定了下边的改进计划</p><p><img src="../assets/image-20240109171817932.png" alt="image-20240109171817932"></p><p>​<strong>改进方案设定</strong></p><h4 id="3-方案前端界面">3.方案前端界面</h4><p>方案设定的前端展示如下，可以对于某个视频中动作进行骨架展示并将对应的数据骨架评分进行展示</p><p><img src="../assets/image-20240109171929541.png" alt="image-20240109171929541"></p><p>​<strong>前端展示图</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;图卷积算法应用——视频行为识别系统落地方案&lt;/h1&gt;
&lt;h2 id=&quot;1-识别系统介绍&quot;&gt;1.识别系统介绍&lt;/h2&gt;
&lt;p&gt;随着AI、web3、短视频的不断发展，现代生活对于图片的注意力开始转向视频数据，AI智能生成视频和各种主题短视频爆火等都是针对于视频数据，不断推动这</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解7骨架序列的目标表示自监督学习框架</title>
    <link href="https://www.fomal.cc/posts/f1740f63.html"/>
    <id>https://www.fomal.cc/posts/f1740f63.html</id>
    <published>2024-01-06T03:31:36.000Z</published>
    <updated>2024-01-06T03:52:23.373Z</updated>
    
    <content type="html"><![CDATA[<h1>Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence骨架序列的目标表示自监督学习框架</h1><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下Abstract段落</p><h2 id="Abstract">Abstract</h2><p>Abstract:<br>Self-supervised pre-training paradigms have been exten-<br>sively explored in the field of skeleton-based action recog-<br>nition. In particular, methods based on masked predic-<br>tion have pushed the performance of pre-training to a new<br>height. However, these methods take low-level features,<br>such as raw joint coordinates or temporal motion, as pre-<br>diction targets for the masked regions, which is subopti-<br>mal. In this paper, we show that using high-level contex-<br>tualized features as prediction targets can achieve supe-<br>rior performance. Specifically, we propose Skeleton2vec,<br>a simple and efficient self-supervised 3D action represen-<br>tation learning framework, which utilizes a transformer-<br>based teacher encoder taking unmasked training samples<br>as input to create latent contextualized representations as<br>prediction targets. Benefiting from the self-attention mech-<br>anism, the latent representations generated by the teacher<br>encoder can incorporate the global context of the entire<br>training samples, leading to a richer training task. Ad-<br>ditionally, considering the high temporal correlations in<br>skeleton sequences, we propose a motion-aware tube mask-<br>ing strategy which divides the skeleton sequence into sev-<br>eral tubes and performs persistent masking within each<br>tube based on motion priors, thus forcing the model to<br>build long-range spatio-temporal connections and focus on<br>action-semantic richer regions. Extensive experiments on<br>NTU-60, NTU-120, and PKU-MMD datasets demonstrate<br>that our proposed Skeleton2vec outperforms previous meth-<br>ods and achieves state-of-the-art results. The source code<br>ofSkeleton2vec is available at <a href="https://github.com/Ruizhuo-Xu/Skeleton2vec">https://github.com/Ruizhuo-Xu/Skeleton2vec</a>.</p><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下1. Introduction段落</p><h2 id="1-Introduction">1. Introduction</h2><p>Introduction:<br>Human action recognition has significant applications in the<br>real world, such as security, human-robot interaction, and<br>virtual reality. The development of depth sensors and ad-<br>vancements in pose estimation algorithms [4, 12, 41] have</p><p>propelled skeleton-based action recognition into a popular<br>research topic, owing to its computational efficiency, back-<br>ground robustness, and privacy preservation. A series of<br>fully-supervised skeleton-based human action recognition<br>methods have been developed using CNNs [10, 19], RNNs<br>[24, 46], and GCNs [5, 43]. Despite their promising per-<br>formance, these methods rely on large amounts of manu-<br>ally annotated data, which is expensive, labor-intensive, and<br>time-consuming to obtain. This circumstance motivates us<br>to explore self-supervised representation learning for 3D ac-<br>tions.<br>Earlier works [21, 29, 33, 47] have employed various<br>pretext tasks, such as motion prediction, jigsaw puzzle<br>recognition, and masked reconstruction, to learn 3D ac-<br>tion representations. Recently, contrastive learning meth-<br>ods [15, 22, 28, 30] have gained prominence. However,<br>these methods often require carefully designed data aug-<br>mentations and tend to encourage the encoder to learn more</p><p>global representations, thereby neglecting local spatiotem-<br>poral information. With the rise of transformer models [37],<br>self-supervised pre-training methods based on masked pre-<br>diction tasks have become mainstream in visual represen-<br>tation learning [15, 22, 28, 30]. Works like SkeletonMAE<br>[39, 42] and MAMP [27] have attempted to transfer MAE<br>[17] methods to the field of 3D action representation learn-<br>ing, achieving promising results. However, these MAE-like<br>methods inefficiently utilize model capacity by focusing on<br>low-level high-frequency details with raw joint coordinates<br>or temporal motion as learning targets, which is subopti-<br>mal for modeling high-level spatiotemporal structures. We<br>believe that using higher-level prediction targets will guide<br>the model to learn better representations and improve pre-<br>training performance.<br>Motivated by this idea, we propose Skeleton2vec, a sim-<br>ple and efficient self-supervised framework for 3D action<br>representation learning. Addressing the limitations of ex-<br>isting MAE-like methods, as illustrated in Fig. 1, Skele-<br>ton2vec leverages contextualized prediction targets. Fol-<br>lowing the work of data2vec [1, 2], we employ a teacher<br>encoder that takes unmasked training samples to generate<br>latent contextualized representations as targets. We then<br>use a student encoder, taking a masked version of the sam-<br>ple as input, combined with an asymmetric decoder to pre-<br>dict data representations at the masked positions. The entire<br>model is based on the vanilla transformer architecture. The<br>self-attention mechanism ensures that the constructed tar-<br>gets are contextualized, incorporating information from the<br>entire sample, making them richer than isolated targets (e.g.<br>raw joint coordinates) or targets based on local context (e.g.<br>temporal motion).<br>Additionally, considering the strong spatiotemporal cor-<br>relations in 3D skeleton sequences, we propose a motion-<br>aware tube masking strategy. Initially, we divide the in-<br>put skeleton sequence along the temporal axis into multiple<br>tubes, where frames within each tube share a masking map<br>to avoid information leakage from neighboring frames. This<br>forces the model to extract information from distant time<br>steps for better prediction. We then guide the sampling of<br>masked joints based on the spatial motion intensity of body<br>joints within each tube. Joints with higher motion inten-<br>sity will be masked with higher probability, allowing the<br>model to focus more on spatiotemporal regions with rich ac-<br>tion semantics. Compared to random masking, our method<br>better utilizes the spatiotemporal characteristics and motion<br>priors of 3D skeleton sequences, effectively improving pre-<br>training performance.<br>In summary, the main contributions of this work are<br>three-fold:<br>• We propose the Skeleton2vec framework, which uses<br>contextualized representations from a teacher encoder as<br>prediction targets, enabling the learned representations to<br>have stronger semantic associations.<br>• We introduce a motion-aware tube masking strategy that<br>performs persistent masking of joints within tubes based<br>on spatial motion intensity, forcing the model to build bet-<br>ter long-range spatiotemporal connections and focus on<br>more semantic-rich regions.<br>• We validate the effectiveness of our method on three<br>large-scale 3D skeleton-based action recognition datasets<br>and achieve state-of-the-art results.</p><h2 id="2-Related-Work">2.Related Work</h2><h3 id="2-1-Self-supervised-Skeleton-based-Action-Recognition">2.1. Self-supervised Skeleton-based Action Recognition</h3><p>Previous studies [21, 33, 47] on self-supervised representa-<br>tion learning for skeleton-based action recognition utilize<br>various pretext tasks to capture motion context. For in-<br>stance, LongTGAN [47] leverages sequence reconstruction<br>to learn 3D action representations. P&amp;C [33] employs a<br>weak decoder to enhance representation learning. MS2L<br>[21] employs motion prediction and jigsaw puzzle tasks.<br>Yang et al. [44] introduce a skeleton cloud colorization<br>task. Contrastive learning methods have gained prominence<br>in 3D action representation learning [14–16, 22, 28, 30].<br>AS-CAL [30] and SkeletonCLR [20] utilize momentum<br>encoder and propose various data augmentation strategies.<br>AimCLR [15] introduces extreme augmentations. ActCLR<br>[22] performs adaptive action modeling on different body<br>parts. Despite their remarkable results, contrastive learning<br>methods often overlook local spatio-temporal information,<br>a crucial aspect for 3D action modeling.<br>The surge in popularity of transformers has led to the<br>mainstream adoption of self-supervised pretraining based<br>on masked visual modeling for visual representation learn-<br>ing [3, 17]. SkeletonMAE [39] and MAMP [27] apply the<br>Masked Autoencoder (MAE) approach to 3D action repre-<br>sentation learning. SkeletonMAE employs a skeleton-based<br>encoder-decoder transformer for spatial coordinate recon-<br>struction, while MAMP introduces Masked Motion Predic-<br>tion to explicitly model temporal motion. In this study, we<br>demonstrate that utilizing higher-level contextualized rep-<br>resentations as prediction targets for masked regions yields<br>superior performance compared to directly predicting raw<br>joint coordinates or temporal motion.</p><h3 id="2-2-Masked-Image-Modeling">2.2. Masked Image Modeling</h3><p>BEiT [3] pioneered masked image modeling (MIM) for<br>self-supervised pretraining of visual models, aiming to re-<br>cover discrete visual tokens from masked patches. Subse-<br>quently, various prediction targets for MIM have been ex-<br>plored. MAE [17] and SimMIM [40] treat MIM as a de-<br>noising self-reconstruction task, utilizing raw pixels as the<br>prediction target. MaskFeat [38] replaces pixels with HOG</p><p>descriptors to enable more efficient training and achieve<br>superior results. PeCo [8] introduces a perceptual loss<br>during dVAE training to generate semantically richer dis-<br>crete visual tokens, surpassing BEiT. These works demon-<br>strate superior performance by utilizing higher-level and se-<br>mantically richer prediction targets in MIM. To further en-<br>hance performance, data2vec [1, 2] employs self-distillation<br>to leverage latent target representations from the teacher<br>model output at masked positions. Compared to isolated<br>targets like visual tokens or pixels, these contextualized rep-<br>resentations encompass relevant features from the entire im-<br>age, enabling improved performance.<br>In this research, we introduce the data2vec framework<br>into self-supervised pretraining of skeleton sequences, uti-<br>lizing latent contextualized target representations from the<br>teacher model to guide the student model in learning more<br>effective 3D action representations.</p><h2 id="3-Method">3.Method</h2><h3 id="3-1-Overview">3.1. Overview</h3><p>3.1. Overview<br>The overall framework of Skeleton2vec is shown in Fig. 2.<br>It takes a skeleton sequence I ∈ RTs×V×Cs<br>as input, where<br>Ts is the the number of frames, V is the number of joints,<br>and Cs is the the coordinates of joints. Similar to most vi-<br>sual transformers [9], the skeleton sequence is first divided<br>into fixed-size patches and then linearly transformed into<br>patch embedding E ∈ RTe×V×Ce<br>. After that, we employ<br>the motion-aware tube masking strategy to guide the mask-<br>ing of joints. The teacher model constructs the full con-<br>textualized prediction targets using unmasked training sam-<br>ples, while the student model receives the masked version<br>of the samples and predicts corresponding representations<br>at the masked positions.<br>As our student model, we adopt an asymmetric encoder-<br>decoder architecture, where the encoder operates solely<br>on non-masked tokens. The lightweight decoder inserts<br>masked tokens into the latent representations outputted by<br>the encoder, forming a full set for predicting the targets. The<br>teacher encoder shares the same model structure as the stu-<br>dent. After accomplishing the aforementioned pre-training<br>task, the teacher encoder is retained for downstream task<br>fine-tuning.</p><h3 id="3-2-Model-Architecture">3.2. Model Architecture</h3><p>3.2. Model Architecture<br>Encoder: Following MAMP [27], we first divide the raw<br>skeleton sequence I ∈ RTs×V×Cs<br>into non-overlapping<br>segments I′ ∈ RTe×V×(l·Cs), where Te = Ts/l and l is<br>the length of each segment. A trainable linear projection is<br>then applied to each joint to obtain the embedding:</p><p>where Ce represents the dimension of the embedding. Tem-<br>poral positional embedding Et ∈ RTe×1×Ce<br>and spatial po-<br>sitional embedding Es ∈ R1×V×Ce<br>are then added to the<br>joint embedding to yield the final input:</p><p>For the teacher encoder, the entire set is flattened as input<br>ET ∈ RNT×Ce<br>, where NT = Te × V represents the total<br>number of tokens in the skeleton sequence. For the student<br>encoder, most tokens are masked, and only the unmasked<br>tokens are utilized as input, flattened as ES ∈ RNS×Ce<br>,<br>where NS = Te × V × (1 − m) denotes the number of<br>visible tokens, and m is the masking ratio. Subsequently,<br>Le layers of vanilla transformer blocks are applied to extract<br>latent representations. Each block comprises a multi-head<br>self-attention (MSA) module and a feed-forward network<br>(FFN) module. Residual connections are employed within<br>each module, followed by layer normalization (LN).<br>Decoder: The decoder input D ∈ RTe×V×Ce<br>contains the<br>full set of tokens, including the latent representations of vis-<br>ible encoded tokens ZS<br>e and the inserted masked tokens.<br>Each masked token is represented by a shared learnable<br>vector EM ∈ RCe<br>, indicating missing information to be<br>predicted at that position. Similar to the encoder, spatial<br>positional embedding E′<br>s and temporal positional embed-<br>ding E′<br>t are added to all tokens to assist masked tokens in<br>locating their positions. The decoder employs an additional<br>Ld layers of transformer blocks for masked prediction.</p><h3 id="3-3-Contextualized-Target-Prediction">3.3. Contextualized Target Prediction</h3><p>Rather than relying on isolated raw joints or temporal mo-<br>tion with limited local context, we employ a transformer-<br>based teacher encoder to construct globally contextualized<br>prediction targets, thereby introducing a diverse training<br>task.<br>Contextualized Target Representations: We extract fea-<br>tures from the output of each FFN block in every layer of<br>the teacher encoder and average them to form our training<br>targets. Following data2vec 2.0 [2], the features from each<br>layer are normalized with instance normalization [36] be-<br>fore averaging. Finally, the averaged features are normal-<br>ized by layer normalization to serve as the prediction tar-<br>gets. Normalizing the targets helps prevent the model from<br>collapsing to a trivial solution, and also prevents any sin-<br>gle layer’s features from dominating. The generation of the<br>target representations can be formulated as:</p><p>where IN and LN refer to instance normalization and layer<br>normalization, respectively. ZT<br>l denotes the output of the<br>FFN block in the lth layer of the teacher encoder.</p><p>Target Prediction: Given the output Hd of the student<br>decoder, we employ an additional linear prediction head<br>to regress the contextualized target representations of the<br>teacher:</p><p>Finally, we adopt L2 loss as our learning objective, cal-<br>culating loss only for the masked positions:</p><p>whereM denotes the set of masked positions.<br>Teacher Parameterization: The student model weights θ<br>are updated through backpropagation on the loss gradients.<br>The teacher model weights ∆ are initialized to be the same<br>as the student weights and parameterized during training by<br>taking an exponentially moving average (EMA) of the stu-<br>dent weights:<br>∆ ← τ∆+ (1 − τ)θ, (6)<br>where τ is a hyperparameter controlling the update fre-<br>quency of the teacher weights using a linearly increasing<br>schedule, gradually increasing from an initial value τ0 to 1<br>throughout training.</p><h3 id="3-4-Motion-Aware-Tube-Masking">3.4. Motion-Aware Tube Masking</h3><p>3.4. Motion-Aware Tube Masking<br>We propose the motion-aware tube masking strategy to ad-<br>dress the issue of high spatiotemporal correlations in skele-<br>ton sequences.<br>Tube Division: The tube masking strategy, initially intro-<br>duced by VideoMAE [35], considers the entire video se-<br>quence along the temporal axis as a single tube, sharing the<br>same masking map across different frames. This mitigates<br>the information leakage issue between adjacent frames. Al-<br>though the skeleton sequence is derived from the video, di-<br>rectly applying this single-tube masking strategy to skeleton<br>data is suboptimal due to the inherent structural differences.<br>In video data, the basic units for masking are image patches<br>in each frame. Due to scene motion or camera viewpoint<br>changes, a masked body part like the hand in the first frame<br>may find its correspondence in unmasked regions in later<br>frames far apart, which facilitates long-range dependency<br>modeling. In contrast, the basic units for masking in skele-<br>ton sequences are the joints in each skeleton frame, where<br>the same-order joints have explicit correspondence across<br>frames. As a result, a body part masked in the first skeleton<br>frame will remain masked in all frames, causing a complete<br>loss of information for that part, which makes the masked<br>prediction task overly difficult and harms the model’s learn-<br>ing capability. To address this, as illustrated in Fig. 2a, we<br>empirically divide the skeleton sequence along the time axis<br>into multiple tubes instead of one tube. Each tube shares the<br>same masking map to force the model to extract informa-<br>tion from farther time steps, while different tubes use differ-<br>ent masking maps to avoid joints being masked throughout.<br>The tube division can be represented as:<br>E′ = Reshape(E) ∈ RN×α×V×Ce  ,(7)<br>where α is tube length and N = Te α is number of tubes.<br>Motion-Aware Sampling: Regions with larger motion in-<br>tensity intuitively contain richer semantic information about<br>actions. Therefore, we utilize the spatial motion intensity of<br>each human body joint within a tube as empirical guidance<br>to generate the masking map.<br>Specifically, we first extract the corresponding motion</p><p>sequence M ∈ RTs×V×Cs<br>from the input skeleton se-<br>quence I ∈ RTs×V×Cs<br>by calculating temporal differ-<br>ences of corresponding joint coordinates between adjacent<br>frames:</p><p>Similar to joint embedding in the encoder, we reshape<br>M into non-overlapping segments M′ ∈ RTe×V×(l·Cs) to<br>match the shape of input sequence I′. We then calculate the<br>motion intensity of each joint within a segment as:</p><p>Afterwards, we compute the spatial motion intensity of<br>each body joint within a tube, normalizing it along the spa-<br>tial dimension:</p><p>Finally, we utilize the normalized spatial motion inten-<br>sity to generate a unique masking map for each tube:</p><p>where η is random noise drawn from a uniform distribu-<br>tion between 0 and 1, β is a hyperparameter controlling the<br>influence of spatial motion intensity on sampling, Mi is<br>the masking map for ith tube, K = V × (1 − m) is the<br>number of joints to be masked, and m is the masking ra-<br>tio. By customizing motion-aware masking maps for each<br>tube, the model is encouraged to focus more on seman-<br>tically richer regions, leading to improved spatiotemporal<br>representations.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence骨架序列的目标表示自监督学习框架&lt;/h1&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解6利用三维热图量改善小规模人体动作识别性能</title>
    <link href="https://www.fomal.cc/posts/ba38a36f.html"/>
    <id>https://www.fomal.cc/posts/ba38a36f.html</id>
    <published>2024-01-03T09:00:35.000Z</published>
    <updated>2024-01-06T04:11:11.170Z</updated>
    
    <content type="html"><![CDATA[<h1>Improving Small-Scale Human Action Recognition Performance Using a 3D Heatmap Volume利用三维热图量改善小规模人体动作识别性能</h1><h2 id="摘要">摘要</h2><p>摘要：近年来，基于骨架的人体动作识别引起了广泛的研究关注，提出的识别或分割方法通常在大规模粗粒度动作数据集上进行验证。然而，在使用深度学习方法识别小规模细粒度人体动作方面仍存在研究不足，而这在实际应用中具有更大的实际意义。为填补这一空白，我们提出了一种基于热图伪视频的新方法，并采用适用于所有模态数据集的统一通用模型。利用人体测量学运动学作为先验信息，通过一个专门预训练的模型提取数据集之间的共同人体运动特征。为了克服关节不匹配问题，我们将人体骨架分为五个部分，这是一种简单而有效的信息共享技术。我们的方法在两个数据集上进行了评估，包括公共的护理活动数据集和我们自建的太极动作数据集。通过线性评估协议和微调评估，结果表明我们的预训练模型有效地捕捉了人体动作之间的共同运动特征，并在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。值得注意的是，我们的模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><p>近年来，骨架基础的人体动作识别备受关注，但主要集中在大规模的粗粒度动作数据集上。</p><p>尚缺乏关于使用深度学习方法识别小规模细粒度人体动作的研究，而这在实际应用中更为实际。</p><p>提出了一种新方法，基于热图伪视频和适用于所有模态数据集的统一通用模型。</p><p>利用人体测量学运动学作为先验信息，通过专门的预训练模型提取数据集之间的共同人体运动特征。</p><p>为了解决关节不匹配问题，将人体骨架分为五个部分，采用简单而有效的信息共享技术。</p><p>在两个数据集上进行评估，包括公共的护理活动数据集和自建的太极动作数据集。</p><p>通过线性评估协议和微调评估，结果表明预训练模型有效捕捉了人体动作之间的共同运动特征。</p><p>在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。</p><p>模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><h2 id="1-Introduction">1. Introduction</h2><p><strong>引言</strong></p><p>人体动作识别领域经历了广泛的研究，导致建立了几个包含各种数据模态的人体动作数据集，包括RGB视频、深度、雷达[1]、红外辐射（IR）和骨架坐标[2]。目前的研究主要集中在利用深度和复杂的神经网络来取得令人印象深刻的结果，这得益于构建了大规模的人体动作数据集，如包含13,000个样本的UCF-101 [3]和包含392,000个样本的Kinetics-600 [4]。然而，这些广泛的多模态人体动作数据集的潜力不应仅限于验证这些算法的性能。相反，必须广泛探索它们的应用价值，例如用于辅助小规模细粒度数据集的识别。</p><p>目前的人体动作数据集通常侧重于粗粒度的日常活动，显示出明显的类间差异，并且仅涉及某些身体部位的主要运动。相反，对特定细粒度动作的研究具有显著的实际价值，因为它们可以被视为粗粒度环境中的一般动作。例如，舞蹈可以包括芭蕾舞、萨尔萨舞和探戈等。一个高效的模型可以识别微妙的差异，并在互动任务中提供适当的反馈，同时在动作评估场景中评估运动表现。然而，由于取样条件有限或取样成本高昂，这类数据集中的样本数量明显少于主流数据集。因此，使用深度学习方法对具有有限训练样本的细粒度动作进行识别仍然是一项具有挑战性且值得关注的努力，这构成了本文的主要贡献。</p><p>在创建这些数据集之后，已经提出了许多与人体动作识别任务相关的方法。早期的方法依赖于手工制作的特征来区分不同的动作类别。然而，随着深度学习的出现，各种方法，如2D卷积神经网络（2D-CNN）、长短时记忆（LSTM）、3D卷积神经网络（3D-CNN）、图卷积网络（GCN）和Transformer，被用于识别样本，并且分类准确性稳步提高。目前，在基于骨架的人体动作识别中，研究人员将GCN或Transformer视为必不可少的工具，而在视频人体动作识别中，3D-CNN和TimeSformer [5]很受欢迎。</p><p>上述方法在特定样本模态方面都有优势和局限性。视频模态样本无法通过GCN的图节点表示有效地，而骨架数据不能直接用于CNN的输入。这种差异在视频和骨架动作之间形成了自然的边界。在骨架模态动作样本的情况下，局部关节可能不存在，并且可能发生多人互动动作。之前使用基于GCN的方法通过在剩余关节上执行线性插值来完成骨架结构 [6]，并将每个执行者的输出的平均值作为动作特征 [7]来解决这些情况。然而，补充的骨架结构可能包含不可靠的信息并引入干扰信息。此外，多人特征的平均化忽略了字符之间的互动信息。如果仅使用小规模的细粒度数据集来从头开始训练深度神经网络，网络容易过拟合。先前的研究在图像处理中采用了自适应锐度感知最小化[8]（ASAM）或使用ImageNet数据集 [9]中的预训练模型来解决这些挑战。基于预训练策略，我们的研究致力于利用公共动作数据集对网络进行预训练，并解决不同数据集之间的关节和模态不匹配问题。</p><p>在使用深度神经网络进行小规模人体动作识别时，一些问题，如缺失的骨架数据、互动样本和过拟合，尚未解决。我们提出了一个统一且通用的模型SSCLS用于小规模动作分类。图1显示了我们识别框架的网络结构。与主流基于骨架的人体动作识别任务不同，我们引入了3D热图体，也称为基于热图的伪视频，作为网络输入，而不是视频图像或骨架序列。来自两种不同格式的代表性热图帧在图2中显示。对于视频输入的任何帧，我们使用高分辨率网络（HRNet）[10]估计人体2D姿势。对于原始的3D骨架坐标，我们将（x，y）2D坐标在每个时间步转换成热图。通过这种方式，我们可以获得一个热图h ∈ RC×H×W，其中C、H和W分别指通道、高度和宽度。在计算每个单帧的热图之后，我们沿着时间维度连接这些热图以形成3D热图体H ∈ RC×T×H×W。如图1所示，公共的NTU RGB+D样本 [11]有17通道的输入，对应于17个骨架关节，而我们自建的太极动作 [12]有72个关节。为了解决在使用完整关节信息的样本时不同数据集之间的骨架关节不匹配问题，我们将骨架关节分为五个部分，包括四肢和一个躯干，并将样本均匀分成5通道输入，如图1中标记为红色、绿色、黑色、紫色和橙色，同时确保每个通道都保持其物理含义。当以3D热图体作为网络输入时，我们使用主流的PoseConv3D模型 [13]作为我们的模型骨干。在这项工作中，我们使用NTU RGB+D数据集进行网络预训练，小规模数据集仅用于最终分类头训练或微调预训练网络。</p><p>我们总结本文的贡献如下： • 一个统一的动作表示。我们引入基于热图的伪视频来统一动作输入的格式。这种格式可以将基于视频和基于骨架的样本转换为统一的输入，消除了这些格式之间的障碍。 • 人体测量学运动先验。我们提出并证明人体动作具有共同的运动特征，因此预训练模型骨干可以帮助提取人体特征，以克服网络过拟合并提高在小规模数据集上的识别性能。 • 一个统一的关节分割方法。我们根据人体结构将骨架关节分为五个部分，以充分利用不同数据集之间的数据信息，而不考虑动作数据的格式和采样器数量。 • 最先进的性能。我们在公共的Nursing Activities和我们自建的太极数据集上分别进行各种配置的实验。识别性能和提取特征的t-SNE [14]可视化表明，与先前的SOTA方法相比，我们的识别准确性得到了很大程度的提高，网络的过拟合现象得到了显著缓解。当融合从关节和肢体提取的特征时，识别准确性进一步提高，证明了我们提出的模型的普遍性。我们的实验记录和结果可在网站https://github.com/eunseo-v/SSCLS（于2023年5月10日访问）上找到。</p><h2 id="2-Related-Work">2. Related Work</h2><p>本节旨在概述基于骨骼的人类动作识别研究的最新进展，包括相关数据集、识别算法、小尺度细粒度动作数据集、训练前策略和评估协议。我们还对我们提出的方法与其他相关工作进行了比较分析，包括先前对太极拳动作识别的研究。</p><h3 id="2-1-基于骨架的人体动作数据集">2.1. 基于骨架的人体动作数据集</h3><p>2.1. 基于骨架的人体动作数据集 本节重点介绍基于骨架的人体动作数据集及其获取方法，可以分为三组：基于深度的、基于可穿戴的和基于提取的。其中，Microsoft Kinect系统[15]广泛用于基于深度传感器的3D人体动作提取。两个大规模的人体日常活动数据集，即NTU RGB+D数据集 [11,16]和PKU-MMD数据集 [17]，包含由Kinect V2传感器收集的骨架坐标，并可以使用姿势估计算法提取多个人体动作。然而，这些数据集的准确性受到光强和背景等因素的影响。另一种类型的数据集是使用可穿戴运动捕捉系统获取的，例如Perception Neuron [18]，它利用惯性传感器测量穿戴者的方向和加速度。我们自建的太极动作数据集 [19] 使用Perception Neuron系统收集，仅包含3D骨架坐标。尽管这种方法可以精确捕捉骨架坐标，但仅限于单个采样器的动作，并且测量精度可能受到惯性传感器漂移的影响。</p><p>在基于提取的数据集领域，姿势估计算法已被广泛用于从视频中检测和估计2D人体姿势，实现从动作视频剪辑直接获取骨架坐标。与3D姿势估计器相比，估计的2D骨架更准确且更稳健。值得注意的是，高分辨率网络（HRNet）[10]被提出作为一种预测更精确和空间准确的关键点热图的手段，而MediaPipe [20]则作为一个跨平台的机器学习（ML）框架，可以从视频中提取人体姿势，如MediaPipe Hands [21]，一个实时的设备上手部跟踪解决方案，可以预测手的骨架。这些方法为调整网络输入格式提供了一种方便的方式，也有效地丰富了人体动作数据集。</p><p>在我们的研究中，我们利用NTU RGB+D数据集的骨架坐标作为我们的预训练数据集，该数据集包含60个跨足日常、相互和健康相关动作的动作类别，采集了三个摄像头视角和众多主体。鉴于我们的假设，即人类活动共享共同特征，我们预计预训练数据将包含丰富的动作信息，并因此有助于我们的小规模动作识别任务。</p><p><strong>要点总结：</strong></p><ol><li>介绍了基于骨架的人体动作数据集，分为深度传感器、可穿戴系统和提取方法三类。</li><li>提到Microsoft Kinect系统广泛用于3D人体动作提取，NTU RGB+D和PKU-MMD数据集是两个使用该系统的大规模数据集。</li><li>另一类数据集使用可穿戴运动捕捉系统，如Perception Neuron，但受到单一采样器和传感器漂移的限制。</li><li>在基于提取的数据集中，介绍了姿势估计算法在从视频中获取2D人体姿势方面的应用，以及使用高分辨率网络（HRNet）和MediaPipe的方法。</li><li>本研究使用NTU RGB+D数据集的骨架坐标作为预训练数据，认为预训练数据将有助于小规模动作识别任务。</li></ol><h3 id="2-2-人体动作识别方法">2.2. 人体动作识别方法</h3><p>2.2. 人体动作识别方法</p><p>由于方法部分不是本文的主要焦点，我们将简要介绍人体动作识别。先前的方法依赖于手工制作的描述符来表示人体动作，成功程度各异。例如，Wang等人[22]尝试通过将摄像机运动纳入其方法来改进基于运动的描述符。与此同时，Oreifej和Liu [23]开发了一种称为4D定向梯度直方图（HON4D）的新型描述符，它创建了时间和空间的4D投影仪。这些描述符是专门设计的，对所有数据集的泛化性较差。近年来，基于深度学习的方法在动作识别研究中变得流行[24]。这些方法主要集中在将基于骨架的序列作为序列问题使用，提出了基于LSTM的模型[25–28]，用于构建序列的不同空间对齐的时间关系。 研究人员[29–32]还探讨了使用伪图像表示3D骨架序列的方法，这允许使用预训练的主流CNN模型进行特征提取，而无需从头开始训练。例如，Wang和Li [31]提出将3D骨架序列转换为关节轨迹图，然后使用CNN核进行分类。然而，这些方法因分解骨架序列的物理结构且缺乏时空动态而受到批评。 为了解决这些局限性，Kipf等人[33]开发了一种有效的卷积神经网络（CNNs）变体，可直接在图上运行，用于处理图结构化数据。该方法将CNNs推广到任意结构的图上，使其非常适用于表示3D骨架序列，因为这可以自然地看作是图数据。Yan等人[34]随后提出了一种称为时空图卷积网络（ST-GCN）的新模型，该模型基于Kipf的方法，并可以自动学习关节的空间配置和时间动态。该模型已成为后续动作识别研究的基准比较。后续基于GCN的工作主要包括自适应空间图卷积核或双向骨架序列[35–38]，以及论文[35]中的Ck核计算每帧中任意两个关节的相似度，类似于注意机制。Qin和Liu [39]在原始输入的通道维度中引入了角度编码，以融合高阶特征。尽管具有自注意模块的Transformer架构[40]在自然语言处理（NLP）任务中已成为事实上的标准，但其结构已在包含计算机视觉[42]和长序列时间序列预测[43]的各种研究领域中得到广泛应用。在基于骨架的人体动作识别领域，研究人员提出了一些方法，例如时空Transformer网络（ST-TR）[7]和群体活动识别网络GroupFormer [44]。这些方法利用Transformer的自注意机制来模拟关节之间的依赖关系，并共同捕获时空上下文信息。观察到与使用骨架坐标作为输入的基于GCN的方法相比，基于Transformer的方法在性能上表现出色。 视频理解是另一个使用视频剪辑作为输入的人体动作识别领域[45]。这些视频数据集中的剪辑通常来自涵盖广泛人体动作相关类别的互联网视频。Karpathy等人[46]收集了Sports-1M数据集，并提出了一种具有上下文流的多分辨率CNN架构，该流模型低分辨率图像，而视觉流模型高分辨率中心裁剪图像。Simonyan和Zisserman [47]提取了光流并提出了一种两流ConvNet，用于使用单帧图像和多帧光流捕获空间和时间信息。Jing等人[48]试图在复杂场景中识别动作。具体而言，他们提出了一个联合损失的时空神经网络模型，用于在视频中识别人体动作，并使用提取光流和外观的两流网络来捕获空间特征，以及使用LSTM网络来建模时态动态。Tran等人[49]将3×3 2D扩展到3×3×3 3D卷积核，并学习由简单线性分类器对齐的C3D特征用于视频理解。由于视频稀缺，3D CNN很难训练。Carreira和Zisserman [4]扩展了2D核并提出了一种两流扩张3D ConvNet（I3D）。I3D模型可以使用在大图像数据集上预训练的相应2D CNN网络初始化，有助于网络训练。由于从视频中提取光流耗时，一些工作[50,51]尝试应用基于幻觉的方法来学习扩充网络输入。例如，Wang等人[50]试图通过一种幻觉步骤将I3D特征图的输出转换为Fisher向量表示。唐等人[51]提出了一种从外观输入中想象光流特征的网络，以节省计算成本。Feichtenhofer等人[52]提出了一个SlowFast网络框架，其中慢路径具有低帧速率和更高通道以捕获空间特征，而快路径可以学习具有高帧速率和更轻通道的时态嵌入。3D CNN核，如C3D和I3D，可以填充到该框架中以进行识别任务。随着注意机制的流行，工作[53]将时空非局部操作应用于视频识别任务，而论文[5,54]的作者则将标准Transformer架构调整到不同技能的大规模视频数据集识别中。 在我们之前的工作[12]中，我们使用3D骨架坐标作为太极动作识别的输入。然而，由于预训练的NTU RGB+D数据集与太极数据集之间的数据格式和收集系统差异，即使在网络管道中应用了批归一化，识别性能仍然相对较差。为了解决这个问题，Duan等人[13]提出使用3D热图体代替传统的图序列来表示人体骨架，这比3D骨架坐标具有几个优势。例如，由于测量可能在不同数据集之间有所不同，使用3D骨架坐标可能导致相同动作的显著差异，需要在数据集之间进行归一化。使用3D热图体可以直接解决这种归一化问题，同时允许应用视频处理技巧，如居中和裁剪，以丰富训练数据集。此外，多人动作骨架可以投影到单个热图上，我们可以利用视频理解领域的主流方法，以在不增加额外计算成本的情况下实现稳定和更好的性能。然而，GCN的计算随着一个动作中采样器数量的增加而呈线性扩展。在这项工作中，我们的目标是将基于图的骨架坐标转换为3D热图体，以实现在不同数据集之间共享人体动作特征，并缓解数据格式不匹配的问题。对于特征提取，我们将使用PoseConv3D网络作为我们的模型骨干，该网络在性能上表现优越，超过了先前的动作识别方法。</p><p><strong>要点总结：</strong></p><ol><li>介绍了人体动作识别的方法，强调了先前依赖手工制作描述符的方法和近年来基于深度学习的方法。</li><li>概述了基于骨架的序列的传统方法，如使用LSTM模型，以及对3D骨架序列使用伪图像的方法，以便利用预训练的CNN模型进行特征提取。</li><li>引入了基于图的CNN变体，特别是ST-GCN模型，以及后续工作中基于GCN和Transformer的方法，这些方法在处理骨架坐标作为输入时表现出优越性能。</li><li>概述了基于视频的动作识别方法，包括对视频理解领域的关键工作，如I3D模型和SlowFast网络。</li><li>提到先前的工作中使用3D骨架坐标的问题，引出了使用3D热图体的解决方案，以便在不同数据集之间共享人体动作特征，并介绍了PoseConv3D网络作为特征提取的模型骨干。</li></ol><h3 id="2-3-小规模细粒度数据集研究">2.3. 小规模细粒度数据集研究</h3><p>2.3. 小规模细粒度数据集研究 研究人员致力于通过新颖的神经网络提高在流行的大规模粗粒度数据集上的识别性能。创建特定的小规模细粒度数据集可能更为有益。Weinland等人[55]创建了一个包含来自多摄像头的11个动作的IXMAS数据集。Nicora等人[56]建立了MoCA数据集，该数据集包含每个活动的3个相机视图中的20个细粒度烹饪动作。此外，MoCA是一个双模态数据集，收集了烹饪场景中的运动捕捉数据和视频序列。这些特定数据集可以证明所提出模型的细粒度识别能力，有助于在特定场景中的应用。Gu等人[57]发布了一个包含注释篮球比赛视频的细粒度篮球动作数据集。他们提出了一个集成了NTS-Net的两流网络，用于提取其细粒度数据集的判别特征。 Wu和Shao [58]提出了一种多最大间隔支持向量机（MMM-SVM），以提高IXMAS数据集的准确性，采用了多视图系统。Wang等人[59]使用内部迁移学习策略增强了小规模数据集的性能。这种策略未使用其他数据集，需要从候选模型列表中选择最佳模型，这在应用中可能需要更复杂。Shen等人[60]提出了一种自动数据增强模型，称为Imaginative Generative Adversarial Network，它可以从学习的数据集分布中采样新数据。增强的数据集可以提高使用相同神经网络的分类准确性。这种方法需要相对较大规模的数据集，而由于样本不足，小规模数据集仍然难以训练。Ijaz等人[6]提出了一种多模态基于Transformer的网络，用于提取和融合骨骼关节和加速度数据的特征信息，以提高小规模细粒度护理活动[61]的识别性能。然而，他们没有使用NTU-RGB+D数据集对网络进行预训练，只引入了自适应锐度感知最小化（ASAM）[8]来收敛他们的Transformer模型。他们还在结论中推断，可以探索预训练骨架分支以进一步提高模型的收敛性，这在我们的工作中已经实施，并陈述了预训练管道的优越性。Goyal等人[62]试图利用大规模预训练表示，假设它们隐含地包含小规模数据集任务的相关线索。该论文还应用了视图智能批量归一化以最小化跨视图动作识别任务的内部协变量转移。这种方法对可以提取为运动特征的哪一层输出进行了实验性分析，并没有详细描述人体数据集之间的隐含相关线索，这些都将在我们的论文中解决。 与普遍的粗粒度动作不同，太极动作可以视为专业的动作类别，每个太极动作由多个与身体运动协调的元动作组成。与日常人体动作相比，需要关注不同人体部位在不同阶段的运动特征。在太极动作中，由于太极动作类别之间的相似性，识别是具有挑战性的。由于太极已成为亚洲运动会的项目，有效的识别网络可以帮助初学者评估他们的活动。在太极相关的动作识别工作中，Lin等人[63]提出了一个包含17个类别的大深度包含人体动作（DHA）视频数据集。他们将所有太极动作视为相同的类别。Sun等人[64]提出了一个包含58个太极动作的细粒度太极数据集，总共有2772个样本。所有视频样本都来自具有动态背景的网站。他们还应用了改进的密集轨迹特征和Fisher向量表示进行识别，并取得了51.39%的识别准确度。Dong等人[65]还提出了一个名为“Sub-Tai chi”的太极数据集，包含15个动作，并应用了带有注意模块的结构LSTM进行识别；他们在自己的数据集上达到了79%的识别准确度。Liu等人[66]在他们自己的太极数据集上应用了ST-GCN模型，取得了89.22%的识别准确度。所有这些研究都创建了自己的太极数据集，并且不能在网站上进行比较。他们提出了具体的或应用了流行的深度学习方法进行识别，对于细粒度动作识别而言并不通用。 我们之前的工作在太极动作上进行了初步研究。我们首先创建了太极动作数据集，并提出了一种使用节点轨迹特征的太极动作识别算法[19]。我们提取了手工制作的特征，并且仅使用了一个单一节点进行特征提取，在小规模训练集下表现不佳，通用性可能更好。我们引入了深度学习方法，并提出了一种用于太极动作识别的空间变换网络[12]。我们使用了NTU RGB+D数据集对模型进行预训练，以解决过拟合网络问题。太极训练样本仅负责使用冻结的模型骨干参数进行最终分类头培训。该算法使用了人体骨架的24个关节点，并相对于第一个传统方法取得了改进。然而，它仅部分利用了太极数据集的72个骨架关节，导致了我们的细粒度太极动作缺失信息。网络识别还需要进行复杂的数据预处理。此外，小规模训练集下的性能仍需要改进，并且结果的分析较少呈现给读者。我们的论文包含准确性提高和消融研究，将解决这些弱点。 我们打算在我们的工作中为这些小规模细粒度动作数据集提供一个通用框架。我们假设样本在人体动作数据集之间共享共同的运动特征，并提出了一个简单但有效的模型框架来解决小规模细粒度动作识别问题，该问题已在公共护理活动数据集和我们自建的太极数据集上进行了评估。该模型框架不引入对抗网络，可以识别小规模细粒度动作数据集。</p><p><strong>要点总结:</strong></p><ol><li>研究人员致力于改善大规模粗粒度数据集上的识别性能，特别关注了小规模细粒度数据集的建立。</li><li>提到了不同研究中创建的小规模细粒度数据集，如IXMAS、MoCA、篮球动作数据集等，以及它们的用途和特点。</li><li>讨论了先前研究采用的一些方法，包括支持向量机、迁移学习、数据增强、多模态Transformer等，以提高小规模细粒度数据集的性能。</li><li>强调太极动作作为细粒度动作类别的挑战性质，以及太极数据集的创建和其他研究。</li><li>总结了先前关于太极动作的研究，包括使用节点轨迹特征和空间变换网络进行识别，指出了它们的局限性和需要改进的地方。</li><li>提出了在工作中旨在为小规模细粒度动作数据集提供通用框架，假设样本在不同数据集之间共享运动特征。</li></ol><h3 id="2-4-预训练策略和评估协议">2.4. 预训练策略和评估协议</h3><p>2.4. 预训练策略和评估协议 在计算机视觉和视频理解领域，许多研究人员通过线性评估协议或微调评估验证他们的方法。在线性评估协议中，我们冻结预训练的网络参数，仅对目标数据集训练最终分类器。此外，在微调评估中，所有网络参数都与新数据集进行微调。例如，MoCo-V1 [67]首先在ImageNet-1M [9]数据集上进行无监督预训练，然后冻结参数并重新训练一个监督线性分类器。这种无监督学习的主要目标是学习可转移的特征。这种策略可以证明提取特征的有效性。Simonyan和Zisserman [47]使用了三种评估方法来衡量其空间流ConvNet的性能，包括在目标UCF-101数据集上从头开始训练，基于ILSVRC-2012数据集的预训练线性评估协议，以及在UCF-101上训练分类器并进行微调。I3D模型 [4]提出了充气的3D CNN核，可以在2D ImageNet模型上进行预训练，并将相关的2D CNN核引导到3D CNN核以进行初始化。I3D模型还通过首先在Kinetics上进行预训练，然后在HMDB-51 [68]和UCF-101数据集上进行微调来评估其出色的迁移学习能力。 如上所述，预训练策略总是与评估协议相配。在计算机视觉领域的下游任务中，如目标检测 [69]、语义分割 [70] 和人体姿态估计 [71]，研究人员将在ImageNet数据集上预训练的网络模型用于微调其下游任务。在视频理解领域，I3D模型使用在ImageNet数据集上预训练的2D卷积网络的参数来初始化其3D卷积网络的权重参数，从而加速网络的收敛。我们观察到，这些下游任务的数据集都是自然图像或视频样本。此外，在ImageNet上预训练的模型具有相似的现象，即从层次网络的第一层到最后一层的提取特征逐渐从通用性变化为任务特定性 [72]。具体而言，前几层可以捕获样本的低级特征，例如图像的颜色斑点特征，而最后一层的输出则是高级和任务特定的。因此，在ImageNet上预训练的模型骨干可以提取自然图像的低级边缘和纹理特征，并加速在其他自然图像或视频数据集的下游任务中的模型收敛，以实现与从头开始训练相比更好的性能。 在本文中，我们打算使用SlowFast构建一个通用且准确的模型框架。与基于骨架的人体动作识别方法不同，我们将基于骨架的动作序列转换为基于热图的伪视频。我们利用大规模数据集提取热图的常见低级运动特征，这有助于改善我们的小规模细粒度动作数据集上的识别性能。以前的基于骨架的动作识别任务只是使用大规模基于骨架的动作数据集NTU RGB+D从头开始训练其提出的模型。对于足够大规模的数据集，从头开始训练的模型也可以展现出令人印象深刻的性能 [7,13]。</p><p>然而，在小规模数据集的下游任务中，预训练策略是提高识别性能的有效方法 [73]。因此，我们基于预训练策略的模型框架对于小规模动作识别任务是必要的，这在先前的工作中从未得到解决 [6,57,59,60]。我们还通过直观的t-SNE [14]可视化给出了完整的分析，以展示预训练策略的有效性。 我们的动作研究使用了两种不同目的的评估。线性评估协议表明我们的预训练模型可以通过基于热图的伪视频样本学习共同的人体运动特征，而微调评估实验证明了我们框架的迁移学习能力，并与从头开始训练网络相比展现出卓越的动作识别性能。</p><ol><li><strong>评估协议：</strong><ul><li>计算机视觉和视频理解领域的研究者使用线性评估协议或微调评估来验证方法。</li><li>线性评估协议中，冻结预训练网络参数，只训练目标数据集的最终分类器。</li><li>微调评估中，对新数据集进行所有网络参数的微调。</li><li>以MoCo-V1为例，首先在ImageNet-1M数据集上进行无监督预训练，然后通过线性评估协议验证特征可转移性。</li></ul></li><li><strong>预训练策略的有效性：</strong><ul><li>在计算机视觉领域的下游任务中，如目标检测、语义分割和人体姿态估计，使用ImageNet上预训练的模型可加速收敛。</li><li>模型在ImageNet上的预训练使其能够在其他自然图像或视频数据集的下游任务中表现更好。</li></ul></li><li><strong>本文模型框架：</strong><ul><li>使用SlowFast构建通用准确的模型框架。</li><li>不同于骨架方法，将基于骨架的动作序列转换为基于热图的伪视频。</li><li>利用大规模数据集提取热图的共同低级运动特征，以改善小规模细粒度动作数据集上的性能。</li></ul></li><li><strong>小规模数据集的挑战：</strong><ul><li>在小规模数据集的下游任务中，预训练策略对提高识别性能是有效的。</li><li>本文模型框架基于预训练策略，专注于解决小规模动作识别任务，这是以往工作未解决的问题。</li></ul></li><li><strong>两种评估方法：</strong><ul><li>线性评估协议展示预训练模型学到共同人体运动特征的能力。</li><li>微调评估实验展示框架的迁移学习能力，相较于从头开始训练，表现出更优越的动作识别性能。</li></ul></li></ol><h2 id="3-Methods">3. Methods</h2><p>我们在3.1节中详细描述了从三维骨骼关节坐标到三维热图体积的动作序列转换。然后我们在第3.2和3.3节中介绍了我们的模型框架和培训策略。</p><h3 id="3-1-从关节坐标到3D热图体积">3.1. 从关节坐标到3D热图体积</h3><ol><li><strong>PoseConv3D方法：</strong><ul><li>对于PoseConv3D方法，动作数据应转换为3D热图体积。</li><li>在PoseConv3D方法[13]中，使用在COCO-keypoints上训练的2D姿势估计器HRNet [10]直接从动作视频剪辑中提取2D人体姿势。</li><li>Nursing Activities和Tai Chi动作数据集仅包含帧内的3D骨架坐标，需要转换为3D热图体积。</li></ul></li><li><strong>坐标转换及数据格式处理：</strong><ul><li>不采用PoseConv3D建议的将3D骨架（x，y，z）分成三个2D骨架的建议，而是使用（x，y）、（y，z）和（x，z）分别，因为有些数据集可能直接由相机收集，3D坐标提取不可行或噪声干扰，影响预训练或微调模型。</li><li>数据格式（y，z）和（x，z）因未固定采样器到相机的距离而缺乏物理含义，因此仅使用2D坐标（x，y）形成3D热图体积，z轴坐标可用于通过沿y轴旋转进行数据增强。</li></ul></li><li><strong>多视角和数据增强：</strong><ul><li>每个动作从三个固定相机视角同时捕捉，可获取相同动作的多视角样本。</li><li>还可以通过沿y轴旋转进行透视变换访问多视角样本，如图3b所示。</li><li>3D骨架坐标序列X ∈ RT×V×C，其中T，V，C表示帧数、关节数和坐标数，包含运动捕捉系统收集的V个骨架关节的三个坐标。</li><li>为了充分利用收集到的3D骨架序列，对每个小规模训练样本执行旋转和剪切操作。</li></ul></li><li><strong>2D热图构建：</strong><ul><li>对于每个2D关节坐标（xk，yk），在方程（3）中，Hkij是像素（i，j）处的关键点热图值，ck是（xk，yk）的置信度分数，σ控制了高斯图的方差；Hvij是肢状热图中像素（i，j）的值。</li><li>利用方程（4）计算所有这些像素的热图值，其中d2_ab是从像素到骨ab的投影距离，因此当像素位于肢上时，该值始终为1。</li><li>计算所有关节和骨的热图值后，像素（i，j）的最终热图值是这些V个热图值的最大值，如方程（5）所示。</li><li>图4说明了原始3D关节坐标和太极动作样本在一个帧上的最终2D关键点和肢状热图。</li></ul></li><li><strong>3D热图体积的处理：</strong><ul><li>完成2D热图构建后，将它们组合成跨所有帧的3D热图体积。</li><li>由于网络的输入是两个具有不同关节数的数据集，需要重新构建热图通道。</li><li>采用两种策略来关联两个数据集的信息：一种是从72个太极关节中选择与NTU RGB+D数据集匹配的17个关节；第二种是将整个骨架分为五个部分 - 四肢和一个主干。</li><li>将所有2D热图沿时间维度连接，构建3D热图体积H ∈ RC×T×H×W。</li><li>进行一系列的数据处理策略，将体积固定为48，采用均匀采样。</li><li>遵循中心裁剪策略[13]，找到包围所有帧的2D姿势的最小框，进行随机裁剪，这是视频理解中的一种流行处理方法。</li><li>热图体积将调整为56×56的分辨率，并有50％的概率翻转骨架的右侧和左侧以丰富数据集。</li><li>与其他识别方法相比，我们的预处理数据方法更为简洁，没有零填充[34]或数据插值策略[12]的限制，对采样器数量也没有限制。</li></ul></li></ol><h3 id="3-2-模型架构">3.2. 模型架构</h3><p>我们的3D热图体积可视为一种特殊的视频格式。我们将关节分割后的3D热图体积作为网络输入，如图1所示。我们的网络骨干显示在图1的中间模型骨干部分，遵循PoseConv3D [13]的配置，详细信息见表1。他们采用了SlowOnly [52]方法，将ResNet层直接从2D扩展到3D的最后两个阶段作为其模型骨干。中间列出了每个残差块的组成部分，右侧说明了相应块之后的输出特征大小。核的尺寸用{T× S2, C}表示，分别代表时间、空间和通道的大小。非退化的时间滤波器（时间核尺寸 &gt; 1）用下划线标示。残差块用括号表示，实例化的骨干是ResNet-50 [76]。从图1中可以看出，在时间维度上没有应用下采样操作。最后，动作嵌入将通过线性分类头传递，其中包含全局平均池化（GAP）层和全连接（FC）层，以生成预测的动作类别。</p><p>相对于ResNet-50模型，我们从模型骨干中移除了ResNet2块，因为我们已经使用骨架坐标从视频中提取了姿势特征。我们在第2.4节详细说明了我们的分层网络逐渐从通用特征提取到任务特定特征。由于基于热图的伪视频样本缺乏背景信息，不包含任何低层特征。因为我们的输入相对于自然图像已经是中层特征，所以具有浅层和窄通道的网络模型更加适用。与X3D [77]和I3D模型 [4]相比，我们的模型骨干更轻便，提高了基于热图的伪视频输入的特征提取效率。我们计划使用每个输入样本中均匀采样的48帧更密集的时间输入，使我们能够捕捉更多的时间动态。在模型框架中，沿时间维度没有下采样操作，其他操作遵循ResNet-50结构以提高模型骨干的鲁棒性。</p><h3 id="3-3-训练策略">3.3. 训练策略</h3><p>在NTU RGB+D动作识别研究中，段等人[13]在基于骨架的跨视角（CV）和跨主体（CS）设置中取得了最先进的准确性。然而，他们使用从视频中提取的2D骨架坐标数据，并丢弃了原始数据集提供的3D骨架坐标，而这些坐标在其他识别方法中被使用。图7a显示了原始3D骨架坐标的可视化，图7b是从RGB样本视频中提取的2D坐标的热图。对于这个喝水的动作，采样环境中有两个人，只有一个人在喝水，另一个是干扰采样者。原始3D骨架坐标有两个采样器，数据过于模糊，难以直观识别。先前的工作筛选了原始数据，并利用去噪后的数据进行动作识别。这可以从采样器中提取正确的采样器，但采样环境总是影响数据的一致性。当他们无法从原始样本中提取正确的采样器时，他们必须将两个采样器的数据输入到网络中，并计算提取特征的平均分数。NTU RGB+D数据集中有交互样本，因此采样器的数量根据动作类别而异。先前的工作必须将单人动作样本的第二个人填充为零值，这对于批量计算很方便。图7b反映了一个更准确的喝水动作，视频格式没有零填充问题，这对应用来说是简洁的。在我们的预训练阶段，我们继续使用从NTU RGB+D视频数据集中提取的2D坐标，这有助于更好地学习普通人类动作特征，并减少网络的过拟合。在训练过程中，如图1所示，我们首先使用NTU RGB+D数据集对模型骨干进行预训练。对于小规模细粒度动作识别，我们分别尝试线性评估协议和冻结所有预训练骨干参数或微调整个网络的微调评估，以验证提出的模型框架的有效性。由于我们可以生成关节和肢体格式的3D热图体积，我们还尝试了伪热图及其融合策略，详细内容在第4节讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Improving Small-Scale Human Action Recognition Performance Using a 3D Heatmap Volume利用三维热图量改善小规模人体动作识别性能&lt;/h1&gt;
&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;摘要：</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>备战1月</title>
    <link href="https://www.fomal.cc/posts/7a7d65fb.html"/>
    <id>https://www.fomal.cc/posts/7a7d65fb.html</id>
    <published>2024-01-02T02:56:51.000Z</published>
    <updated>2024-02-22T02:22:46.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>如何快速精通一个领域</p><blockquote><p>尝试更新一下自己的个人学习系统</p><p>自己之前都是过多关注于刷了多少课，却没有关注学习效果 同时导致学的太多 累了之后懒得去复习和反馈应用，应该关注学到的知识和应用</p></blockquote></blockquote><p><img src="../assets/image-20240102105700646.png" alt="image-20240102105700646"></p><h1>备赛</h1><blockquote><p>先练手 开始刷题</p><p>在深入学习对应的不会的知识点</p></blockquote><p>java 公开课<a href="https://www.bilibili.com/video/BV1MA4116793?p=28&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">试题一：指数计算_哔哩哔哩_bilibili</a></p><p><a href="https://pan.baidu.com/s/1lELNX9iYDW6FN5b1pOX4Gg?pwd=6fo6">2020年蓝桥杯JavaC组省考题目.pdf_免费高速下载|百度网盘-分享无限制 (baidu.com)</a></p><ul><li>[ ] 19年老视频<a href="https://www.bilibili.com/video/BV1Lb4y1k7K3?p=23&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">22-动态规划入门视频讲解_哔哩哔哩_bilibili</a></li></ul><p>java备考方法论</p><blockquote><p>主要是关于对应的算法题目的一些拿分技巧 建议后期掌握一下</p><p><a href="https://www.bilibili.com/video/BV1NZ421z7WV/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">Java组经验分享| 蓝桥杯拿分技巧_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p>下边方法来源于<a href="https://www.bilibili.com/video/BV1Hb4y1W7Pn/?spm_id_from=333.880.my_history.page.click">【蓝桥杯】蓝桥杯备赛攻略/算法学习路线/经验分享_哔哩哔哩_bilibili</a></p></blockquote><blockquote><p>排序算法 查找算法</p><blockquote><p>八大排序 + 主要练习归并+ 快速排序</p></blockquote><p>难度低的通过率高的题目</p><p>二分查找+双指针+ 练习</p><p>递归+暴力学习+练习【蓝桥杯的基础】</p><p>贪心算法</p></blockquote><blockquote><p>最后学习完算法之后在学习一边数据结构 加强能理解</p><p>搜索【深度搜索【暴力基础 打蓝桥杯最重要的算法，可以骗到很多的分数】+ 广度搜索（练习几道题即可）】</p><p>动态规划杯</p></blockquote><p>暴力 递归 暴力搜索这些练好刷题之后 直接刷蓝桥杯真题</p><p>学习路线<a href="https://www.bilibili.com/video/BV1u84y1w7xt/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">（新生必看）小白或新手如何准备蓝桥杯Java组，以及如何避坑_哔哩哔哩_bilibili</a></p><blockquote><p>java 可以减少一些底层的大佬，好好备赛，应该能拿奖</p><blockquote></blockquote></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100118769.png" alt="image-20240218100118769"></p><blockquote><p>先用洛谷刷题  去练习基础题目</p><p>再用对应真题刷题系统来刷真题</p></blockquote><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100129245.png" alt="image-20240218100129245"></p><p><img src="D:%5C2023.6.2hexo%5Ctest%5Csource%5Cassets%5Cimage-20240218100103648.png" alt="，"></p><blockquote><p>必须要用Main 函数 来实现所有的代码提交，不能写package 包</p></blockquote><p>OA 赛制   有步骤分，也有技巧</p><p>日期计算器  excel 表格 写代码</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;如何快速精通一个领域&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;尝试更新一下自己的个人学习系统&lt;/p&gt;
&lt;p&gt;自己之前都是过多关注于刷了多少课，却没有关注学习效果 同时导致学的太多 累了之后懒得去复习和反馈应用，应该关注学到的知识和应用&lt;/p&gt;
&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>2024.1月</title>
    <link href="https://www.fomal.cc/posts/668a32f0.html"/>
    <id>https://www.fomal.cc/posts/668a32f0.html</id>
    <published>2024-01-02T01:15:04.000Z</published>
    <updated>2024-01-18T07:39:58.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="12月末尾">12月末尾</h2><p>🗓️2023.12.30【日知日进】日历<br>📚日知书目：《人是如何学习的》</p><p>✨日进思考：“普通人和高手的认知差距在哪里?”</p><pre><code> 🔹普通人和高手之间的差距在于知识迁移能力的高下，高手总是能够更有效地迁移知识。是否了解原理决定了能否进行知识的迁移，而高手总是乐于探究事物运行背后的原理。高手往往也具备“元认知”的能力，元认知即“对自己认知的认知”，这种能力相当重要。</code></pre><p>💫共读推荐：<br>🔹《知识大迁移》<br>🔷学习者必须掌握足够多的事实，才能对自己掌握的知识及欠缺部分心中有数。</p><p><img src="../assets/image-20240102220712895.png" alt="image-20240102220701767"></p><p>今天是2023年最后一天啦！<br>年初的目标是否已经完成？未来的日子，又有哪些梦想要实现？</p><p>凡是过往，皆为序章。<br>每个终点也是新的起点，只要拼搏过，时光会看见。<br>2023最后一天，早安！</p><p>今日简报</p><p>12月31日   星期日</p><p>1、阿根廷正式拒绝加入金砖国家。</p><p>2、董宇辉宣布由东方甄选100%控股的新账号1月9日开播。</p><p>3、直播PK输了给对方洗脚？线下约架签“生死状”？长沙警方整治网红乱象。</p><p>4、央行同意支付宝变更为无实际控制人。</p><p>5、元旦假期首日跨省游占55%，中国香港成最火跨年境外目的地。</p><p>6、北京城市轨道交通运营总里程达836公里，超越上海跃居全国第一。</p><p>7、因假冒大凉山农特产品，凉山包括赵灵儿、曲布等多名百万级网红被判刑！</p><p>8、易建联宏远9号球衣正式退役！传奇生涯不会落幕。</p><p>9、一眼望不到头！也门首都大规模集会声援巴勒斯坦。</p><p>10、2024年起USB-C将成为欧盟电子设备通用标准。</p><p>11、马斯克强势回归领跑2023年全球富豪榜。</p><p>12、哈马斯官员称停火前不再交换被扣押人员。</p><p>✨此心不动，随机而动。<br>来帆书APP 听《心学的诞生》</p><p>🎈今天是三末:<br>周末，月末，年末。<br>把过去所有的遗憾、烦恼、病痛通通抹去！</p><p>明天是三新:<br>日新，月新，年新。<br>让喜乐、梦想、健康都新新启程！<br>让2023的一切不顺都变成2024惊喜的铺垫！</p><h1>2024年1月</h1><h2 id="1-1日周一">1.1日周一</h2><h3 id="英语角">英语角</h3><p>无</p><h3 id="1-1晨读">1.1晨读</h3><p>无</p><h3 id="今日简报">今日简报</h3><p>今日简报</p><p>1月1日，星期一，元旦快乐</p><p>1、国家主席习近平发表二〇二四年新年贺词。</p><p>2、家用神车桑塔纳月销仅1辆，又一代经典车型将退出历史舞台。</p><p>3、爱国主义教育法2024元旦起施行：将爱国主义教育贯穿学校教育全过程。</p><p>4、我国调整部分商品进出口关税，对部分抗癌药实施零关税。</p><p>5、国家统计局：2022年全国女性就业人员3.2亿人，占全部就业人员的比重为43.2%。</p><p>6、段永平向浙大再捐10亿+，支持母校“双一流”建设。</p><p>7、经济日报：摘牌倒逼景区提质升级。</p><p>8、7500米深，日本发现大地震海底断层悬崖，此次发现的断层有助于提高地震、海啸等灾害预测的准确度。</p><p>9、印度法院下令释放vivo两高管。</p><p>10、国家统计局：2022年中小学生《国家学生体质健康标准》达到优良的比例为55.1%。</p><p>11、以总理称加沙地带军事行动将持续数月。</p><p>12、美军研发无线能量“网”为无人机空中充电。</p><p>✨以自己的想法安排人生，即便在别人眼里有些离经叛道，至少还有机会活出自我。——《不安的哲学》</p><h3 id="名人语录">名人语录</h3><p>🗓️2024.01.01漫画国学👣<br>『元旦佳节』</p><p>💫【诗歌欣赏】</p><p>《今日歌》<br>【明】文嘉<br>今日复今日，今日何其少。<br>今日又不为，此事何时了。<br>人生百年几今日，<br>今日不为真可惜。<br>若言姑待明朝至，<br>明朝又有明朝事。<br>为君聊赋今日诗，<br>努力请从今日始。</p><p>💫【诗歌鉴赏】<br>这是一首劝勉诗。语言朴实无华，道理浅显易懂，就是叫人珍惜光阴，努力奋进。因为我们整个人生就是由每一个今日组成，每一天都做好今日事，“明日”才不会因为虚度了“昨日”而感到遗憾，那么，由无数个“今日”组成的今生，才会有所作为。</p><h3 id="日知日进">日知日进</h3><p><img src="../assets/image-20240102220932012.png" alt="image-20240102220932012"></p><h2 id="1-2日周二">1.2日周二</h2><p>每个人的花期不同，不必因为有人提前拥有而焦虑。重要的是，当你选择了你要的方式，那就坚定地走下去。</p><h3 id="英语角-2">英语角</h3><h3 id="今日晨读">今日晨读</h3><p>🍁2024.1.2晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。<br>——来自帆书《十分钟冥想》</p><p>👉【精彩选段】<br>冥想不是思考！它所做的不过是将一道明亮的光照耀在你的心灵上，所以你才更清楚地看到一切。</p><p>👉【作者之语】<br>无论我们感受如何，心灵的深层本质就像蓝色的天空一样，并没有发生变化。</p><p>👉【樊老师之语】<br>本书作者用生动故事和精妙比喻帮你破除误区，理解冥想精髓，带领你学习一些简单易学且非常有效的冥想方法，将其融入你的生活、工作和内心。每天给自己十分钟，拥抱清醒、平和与快乐，让我们的生活变得更加丰富而细腻。</p><h3 id="今日简报-2">今日简报</h3><p>今日简报</p><p>1月2日，星期二</p><p>1、百城二手住宅平均价格去年累计下跌3.53%，今年市场有望逐渐筑底企稳。</p><p>2、各地推动开展根治欠薪冬季专项行动，让农民工安“薪”过节。</p><p>3、新疆三文鱼火出圈，尼勒克县年产6000吨三文鱼。</p><p>4、中美两国领导人互致贺信庆祝两国建交45周年。</p><p>5、南方人涌入哈市洗浴店爆满，有游客排队搓澡要等两三小时。</p><p>6、雪乡住宿2床1炕一晚3000元？价格被指“高攀不起”，当地回应雪乡住宿价格被指过高。</p><p>7、多地降温降雪低温津贴引关注，多名户外劳动者称“没听说过”。</p><p>8、2023全国电影票房超549亿元。</p><p>9、国家移民局：去年12月6国超11万人次免签入境中国，观光休闲占七成。</p><p>10、日本本州西岸近海发生7.4级地震。</p><p>11、2023年泰勒巡演带动50亿美元，消费者支出泰勒巡演经济效益或超50个国家GDP。</p><p>12、丹麦女王玛格丽特二世宣布退位，王位将传给儿子。</p><p>✨最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。——《十分钟冥想》</p><h3 id="名人语录-2">名人语录</h3><p>💞拉卡拉创始人孙陶然曾说：<br>“我所有的成就，<br>一半来源于天资，<br>一半来源于复盘。”<br>可见复盘对人生的重要性。</p><p>人生没有白走的路，每一步走过的路，每一件经历过的事，都是人生的财富。拉卡拉创始人孙陶然曾说：“我所有的成就，一半来源于天资，一半来源于复盘。”</p><p>反思自己的不足之处，总结出过往的经验。好的方面继续保持，坏的方面及时改正。</p><p><img src="../assets/image-20240102221129250.png" alt="image-20240102221129250"></p><p><img src="../assets/image-20240102221119189.png" alt="image-20240102221119189"></p><blockquote><p>✍🏻如果用五到十年坚持做一件事，也许生活会发生巨变。</p></blockquote><h3 id="日知日进-2">日知日进</h3><p>🗓️2024.01.02漫画国学👣</p><p>🔹生活小困扰：朋友误会我了，该怎么办?</p><p>💫国学大智慧：人不知，而不愠，不亦君子乎?</p><p>🔸译文诠释：别人不了解我，我不怨恨(恼怒)，不也是君子吗?</p><p>✨迁移阅读：参差多态，乃幸福之源。（《不抱怨的世界》）</p><p><img src="../assets/image-20240102221307687.png" alt="image-20240102221307687"></p><p>农夫种地以前有一位农夫，做事总爱瞻前顾后。春天的时候，有人问农夫：“春天到了，你种麦子了吗？”农夫回道：“没，我担心天不下雨。”那人又问：“那你种棉花没？”农夫又回道：“没，我担心虫子吃了棉花。”那人再问：“那你种了什么？”农夫说：“什么也没种，我要确保安全。”</p><p>感悟：我们总是想的太多而做的太少。顾虑重重只会束手束脚，一事无成。生活永远充满不确定，有时放手一搏，大胆尝试，方能有所收获。</p><h2 id="1-3日周三">1.3日周三</h2><h3 id="英语角-3">英语角</h3><p>Life is too short for long-term grudges.<br>人生短暂，何必长期心怀怨念。</p><h3 id="今日晨读-2">今日晨读</h3><p>🍁2024.1.3晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生中最大的发现就是发现自己。<br>——来自帆书《心态》</p><p>👉【精彩选段】<br>我们无须思考我们是否“有成功特质”和“无成功特质”，那是固定型心态所关注的。一个更健康、有效的成长型观念是，“如果成功没有如期而至，那只是因为我没有付出足够的努力”。</p><p>👉【作者之语】<br>解决问题的方法永远都在问题之外。</p><p>👉【樊老师之语】<br>只有真正了解自己的心态，你才能更好地驾驶人生航船。听了这本书，你将从4个维度了解自己的心态，并收获一套科学改善心态的行动方案，从而更积极地面对工作与生活。</p><h3 id="今日简报-3">今日简报</h3><p>今日简报</p><p>1月3日星期三</p><p>1、中泰将从3月起永久互免对方公民签证。</p><p>2、日本石川县能登地区7.6级地震已致48人死亡。</p><p>3、印度人口76年增长近11亿！印度成世界人口第一大国。</p><p>4、东航第四架C919大型客机交付入列。</p><p>5、日媒：客机与海上保安厅飞机相撞，保安厅飞机死亡人数增至5人。</p><p>6、韩媒：袭击李在明嫌疑人为60多岁男性，供述中承认有谋杀意图。</p><p>7、乌称基辅等地遭袭，俄方对此无回应。</p><p>8、因反垄断未获批，百度宣布终止收购YY直播。</p><p>9、驻美大使馆回应中国留学生疑似遭绑架：系遭遇“虚拟绑架”电信诈骗。</p><p>10、强震造成日本多个核电站燃料池水溢出。</p><p>11、广东一地公示8名躺平休闲人员，当事人称不便回应，专家称躺平者标准并无法律支撑：应谨慎。</p><p>12、古茗向港交所提交上市申请。</p><p>✨人生中最大的发现就是发现自己。——《心态》</p><h3 id="名人语录-3">名人语录</h3><p>生活就像一面镜子，你对它微笑，它便会为你绽放欢颜。<br>从今天开始，用乐观代替沮丧，用行动代替懒散。<br>调节好自己的心态，成功和幸福才会不请自来。</p><p>你人生的起点并不是那么重要<br>重要的是你最后抵达了哪里<br>———— 巴菲特</p><h3 id="漫画国学">漫画国学</h3><p>🗓️2024.01.03漫画国学👣</p><p>🔹生活小困扰：对新事物，我总是不敢尝试，怎么办?</p><p>💫国学大智慧：司马牛问君子。子曰：“君子不忧不惧。”</p><p>🔸译文诠释：司马牛问怎样才是君子。孔子说：“君子不忧愁，不恐惧。”</p><p>✨迁移阅读：儒家学说的终极目标不是建立道德规范来束缚人，而是提供一种让人美好、艺术地生活的指南。</p><p>📔推荐书目：《孔子：人能弘道》</p><p><img src="../assets/image-20240103143404860.png" alt="image-20240103143404860"></p><hr><h2 id="1-4日周四">1.4日周四</h2><h3 id="英语角-4">英语角</h3><p>It’s no use going back to yesterday, because I was a different person then.<br>回到昨天毫无用处，因为今天的我已和过去有所不同。</p><h3 id="今日晨读-3">今日晨读</h3><p>❄️2024.01.04晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>只要人活得高兴，穷也不怕。<br>——来自帆书·李蕾讲经典《活着（上）》</p><p>👉【精彩选段】<br>我看着那条弯曲着通向城里的小路，听不到我儿子赤脚跑来的声音，月光照在路上，像是撒满了盐。</p><p>👉【作者之语】<br>人是为活着本身而活着，而不是为了活着之外的任何事物所活着。</p><p>👉【李蕾老师之语】<br>《活着》是一本真实的书，同时也很坚韧，它讲了一个人和他命运的复杂友情。不管遭遇什么，活着是不该也不能放弃的事情。</p><p><img src="../assets/image-20240105081848787.png" alt="image-20240105081848787"></p><h3 id="今日简报-4">今日简报</h3><p>今日简报</p><p>1月4日星期四</p><p>1、日本地震死亡升至73人。</p><p>2、成品油价迎来2024年首次上调，加满一箱92号汽油将多花8元。</p><p>3、科技部发文规范AI使用，禁用生成式人工智能直接生成申报材料。</p><p>4、12306客户端推出购票需求预填及起售提醒订阅。</p><p>5、日本撞机前通话记录公布，日本海上保安厅飞机未被允许进跑道。</p><p>6、俄罗斯钻石不再被允许进入欧盟。</p><p>7、美国国债总额首次达到34万亿美元。</p><p>8、伊朗克尔曼市发生爆炸致多人死伤。</p><p>9、无人机未经实名登记飞行最高罚2万。</p><p>10、哈佛历史上首位黑人校长辞职，卷入抄袭丑闻和校园反犹争议。</p><p>11、303名抖音主播被关闭收礼物权限，抖音直播新版健康分处罚正式生效。</p><p>12、哈尔滨市政府提醒宾馆酒店珍惜“出圈”机遇：不盲目调整价格。</p><p>✨只要人活得高兴，穷也不怕。——《活着（上）》</p><h3 id="名人语录-4">名人语录</h3><p>越是优秀的人，越是知道这世间没有任何捷径可走。<br>把每一件事情坚持做、用心做，才可能厚积薄发，收获意料之外的惊喜！</p><h3 id="漫画国学-2">漫画国学</h3><p>🗓️2024.01.04漫画国学👣</p><p>🔸生活小困扰：能说会道和踏实肯干，究竟哪个比较重要？</p><p>💫国学大智慧：子贡问君子。子曰：“先行其言而后从之。”</p><p>🔅译文诠释：子贡问怎样才能做一个君子。孔子说：“对于自己要说的话，要先实行了，然后再说出来，这样就够算是一个君子了。”</p><p>✨迁移阅读：曾国藩之于后人最大的意义是，他以自己的实践证明，一个中人，通过“陶冶变化”，可以成为超人。</p><p>📔推荐书目：《曾国藩的正面和侧面》</p><p><img src="../assets/image-20240105082045243.png" alt="image-20240105082045243"></p><h3 id="日知日进-3">日知日进</h3><p>📯你是否也有这样的情况？<br>想和家人好好说话，<br>可一开口就管不住脾气。<br>想换一份工作，<br>但一直没有勇气辞职。<br>在一段感情里内耗许久，<br>却又舍不得放手。</p><p>🌟为什么我们都清楚需要改变，却没法做到呢？<br>如果你也有类似这样的困境，<br>想要改变却不知道如何开始。<br>不妨读读这本《5%的改变》。<br>试着从5%的改变里，<br>去感受自己的变化，<br>去发现生活的不同。</p><p>萧伯纳曾说：很多时候，完成比完美更重要，在一次次完成中迭代，就是进步。</p><p>苛求完美，像是一场与自己的无穷斗争，只会搞得精疲力竭。而真正的蜕变，是以轻松的心态去接纳原来的我，以5%的改变去迎接崭新的我</p><h2 id="1-5日周五">1.5日周五</h2><h3 id="英语角-5">英语角</h3><p>Nothing is impossible, the word itself says “I’m possible”!<br>没有不可能，连“不可能”这个词自己都说：“不，可能！”</p><h3 id="今日晨读-4">今日晨读</h3><p>❄️2024.1.5晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>我们不能仅仅依靠善心和意志力就成为更好的人。<br>——来自帆书·非凡精读《善恶之源》</p><p>👉【精彩选段】<br>由共情直接激发出的助人行为，往往会有“只见树木不见森林”的效果，它把我们的注意力引导到一个个案上，但往往没法兼顾公平。</p><p>👉【作者之语】<br>我们创造出来的社会环境，可以把一个只拥有部分道德意识的婴儿，变成一个拥有强烈道德感的成年人。</p><p>👉【主讲老师之语】<br>这本书帮助我们认识到天生的道德直觉并不完美。道德本能很强大，我们需要好好理解它。个人的道德观、社会的道德规范都是从这些道德本能里发芽生长出来的。</p><h3 id="今日简报-5">今日简报</h3><p>今日简报</p><p>1月5日星期五</p><p>1、2024年购买新能源车免征购置税。</p><p>2、我国自研电动飞机成功首飞：最大平飞速度218km/h，航程1100km。</p><p>3、外交部回应中国留学生被美方强制遣返：敦促美方停止打压限制中国留学生。</p><p>4、我国将开展“信号升格”专项行动。</p><p>5、多家日媒：日本石川县震后失联人数已增至179人。</p><p>6、首个石墨烯制成的功能半导体问世，天津大学团队承担主要研究与攻关。</p><p>7、圆通、韵达和京东物流违规寄递烟花被约谈。</p><p>8、伊朗克尔曼省发生两起爆炸已致84人死亡。</p><p>9、四部门：大力推广智能有序充电设施，原则上新建充电桩统一采用智能有序充电桩。</p><p>10、美国多个州议会大厦收到炸弹威胁。</p><p>11、高校严控重复设置过热专业，2035年全国或有近200万教师过剩。</p><p>12、湖南将保障支付农民工工资切实纳入对市州人民政府考核。</p><p>✨我们不能仅仅依靠善心和意志力就成为更好的人。——《善恶之源》</p><h3 id="名人语录-5">名人语录</h3><p>总是充满期待的人，每一天都是全新的开始~</p><p>山不却垒土之功，故能成其高；海不避涓涓细流，故能成其大。这世上从来就没有一步登天的神话，有的只是日积月累、滴水穿石。</p><h3 id="漫画国学-3">漫画国学</h3><p>🗓️2024.01.05漫画国学👣</p><p>🔸生活小困扰：我们如何获得他人的尊敬?</p><p>💫国学大智慧：君子以仁存心，以礼存心。</p><p>🔅译文诠释：君子心里存在着仁爱，存在着礼义。</p><p>✨迁移阅读：我们希望对方以何种方式对待我们，我们也就必须以同样的方式对待他们——己所不欲，亦施于人。</p><p>📔推荐书目：《人性中的善良天使》</p><p><img src="../assets/image-20240105111556361.png" alt="image-20240105111556361"></p><h3 id="日知日进-4">日知日进</h3><blockquote><p>糟糕的坚持 也是一种坚持，也要比轻易的放弃要好</p><p>两次放弃坚持的东西   就是一种坏习惯的开始了</p></blockquote><blockquote><p>糟糕的坚持  也能够帮你保持一种身份，帮助你对抗这种世界外部的无序</p><blockquote><p>跑5分钟 走一圈 因为今天实在是不想动，那就溜一圈，也比放弃要好，你要对自己宽容一点，</p><p>至少你现在所做的一切还能帮助你保持一种身份</p><p>我们宁肯要糟糕的坚持，也要对抗这种轻易的放弃</p></blockquote></blockquote><p>🌻你的孩子在外遇到难题了，<br>你会是他第一个求助的人吗？<br>你的孩子犯错了，<br>他会有勇气跟你说真话吗？<br>你的孩子会主动跟你袒露自己的脆弱吗？</p><p>:sun_with_face:前几天，看到这样一则故事：<br>“有个7岁的小男孩和朋友打篮球时摔倒了，<br>把手给摔折了。他害怕被父母骂，<br>生生地忍着疼痛，没跟大人说。”<br>直到后面忍不了了，被父母发现了，<br>才送往医院治疗。</p><p>:full_moon:到底是什么原因，<br>让孩子有事宁愿自己扛，<br>也不愿告诉父母？</p><p>📚分享内容｜简单人生<br>📆分享递增｜Day251<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.5</p><p>禅宗思想里，有一个“看山理论”。它将人生分为了三重境界：看山是山，看山不是山，看山还是山。第一重境界的人，心思单纯，没啥城府，看见什么便是什么；第二种境界的人，经历了世事的摧残，觉得生活复杂，看什么都是雾里看花；而最高境界的人，饱经沧桑后，依旧能淡看人生起落，把一切看得简单而自然。人生最好的状态，是在复杂的世界里，做一个简单的人。的确，人生短短三万天，学会看轻得失，看淡纷扰，将复杂的生活简单过，就是最好的处世之道！</p><h2 id="1-6日周六">1.6日周六</h2><h3 id="英语角-6">英语角</h3><p>would die for you.<br>But I won’t live for you.<br>我愿意为你赴死，但不会指望你而活。</p><h3 id="今日晨读-5">今日晨读</h3><p>☃️2024.1.6晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>除了吃饭，生命还有更重要的事!<br>——来自帆书·李蕾讲经典《海鸥乔纳森》</p><p>👉【精彩选段】<br>天堂不是一个地点，也不是一段时间。天堂是一种完美的状态。</p><p>👉【作者之语】<br>不要只相信你眼睛看到的东西。要用你的悟性去看，理解你已经知道的东西，然后你会发现飞翔的真理。</p><p>👉【李蕾老师之语】<br>《海鸥乔纳森》是一部关于梦想与追求的寓言故事，故事里的海鸥乔纳森是个“异类”。对于他来说，飞翔远比吃饭更重要。因此在命运转折的时候，大多数海鸥选择安于现状，而他选择成为想要成为的自己。</p><h3 id="今日简报-6">今日简报</h3><p>今日简报</p><p>1月6日，星期六</p><p>1、市场监管总局：特斯拉召回超160万辆存在安全隐患电动汽车。</p><p>2、央视曝河南距今2300年的赵长城被拦腰截断。</p><p>3、IS宣布对伊朗克尔曼市的连环爆炸案负责。</p><p>4、无锡全面施行经常居住地登记户口制度。</p><p>5、快递新规禁止未经同意擅用快递柜，未经同意代收货最高罚3万元。</p><p>6、韩方称朝鲜发射200多发海岸炮，外交部：正密切关注形势发展。</p><p>7、美媒：美军一架B-1轰炸机试图降落时坠毁，事发时正执行训练任务。</p><p>8、深圳拟出新规推进城中村改造，涉全市约40%建面。</p><p>9、阿根廷法院发布禁制令，暂停阿根廷总统米莱提出的涉及300多项经济改革中的劳工法令。</p><p>10、浙江将组织千名科学家任中小学校科学副校长。</p><p>11、福彩回应网曝福彩主持人提前播报中奖球号：视频系拼凑嫁接，已报警。</p><p>12、广西警方通报“女大学生摆摊卖糖葫芦被壮汉威胁”：行拘3人。</p><p>✨人生是一场无人可替代的修行，生活则是修炼心性的真经。——《给孩子一生的安全感》</p><h3 id="名人语录-6">名人语录</h3><p>有时候，成长就如种子发芽一般，需要积蓄足够多的力量，才能冲破土壤。在人生的跑道上，一旦认准目标，就只管努力，总有一天会惊艳所有人。</p><h3 id="漫画国学-4">漫画国学</h3><p>🗓️2024.01.06❄️漫画国学👣</p><p>『小寒已至』</p><p>❄️小寒，是踏雪赏梅的好时节，也标志着一年中最寒冷的日子就要来了。出门游玩的时候，小朋友们一定要做好保暖工作，日常也要多食用一些温热的食物来滋补身体。相传，古时的南京人对小寒颇为重视，有吃菜饭的习俗。但每家每户的菜饭并不相同，比如有用青菜、咸肉片、生姜粒和糯米一起煮的，咸香可口。大家可以在家里邀请爸爸妈妈一起来制作哦!</p><p>📔小寒谚语:<br>🔸小寒暖，立春雪。<br>🔸小寒不寒，清明泥潭。<br>🔸小寒蒙蒙雨，雨水还冻秧。<br>🔸小寒一场白，来年收小麦。<br>🔸小寒胜大寒，常见不稀罕。<br>🔸小寒暖立春雪，小寒不寒立春雪。<br>🔸小寒时处二三九，天寒地冻冷到抖。<br>🔸小寒不寒大寒寒，大寒不寒倒春寒。</p><p><img src="../assets/image-20240106210118345.png" alt="image-20240106210118345"></p><h2 id="1-7日周天">1.7日周天</h2><h3 id="英语角-7">英语角</h3><p>Years fly by, but the heart stays in the same place.<br>时光飞逝，我心依旧。</p><h3 id="今日晨读-6">今日晨读</h3><p>❄️2024.1.7晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>生活本有千万可能，别让过去束缚了你。<br>——来自帆书·非凡精读《拥抱可能》</p><p>👉【精彩选段】<br>被动就是让别人替你做决定，争强好胜就是为别人做决定，坚定自信就是为自己做决定。</p><p>👉【作者之语】<br>生命中，所有能让你惊喜若狂的事情都来自于内心。没有人能把你心里的东西拿走。</p><p>👉【推荐之语】<br>《拥抱可能》是埃格尔对自己一生的回望，记录了坚韧、治愈、有尊严地生存、充满勇气地拥抱自由和幸福。</p><h3 id="今日简报-7">今日简报</h3><p>今日简报</p><p>1月7日，星期日</p><p>1、首张房票已发放！广州打响一线城市房票安置“第一枪”。</p><p>2、境外机构免费送航空爱好者设备窃取我国信息。</p><p>3、江苏新发现1例小p血型，目前我国仅有9人。</p><p>4、世界最大冰雪主题乐园！哈尔滨冰雪大世界拿下吉尼斯世界纪录。</p><p>5、不分节假日、不限身份，山西21所高校向社会开放。</p><p>6、学历入户放宽至大专！成都户籍新政来了，2月20日起施行。</p><p>7、中美联合研制全球首个由石墨烯材料制成的功能性半导体。</p><p>8、上海与嘉兴实现公共交通乘车码互联互通，后续将扩大至长三角11城。</p><p>9、美国阿拉斯加航空公司宣布：暂时停飞65架波音737 MAX 9飞机。</p><p>10、日本羽田机场撞机事故影响波及逾15万人，跑道预计8日重启。</p><p>11、美邦服饰“二代”接任7年亏损超32亿，创始人重新被提名董事。</p><p>12、英国多地洪水，气象局连发三百多个预警。</p><h3 id="名人语录-7">名人语录</h3><p>✨大脑的疲劳和压力都来自过去和未来：对过去的事情心有不甘，对未来的事情充满不安。——《高效休息法》</p><p>生命是一棵长满可能的树 ————米兰·昆德拉</p><p>每一天都是新的旅程，<br>不需要去改变任何人，<br>只需要不断的提升自己，<br>自然有同频人与你同行。</p><h3 id="漫画国学-5">漫画国学</h3><p>🗓️2024.01.07漫画国学👣</p><p>📖一周复习：</p><pre><code>🔸人不知，而不愠，不亦君子乎?     ——《论语·学而》🔸司马牛问君子。子曰:“君子不忧不惧。”     ——《论语·颜渊》🔸子贡问君子。子曰:“先行其言而后从之。”    ——《论语·为政》🔸君子以仁存心，以礼存心。     ——《孟子·离娄下》</code></pre><p><img src="../assets/image-20240108193200012.png" alt="image-20240108193200012"></p><p>💞美好的亲子共读时光开始啦<br>🎤今日共读内容<br>大禹治水</p><p>很久很久以前，<br>洪水经常泛滥。<br>大水淹没了田地，<br>冲毁了房屋，<br>毒蛇猛兽到处伤害百姓和牲畜，<br>人们的生活痛苦极了。<br>洪水给百姓带来了无数的灾难，<br>必须治好它。</p><p>当时，<br>一个名叫鲧(gǔn)的人领着大家治水。<br>他只知道筑坝挡水，<br>九年过去了，<br>洪水仍然没有消退。</p><p>他的儿子禹继续治水。<br>禹离开了家乡，<br>一去就是十三年。<br>这十三年里，<br>他到处奔走，<br>曾经多次路过自己家门口。<br>可是他认为治水要紧，<br>一次也没有走进家门看一看。</p><p>禹吸取了鲧(gǔn)治水失败的教训，<br>采用疏导的办法治水。<br>他和大家一起，<br>疏通了很多河道，<br>让洪水通过河道，<br>最后流到大海里去。<br>洪水终于退了，<br>毒蛇猛兽被驱赶走了，<br>人们把家搬了回来。</p><p>大家在被水淹过的土地上耕种，<br>农业生产渐渐恢复了，<br>百姓安居乐业，<br>重新过上了幸福的生活。<br>由于禹为人们作出了很大的贡献，<br>大家便把他称为大禹。</p><h2 id="1-8日周一">1.8日周一</h2><h3 id="英语角-8">英语角</h3><p>May we be together till forever.<br>朝暮与岁月并往，愿你我共至光年。</p><h3 id="今日晨读-7">今日晨读</h3><p>❄️2024.1.8晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生在世，做事只求尽心，而成败不必强求。<br>——来自帆书《战安庆》</p><p>👉【精彩选段】<br>所谓中年危机，无非就是人到中年因事业无成而产生的焦虑和惶恐，一旦跳出功名利禄的圈子，从宇宙的维度来看待世事，以恬淡冲融的态度来重新看待人生，危机带来的抑郁也就消泯了。</p><p>👉【作者之语】<br>乱极时站得定，才是有用之学。</p><p>👉【樊老师之语】<br>打开本期新书，为你还原曾国藩那段最艰难的时光，看他如何用谦虚自省的心态，战胜中年危机，走向完满的人生境界。</p><h3 id="今日简报-8">今日简报</h3><p>今日简报</p><p>1月8日   星期一</p><p>1、教育部：网传义务教育教学改革实验区“取消中考”等说法不实。</p><p>2、上海税务：个人通过网络买卖虚拟货币需要缴纳个人所得税。</p><p>3、外交部：中方决定对5家美国军工企业实施制裁。</p><p>4、广西河池回礼东北老铁：16个景区免首道门票。</p><p>5、今年首批银行业罚单开出，3家银行合计被罚千万！去年千万以上罚单超20张。</p><p>6、陕西省定边县一住户发生煤炭炉取暖中毒，致4人死亡。</p><p>7、预计最快10天可找到马航MH370！最新研究指向未经搜索区域。</p><p>8、日本气象厅：1日以来能登地区已发生1000余次地震。</p><p>9、腾讯微信团队致歉：私密朋友圈bug已彻底修复。</p><p>10、流感病例数激增，西班牙多地医疗系统超负荷！部分地区重启口罩强制令。</p><p>11、OpenAI和微软遭集体诉讼，被指控“窃取”他人作品训练AI模型。</p><p>12、中国在极地布放首个生态潜标。</p><p>✨人生在世，做事只求尽心，而成败不必强求。</p><h3 id="名人语录-8">名人语录</h3><h3 id="漫画国学-6">漫画国学</h3><p>🗓️2024.01.08漫画国学👣</p><p>🔸生活小困扰：别人总是看不到我的优点，我该怎么办?</p><p>💫国学大智慧：君子病无能焉，不病人之不己知也。</p><p>🔅译文诠释：君子只会惭愧自己没有能力，不怨恨别人不了解自己。</p><p>✨迁移阅读：问题不能在产生问题的维度得到解决，升一个维度，才能解决问题。</p><p>📔推荐书目：《升维》</p><p><img src="../assets/image-20240108193443684.png" alt="image-20240108193443684"></p><h2 id="1-9日周二">1.9日周二</h2><h3 id="英语角-9">英语角</h3><p>Through the crowd to you.<br>穿越人群，奔赴于你。</p><h3 id="今日晨读-8">今日晨读</h3><p>❄️2024.1.9晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>——来自帆书《减压生活》</p><p>👉【精彩选段】<br>无论是我们自己，还是身边的人，在改变自己的时候，都要学会借助外力和改变认知。改变，并没有使我们失去什么，而是获得了一个更好的自己。</p><p>👉【作者之语】<br>健康的四大基石：乐观的心情、均衡的饮食、充足的睡眠、适度的运动。</p><p>👉【樊老师之语】<br>本书作者从神经外科医生的视角提出减压的根本是通过改变生活习惯，来干预和调节自主神经，给神经“松松绑”，让你由内而外，击退压力。更从四大维度提出十余种减压实操法，让你告别疲劳、焦虑，重新回归愉悦的生活。</p><h3 id="今日简报-9">今日简报</h3><p>今日简报</p><p>1月9日   星期二</p><p>1、多国停飞波音737MAX9型客机。</p><p>2、广西龙门大桥顺利合龙。</p><p>3、贵州村超新赛季打响！村超带动旅游综合收入近60亿元。</p><p>4、东航C919机型9日起开始执飞京沪航线。</p><p>5、深圳欢乐谷过山车碰撞事故调查报告公布，9人被追责问责。</p><p>6、字节与腾讯谈判出售多款游戏。</p><p>7、日本一核电站因地震漏约1.98万升油入海。</p><p>8、黎巴嫩真主党一高级指挥官在黎南部被以方打死。</p><p>9、我国破获一起英国秘密情报局（MI6）间谍案，MI6利用第三国人员从事对华间谍活动。</p><p>10、巴基斯坦发生爆炸袭击事件，致5名警察死亡。</p><p>11、广州“房票安置”政策细化：广州房票全市通用可转让。</p><p>12、北京新增设集成电路专业职称，放宽个人企业参评限制。</p><p>✨整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>——来帆书APP 听《减压生活》</p><h3 id="名人语录-9">名人语录</h3><p>无论年纪，只有不断提升自己，眼界才会愈加开阔，也才能更有底气、更加从容地面对人生。<br>保持学习、持续向上，就是我们给生活最好的回馈</p><p>与其千方百计去改变别人，让自己陷入麻烦，不如专注于对方的优点，放过别人，也放过自己。</p><p>德国哲学家康德说：请接受任何一个独立灵魂的存在，哪怕有些你并不认可，但也尽可能试着去理解。</p><p>允许对方有不同于我们的喜恶，生活才不会只有一种颜色。</p><h3 id="漫画国学-7">漫画国学</h3><p>🗓️2024.01.09漫画国学👣</p><p>🔸生活小困扰：我有些偏科，只喜欢学数学，这样不可以吗?</p><p>💫国学大智慧：君子不器。</p><p>🔅译文诠释：君子不能像器皿一样(只有一种用途)。</p><p>✨迁移阅读：风会熄灭蜡烛，却会使火越燃越旺。对随机性、不定性和混沌也是一样：你要利用它们，而不是躲避它们。你要成为风，渴望得到风的吹拂。</p><p>📔推荐书目：《反脆弱》</p><p><img src="../assets/image-20240109111933885.png" alt="image-20240109111933885"></p><h2 id="1-10日周二">1.10日周二</h2><h3 id="英语角-10">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-9">今日晨读</h3><h3 id="今日简报-10">今日简报</h3><h3 id="名人语录-10">名人语录</h3><h3 id="漫画国学-8">漫画国学</h3><h2 id="1-11日周二">1.11日周二</h2><h3 id="英语角-11">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-10">今日晨读</h3><h3 id="今日简报-11">今日简报</h3><h3 id="名人语录-11">名人语录</h3><h3 id="漫画国学-9">漫画国学</h3><h2 id="1-12日周二">1.12日周二</h2><h3 id="英语角-12">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-11">今日晨读</h3><h3 id="今日简报-12">今日简报</h3><h3 id="名人语录-12">名人语录</h3><h3 id="漫画国学-10">漫画国学</h3><h2 id="1-13日周二">1.13日周二</h2><h3 id="英语角-13">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-12">今日晨读</h3><h3 id="今日简报-13">今日简报</h3><h3 id="名人语录-13">名人语录</h3><h3 id="漫画国学-11">漫画国学</h3><h2 id="1-14日周二">1.14日周二</h2><h3 id="英语角-14">英语角</h3><p>Live up to the tender years<br>不负人间温柔岁月。</p><h3 id="今日晨读-13">今日晨读</h3><p>❄️2024.1.14晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>四大运动指南 ，教你如何正确锻炼。<br>——来自帆书·非凡精读《锻炼》</p><p>👉【精彩选段】<br>人体哲学与人生哲学同样重要。在健康面前，每个人都不是一座孤岛，我们良好的健康状况是可以相互影响的。</p><p>👉【作者之语】<br>不管是快走、短跑，还是跳舞，挑一个适合自己、自己喜欢的动起来，我们的身体将会因此受益无穷。</p><p>👉【馆长推荐】<br>本书从人类学和进化学的角度探讨我们的身体，为我们解答关于锻炼的种种问题。</p><h3 id="今日简报-14">今日简报</h3><p>今日简报</p><p>1月14日    星期日</p><p>1、发改委推出第七批重大外资项目，计划总投资超150亿美元。</p><p>2、六部门明确：进一步加强适老化无障碍出行服务，优先给老年人安排下铺。</p><p>3、气象台：北方大部地区将迎来大范围雨雪天气，局地降温可达15℃。</p><p>4、多地探索教师退出机制，以转变岗位、待岗培训、解聘为主要途径。</p><p>5、全球新冠加速蔓延！感染科专家：春运将至护好“脆弱人群”。</p><p>6、2024年台湾地区领导人和民意代表选举结果揭晓：民进党候选人赖清德、萧美琴当选台湾地区正副领导人。</p><p>7、上海放松限购：青浦、奉贤两区取消单身限购，社保3年即可买房。</p><p>8、河南平顶山煤矿事故已确认10人遇难，6人失联，其余人员全部脱险。</p><p>9、2023中国城市海外网络传播力前三名为上海、北京、杭州。</p><p>10、2.89万亿美元！微软超越苹果重新成为全球市值最高的公司。</p><p>11、加沙地带逾2.3万名巴勒斯坦人死亡，190万平民流离失所。</p><p>12、美军再次空袭胡塞武装，拜登政府考虑重新将也门胡塞武装列为恐怖组织。</p><p>✨审视自己的想法，而不是根据自己的想法来看问题。<br>——来帆书APP 听《跳出头脑，融入生活》</p><h3 id="名人语录-14">名人语录</h3><h3 id="漫画国学-12">漫画国学</h3><p>🗓️2024.01.14☀漫画国学👣</p><p>🔸国学小课堂：孔子推崇的同时代君子有哪些?</p><pre><code>在周朝是老子，在卫国是蘧伯玉，在齐国是晏子，在楚国是老莱子，在郑国是子产，在鲁国是孟公绰。</code></pre><p>✨迁移阅读：看2000年前的圣人如何练成自我。</p><p>📔推荐书目：《孔子如来》</p><p><img src="../assets/image-20240114172254529.png" alt="image-20240114172254529"></p><h3 id="故事角">故事角</h3><p>三国时期，<br>孙权送给曹操一只大象。<br>曹操带领大臣们和小儿子曹冲，<br>一同去看。</p><p>大象又高又大，<br>人站在它跟前，<br>只能够到它的肚子，<br>曹操对大臣们说：<br>“这只大象真是大，<br>可是到底有多重呢？<br>你们哪个有办法称一称？”<br>大臣们纷纷议论起来，<br>一个大臣说：<br>“只有造一杆很大很大的秤来称。”<br>而另一个大臣说：<br>“这可要造多大的一杆秤呀！<br>我看只有把它宰了，切成块儿称。”<br>大臣们想了许多办法,<br>但都行不通。</p><p>这时，<br>曹冲对曹操说:“我有一个办法。”<br>曹操就让他说来听听。<br>曹冲把办法说了，<br>曹操一听连连叫好。<br>对大臣们说:“走！咱们到河边看称象去!</p><p>众大臣跟随曹操来到河边，<br>河里停着一只大船，<br>曹冲叫人把大象牵到船上，<br>等船身稳定了，<br>在船舷上齐水面的地方，<br>刻了一条线。</p><p>他再叫人把大象牵到岸上来，<br>把石头往船上装，<br>船身就一点儿一点儿往下沉。<br>等船身沉到刚才刻线的地方，<br>曹冲就叫人停止装石头。<br>大臣们起先还摸不清是怎么回事，<br>看到这里不由得赞叹不已：&quot;好办法!好办法！”<br>现在谁都明白了，<br>只要把船里的石头分别称一下，<br>再把重量加起来，<br>就知道大象有多重了。</p><h2 id="1-13日周二-2">1.13日周二</h2><h3 id="英语角-15">英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-14">今日晨读</h3><h3 id="今日简报-15">今日简报</h3><h3 id="名人语录-15">名人语录</h3><h3 id="漫画国学-13">漫画国学</h3><h1>独白记录</h1><h3 id="1-1日独白">1.1日独白</h3><blockquote><p>从家里回来</p><p>累并快乐着</p><p>从姨家里吃晚饭 回家 回济南</p></blockquote><h3 id="1-2日独白">1.2日独白</h3><p>成大事不在于力量的大小而在于能坚持多久？ 五到十年之内如果你坚持做一些事情 你的生活会发生巨变！ 就是我们日常生活当中的一点点 微小的改善</p><p>这个让我想到了坚持晨读和夜读的力量</p><blockquote><p>早上果然不能看 各种的  会想一天</p><p>炮</p></blockquote><p>正念冥想应对胡思乱想</p><p>把想法当成天空的云，看着它，来来去去，时而乌云密布，时而蓬松轻柔</p><h3 id="1-3日独白">1.3日独白</h3><blockquote><p>关于习惯</p></blockquote><p><img src="../assets/image-20240103190120380.png" alt="image-20240103190120380"></p><blockquote><p>关于产品价值</p></blockquote><p>情绪，是下一个赛道。<br>【原文引用】<br>一个产品有三种价值。一是资产价值，二是功能价值，三是情绪价值。<br>【核心概念转述】<br>每一个产品都可以挖掘它三个方面的价值。第一方面是资产价值，资产价值也就是这个产品的天然属性。二是功能价值，也就是这个产品所带来的实用性。三是情绪价值，也就是为了获得某种情绪或感受而愿意支付的价值。这个产品的增值价值、附加价值。<br>【个人体验】<br>我想用几个例子来进一步理解如何去看待一个产品的三种价值，比如说一杯奶茶。它的售价是15块钱，这就是它的资产价值，它的价值是由它的人工成本，它的配料来确定的。同时一杯奶茶，它可以解渴，也可以饱腹，可以当早餐，也可以做下午茶，这就是它的功能价值。那么如果一杯奶茶是在冬天的清晨，由你的男朋友专门为你点送的，他就具有了情绪价值，是温暖，是爱，是无价的。<br>这样我们就比较好理解，为什么所有迪士尼的产品都会有溢价，再比如说像去年的冰墩墩大火大卖，溢价不止十倍以上。这也是我们常说的，要讲好一个故事，把历史，把文化，把情感带进去，也是现在IP打造的核心内涵。<br>我们经常会强调客户的满意度，客户的满意度也分几种。第一种不达预期的，那一定是不满意，第二种符合预期。其实只能算接受、认可。只有超出预期给客户带来惊喜才能是真正的满意。海底捞的服务为什么被奉为教科书式的经典，同样是因为被赋予了超值的情绪价值。<br>从银行的角度来看，产品的价格越来越低。各种的减费让利尽可能多的惠及客户。 同时也通过各种活动，各种权益来吸引客户。价格战已经卷到不能再卷。而产品的功能属性不断的在开发迭代，基本上是属于同质化的。谁能为客户带来超预期的情感体验，才是能够吸引客户，稳定客户，获取最大份额的利器。<br>【行动指南】<br>直营经理作为线上经营的主力。从产品的价格、产品的功能上和线下的营业机构都是同质化的。但是又缺乏和客户面对面的交流，客户的信任感天然是有差距的。<br>直营经理的人设打造，不断注入有情感，有温度的服务。将是需要我们不断去思考，去尝试，改进，迭代的方向。<br><a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=255360&amp;tag_name=%E5%88%98%E6%B6%A6&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#刘润 </a>/ <a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=240257&amp;tag_name=%E8%90%A5%E9%94%80&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#营销 </a></p><blockquote><p>关于怎样选择 看书 和选择一个是否要做的东西</p><blockquote><p>跃迁知识体系+ 读书投资</p></blockquote></blockquote><blockquote><p>关于笔记</p><blockquote><p>探索下浮墨卡片 + 成段的笔记</p></blockquote></blockquote><p><img src="../assets/image-20240103190109567.png" alt="image-20240103190109567"></p><blockquote><p>精选出所有的点赞量大的文章 进行写文+ 应用+ 反馈【社区里边的】</p></blockquote><p>认同共读的三大原则：<br>1、成年人原则<br>2、自助餐原则<br>3、以自己为圆心<br>先让自己快乐起来，再去体验：善思会写的快乐！</p><p>长期主义，乐观主义</p><blockquote><p>关于金钱</p></blockquote><p>金钱心理学：财富的转机和钱给你带来什么？<a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=254525&amp;tag_name=%E9%87%91%E9%92%B1%E5%BF%83%E7%90%86%E5%AD%A6&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#金钱心理学 </a><br>个人转述：什么时候你会感觉到财富转机的出现，不一定是说你需要拥有多少钱？而是你开始拥有的能力或技能（不是体力或时间），能轻松地换取到满足你生存所需的钱，这代表财富转机的出现。而钱最终能给人带来什么？一是允许你能自由控制自己的时间，二是可以不用为了生存做讨厌的事，接触不喜欢的事和人。概括两字就是“选择”的底气。<br>个人体验：刚从体制内辞职的一两年，我会有一种失重感，这种失重是彷徨和迷茫，满腔热血但迷茫，深耕哪个行业哪个岗位？广告行业、商务拓展、政务公关？每个我都细细思量过，适不适合自己，有没有前途，容不容易被替代？直到在财富管理领域保险行业获取一批客户，以及自己的公司进入平稳后，知道自己的财富转机已经到来，我不再需要再找工作，不再面临中年危机。我的转型第一步目标已经实现。感谢过去支持信任的朋友们，感恩！<br>行动指南：基础的架构已经搭建，那就要去做有意义和扩大规模的事。要再持续输入和输出，在亲子教育方面，不断积累，输出一本客观、理性，分享当父母的书和课。在财务规划团队建设方面，2024要新上台阶，更加珍惜每个合作伙伴，当个好教练，而不仅仅是好人！</p><p>📚分享内容｜活出心花怒放的人生<br>📆分享递增｜Day249<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.3</p><p>活出心花怒放的人生的有几点收获：<br>1 人需要从多个维度来认识自己的生活。健康、愉悦、良好的人际关系、一定程度的社会认可，这是很重要的。对于人来讲，这样的需要才是我们真正的需要。<br>2 你的行动反过来会影响你的想法。多则惑，简单的生活更容易快乐；闻香和深呼吸是调整情绪的最快方法！<br>3 找到工作的意义，除了收入之外，它的价值，对自己的帮助，对社会的帮助，好的团队关系。工作的幸福感，很重要。<br>4 爱需要你去感受才能够有。感谢亲人的付出，望着他们的眼睛，主动拥抱他们！<br>5 孩子是爱的结晶，爱的寄托。不是自我超越的工具。培养孩子的审美、创造和同理心。千万别和别的孩子比。<br>6 少而精才是好，比如孩子的玩具等。现在结婚恋爱的欲望下降，也是看到的美女太多了。<br>7 不开心的时候要立马做点事。嗅觉是让自己开心最快的方法，立竿见影。母亲闻孩子的味道也能感受幸福。副交感神经调动会让人开心，如深呼吸。<br>8 最后借用彭凯平教授的话“再好的方法你不学，你更不信，一点作用都没有的。”共勉！</p><h3 id="1-4日独白">1.4日独白</h3><blockquote><p>跟广阔合作 完成项目可视化 和 对应的代码框架 【不得不说 跟人合作 做项目确实会比较高效】</p></blockquote><h3 id="1-5日独白">1.5日独白</h3><blockquote><p>项目推进 做完ppt</p><p>零零散散 做了一天</p><p>也是不知道咋做，也是不知道怎样面对</p></blockquote><h3 id="1-8日独白">1.8日独白</h3><p><img src="../assets/image-20240108221205689.png" alt="image-20240108221205689"></p><h3 id="1-12日独白">1.12日独白</h3><h4 id="积极主动，操作在我">积极主动，操作在我</h4><p>在自己的选择和回应之间 不断抓住自己的自由度和空间，给与主动的、建设性的回应，并不断的为所有的选择做出积极的行动，不断的把自己的掌控权来进行扩大【方法论包括三个圈的应对方法】来不断的实现自己的人生。</p><p>同时观察并察觉自己的负面言论，不断的去停下来思考这些言论背后的原因和解决这些负面语言的可能性，并问自己能为这些可能性去做些什么，一旦问出这些话就能够不断的扩宽自己人生的可能性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;12月末尾&quot;&gt;12月末尾&lt;/h2&gt;
&lt;p&gt;🗓️2023.12.30【日知日进】日历&lt;br&gt;
📚日知书目：《人是如何学习的》&lt;/p&gt;
&lt;p&gt;✨日进思考：“普通人和高手的认知差距在哪里?”&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 🔹普通人和高手之间的差距在于知识迁移能力</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解5基于图扩散卷积网络的二人动作语义识别</title>
    <link href="https://www.fomal.cc/posts/d937b92.html"/>
    <id>https://www.fomal.cc/posts/d937b92.html</id>
    <published>2023-12-28T09:44:39.000Z</published>
    <updated>2023-12-29T09:48:50.126Z</updated>
    
    <content type="html"><![CDATA[<h1>Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition of Two-Person Actions基于图扩散卷积网络的二人动作语义识别</h1><h2 id="Abstract">Abstract</h2><p>​         Graph Convolutional Networks (GCNs) have successfully boosted skeleton-based human action recognition. However, existing GCN-based methods mostly cast the problem as separated person’s action recognition while ignoring the interaction between the action initiator and the action responder, especially for the fundamental two-person interactive action recognition. It is still challenging to effectively take into account the intrinsic local-global clues of the two-person activity. Additionally, message passing in GCN depends on adjacency matrix, but skeleton-based human action recognition methods tend to calculate the adjacency matrix with the fixed natural skeleton connectivity. It means that messages can only travel along a fixed path at different layers of the network or in different actions, which greatly reduces the flexibility of the network. To this end, we propose a novel graph diffusion convolutional network for skeleton based semantic recognition of two-person actions by embedding the graph diffusion into GCNs. At technical fronts, we dynamically construct the adjacency matrix based on practical action information, so that we can guide the message propagation in a more meaningful way. Simultaneously, we introduce the frame importance calculation module to conduct dynamic convolution, so that we can avoid the negative effect caused by the traditional convolution, wherein the shared weights may fail to capture key frames or be affected by noisy frames. Besides, we comprehensively leverage the multidimensional features related to joints’ local visual appearances, global spatial relationship and temporal coherency, and for different features, different metrics are designed to measure the similarity underlying the corresponding real physical law of the motions. Moreover, extensive experiments and comprehensive evaluations on four public large-scale datasets (NTU-RGB+D 60, NTU-RGB+D 120, Kinetics-Skeleton 400,SBU-Interaction) demonstrate that our method outperforms the state-of-the-art methods.</p><p><strong>摘要:</strong></p><p>图卷积网络（GCNs）已成功提升基于骨架的人体动作识别。然而，现有的基于GCN的方法大多将问题视为单独的人物动作识别，忽视了动作发起者和动作响应者之间的相互作用，尤其是对于基本的双人互动动作识别。有效考虑双人活动的固有局部-全局线索仍然具有挑战性。此外，GCN中的消息传递依赖于邻接矩阵，但基于骨架的人体动作识别方法往往使用固定的自然骨架连接性计算邻接矩阵。这意味着消息只能沿着网络的不同层或不同动作的固定路径传递，这极大地降低了网络的灵活性。为此，我们提出了一种新颖的图扩散卷积网络，通过将图扩散嵌入GCN，用于双人动作的语义识别。在技术上，我们基于实际动作信息动态构建邻接矩阵，以便以更有意义的方式引导消息传播。同时，我们引入了帧重要性计算模块进行动态卷积，以避免传统卷积引起的负面影响，其中共享权重可能无法捕捉关键帧或受到嘈杂帧的影响。此外，我们全面利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并针对不同特征设计不同的度量标准，以衡量相应运动的真实物理法则的相似性。此外，对四个公共大规模数据集（NTU-RGB+D 60、NTU-RGB+D 120、Kinetics-Skeleton 400、SBU-Interaction）进行了广泛的实验证明和全面的评估，证明我们的方法优于现有方法。</p><p><strong>总结:</strong></p><p>本研究针对基于骨架的人体动作识别中双人互动的问题，提出了一种新颖的图扩散卷积网络（GCNs）方法。与现有方法不同的是，该方法考虑了动作发起者和动作响应者之间的相互作用，并通过在GCNs中嵌入图扩散来更有效地利用双人活动的局部-全局线索。在技术上，研究动态构建邻接矩阵以引导有意义的消息传播，并引入帧重要性计算模块进行动态卷积，以避免传统卷积的负面影响。综合利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并设计不同的度量标准以测量相应运动的相似性。实验证明，在四个大规模数据集上，该方法在双人互动动作识别方面优于现有方法。</p><h2 id="I-INTRODUCTION">I. INTRODUCTION</h2><p><strong>I. 引言</strong></p><p>近年来，随着计算机视觉的快速发展，人体动作识别引起了越来越多的关注，这对于自然人机交互、动态行为分析和安全监控至关重要。由于动作理解依赖于信息丰富的表示，视频中的人体动作识别可以利用各种特征，如颜色、深度和骨架。许多方法尝试通过从原始视频帧中提取时间上的视觉运动线索来分析动作。然而，这些方法很容易受到复杂的背景、间歇性遮挡、自遮挡、视角变化和计算成本的挑战。</p><p>随着3D深度摄像机和先进的人体姿势估计算法的快速发展，获取动态3D骨架序列变得方便。骨架基动作识别方法在过去十年中取得了显著的进展，具有对复杂背景和动态环境的灵活适应性，以及计算和存储的高效性。然而，现有的基于骨架的人体动作识别方法通常侧重于单个人的动作识别。在现实生活中，群体活动和多人互动对于理解人类行为更为重要。作为群体活动的基础，双人互动识别成为本文的技术焦点。</p><p>近年来，基于深度学习的方法取得了显著的性能提升。通常，循环神经网络（RNN）通常学习骨架序列的时间上下文，而卷积神经网络（CNN）更倾向于空间线索。然而，同时考虑时间和空间信息仍然很难。因此，提出了在骨架结构上进行深度学习，通过这种方式，网络可以同时集成时间和空间信息。例如，时空图卷积网络（ST-GCN）已经成为骨架识别的最先进骨干。ST-GCN连接具有自然空间连接性的人体关节，并在连续帧中建立相同关节之间的时间连接。其邻接矩阵是通过构建的图形建立的，然后分为三部分以捕获三种运动形式：向心、静态和离心。自那时以来，基于各种ST-GCN的工作提供了一种新的语义动作识别方向，并取得了令人印象深刻的成果，但仍存在一些局限性。</p><p>首先，ST-GCN及其变体的骨架结构是固定的，仅源自身体的自然连接性，在某些需要关节之间进行长距离消息传递的动作（例如“拍手”）时，信息将大大减弱。其次，对于双人行为，ST-GCN及其变体将分成两个独立的样本进行处理，忽略了人体关节之间的连接。第三，在时间上，上述方法中每一帧的关键性相同，这使得难以处理现实中多个连续动作和“小动作”的影响。第四，ST-GCN仅使用关节作为特征，其变体添加了骨骼和运动特征，但所有上述方法都使用了距离导出的高斯相似性作为相似性测量。这很难捕捉物理运动中的相对位置关系。第五，对于ST-GCN的改进往往引入了注意机制以提取用于识别的关键区域，或者设计复杂的网络以捕获关键信息。这些可学习的方法意味着更复杂的网络结构。</p><p>为了缓解上述问题，如图1所示，我们提出了一种用于基于骨架的双人动作语义识别的图扩散卷积网络。在本文中，引入图扩散的思想以建立任意两个关节之间的连接，使得图结构不仅连接同一身体的远距离关节，还连接两个互动人的关节。使用固定的图结构作为先验知识来引导网络学习，并使用通过图扩散获得的图结构作为学到的知识来引导网络快速收敛。通过将关键关节的轨迹叠加来计算帧的重要性，使得时间维度上的卷积具有动态性，并且动态搜索时间扩散的源帧可以促进识别关键动作并屏蔽小动作。对于连续动作，此操作可以自适应地对不同动作进行识别而无需预先分割，这接近于现实生活中对视频的语义理解。</p><p>同时，受2s-AGCN的启发，我们引入“骨”特征，为了与真实物理运动保持一致，我们考虑3D坐标的更高阶导数，如速度、加速度。通过大量实验证明，更高阶导数会引入噪声，而平滑的轨迹会大大降低区分度。因此，我们使用3D坐标、骨骼、速度作为三个特征，并使用余弦相似性以及高斯相似性来测量肢体的相对位置，以获得更具物理意义的结果。</p><p>值得一提的是，空间和时间均以手工方式设计，也就是说，我们没有引入复杂的网络，而是通过统计分析以数学方式提取关键信息。特别是，本文的显著贡献可总结如下：</p><ul><li>我们通过在基于自然人体骨架连接性构建的先验骨架图上进行扩散来提取扩散图结构。先验骨架图用于使网络收敛到全局最优解，而扩散图用于加速收敛和提高灵活性。</li><li>我们提出了基于图扩散的动态卷积，通过在远距离关节和不同身体之间建立直接连接，使GCN的消息传递超过直接邻居。</li><li>我们为时间扩散的源帧引入了动态搜索方案，既可以屏蔽由小动作引起的噪声，又可以促进对连续多个动作的自适应分割，这与现实生活中对视频的语义理解接近。</li><li>我们设计了不同的相似性度量来挖掘与不同特征相关的不同潜在空间，使特征融合更符合动作的相关物理规律。</li></ul><p><strong>引言总结：</strong></p><p>本文介绍了对于实际骨架动作识别的一个新的子领域，并提出了基于信息理论的原则，引入了实际图卷积网络（RW-GCNs）来在NTU-RGB-D-120数据集上实现94.16%的最新准确度，相较于ST-GCN基线具有3.02倍的低延迟。此外，RW-GCNs可以在基线实现的基础上实现10倍的低延迟，并且准确度仅降低了3.8%。对Northwestern UCLA数据集的评估显示，RW-GCNs可以实现90.4%的准确度，比基线ST-GCNs低32.5倍的延迟。这是尽管在验证和训练中存在空间关键点噪声。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，其在Northwestern UCLA数据集上比基线ST-GCNs低32.5倍的延迟，并在保持71.8%准确度的同时。这一切都是通过隐私感知和可扩展的边缘计算为中心的方法实现的，在这种方法中，每个节点的系统成本可以降低10倍，同时仍然保持在场景复杂性（15.6至5.5 ApS）范围内的吞吐量。这项工作标志着实际骨架动作识别子领域的开始。通过设计RW-GCNs，本文希望促进以前不可行的新边缘计算应用的设计和创建。然而，我们认为这个新兴领域仍然存在许多挑战，如时间变化、环境特定场景动态和进一步的应用特定约束。</p><h2 id="II-RELATED-WORKS">II. RELATED WORKS</h2><h3 id="A-Skeleton-Based-Action-Recognition">A. Skeleton-Based Action Recognition</h3><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别</strong></p><p>早期的基于骨架的人体动作识别方法通常采用手工设计的特征来捕捉关节的运动规律[12]，然而，这种手工设计的特征往往不够丰富，因此结果通常令人不满意。实际上，动作识别强烈依赖于人体的表示和分析[13]。随着深度神经网络的发展[14]，数据驱动的方法引起了广泛关注。例如，基于RNN的方法使用坐标作为序列数据来表示人体关节[15]。长短时记忆网络由于其在建模序列数据的依赖性和动态性方面的优势，展现出良好的性能[16][17]，用于提取长期语义信息[18]。将3D坐标映射到基于CNN的伪图像上[19]，而伪图像的第三维用于表示时间动态。</p><p>图卷积的引入使得动作识别更加专注于基于骨架的方法，从而不受复杂背景的干扰，并且极大地提高了识别准确性。2s-AGCN[8]提出了一种两流自适应图卷积网络，并通过注意模块以数据驱动的方式学习新的拓扑结构，增强了网络的灵活性。AS-GCN[7]采用编码器-解码器结构捕获特定于动作的潜在依赖性，建立了无自然连接性的关节之间的连接，形成广义骨架图以获取更丰富的信息。ST-TR[5]引入Transformer[20]处理长时依赖关系，同时在时空中引入自注意机制以提取关键信息。FDGCN[21]设计了一个聚焦扩散图卷积网络，充分探索时空上下文。MS-G3D[22]提出了一个强大的特征提取器，捕获了先前未被注意到的多尺度时空特征。DC-GCN[10]构建了一个解耦图卷积以增强骨架动作识别的图建模能力，并引入了注意引导的DropGraph模块，有效缓解了图卷积网络中的过拟合问题。AGC-LSTM[23]提出了一个增强型注意图卷积LSTM网络，不仅可以捕获空间配置和时间动态中的判别特征，还可以探索时空域之间的共同关系。STGR[24]设计了一种新颖的时空图路由方案，用于骨架动作识别，自适应地学习物理上相距较远的骨架关节的高阶连接关系。</p><p>人与人之间的相互动作识别是人体活动分析的重要研究分支[25]，但由于相互遮挡的原因仍然具有挑战性。在人体动作识别中，如果将动作划分为单个人进行识别，将缺乏对人体各关节之间关联的关注。许多文献已经证明，单个人体的远端关节对于不同动作具有不同的重要性。同样，在两人互动中，需要更多关注两者关节之间的关系。关节的时间运动轨迹应是区分相似动作的关键因素，而大多数先前的方法未能充分利用视频段之间的时空关系。基于图的方法可以以灵活的方式处理骨架数据，以探索骨架关节之间的关系。时空图卷积网络（ST-GCN）[4]是第一个使用图卷积进行动作识别的模型，由于骨架数据的简单性和高效性，ST-GCN实现了最先进的准确性。因此，在本文中，我们采用ST-GCN作为基准模型。</p><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别总结</strong></p><p>在骨架基础动作识别领域，传统方法采用手工设计的特征，但这些特征往往信息不足，效果不佳。随着深度神经网络的兴起，数据驱动的方法备受关注，如RNN和CNN等。引入图卷积后，骨架方法更专注于动作识别，避免了复杂背景的干扰，并大幅提高了准确性。</p><p>具体模型包括2s-AGCN、AS-GCN、ST-TR、FDGCN、MS-G3D、DC-GCN等，它们通过自适应卷积、注意机制、编码器-解码器结构等方式增强网络灵活性，捕获更丰富的时空特征。这些模型在骨架动作识别中取得显著成果，但也存在一些局限性，如固定的骨架结构、对两人互动关系的忽略等。</p><p>为解决这些问题，文章提出了基于图扩散的动态卷积网络，引入图扩散建立关节连接，通过动态卷积适应不同距离和不同身体之间的连接，提高信息传递效率。同时，通过关键关节轨迹的叠加计算帧的重要性，使时间维度的卷积具有动态性，能够识别关键动作并抑制小动作的噪音。该方法综合利用了多维特征，通过不同相似度度量捕捉特征之间的物理关系。总体而言，该方法通过图扩散和动态卷积有效地应对了骨架动作识别中的挑战。</p><h3 id="B-Graph-Neural-Network">B. Graph Neural Network</h3><p><strong>B. 图神经网络</strong></p><p>图神经网络[31]使用节点表示对象，使用边表示对象之间的关系，并在图上应用图卷积。图卷积网络的构建通常遵循两个原则：空间视角[32]和频谱视角[33]。基于频谱理论的方法计算图拉普拉斯矩阵[34]的特征值和特征向量，并借助图傅里叶变换在频域进行图卷积，而无需从图中提取局部区域的连接。与频谱理论相反，基于空间理论的方法需要知道节点及其邻居以执行卷积操作[34]。在本文中，采用空间理论构建图卷积核，同时引入图扩散使我们的模型在频谱视角上更具可解释性。</p><p>图卷积是图神经网络的核心，通常采用直接邻居进行消息传播[37]。动作识别任务的最关键因素在于关节共存的帧内表示和骨架时间演变的帧间表示[38]。关节之间的连接不仅限于直接邻居，还包括更本质相关的关节。例如，在拍手动作中，两只手没有自然连接，但却密切相关[5]。因此，直接邻居的传播将削弱远离但密切相关关节之间的消息[6]。图扩散卷积（GDC）[37]支持超出直接邻居的消息传递，可以有效提取关键区域的不同动作，削弱无用的边。同时，GDC与频谱理论密切相关，可以结合空间理论和频谱理论的优势，突破直接邻居消息传递的局限性。因此，我们将采用图扩散卷积的思想，自适应地构建不同动作的骨架图，以提高动作识别的准确性。</p><p><strong>总结：</strong> 图神经网络是通过节点和边表示对象及其关系，并应用图卷积进行信息处理的模型。构建图卷积网络可基于空间或频谱视角，本文选择空间理论。图卷积是核心，通常使用直接邻居进行消息传播，但对于关节间的复杂关系，引入图扩散卷积（GDC）支持超出直接邻居的消息传递，提高了对不同动作关键区域的识别能力。GDC结合了空间和频谱理论的优势，通过自适应构建不同动作的骨架图，有效突破了传统直接邻居消息传递的限制，提高了动作识别的准确性。</p><h2 id="III-PRIOR-SKELETON-GRAPH">III. PRIOR SKELETON GRAPH</h2><p>我们的先验骨架图以关节为图节点，以人体的自然连通性为空间边，相邻框架中相同的关节连接为时间边。除了关节特征外，我们还使用了骨骼数据的二阶信息(骨骼的长度和方向)，这自然对动作识别[8]更具信息和鉴别性。为了引入物理运动方向的大小，我们添加了速度特征，所以涉及到的特征是关节(3d坐标)，骨骼(骨骼数据)和速度。</p><h3 id="A-Skeleton-Graph-Construction">A. Skeleton Graph Construction</h3><p>构建的两人交互骨架模型分为两部分:一部分是来自人体自然连接的先验知识，另一部分是通过数据驱动的方式获得的个性化模型。先验模型主要用于利用先验知识引导网络学习，而个性化模型产生动态骨架，通过图扩散获得动态骨架，主要用于引导网络更快地收敛，同时增强固定骨架的远距离消息传递。图2为两人动作的先验骨架图，其中棕色实心圆表示人体关节，棕色线表示人体自然连通性，蓝色线表示相同关节的时间轨迹。</p><h3 id="B-Feature-Selection">B. Feature Selection</h3><p><strong>B. 特征选择</strong></p><p>动作特征元素通常包括轨迹、速度、方向等。我们将其分解为关节、速度和骨骼。</p><p>关节特征通常是人体动作识别中最常用的特征之一[4]，[6]，[8]。关节之间的位置关系反映了运动的幅度和轨迹。在空间维度上，关节的相对位置可以有效区分不同的运动，而在时间维度上，关节的轨迹反映了运动的周期和幅度。</p><p>速度特征被引入以反映实际物理运动中力的重要性，它是一个带有方向和大小的矢量。在一些文献中[6]，[39]，位置、速度和加速度经常被用作动作识别的三个特征。基于大量实验，作为二阶导数的加速度可能会受到噪音的影响（例如，小动作），如果在时间上平滑，加速度的区分能力将会减弱，因此我们舍弃了加速度特征。与空间点的位置相比，作为3D坐标的一阶导数，速度更关注力的方向和大小，并且与人体运动学更密切相关。大量实验证明，速度的区分能力不逊于3D坐标，并且通过考虑不同的注意力隐藏空间后，可以显著提高其区分能力。</p><p>骨骼特征。实际上，骨骼和关节将两人互动动作的骨架模型分为两部分，如图3所示，这也是骨架动作识别中通常使用的两个基本特征[6]，[8]，[9]，[11]，[40]。骨骼是人体关节之间的自然连接的反应，是人体运动的最小单位，也是运动中姿势的重要载体。骨骼特征在人体动作识别中具有很强的区分能力。特别是在两人互动中，骨骼的相对关系比其他特征具有更多的物理意义，并且导致更高的识别准确性。</p><p><strong>总结：</strong> 本节介绍了动作特征的选择，主要包括关节、速度和骨骼。关节特征通过反映运动的幅度和轨迹在空间和时间维度上进行区分。速度特征引入了对实际物理运动中力的重要性的考虑，通过矢量表示方向和大小。骨骼特征将骨架模型分为关节和骨骼两个基本部分，对于人体动作识别，尤其是在两人互动中，骨骼的相对关系具有更多的物理意义，导致更高的识别准确性。</p><h3 id="C-Feature-Similarity-Measurement">C. Feature Similarity Measurement</h3><p><strong>C. 特征相似度测量</strong></p><p>不同的特征具有不同的关注特性，使不同的特征在其专业领域发挥出较大的区分能力非常重要。在本文中，为不同的特征引入了不同的相似度度量方法。实验证明，可以使用高斯相似度测量3D位置坐标，而速度更关注余弦相似度。动作识别实际上涉及对人体每个骨骼的力进行分析。这种力不仅具有大小，还具有方向。我们可以使用高斯相似度进行幅度的相似性分析，使用余弦相似度来计算方向的相似性。通过这种方式，我们可以更好地捕捉与不同特征相对应的不同隐藏空间。</p><p>高斯相似度测量多维空间中每个点的绝对距离。为了将欧几里德距离映射到0-1范围，我们使用高斯核函数。当两点之间的欧几里德距离为0时，高斯核函数的值接近1；当距离无穷大时，它接近于0。参数σ用于调整下降速度。σ越小，变化越剧烈。</p><p>余弦相似度计算两个向量之间的夹角的余弦值：余弦值可用于表示两个向量的相似性。夹角越小，余弦值越接近1，它们的方向越匹配，相似性越高。余弦相似度与向量的大小无关，只与向量的方向有关。</p><p><strong>总结：</strong> 本节介绍了针对不同特征引入的不同相似度度量方法。通过使用高斯相似度和余弦相似度分别进行幅度和方向的相似性分析，可以更好地捕捉不同特征对应的隐藏空间，提高动作识别的准确性。</p><h2 id="IV-GRAPH-DIFFUSION-CONVOLUTIONAL-NETWORK">IV. GRAPH DIFFUSION CONVOLUTIONAL NETWORK</h2><p>我们的图扩散卷积神经网络以STGCN为基线。我们首先简要回顾了ST-GCN，然后详细介绍了我们在ST-GCN上的改进。ST-GCN将图形卷积应用于基于骨骼的人体姿态识别。对于单帧，有areNjointsVt, joint对sh = {(i, j)|i, j∈N}，其连接是根据人体关节的自然连度定义的，因此，骨架边集可以定义为Et = {(vti,vtj)|t∈τ， (i, j)∈H}，其中τ为视频帧总数。如图1所示，整个网络采用端到端训练方法，并通过反向传播进行优化。网络的前三层用于自学习，第四层和第五层用于以数据驱动的方式引导网络达到全局最优，后两层用于收敛，最后通过标准的SoftMax分类器对每个类别进行关联</p><h3 id="A-Generalized-Graph-Diffusion-and-Graph-Diffusion-Convolution">A. Generalized Graph Diffusion and Graph Diffusion Convolution</h3><p><strong>A. 广义图扩散和图扩散卷积</strong></p><p>对于单帧，根据上述定义，我们现在有一个无向图 G = (V，E)，其中 V 是点集，E 是边集，广义图扩散的扩散矩阵定义如下：</p><p>其中 θk 是权重系数，T 是广义转移矩阵。为了确保收敛性，限制必须满足 ∑（从 k=0 到 ∞）θk = 1，θk ∈ [0, 1]，且T的特征值被限制在 λ ∈ [0, 1]。</p><p>常用作转移矩阵的随机游走转移矩阵 Trw = AD^(-1) 和对称转移矩阵 Tsym = D^(-1/2)AD^(-1/2)，其中 D 是节点度矩阵，A 代表邻接矩阵。为了确保转移矩阵列随机，我们在转移矩阵中添加了具有权重 wloop 的自环 IN。现在，随机游走的转移矩阵可以定义为：</p><p>实际上，图扩散卷积将原始邻接矩阵 A 替换为广义图扩散矩阵 S，这也可以理解为在图扩散之后，我们构建了一个新图用于后续网络卷积。</p><p><strong>总结：</strong> 本节介绍了广义图扩散和图扩散卷积的定义。对于单帧，通过定义广义图扩散矩阵，构建了一个无向图，并引入了相应的转移矩阵。图扩散卷积通过将原始邻接矩阵替换为广义图扩散矩阵，实际上是在图扩散后为后续网络卷积构建了一个新的图。</p><h3 id="B-Spatial-Graph-Diffusion-Convolution-Neural-Network">B. Spatial Graph Diffusion Convolution Neural Network</h3><p><strong>B. 空间图扩散卷积神经网络</strong></p><p>图卷积的最重要部分是采样函数和权重函数的定义。在ST-GCN中，采样函数获取与关节点vti相隔一跳的直接邻居。邻居集合可以表示为B(vti) = {vtj | d(vti, vtj) ≤ 1}，其中d(vti, vtj)表示vti和vtj之间的最短距离。</p><p>由于图中节点的邻居与图像相比没有顺序，合理的划分策略和权重设置的定义成为图卷积的关键。根据人体运动学，ST-GCN将节点vti的邻居分为三类，如图4所示。节点本身被划分为静态运动集。靠近骨架重心的节点被划分为向心运动集，远离中心的节点被划分为离心运动集。划分策略可表示为：</p><p>其中ri是训练集中所有帧中从重心到关节点i的平均距离。根据划分策略，权重函数定义为：</p><p>因此，ST-GCN中的空间图卷积定义为：</p><p>其中Zti(vtj)=|{vtk | lti(vtk)= lti(vtj)}|是归一化项，表示相应子集的基数。 根据上述公式，我们的图扩散卷积网络需要设计两个部分，采样函数和权重函数。</p><p>我们仍然采用ST-GCN采用的空间配置划分方法。实际上，由于ST-GCN中的固定骨架结构，不同帧的权重函数是固定的，这意味着不同关节具有相同的重要性，与不同部位协同执行不同行为的思想相悖。因此，我们将引入图扩散到空间图卷积，以便不同行为采用不同的权重函数。</p><p>图扩散的示例包括个性化PageRank（PPR），热核等，我们采用了PPR，它使用具有转移矩阵T = Trw和系数θPPR k = α(1 − α)k（我们设置alpha = 0.05）的随机游走转移矩阵。在复杂社交网络挖掘中通常采用PPR，通常数据漫游不是随机的，而是具有用户偏好。因此，我们应考虑如何将PPR应用于两人互动识别模型。</p><p>按照这个思路，我们将节点传输过程转化为基于前述节点相似性计算的数据驱动过程。这意味着节点vi在行走过程中停留在原地的概率是wloop/Di，而权重wloop可以由我们的相似性矩阵定义。</p><p>通过图扩散，我们可以将骨架图转化为任意两个关节之间的关系矩阵。这种关系不是通过相似性计算直接获得的，而是在整个骨架图上使用相似性矩阵进行扩散。消息传递到整个图的每个关节，甚至传递到另一个人的关节。这导致了一个新的骨架结构，包含了在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。图5显示了我们的扩散结果。</p><p><strong>总结：</strong> 本节介绍了空间图扩散卷积神经网络的关键概念，包括采样函数和权重函数的定义。在空间图卷积中，通过引入图扩散，改变了原始的图结构，使不同行为采用不同的权重函数。图扩散通过在整个骨架图上进行相似性矩阵扩散，获得了不同关节之间的关系，包括在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。</p><h3 id="C-Temporal-Dynamic-Convolution-Neural-Network">C. Temporal Dynamic Convolution Neural Network</h3><p><strong>C. 时间动态卷积神经网络</strong></p><p>在一系列视频帧中，通常会存在一些无关的动作[41]。例如，在“拥抱”动作中，“接近”动作首先执行，然后是“拥抱”。因此，我们希望将振幅最大的动作标识为扩散和识别的焦点。同时，在一些动作中，会有一些“小动作”作为插曲，例如，有些人会习惯性地“拿眼镜”、“拿耳朵”或“摇腿”等。这些小动作不应成为识别的重点。而且，有些动作非常相似，振幅略有不同，例如“挥手”和“打排球”等。在实际场景中，动作通常是连续的，因此在分割后识别动作具有重要的实际意义。</p><p>在许多相关研究中，通常通过简单的卷积进行知识融合，并将关节的变化抽象为时间维度的隐藏空间，但忽略了每帧的重要性的差异[42]。由于卷积操作共享卷积核的特性，扩散的源帧与其他帧没有差异。然而，对于一些相似的动作，动作的振幅成为识别的关键。因为大多数动作是由身体部位执行的，而身体部位具有简单的图结构，对于动作来说，主要的区别在于哪些部位适合在一起以及每个部位的振幅。在前一种情况下，我们通过扩散在空间维度的组件之间建立了空间连接。后者是时间维度的问题。因此，我们通过在时间维度中寻找扩散的源帧来放大扩散过程中源帧的权重，使模型收敛更快、更准确，提高相似动作的识别能力。</p><p>在ST-GCN中，时间卷积操作是通过考虑周围9帧来更新当前帧的关节。由于卷积过程的局部感知和参数共享特性，由于对不同帧具有相同的感知强度，很难捕捉关键帧。</p><p>因此，我们研究了关节的时间活动轨迹，以改变不同帧的重要性，这意味着我们的时间卷积是动态的。时间的帧重要性计算可以表示为：</p><p>在这里，我们选择了所有关节进行重要性计算，并采用了原始未处理的特征。在后续的消融实验中，我们将讨论一系列更有意义的堆叠方法。现在，我们的时间卷积可以表述为：</p><p>准确捕捉源帧还将抑制小动作和噪音帧的影响。同时，帧重要性计算可以以细粒度捕捉人体运动趋势的变化，例如从“前进”到“后退”，从“外”到“后”等，这些常常在运动轨迹中呈现为拐点。通过这些拐点，我们可以分割多个连续的动作，以处理现实中复杂而连续的视频。</p><p><strong>C. 时间动态卷积神经网络总结</strong></p><p>在时间动态卷积神经网络中，通过识别动作的振幅最大的源帧来作为扩散和识别的焦点，以区分动作中的“小动作”和相似动作。传统卷积操作的问题在于对不同帧的感知相同，而时间动态卷积通过考虑关节的时间活动轨迹，使模型能够动态地调整每帧的重要性。通过在时间维度中寻找源帧，扩散的源帧在卷积过程中的权重得以放大，提高了模型的收敛速度和准确性，同时消除了小动作和噪音的影响。这种动态的卷积操作能够更好地捕捉人体运动趋势的变化，为处理连续和复杂的实际视频提供了更有效的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition of Two-Person Actions基于图扩散卷积网络的二人动作语义识别&lt;/h1&gt;
&lt;h2 id=&quot;Abstr</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解4真实世界的图卷积网络用于智能视频监控中的动作识别</title>
    <link href="https://www.fomal.cc/posts/69357615.html"/>
    <id>https://www.fomal.cc/posts/69357615.html</id>
    <published>2023-12-28T03:08:41.000Z</published>
    <updated>2023-12-29T09:48:50.121Z</updated>
    
    <content type="html"><![CDATA[<h1>Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance</h1><h1>abstract</h1><p>摘要： 动作识别是新兴的边缘智能视频监控和安全系统中的关键算法部分。基于骨架的动作识别是一种吸引人的方法，它不使用RGB像素数据，而是依赖于人体姿势信息来分类适当的动作。==然而，现有的算法通常假设理想条件，这些条件不代表真实世界的限制，如嘈杂的输入、延迟要求和边缘资源约束。==</p><p>为了解决现有方法的局限性，本文提出了Real-World Graph Convolution Networks（RW-GCNs）<strong>，这是一个面向真实世界骨架动作识别域约束的架构级解决方案。<strong>受人类视觉皮层中反馈连接的启发，RW-GCNs利用对现有近最先进（SotA）的时空图卷积网络（ST-GCNs）<strong>进行专注反馈增强</strong>。ST-GCNs的设计选择</strong>源于信息理论中心的原则，以解决端到端实时和边缘智能视频系统中通常遇到的空间和时间噪声</strong>。我们的结果表明，RW-GCNs能够通**过在NTU-RGB-D-120数据集上实现新的SotA准确性（94.1%）**来服务这些应用，并在存在空间关键点噪声的情况下，<strong>在Northwestern UCLA数据集上实现90.4%的准确性</strong>，比基线ST-GCN应用的延迟低32倍。RW-GCNs通过在成本效益高达10倍的NVIDIA Jetson Nano上运行（而不是NVIDIA Xavier NX），同时在资源受限的设备上保持可观的吞吐量范围（15.6到5.5个动作每秒），进一步展示了系统的可扩展性。代码可在此处获取：<a href="https://github.com/TeCSAR-UNCC/RW-GCN%E3%80%82">https://github.com/TeCSAR-UNCC/RW-GCN。</a></p><p>总结： 本文介绍了一种名为Real-World Graph Convolution Networks（RW-GCNs）的架构，用于解决真实世界骨架动作识别中的各种约束。受人类视觉皮层中反馈连接的启发，RW-GCNs通过对现有时空图卷积网络（ST-GCNs）进行专注反馈增强，==以应对噪声、延迟和资源限制等挑战==。实验证明RW-GCNs在NTU-RGB-D-120数据集上==实现了新的最先进准确性==，==同时在较低的延迟下仍保持了高准确性==。RW-GCNs还通过在成本效益高的边缘设备上运行，展示了系统的可扩展性。</p><h1>I. INTRODUCTION</h1><p>动作识别，即在视频片段中识别演员执行的动作，是计算机视觉领域一个具有挑战性且繁荣的领域。在边缘实时应用的约束条件下，这一挑战变得更加严峻。边缘计算是许多旨在维护隐私和可扩展性的应用的要求。深度神经网络（DNNs）的最新进展使该领域迎来了一场复兴，将模型的准确性推动到远远超出以往可能的水平。这在基于骨架的动作识别中尤为显著，该方法不使用RGB像素数据，而是依赖于人体姿态信息来分类适当的动作。特别是，源自图卷积网络（GCNs）[22]，具体来说是时空图卷积网络[64]（ST-GCNs）的方法，将最先进（SotA）的动作识别推向了新的高度。目前，即使对于相对简单的基线ST-GCN算法，也可以实现近SotA的准确性，达到了87%的准确性。有了这么高的准确性，现在可以在理解人们动作的基础上实现依赖于复杂计算机视觉的应用，例如视频监视、自动驾驶车辆、患者监测等。然而，许多这些应用都具有尚未得到适当解决的实际约束条件，例如需要在边缘端到端系统中满足隐私和可扩展性要求以及实时延迟的约束条件。这些现实世界的约束条件阻碍了现有基于骨架的动作识别工作的部署。</p><p>现有致力于基于骨架的动作识别的工作往往假设并非真实世界的理想条件。通常，网络使用的姿势信息被假定为完美的，并且通常基于手动注释的数据。然而，在实际系统中，骨架姿势信息将来自于不完美的人体姿势估计器，导致不完美的姿势数据。即使是最先进的SotA姿势估计器，具有数千万参数和数百亿次操作[4, 37]，也无法产生无瑕疵的姿势数据。这些姿势估计器的准确性受到边缘设备资源约束的进一步限制。此外，这些工作通常忽视了在边缘设备上满足这些现实世界应用的潜在延迟约束，这明显限制了它们在实际世界中的适用性。</p><p>虽然大多数作品主要关注实现高准确性，但也有一些尝试解决实际条件的特定方面，特别是关于噪声数据和端到端边缘系统的实时延迟约束。噪声数据可以看作是由于场景遮挡引起的不完整的人体骨架，或由于ID混淆导致多人设置中缺失的骨架。通过在Kinetics-Skeleton数据集[21]上进行评估，可以看出模拟实际分析的主要方法。这是通过使用OpenPose[4]从原始RGB图像中提取人体姿势信息而导出的流行Kinetics动作数据集的增强。然而，对于该特定数据集，准确性往往较低，SotA低于50%。部分原因可以通过使用不完美的姿势估计器引入的噪声来解释，但主要因素是对于该特定数据集，姿势数据不包括许多动作相关信息。这些原因包括带有移动摄像机的场景、第一人称视角的互动以及依赖于场景背景的信息。</p><p>总体而言，现有方法未能解决包括延迟约束和噪声数据在内的现实世界动作识别约束。这一失败阻碍了在具有边缘和实时要求的实际应用中的实现。这需要一种新的设计思维方式、形式化和领域规范，以应对甚至解决这些挑战，并同时保持强大的准确性。本文提出了实际图卷积网络（RW-GCNs）作为解决方案，以应对实际动作识别的固有约束。通过这样做，我们正在实现在边缘设备上部署实际动作识别。我们关注深度学习的信息瓶颈理论[58]，并受到神经科学的启发[24]，从而提出了关于处理边缘设备上的实时延迟和关键点噪声的注意反馈增强作为解决方案。RW-GCNs的反馈通过存储和重复使用过去的特征以及对重要特征的隐式时间采样，解决了在边缘设备上操作时保持准确性的问题。关注性反馈重新审视过去的特征并增强模型的区分能力。我们进一步推导了RW-GCNs的两个反馈变体：（1）语义反馈和（2）控制反馈。</p><p>为了全面评估与边缘实际约束条件相关的问题，我们为基于骨架的实际动作识别确定了一个现实的边缘系统，该系统满足现有云系统无法满足的隐私和可扩展性约束。此外，我们引入了两个新的计算性能指标：每秒动作（ApS），一种分析在多个延迟约束下的吞吐量的方法，以及动作产品延迟（APD），一种端到端系统延迟度量。然后，我们定量评估了RW-GCNs，包括语义反馈和控制反馈，以应对在实际边缘系统引起的延迟约束和噪声输入条件下的现实世界动作识别挑战。我们的结果显示，RW-GCNs在NTU-RGB-D-120数据集[30]上实现了94.16%的新SotA准确性，其延迟比ST-GCN基线少3.02倍。对Northwestern UCLA数据集[61]的评估显示，RW-GCNs可以在验证和训练中存在空间关键点噪声的情况下，以32.5倍的较低延迟实现90.4%的准确性。总的来说，RW-GCNs能够在完全端到端系统噪声存在的情况下以32.5倍的较低延迟运行，同时在Northwestern UCLA数据集上保持71.8%的准确性。总之，这项提议的工作做出了以下贡献：</p><ul><li>我们定义了领域约束，并引入了针对实际世界基于骨架的动作识别领域的新评估方法。</li><li>引入RW-GCNs作为解决同时处理延迟和噪声数据的实际约束的解决方案，通过基于信息理论的关注反馈架构增强。</li><li>在与现有实际边缘设备上的真实无约束工作的系统性能之间进行系统评估。</li><li>对端到端动作识别系统以及视频监控导向数据集上的紧急系统噪声进行了割离研究。</li></ul><p>接下来，第2节回顾相关工作。第3节介绍我们的领域约束和评估指标，以及与实际端到端边缘系统及其相关的新兴系统噪声。第4节提出了RW-GCNs作为在边缘和实时情况下启用基于骨架的动作识别的解决方案，并解释了语义和控制反馈的RW-GCNs的理论基础和实施细节。第5节呈现了该工作的结果、分析和评估。第6节包含我们的结论性意见。</p><h1>2 Related Works</h1><p>动作识别通常可以通过直接从视频信息中进行 [28]，或者通过时间利用人体姿势信息来完成。前一种方法可以称为基于像素的动作识别，并已在大型数据集上进行了广泛探讨 [21]。基于像素的动作识别方法必须对个体演员的空间外观特征具有鲁棒性，同时捕捉演员的时间动态，正如江等人在 [20] 中所做的那样。由于空间域中的噪声，这变得更加具有挑战性。现有作品在转移到动作数据集之前，利用大型图像数据集进行预训练，正如Tran等人在 [59] 中所示。动作识别的时间动态也需要被考虑，正如Liu等人在视频片段中以多个粒度查看的情况 [32]。</p><p>基于像素的动作识别的另一种选择是基于骨架的动作识别，从直观上来看，它与RGB视频流相比具有更少的噪声输入。正确提取该信息并对特定动作进行分类一直是一项持续的研究挑战。先前的方法依赖于基于循环神经网络（RNN）的模型的表达能力来处理图结构化输入 [25]。图卷积网络（GCNs），如Kipf等人的工作中所见 [22]，利用对非欧几里德图进行卷积，而不是在图像中找到的规则图。这个想法被应用于人体姿势图，用于动作识别，如Yu等人的工作中所见 [64]。最终的结果是ST-GCNs [64]。RW-GCNs可以被看作是RNN和基于ST-GCN的方法之间的中间体，类似于AGC-LSTMs [54]。</p><p>基于骨架的动作识别领域的当前最先进方法主要源自（或至少与）Yang等人的ST-GCN [64] 的工作有关。这些作品在ST-GCNs的基础上进行了多方面的创新。Liu等人的工作 [33] 解决了基线ST-GCN公式的现有偏见。第一个偏见是针对表示远程关节关系的。由于邻接矩阵中的冗余连接在ST-GCN的特征聚合中占主导地位，导致了这种偏见。提出的解决方案是删除现有的冗余连接。第二个偏见是针对复杂的时空特征传播的。这是由于将时空图卷积的因子分解为单独的层。这项工作另外提出了一个统一的运算符，同时处理数据的空间和时间方面。</p><p>一些作品没有将手工设计的设计原则编码到网络中，而是利用自适应图结构，使网络能够学习导致更稳健表示的设计原则。Plizzari等人的工作 [44] 将变压器引入，如自然语言处理领域所见 [60]。变压器的灵活性，最终的核心组件自我关注，使ST-GCN能够动态建模关节连接并学习依赖于数据的重要连接。虽然Plizzari等人 [44] 利用自我关注来克服基线ST-GCN公式的局限性，但仍然利用了图卷积形成（即在不规则图上进行显式卷积）。Shi等人的工作 [52] 超越了这一点，并完全用解耦的自我关注（解耦的自我关注在空间和时间上分解）取代了GCNs。通过利用位置编码，这项工作依赖于自我关注隐式模拟GCNs的能力。与增加图结构的灵活性不同，一些其他方法通过使输入表示更具信息性来改进输入表示 [26, 50, 51]。这些作品通过预处理现有的关节信息，进一步以骨骼和运动的方式明确定义输入。</p><p>尽管这些创新有助于更健壮的泛化和在关键数据集上实现整体更高的准确性，但很少有作品尝试解决在现实世界中部署ST-GCNs时面临的现实约束和困难。因此，大多数现有作品未能实现现实世界的部署，特别是考虑到某些应用所需的边缘设备的计算限制。Song等人的工作 [55] 是一个例外，该工作改进了输入表示，类似于 [26, 50, 51]，然后构建了一个专注于部分注意力的高效架构。Cheng等人的工作 [7] 也通过利用时间移位模块（TSMs）[28]来提高实时性能，作为时序卷积的有效替代方案。Yang等人的工作 [65] 利用反馈机制增加了时序采样策略的灵活性，并进一步减少了分类所需的帧数。最后，Yu等人的工作解决了噪声关键点的真实世界效应，并根据使用人工噪声进行训练的类型提出了解决方案 [67]。</p><p>与考虑现实世界部署要求时的工作方向不同。与Cheng等人和Song等人的工作不同 [7, 55]，我们没有为高效模型做出明确的贡献。相反，我们专注于输入数据依赖性，并在满足延迟约束的同时将我们的模型与大量输入要求解耦，类似于Yang等人 [65]。然而，我们并非利用密集连接层来控制必要的反馈，而是提出了一种基于注意机制的反馈机制，类似于Plizzaris等人 [44] 中发现的机制。此外，我们还考虑与Yu等人 [67] 类似的关键点噪声，尽管我们是在现实端到端动作识别系统的背景下这样做。通过这样做，我们希望实现在实时边缘应用中部署基于骨架的动作检测。</p><h1>3 Real-World Action Recognition: Domain Constraints, Gaps, and Methods</h1><p>虽然基于骨骼的动作识别领域取得了越来越多的成功，但与现实世界的约束有关的算法性能在很大程度上还未被探索。我们的工作希望能够启动现实世界中基于骨骼的动作识别的探索，这将使基于骨骼的动作识别能够部署在边缘上。在本节中，我们将分析诸如患者监控和安全等应用程序的实际部署限制，并定义子领域真实世界的骨骼动作识别。首先，我们简要回顾了现实世界的行动识别约束和边缘计算范式的必要性。然后，我们引入两个新的评估指标来捕获延迟约束。最后，我们将深入研究边缘视频监控应用中完整的端到端视觉处理管道背景下的噪声数据的细节。</p><h2 id="3-1-Real-World-Constraints">3.1 Real-World Constraints</h2><p>嘈杂输入：这个新的子领域的第一个主要约束是现实世界的数据。基于骨架的动作识别高度依赖以关键点形式呈现的人体姿势数据。通常，该领域中发布的作品假设提供了完美的人体姿势数据。这种完美的数据通常是通过耗时的手动标注或在实验室环境中使用昂贵且高度准确的传感器获得的。在实际部署中，姿势数据通常需要由专用的机器学习算法即时生成，导致数据不完美。这被称为噪声数据，在本文中我们假设这种噪声是由基于深度神经网络的多人姿势估计器生成的。我们采用了他们关于空间噪声的定义，即应该在帧中但未提供的人体关键点关节。然而，我们对他们对于时序噪声的定义进行了扩展，将其分为两个不同的类别。第一类是帧级时序噪声。当个体的数据完全缺失时，就会发生这种情况，无论是由于未能检测到一个人，还是由于在多人环境中误认为他们。第二类是感受野级时序噪声。这是一种隐式的噪声，定义为网络在能够查看整个动作剪辑的持续时间方面的能力的限制。</p><p>延迟约束：现实世界基于骨架的动作识别的第二个主要约束是延迟。虽然一些现有的作品关注基于骨架的动作识别的计算复杂性，但许多忽视了执行单个动作识别所需的帧数。通常，现有作品中用于分类单个动作（即剪辑大小或T）的帧数为300。如果我们假设神经姿势估计器以FPSin = 30运行（这取决于硬件，是一项非常庞大的成就），那么300帧的剪辑对应于10秒的延迟。这对于需要实时响应的应用，如患者监测、公共安全和一般视频监控而言，是无法接受的昂贵。</p><p>边缘计算约束：最后，由于考虑到现实世界基于骨架的动作识别需要额外的隐私和可扩展性考虑，边缘计算范式变得必不可少。大多数现有的视频监控实现从边缘的本地摄像头流式传输图像数据到云服务器，云服务器执行大部分计算。这使用户隐私无法得到保障，并限制了可扩展性。隐私对于启用智能边缘视频应用至关重要，其他作品为确保用户隐私而专门致力于使用计算昂贵的深度神经网络 [57]。在我们的工作中，靠近传感器的低功耗边缘设备用于进行姿势估计和动作识别的计算。通过直接在边缘处理视频流，传送到网络的唯一数据是经过预处理的，不包含个人身份信息。</p><p>这从根本上解决了通过云端传输个人数据的不安问题。此外，使用边缘设备来处理大部分计算正面影响系统的可扩展性。在基于云计算的系统中，可扩展性受云服务器的处理能力和网络吞吐量的限制。增加摄像头会导致对服务器的更多请求，这可能导致要么昂贵的服务器升级，要么由于无法满足系统计算需求而增加系统范围的延迟。当系统实现成本太高时，它会限制智能视频应用在边缘的推广，以及整体人工智能的普及。通过减少昂贵的云服务器，采用更便宜的本地边缘计算解决了这个问题，并已经在现有的深度神经网络服务平台中使用 [34, 35]。然而，边缘解决方案通常受到资源限制，这导致其他实现使用混合边缘/云解决方案 [38]，或者优化DNN以提高性能 [40, 45]。</p><p><strong>总结：3.1 现实世界约束</strong></p><p><strong>1. 噪杂输入：</strong></p><ul><li>真实世界的数据对于基于骨架的动作识别构成首要约束，依赖于以关键点形式呈现的人体姿势数据。</li><li>大多数文献中假设提供完美的人体姿势数据，但在实际部署中，姿势数据往往由专用机器学习算法即时生成，导致数据不完美，即噪声数据。</li><li>噪声数据主要由基于深度神经网络的多人姿势估计器生成，包括空间噪声和时序噪声（分为帧级和感受野级）。</li></ul><p><strong>2. 延迟约束：</strong></p><ul><li>实际世界基于骨架的动作识别的第二个主要约束是延迟。</li><li>许多现有作品关注计算复杂性，但忽略了执行单个动作识别所需的帧数。</li><li>典型情况下，现有作品的帧数为300，而这将导致10秒的延迟，对于需要实时响应的应用来说是昂贵的。</li></ul><p><strong>3. 边缘计算约束：</strong></p><ul><li>由于隐私和可扩展性的考虑，现实世界基于骨架的动作识别需要采用边缘计算范式。</li><li>大多数现有视频监控实现通过从边缘传输图像数据到云服务器进行计算，这违反了用户隐私且限制了可扩展性。</li><li>我们的工作使用低功耗边缘设备进行计算，解决了云端传输个人数据的问题，同时提高了系统的可扩展性。</li></ul><h2 id="3-2-Real-World-Evaluation-Metrics">3.2 Real-World Evaluation Metrics</h2><p>为了解决资源受限边缘系统的延迟约束，我们定义了两个新的指标，用于分析和研究动作识别算法在现实世界中的部署能力。第一个指标是每秒动作数（ApS）。</p><p>该指标表示模型在单个秒内能够完成多少工作（即吞吐量）。其中，T是剪辑大小，W是窗口大小，CpSin是每秒剪辑数，或者是完整剪辑被输入到网络的速率。这个指标是为了将受到延迟约束的作品（如我们的作品）与无约束的作品进行比较而需要的，因为它们在单个剪辑中完成了不同数量的工作。虽然模型接受的理论输入吞吐量（FPST）对于像自动驾驶汽车这样的快速应用非常重要，但视频监控领域的要求要宽松得多（对于人体运动为30 FPS）。相反，第二个指标动作产品延迟（APD）更具预测性，特别适用于视频监控等领域的实际部署性能。</p><p>APD是输入帧速率FPSin的倒数与网络窗口大小W之间的乘积。该指标表示处理单个动作所需的延迟量。在监测患者或危险工作环境等应用中，检测事故（例如摔倒、绊倒）的响应时间至关重要，其下限即为APD。</p><p><strong>总结：3.2 现实世界评估指标</strong></p><p><strong>1. 每秒动作数 (ApS)：</strong></p><ul><li>用于衡量模型在单秒内完成的工作量，即吞吐量。</li><li>考虑剪辑大小（T）、窗口大小（W）和每秒剪辑数（CpSin）。</li><li>用于比较延迟受限作品与无约束作品，因为它们在单个剪辑中执行不同数量的工作。</li><li>对于视频监控等领域，与模型接受的理论输入吞吐量（FPST）相比，具有更宽松的要求。</li></ul><p><strong>2. 动作产品延迟 (APD)：</strong></p><ul><li>是输入帧速率（FPSin）的倒数与网络窗口大小（W）之间的乘积。</li><li>表示处理单个动作所需的延迟量。</li><li>在监测患者或危险工作环境等应用中，检测事故的响应时间至关重要，其下限为APD。</li><li>更具预测性，特别适用于视频监控等领域的实际部署性能评估。</li></ul><h2 id="3-3-End-to-End-Video-Surveillance-and-Noisy-Data">3.3 End-to-End Video Surveillance and Noisy Data</h2><p>在现实世界的环境中，基于骨骼的动作识别将成为更大端到端视觉管道的一部分。图1展示了智能视频监控应用的典型视觉管道。该图受到[36]和[23]中提出的现有视觉管道框架的启发。如图1所示，这些框架通常包括三个概念阶段。我们还增加了一个额外的阶段，其中包含我们的RW-GCN网络。 RW-GCN的详细描述可以在第4.2节中找到。第一个阶段是检测阶段。多人体姿势估计器用于检测人体关键点并从输入视频的单个帧中生成边界框。接下来，在提取阶段，使用先前阶段的边界框生成人体图像裁剪。这些人体裁剪通过特征提取网络，为每个人体图像生成嵌入式特征表示。这些边界框还用于计算每个个体在与先前帧相比的帧中的时间位置。在匹配阶段，匹配算法测量当前帧特征与先前帧特征之间的欧几里得外观相似性，并考虑先前计算的时间位置，确定要分配给哪个人的ID。</p><p>这一端到端的视觉管道完全在靠近摄像机的边缘节点上运行，通过从多个节点聚合非敏感信息到边缘服务器，实现了[36]中所做的操作。</p><p><strong>空间噪声：</strong> 基于骨骼的动作识别需要人体姿势信息，以关键点骨架的形式作为输入。在封闭环境中的动作识别中，假定这些骨架是演员场景的无缺陷、完美的表示。然而，在现实世界中，由人体姿势估计器提供给网络的骨架通常存在缺陷。即使是最先进的方法也不是完美的，有时会出现错误，要么是误检测关键点，要么完全未检测到。这些错误在整体上可能是不可避免的，但当应用环境与姿势估计器训练的数据集不同时，错误会进一步加剧，而在现实世界中几乎总是这种情况。对于需要考虑硬件资源的系统，或者存在实时约束的系统，使用大型、高精度的最先进模型可能不是一个选择。在这些情况下，需要做出权衡，使用轻量级的姿势估计器，并经常牺牲准确性以满足应用要求，这可能进一步增加这些错误的频率。</p><p>这些缺失和错误检测的关键点在系统中形成了一种空间噪声，如图2所示。当传播到动作识别时，这种噪声可能会带来很大问题。图2上半部分的演员正在弯下身子将物品放入桶中。这对于人眼来说是清晰的。然而，用于动作识别的关键点骨架在这个场景中存在明显错误，特别是涉及演员右臂的部分。除了在检测方面通常不一致之外，在图中最右边的图像中，演员的右手被错误地检测到了他的脚踝。如果动作识别系统被设计为假定接收到的数据是完美的，这种类型的错误很容易使网络混淆，并导致不正确的动作分类。因此，在现实世界的动作识别中需要考虑这种类型的噪声。</p><p>为了在准确性和实时约束之间取得平衡，我们使用EfficientHRNet [37]来获取人体姿势骨架。</p><p><strong>时间噪声：</strong> 现实世界动作识别中常见的另一类噪声是时间噪声。时间噪声可以通过两种方式之一发生。第一种是演员的骨架在视频片段的一个或多个帧中丢失。通常，这是因为姿势估计器没有检测到骨架，或者因为演员的骨架在这一帧中没有与演员正确匹配。在这两种情况下，这导致动作识别网络在应该有骨架信息的情况下没有该演员的骨架信息。在封闭环境中的动作识别中，其中提供了完美的骨架和ID数据，这不是问题，但在现实世界的动作检测中，使用不完美的算法生成关键点和ID，这种类型的错误相当普遍。</p><p>第二种形式的时间噪声发生在边缘系统没有同时将动作的所有帧馈送到网络中。虽然这对于封闭环境的动作识别不是问题，在现实世界中，在延迟约束 dictate 下，只能一次处理少数帧的应用程序中，这种情况经常会遇到。</p><p>图2说明了第一类时间噪声的一个例子；关键点骨架丢失。虽然图2下半部分的演员在第一帧中被正确识别，但中间两帧的关键点骨架变灰，表明它们与演员没有正确匹配。这是由重新识别算法引起的错误。此外，在最后一帧中，根本没有检测到关键点骨架；这是神经姿势估计器的失败。凭借仅有的一帧姿势数据，动作识别网络将无法理解演员的运动和位置情况。直观地说，这将使得对于场景的动作识别非常具有挑战性，设计现实世界的动作识别时必须考虑这些情况。</p><p>我们选择使用与[36]中相似的Re-ID方法，这是一种不依赖于面部识别的现实解决方案。</p><p><strong>总结：</strong></p><p>这一段描述了基于骨架的动作识别在现实世界环境中的应用，作为更大端到端视觉管道的一部分。该视觉管道包括检测、提取、匹配等阶段，其中使用多人体姿势估计器进行关键点检测，并通过边缘节点进行端到端的实时处理。文章指出了两种主要噪声：空间噪声和时间噪声。空间噪声源于人体姿势估计的不完美，包括关键点缺失和错误检测，可能导致动作识别错误。为了平衡准确性和实时性，文章采用了EfficientHRNet等轻量级姿势估计器。时间噪声涉及骨架在视频中的丢失，以及边缘系统无法同时处理所有帧的情况。这种噪声使得动作识别在现实场景中更具挑战性，需要针对性的解决方案，如采用Re-ID方法。最后，介绍了Real-World GCNs（RW-GCNs）作为一种解决方案，通过端到端处理空间和时间噪声，提高对于现实世界骨架动作识别的适用性，尤其是在边缘计算领域的应用。</p><h1>4 Real-World-GCNs (RW-GCNs) through Attentive Feedback Augmentation</h1><p>为了满足现实世界骨骼动作识别的领域约束，本节介绍了实际世界GCNs（RW-GCNs）。在RW-GCN中，我们利用一个具有预定义结构的静态图，模仿了COCO数据集关键点格式[29]，用于空间图表示。时间组件由每个顶点通过相邻帧之间的一对一连接组成。这个时空人体姿势图被网络视为一个张量X ∈ RN×T×C×M，其中N是节点数，T是帧数，C是通道数，M是整个剪辑中单帧中分开的人体姿势的最大数量。此外，网络使用的邻接矩阵A ∈ RN×N用于定义图上的卷积。</p><p>我们基于ST-GCN的网络的空间组件是通过在输入缓冲区的单个帧上执行1×P标准的2D卷积来实现的，其中P决定了权重矩阵和邻接矩阵的分区数。然后，结果与一组归一化的P邻接矩阵相乘。根据定义的分区策略，权重矩阵和邻接矩阵可能被分割成子集，并按方程（3）中所示相加，该方程在[7]中进一步解释。RW-GCN利用了在[64]中定义的空间分割策略。RW-GCN通过细致的反馈增强了现有的ST-GCN架构。反馈可能增强ST-GCN在处理嘈杂输入方面的能力。馈送回架构的特征将包括更高级别的语义，如果通过时间缺乏变化，则可能是冗余的。如果神经姿势估计器的噪声与其估计相关，这些在时间上冗余的特征可能在空间上产生协同作用，这在定性上是成立的。总体而言，细致的反馈增强使RW-GCN能够应对各种噪声，特别是由现实世界的延迟约束引起的时间噪声。因此，RW-GCN为边缘计算领域的基于骨骼的动作识别应用打开了可能性。对于大多数计算在接近传感器边缘节点的系统，例如REVAMP2T [36]，RW-GCNs添加了一个灵活的网络，可以根据其他网络的计算强度使用不同数量的帧。对于像EdgeEye [31]这样将所有数据发送到本地边缘服务器的系统，RW-GCNs增强了边缘系统在处理网络带宽和传感器存储限制方面的灵活性。最后，对于更分布式的边缘系统，例如VideoPipe [46]，RW-GCNs相对于本地存储容量和网络带宽的灵活性将能够实现许多不同的流水线实现，每个都专门针对特定的应用。</p><p><strong>总结:</strong></p><p>上述段落介绍了Real-World GCNs（RW-GCNs），这是为了满足现实世界骨骼动作识别领域的约束而提出的一种方法。RW-GCN采用静态图和预定义结构，模拟了COCO数据集的关键点格式，用于构建空间图表示。时间组件由相邻帧之间的一对一连接组成。网络将这个时空人体姿势图视为张量，其中包含节点数、帧数、通道数和单帧中人体姿势的最大数量。空间组件通过在输入缓冲区的单个帧上执行标准的2D卷积来实现，并结合了一组归一化的邻接矩阵。RW-GCN采用了一种空间分割策略，并引入了有意义的反馈，以增强网络对嘈杂输入的处理能力。这种反馈包括高级语义特征，有助于处理时间上的冗余。最终，RW-GCN通过处理各种噪声，尤其是由于现实世界延迟引起的时间噪声，为边缘计算领域的基于骨骼的动作识别应用提供了可能性。这对于不同计算强度和网络带宽的系统提供了灵活性，为实时应用和边缘设备上的动作识别提供了解决方案。</p><h2 id="4-1-Theoretical-Aspects">4.1 Theoretical Aspects</h2><p><strong>4.1 理论方面</strong></p><p>我们关注的自注意力反馈增强的理论基础基于深度神经网络的信息瓶颈理论，由Tishby等人提出[58]。</p><p>虽然Saxe等人的工作[48]最初对这一理论提出了质疑，但Noshad等人的工作[41]随后证实了该理论，指出存在对互信息的计算问题，因为使用了较差的互信息估计器，Nosahd等人提出了他们自己的互信息估计器。此外，还有其他尝试验证信息瓶颈理论在更复杂架构上的应用[9]。尽管直接将信息瓶颈应用于深度学习存在困难，但其他研究已经表明，基于信息瓶颈理论的解决方案在开发时在泛化方面取得了明显的改进[19, 43]。我们的工作采用相同的方法，基于信息瓶颈理论的基础，开发了面向现实世界实时动作识别的体系结构级解决方案。</p><p>信息瓶颈理论假设深度神经网络可以被表示为马尔可夫链： X → T1 → … → Tz → Y 每一层的输出在马尔可夫链中由一个随机变量表示。例如，X是输入特征，T1是第一层的隐藏表示，Tz是第Z层的隐藏表示，Y是输出随机变量。</p><p>应用于深度学习的信息瓶颈理论认为学习等效于最小化以下拉格朗日函数：</p><p>我们可以解释方程（4）为学习具有两个明显且独立的阶段，通过最小化−I(T; Y)（我们可以将其视为最大化其倒数）和最小化I(X; T)。拟合阶段是第一阶段，如方程（5）所示： max I(T; Y) = H(T) − H(T|Y) （5） 拟合阶段由β调节，通过最大化隐藏特征的熵H(T)来学习信息性特征，使隐藏表示更具信息性。此外，拟合阶段最小化H(T|Y)，这最小化了当已知标签信息时隐藏表示的信息。从直观上看，这相当于删除与隐藏表示无关的特征。</p><p>学习的第二阶段是压缩阶段，如方程（6）所示： min I(X; T) = H(X) − H(X|T) （6） 压缩阶段最小化H(X)，即源数据的熵，同时最大化隐藏表示对输入的信息量H(X|T)。这导致了一个模型，它对输入信息的依赖较少。我们的工作假设利用具有助于拟合或压缩学习阶段的特性的解决方案，可以导致类似于Jeon等人[19]的强大算法解决方案。</p><p>RW-GCNs的第一个体系结构级解决方案基于自注意力。Bloem等人的工作[2]指出，注意力使网络能够通过捕获数据相关的不变性来学习和建模。了解信息瓶颈可以被看作是查找最小充分统计量的框架[53]，我们假设注意力使模型更有效地捕获数据集的最小充分统计量，这直接有助于模型与信息瓶颈框架相关的方面。这进一步得到了支持，因为注意力可以作为一种形式的预训练压缩[2]。我们此外强调了早停止（仅在验证准确性不再增加时停止训练）通常用于训练深度神经网络。其他研究发现，早停止通常（但不总是）在压缩阶段之前停止训练，暗示压缩阶段可能导致过拟合[43, 63]。鉴于此，可以将压缩阶段解释为学习要关注的信息。我们认为自注意力是一种足够灵活的归纳偏置（由于模仿和扩展归纳偏置的卷积的能力[1, 8]），可以实现非过拟合的压缩阶段。</p><p>反馈还可以协助模型的泛化，不是通过压缩阶段，而是通过拟合阶段。事实上，RW-GCNs的主要前提是保持良好的表示或最大化H(T)，尽管存在感受野级别的时间噪声（由延迟约束引起）。在考虑到这一前提的基础上，我们将目光投向拟合阶段，max I(T; Y)，将其视为一个经典的信道编码问题，其中我们的网络既是编码器又是信道本身，信道容量是互信息I(T; Y)。我们理论上认为，如果反馈可以协助经典通信信道中的信道编码，那么它可以通过改善神经网络训练的拟合阶段来协助深度神经网络。</p><p>已知，当反馈应用于具有不相关高斯噪声信道的传统通信通道时，信道容量不会增加[12]。但是，当噪声与输入相关（即带有记忆的信道）时，反馈可以导致信道容量的小幅提高。此外，反馈还可以显著降低信道编码器和解码器（即神经网络）的复杂性[6]，通过降低传统通信通道中接近容量所需的平均块长度。我们认为这相当于减少深度神经网络在单个时间步中需要看到的特征数量。我们将这与卷积神经网络层具有具有受限感受野的神经元（即更少的空间特征）以及网络深度作为一种模仿反馈能力以用更简化的复杂性实现合理准确性的机制相联系。已知，由于深度神经网络通常具有高度的层间权重共享[3, 47]，深度可以模仿反馈。</p><p>反馈还可以在无记忆多路访问信道（MACs）的容量中提供附加增益，其中多个信号传递到同一信道[15]。这可以看作是神经网络处理多个特征。通过反馈的能力增强信号的合作，可以实现容量的提高，使神经网络更好地共同优化多个竞争特征。当仅希望部分信息的多个信号时，可以实现信道容量的乘法增益[56]。我们将语义反馈视为一种试图通过利用高级语义信息（例如深度神经网络特征）来最大化H(T)或等效地增加特征表示的信息的机制，如在CliqueNets[66]中所见。我们认为这种高级语义信息通过反馈的能力降低编码器（即网络）的复杂性，从而允许我们使用更小的时间感受野。</p><p>此外，我们将控制反馈视为一种使统计信息提取与特定随机变量相关但与其他变量无关的机制。这种类型的反馈专注于最小化H(T|Y)，从而在已知目标分类的某些信息时减少隐藏特征的冗余。这可以看作是在传统多路访问通信信道中协调多个信号的反馈的类似物。此外，已知信息瓶颈在有条件地优化适用于不同输出随机变量的模型时具有交替公式[5]。根据反馈的不同，可能可以学习专注于不同输出随机变量的条件模型。直观地说，如果我们假设一个具有树、狗和人类类别的深度神经网络模型，最佳模型将不会将树的高级特征暴露给人类类别的分类。控制反馈可能有助于实现这种条件信息流。</p><h2 id="4-2-Implementation-details">4.2 Implementation details</h2><p>以下是段落的逐句翻译：</p><p><strong>4.2 实施细节</strong></p><p>为了解决第3节中讨论的领域限制，我们使用注意反馈增强了现有的ST-GCNs，创建了Real-World Graph Convolutional Networks（RW-GCNs）。在所有变种的RW-GCNs中，我们假设存在一种窗口大小W的延迟约束。当W等于动作片段T中的总帧数时，RW-GCN充当通用的ST-GCN。对于RW-GCN的基线实现，将包含T帧的片段分解为W个窗口，每个窗口包含T/W帧，并对每个窗口生成一个动作分类。RW-GCN采用滑动窗口操作，其步幅等于T/W。</p><p>这个有限的窗口大小导致了有限的时间感受野。为了克服这一问题，RW-GCN在处理当前窗口的同时，将先前窗口的信息传递到网络中。为此，我们探讨了两种专门的反馈形式，即（1）语义反馈和（2）控制反馈用于RW-GCNs。每种公式都使用过去的特征作为输入（除了当前特征），并实现两种专门的注意力形式（语义注意力和通道注意力）。接下来，我们详细讨论每种形式。</p><p><strong>总结：</strong></p><p>在这一部分，作者介绍了为了解决第3节中提到的领域限制而引入的Real-World Graph Convolutional Networks（RW-GCNs）的实施细节。为了处理潜在的延迟约束，他们采用注意反馈来增强现有的ST-GCNs。RW-GCNs的不同变体在窗口大小W的情况下操作，可以作为通用ST-GCN，但作者也介绍了基线实现，其中通过滑动窗口操作处理T帧片段，以克服有限的时间感受野。为了更好地处理这一问题，他们引入了两种专门的反馈形式：语义反馈和控制反馈。每个形式都使用过去的特征作为输入，并实现了两种专门的注意力形式。这些细节旨在提高RW-GCNs在处理实际世界骨架动作识别时的性能。</p><h3 id="4-2-1-Semantic-Feedback">4.2.1 Semantic Feedback:</h3><p><strong>4.2.1 语义反馈:</strong></p><p>我们的语义反馈设计有两个目标。第一个目标是在考虑延迟约束的同时解决空间和时间噪声。通过首先仅在输入的一个窗口（或时间切片）上操作，并保留上一个窗口的高级语义特征以供下一个窗口的处理，实现了这一目标。然后，我们将这些特征馈送到我们的语义空间-时间注意块（语义注意块）中，如图3所示。</p><p>语义注意块与原始的自注意机制有显著的不同，后者通常以方程（7）的形式存在。 自注意(<em>Q</em>,<em>K</em>,<em>V</em>)=<em>so<strong>f</strong>t<strong>ma</strong>x</em>(<em>Q<strong>K</strong>T</em>/√<em>d**k</em>)×<em>V</em> (7) 这个实现是通过过去的特征和当前特征之间的点积来实现的，而不仅仅是当前特征。这是一种交叉关注。如果X中的时间信息高度冗余，那么我们的语义关注公式几乎等同于简化的自关注[11]。这可以为模型提供捕获更强大的空间特征的机会，并在理论上处理空间噪声。然而，在存在多样化的时间特征（即运动非常快）的情况下，语义关注块使得可以捕获跨帧的成对交互，从而通过最大化H(T)实现更精细的特征表示。</p><p>此外，我们的语义关注旨在具有计算效率。通过简化方程（7）的公式，我们的语义关注块将键和值的计算合并为X的单一投影，而不是两个[11, 16]。我们还受到[11]中的工作的启发，不对输出进行规范化。当我们将X的输入之一替换为我们的过去特征FB时，这使得我们的语义关注块能够捕获语义反馈。</p><p>为了进一步提高计算效率，我们将过去的特征压缩成1X32张量，然后应用语义关注。尽管关注机制的计算复杂度呈二次扩展，但这限制了计算成本。最后，我们通过1×1卷积加上以零初始化的批归一化，以及在注意特征上带有乘法门的残差，添加了门，有助于网络的收敛。我们反馈增强的语义关注块的最终公式如图4所示，这个公式使我们的ST-GCN能够实现语义反馈。 语义关注块放置在标准ST-GCN之前，如图3所示。在这项工作中，我们保留了Yan等人的相同基础架构[64]，如表1所示。重要的是要注意，ST-GCN块由GCN层和TCN层组成。GCN层的工作与本节前面描述的相同。时态卷积只是通过时间维度进行的1×9卷积，具有可变的步幅。</p><p><strong>总结：</strong></p><p>本节介绍了语义反馈的实现细节。语义反馈旨在解决空间和时间噪声，并考虑了延迟约束。该机制通过在输入的窗口上操作并保留上一个窗口的高级语义特征，然后将这些特征馈送到语义空间-时间注意块中。与原始的自注意机制不同，语义关注块采用了一种交叉关注的实现，其中点积考虑了过去和当前的特征。这允许模型更好地捕获空间特征，并理论上处理时间噪声。此外，语义关注块设计为计算效率高，通过将过去的特征压缩为1x32张量并添加门来进一步提高效率。最终，该机制被放置在标准ST-GCN之前，以实现语义反馈。</p><h3 id="4-2-2-Control-Feedback">4.2.2 Control Feedback</h3><p><strong>4.2.2 控制反馈：</strong></p><p>与实施语义反馈不同，RW-GCN 也可以实施控制反馈。控制反馈仅使用图4中所示的语义关注公式的压缩反馈特征。与通过时间和空间对特征进行成对交互的建模不同，我们的控制反馈实现使用过去的时空特征来主动加权通道，并有效地控制哪些特征传播更强。我们通过利用简单高效的通道关注[62]来实现这一点。这可以解释为特征选择，并允许整个网络在理论上根据过去的信号调整其信息流，以预期未来的信号（即最小化H(T|Y)或等效地减少冗余特征）。</p><p><strong>总结：</strong></p><p>本节介绍了控制反馈的实现。与语义反馈不同，控制反馈仅使用语义关注公式的压缩反馈特征。控制反馈利用过去的时空特征，通过通道关注主动加权通道，从而有效地控制特征的传播。这实际上是一种特征选择，允许整个网络根据过去的信号理论上调整其信息流，以预测未来的信号。</p><h1>5 Experimental Results and Evaluation</h1><p>这项工作评估了我们提出的RW-GCN在现实世界骨架动作识别领域的几个变体，重点是视频监控应用。为此，我们将RW-GCN分为三种变体:(1)基线，(2)受控反馈，(3)语义反馈。RW-GCN的基线实现不使用语义或控制反馈，而是简单地将过去的框架窗口与当前的框架窗口平均起来。我们称这种变化为共识RW-GCN。相反，我们的语义反馈变异被称为RW-GCN-SF，我们的受控反馈被称为RW-GCN-CF。所有实验都在Nvidia-V100 GPU上进行训练，使用Nesterov加速随机梯度下降，动量项为M = .9，权值衰减为WD = 10−4。除非另有说明，否则基础开始学习速率是lr = .01，并有10的衰减。此外，当训练任何反馈增强的RW-GCNs(不是共识)时，我们在特定时期实现了一个不断增长的架构策略。我们取基础RW-GCN的权重，对语义反馈和控制反馈分别“增长”语义注意和有效渠道注意。我们相信，通过更好地捕捉数据的不变性，这些注意力模型可以导致更鲁棒的压缩阶段[2,63]。</p><h2 id="5-1-Latency-Constraint-Evaluation-and-Analysis">5.1 Latency Constraint Evaluation and Analysis</h2><p><strong>5.1 延迟约束评估和分析：</strong></p><p>对于延迟受限准确性实验，我们关注 NTU-RGB-D-120 [30] 数据集。NTU-RGB-D-120 数据集是当前最大的姿势 RGB+D 人体动作识别数据集。该数据集包含 120 种不同的室内动作类别，是评估骨架动作识别作品的当前事实标准。每个样本片段在 30 FPS 下最多有 300 帧。此外，该数据集在单个帧中最多有 2 人。数据集由 106 个独特的主体组成，配有 32 种可能的设置。总体而言，它包含超过 114,000 个视频样本和 8 百万帧。原始 NTU-RGB-D-120 数据集的姿势格式包含 3D 坐标空间中的 25 个节点 (x, y, z)。我们忽略深度维度，并将节点数量减少到 18，以与 REVAMP2T 等端到端系统兼容。 此次训练的具体超参数使用了 N = 32 的批处理大小。基线 RW-GCN 模型在第 30 和第 60 个时期进行了学习速率衰减（10^(-1)）。对于该数据集，我们仅关注第一个骨架序列，即使场景中可能有两个演员。RW-GCN-SF 变体在第 40 个时期采用学习率重新启动，并训练 45 个时期，总共 85 个时期。所有共识模型均在 60 个时期内进行了训练。我们没有对此数据集进行 RW-GCN-CFs 的测试。在验证结果时，我们使用交叉设置和交叉主体验证策略[30]。 我们分析的第一个实验结果是训练基线 RW-GCN 共识模型的准确性曲线与使用语义反馈的 RW-GCN-SF 相比。由于用于训练反馈增强网络的时期较多，有必要将训练超参数的贡献与我们的关注反馈增强的贡献分开。图 5 显示了当共识模型的训练延长到相似数量的时期时，其准确性停滞在其现有准确性。这些学习曲线使用 50 和 90 时期的学习率重新启动，并且没有使用增长的训练策略。我们发现，当使用增长的反馈增强注意力进行训练时，RW-GCN 可以在更少的时期内达到相似的准确性，避免过拟合。 接着，我们将受延迟约束的 RW-GCN 的准确性与第 2 节中先前提到的多个现有作品进行比较。这不包括具有额外模态的作品，如 RGB 补丁。表 2 显示，仅通过利用 COCO 格式的关键点，RW-GCN 就能够实现 87.7% 的跨主体准确性，比常规 NTU-RGB-D 骨架基线高出 16.4%。这是在没有延迟约束的情况下完成的，距离之前的 SotA 作品 DSTA-net [52] 仅有 1.6% 的差距。我们认为性能提高是由于输入的维度减少所致。通过将节点数量从 25 降低到 18 并去除深度通道，我们将输入的维度降低了 48%。直观地说，这些维度被解释为冗余的，使模型能够专注于学习而不是降维和特征选择。 跨主体的结果进一步显示，我们具有延迟约束的 RW-GCN 在 W = 30 的情况下表现为 70.6%，比 SotA 低了 17% 以上。这是在拥有 3 倍较少帧的情况下完成的，因此在动作分类之间有 3 倍较少的延迟。事实上，假设这项工作将基于 NTU-RGB-D-120 数据集的 30 FPS 输入，SotA（以及除 FGCN [65] 外的所有现有作品）将具有相当于 10 秒延迟的动作产品延迟，这在实时视频监控等真实应用中可能是 prohibitively expensive。RW-GCN 具有 1 秒的延迟，我们将其分类为次实时。当使用语义反馈时，RW-GCN-SF 实现了 86.4% 的准确性，距离之前的 SotA 仅有 1.3% 的差距。这显著缩小了与 SotA 之间的差距，同时满足次实时延迟约束，从而使新兴的边缘应用得以实现。当放宽延迟约束以允许 3.33 秒的延迟时，我们成为新的 SotA（就交叉主体验证而言），准确性为 94.16%。这表明，我们的方法不仅满足了实际骨架动作识别的子领域，而且为基于骨架的动作识别作出了贡献。在进行交叉设置验证时，我们看到我们的模型并没有定义新的 SotA，而是仅比 SotA 少 0.4%。交叉主体和交叉设置结果之间的这种差异可能可以通过两个因素来解释。第一个因素是缺乏深度。由于我们依赖于 COCO 关键点，由于训练集和验证集之间的视点不同（以及缺乏特征来通知模型样本视点），我们在域中经历了更多的转变。第二个因素是跨主体的时间方差，这也导致验证和训练集样本分布之间的差异。由于我们的注意力反馈增强具有更大的灵活性来适应时间变化，因此可以学会对个体主体时间变化不变，最小化训练集和验证集之间的分布转移。这种解释在基线 ST-GCN 在交叉设置验证中的更好性能方面受到轻微质疑。这表明在存在时间接收场噪声的情况下，缺乏深度只是一个问题。</p><p>我们相信这种准确性是由于语义关注块在时间信息高度冗余或无关时（即窗口内的帧受到接收场噪声的影响较小）能够编码空间成对特征交互的能力，稀疏的时间特征交互需要对表示进行编码。因此，我们认为在时间噪声较低时，我们的模型编码了更有区别性的空间特征，从而实现了高准确性。我们还分析了其他作品在延迟约束方面与我们的性能匹配的能力，如表 3 所示。对于这个分析，我们没有训练这些现有方法，而是使用基本的共识反馈对它们进行验证。我们发现，如果没有训练，这些作品无法在真实世界的领域中执行，最好的作品仅达到 10.12%。 我们在 NTU-RGB-D-120 数据集上的最后一个延迟约束分析是对专注语义反馈增强到 ST-GCN 上的概括改进的消融研究。这种消融允许我们看到语义反馈执行两种操作模式。在 W = 30 的第一个延迟约束中，语义反馈发挥其预期功能，通过关注成对的特征交互保留模型的过去特征。W = 30 的延迟约束可能缺乏时间冗余，并隐含地强制执行类似于 FGCN [65] 的稀疏采样策略。因此，模型保留了其在长程时间特征交互方面的能力，并且准确性仅比其基线无约束 ST-GCN 实现低 1.3%，如表 2 所示。 当受到 W = 60 的窗口约束时，ST-GCN 的操作模式开始转变。它记录的特征更具有时间冗余性，而模型自然具有存储更长程时间相互作用的能力。因此，该模型改善了无约束基线 +2.3%。我们认为，这种窗口约束平衡了提高其空间判别能力与保持暂时性特征保持能力之间的任务。由于这两项任务的复杂性，与 W = 30 测试中的 16.4% 改进相比，模型在共识模型的准确性改善方面仅有 6.4%。最后，使用 W = 100 的约束，语义关注块可以保持最少量的时间特征表征力量，并专注于更有效地编码空间特征。由于共识模型无法提高 ST-GCN 的空间表征能力，共识模型的准确性停滞不前。</p><p><strong>总结：</strong></p><p>在延迟约束评估和分析章节中，研究侧重于使用 NTU-RGB-D-120 数据集进行实验。该数据集是目前最大的骨架动作识别数据集之一。实验中采用了 RW-GCN 和 RW-GCN-SF 变体，其中 RW-GCN-SF 引入了语义反馈机制。实验结果显示，通过利用 COCO 格式的关键点，RW-GCN 在无延迟约束情况下就能够实现较高准确性，较之前的 SotA 作品有显著提升。在具有延迟约束的情况下，RW-GCN 在较少的帧数下仍能取得较高的准确性，相比于其他作品具有更低的延迟。语义反馈机制的引入进一步提高了模型的准确性，特别是在满足次实时延迟约束的情况下。此外，与其他现有方法相比，该模型在没有进行训练的情况下也能够在真实世界领域表现出色。最后，通过对语义反馈的消融研究，显示了其在不同延迟约束条件下的作用，从而揭示了其对模型性能的影响。</p><h2 id="5-2-End-to-End-System-Error-Propagation-Analysis">5.2 End-to-End System Error Propagation Analysis</h2><p><strong>5.2 端到端系统误差传播分析</strong></p><p>端到端系统噪声分析超越了延迟约束分析，着眼于现实端到端边缘系统的新兴系统噪声。使用的数据集是 Northwestern-UCLA 数据集，也是一个骨架动作识别数据集。它包含 1494 个视频片段，每个片段平均包含 18.5 帧，标准差为 ±13.25。该数据集有 10 个可能的动作类别，非常适用于视频监控应用。采用了 [65] 使用的 Cross-View 验证策略，其中略去最后一个摄像头视图。</p><p>为了真正评估适用于视频监控应用的端到端系统噪声，我们将该数据集的 RGB 图像通过 REVAMP2T 处理，使用 EfficientHRNet-H0 [37] 和 REVAMP2T 论文 [36] 中使用的 MobileNetV2 特征提取器。这导致由于神经姿势估计器引入的关键点的空间噪声，以及由 REVAMP2T 的 Re-ID 模型带来的帧级时间噪声，另外还有极端的延迟约束 W = 10。</p><p>由于 Re-ID 改变了检测到的人数以及数据集的自然时间变化，输入张量 X ∈ RN×T×C×M 具有广泛变化的 T 和 M 维度。我们利用动态批处理步骤来处理这一问题，首先使用重复剪辑填充 T 维度，直到批处理中最大剪辑的长度。张量的 M 维表示多个人。由于 Re-ID 的误差传播到动作识别，我们可能在 M 维度中以不连续和不规则的方式中有单人剪辑。为解决这个问题，动态批处理会自适应地将输入求和到较小的 M = λ。如果 λ = 1，则称为 ID 无关场景，此时我们不再建模来自 Re-ID 的误差传播。在样本的 M &lt; λ 的情况下，我们将张量填充为 M = λ。</p><p>对于训练，所有共识模型最初训练了 30 个 epoch，然后在 epoch 30 时在共识模型的权重上添加了反馈模型，并进行额外的 40 个 epoch 的训练。由于动态批处理引入了时间噪声，我们使用批量大小为 8。验证批处理大小设置为 1，以最小化动态批处理带来的时间噪声。这组实验还利用了附加到每个 FB-Augmented 注意力的可学习乘性门，从零开始初始化。这使得反馈增强模型能够顺利地集成到共识模型中。</p><p>表 4 显示了在 NW-UCLA 动作数据集上的噪声输入下的准确性结果。它还将我们的方法与 FGCN [65] 进行了比较，后者是一个同样引入反馈的现有作品，作为 SotA。在没有空间关键点噪声和 Re-ID 噪声的情况下，我们超越了 SotA 1.3%。此外，结果表明，尽管 RW-GCN-SF 具有 W = 10 的延迟约束、EfficientHRNet-H0 的嘈杂关键点和 REVAMP2T Re-ID 带来的帧级时间噪声，但我们的方法仍能够与 FGCN 架构竞争。我们报告了 90.4% 的准确性，仅比 FGCN 的 SotA 低 4.9%。此外，Yang 等人 [65] 报告了他们的准确性，考虑到一个 64 帧的延迟约束（与我们的 10 帧相比），并随机采样。</p><p>我们相信，尽管存在上述噪声，我们能够在超过 90% 的准确性下进行动作识别，这对于现实世界的骨架动作识别和智能边缘视频监控应用至关重要。这一点在表 3 的结果中得到了进一步的加强，表明现有作品无法适应延迟约束的感受场噪声。此外，我们继续分析控制反馈和语义反馈模型之间准确性的差异。表 2 强调了控制反馈的表现甚至不如基线共识模型，而且无法与语义反馈一起使用。我们的主要假设是，由于 RW-GCN-SF 建模了成对的双线性特征交互，而 RW-GCN-CF 变体仅建模了线性交互，因此控制反馈的效果不如语义反馈。由于特征的极端延迟约束导致缺乏时间冗余，这一情况更加恶化。此外，通过分析动态批处理注入的时间噪声（通过将验证批处理 N &gt; 1），控制反馈方法达到了 84%，而语义反馈为 83.1%。我们的逻辑推测是，动态批处理过程中特征的不规则复制模拟了嘈杂的循环反馈，即通过相同的网络权重反复推送相同的特征，并且这充当了信息处理的形式。这种信息处理方式更类似于传统循环神经网络处理时间信息的方式。可以说，我们的 RW-GCNs 存在于一个模型家族中，其中网络深度和循环处理的依赖因每个变体而异。AGC-LSTM [54] 模型将是与我们的模型相反的变体，它在循环和注意力上具有较大的依赖，但对网络深度的依赖较小。对于像 AGC-LSTM 这样的模型，控制反馈可能比语义反馈更可靠，因为它控制信息传播。</p><p>Northwestern-UCLA 数据集的最后一项分析是关于 Re-ID 误差传播效应的消融研究。我们在两种情况下测试了 RW-GCN 架构。第一种是 ID 无关场景，其中 RW-GCN 试图通过对输入的 M 维度求和来人为地消除时间噪声，将被剪辑分开的时间特征重新连接成一个单一的 1 × T × V × C 张量。尽管这在人多、执行不同动作的拥挤场景中理论上可能效果较差，但 Northwestern-UCLA 数据集 [61] 的这类样本很少。我们看到 RW-GCN-SF 的最佳性能达到了 90.4% 的动作识别准确性。在第二种情况下，我们分析了与 Re-ID 噪声相关的真实动作识别。我们变化了衡量时间 IoU 过滤器与外观特征距离的超参数。我们发现 RW-GCN-SF 在 71.8% 的准确性上最为弹性。我们结果的一个有趣方面显示，以外观为中心的 Noise 1 对 RW-GCNs 的抑制更为明显。虽然这可能只是数据集的一个方面，但我们认为这值得未来进一步探讨。</p><p>总的来说，我们得出结论，通过沿 M 维度对输入特征进行零填充会引入太多噪声，从而降低了所有模型在真实场景中的准确性。预期基于 IoU 的 Re-ID 将优于基于外观的 Re-ID，但从外观噪声测试的顶级网络中仅获得了较小的改进。</p><p><strong>总结：</strong></p><p>本节继续对提出的骨架动作识别模型进行详细分析，特别是在端到端系统的噪声和误差传播方面进行了探讨。使用 Northwestern-UCLA 数据集，作者模拟了真实世界的视频监控应用场景，通过 REVAMP2T 处理 RGB 图像，并引入了空间关键点和 Re-ID 模型的噪声。在不同的延迟约束下进行了实验，并使用动态批处理处理由 Re-ID 引入的时间噪声。</p><p>实验结果表明，在实际场景中，即使存在大量噪声，提出的 RW-GCN-SF 模型仍能实现超过 90% 的动作识别准确性，超越了同类模型 FGCN。此外，作者分析了控制反馈和语义反馈模型之间的性能差异，发现语义反馈相对于控制反馈更为有效，特别是在极端的延迟约束下。最后，通过消融研究，作者进一步验证了模型对 Re-ID 误差传播的鲁棒性，并提出了对动作识别系统设计的有价值见解。</p><p>总体而言，提出的模型在复杂实际场景下表现出色，为骨架动作识别在视频监控等领域的应用提供了强大的性能和鲁棒性。</p><h2 id="5-3-Real-Time-Performance-Analysis">5.3 Real-Time Performance Analysis</h2><p>本节对模型在边缘设备上的实时性能进行了交叉分析，使用的是 Nvidia Jetson Xavier NX 设备。为了公平评估实时性能，引入了两个度量标准：每秒动作数（ApS），由方程（1）计算，以及动作产品延迟（APD），由方程（2）计算。值得注意的是，ApS是特定于设备的，而APD是端到端系统的。</p><p>该模型直观地支持更高的ApS，因为它在一个剪辑中产生多个动作分类。虽然在考虑实际场景时可能会贬低每个剪辑的多个输出的必要性，但要承认动作执行的速度既未知又依赖于数据。此外，动作识别模型通常不知道输入的帧率（除非经过训练，大多数模型没有），这导致了需要支持比理想训练场景中更大的ApS，并为将来训练模型以对不同输入帧率进行不可知处理提供了潜在可能性（如果不加以控制，可能导致准确性下降）。RW-GCN以 41.7 ApS 报告了最高性能，比现有模型高出 1.6 倍。从性能的角度来看，语义反馈的开销在ApS和参数方面都很小。正如在第4节中提到的，语义反馈被设计成轻量级，只在32个通道上工作。Shift-GCN 是参数效率最高的模型，因为它是专门设计用于轻量级和高效推断的，通过利用时间步长之间的数据移动。这种模型设计与 RW-GCNs 不是相斥的，在未来可以进一步探讨。</p><p>值得注意的是，RW-GCN 模型的 ApS 随着帧大小的变化而变化很大。这是由基于 GPU 的系统的执行偏差导致的。当我们减小窗口大小时，RW-GCNs 的数据依赖性被序列化并失去了性能效率。另一方面，当窗口大小增加时，由于需要更多帧来计算单个动作，我们面临与其他模型相同的问题。APD 的计算依赖于 EfficientHRNet 的 H0 FPS 为 22.95。RW-GCN 实现了最低的 APD，为 0.4 秒，这是从 Northwestern-UCLA 数据集得出的极端延迟结果。</p><p>最后，我们分析了 RW-GCNs 在轻量级和廉价边缘设备上的可扩展性。我们关注 Nvidia Jetson Nano，而不是之前在表6中展示的 Nvidia Xavier NX。截至本文撰写时，Jetson Nano 的价格几乎是 Xavier NX 的 10 倍。价格是确定系统成本可扩展性的重要因素，实际上通过负担得起的价格推动了人工智能的民主化。然而，Nano 的资源更为有限，只有一个 128 核的 Maxwell GPU，而 Xavier NX 则有一个 384 核的 Volta GPU 和 48 个 Tensor 核。</p><p>这是在更大的 RAM 带宽（约2倍）和更大的内存容量（约4倍）的基础上。我们使用 ApS 衡量我们的计算如何映射到这样一个受限环境。我们使用剪辑大小为 30 和窗口大小为 10 来计算 ApS。当一个场景中只有两个人时，我们看到 ApS 下降了 2.67 倍。当扩展到 8 人时，这一差距增加到 7.6 倍的 ApS 减少。我们认为这对于实时动作检测来说已经足够了，实际上我们相信可以进一步牺牲 ApS 来支持更多人数的场景。从理论上讲，我们可以顺序地跨越与我们的滑动窗口解决方案相似数量的人，以较大的时间框架处理场景。这可能允许我们以 1 的 ApS 处理包含 36 人的场景。未来的方向可以通过将更深的层级卸载到类似于[39]的服务器中来进一步优化。通过选择卸载深层次，可以保护隐私。另一个方向是利用输入自适应计算，类似于 Fang 等人的工作。【13】</p><p><strong>5.3 实时性能分析总结：</strong></p><p>本节详细分析了模型在边缘设备上的实时性能。使用 Nvidia Jetson Xavier NX 进行测量，并引入了两个度量标准：每秒动作数（ApS）和动作产品延迟（APD）。结果显示，RW-GCN 在 ApS 方面表现最佳，达到 41.7，比现有模型高出 1.6 倍。在考虑语义反馈的开销时，性能仍然相对较高。Shift-GCN 在参数效率上领先，专为轻量级和高效推断设计。值得注意的是，RW-GCN 模型的 ApS 随帧大小变化而变化较大，这与 GPU 执行的特性有关。最终，RW-GCN 在动作产品延迟方面取得了最佳结果，为 0.4 秒，这是在 Northwestern-UCLA 数据集上得出的。</p><p>最后，对于在轻量级和廉价边缘设备上的可扩展性，使用 Nvidia Jetson Nano 进行测试。虽然 Nano 相对于 Xavier NX 更便宜，但其资源更有限。在不同人数的场景下，ApS下降了，但仍然足够支持实时动作检测。此外，对于将模型部署到更大范围的场景，可以通过进一步优化和利用服务器进行深层次卸载来实现。</p><h1>6 Conclusion</h1><p>总的来说，本文定义了真实世界骨架动作识别的子领域，并利用基于信息理论的原理引入了Real-World Graph Convolutional Networks（RW-GCNs），在NTU-RGB-D-120数据集上取得了新的SotA准确率，达到94.16%。相比ST-GCN基准，RW-GCNs的延迟降低了3.02倍。此外，RW-GCNs在仅有3.8%准确率损失的情况下，可以比基准实现降低10倍的延迟。 通过对Northwestern UCLA数据集的评估显示，RW-GCNs在验证和训练中存在空间关键点噪声的情况下，可以达到90.4%的准确率，延迟比基准ST-GCN降低了32.5倍。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，在Northwestern UCLA数据集上的延迟比基准ST-GCN降低了32.5倍，并保持71.8%的准确率。所有这些都是通过一种注重隐私和可扩展的边缘计算方法实现的，其中系统成本每个节点可以降低10倍，同时仍然保持了一定范围的吞吐量，具体取决于场景的复杂性（从15.6到5.5 ApS）。 这项工作标志着真实世界骨架动作识别子领域的开始。通过设计RW-GCNs，本研究希望促进先前不可行的新边缘计算应用程序的设计和创建。然而，我们认为这一新兴领域仍然存在许多挑战，如时间变化、特定环境场景动态和进一步的应用特定约束。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance&lt;/h1&gt;
&lt;h1&gt;abstract&lt;/h1&gt;
&lt;p&gt;摘要： 动作识别是新兴</summary>
      
    
    
    
    
  </entry>
  
</feed>
