<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fomalhaut🥝</title>
  
  
  <link href="https://www.fomal.cc/atom.xml" rel="self"/>
  
  <link href="https://www.fomal.cc/"/>
  <updated>2024-09-17T03:01:52.996Z</updated>
  <id>https://www.fomal.cc/</id>
  
  <author>
    <name>Fomalhaut🥝</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>boss_and_job</title>
    <link href="https://www.fomal.cc/posts/c0bca80f.html"/>
    <id>https://www.fomal.cc/posts/c0bca80f.html</id>
    <published>2024-09-17T02:42:34.000Z</published>
    <updated>2024-09-17T03:01:52.996Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>网站重建，梦想开启</title>
    <link href="https://www.fomal.cc/posts/c318edab.html"/>
    <id>https://www.fomal.cc/posts/c318edab.html</id>
    <published>2024-09-06T14:10:38.000Z</published>
    <updated>2024-09-07T14:14:37.406Z</updated>
    
    <content type="html"><![CDATA[<h1 id="网站重建，梦想开启"><a href="#网站重建，梦想开启" class="headerlink" title="网站重建，梦想开启"></a>网站重建，梦想开启</h1><h2 id="博客重启的巴拉巴拉（必备工具）"><a href="#博客重启的巴拉巴拉（必备工具）" class="headerlink" title="博客重启的巴拉巴拉（必备工具）"></a>博客重启的巴拉巴拉（必备工具）</h2><p><a href="https://blog.cuijiacai.com/blog-building/">https://blog.cuijiacai.com/blog-building/</a></p><p><a href="https://www.bilibili.com/video/BV1qD4y1z783/?vd_source=3d5976f4f96ce120bf88891647d386cb">超详细的个人博客搭建教程（无需服务器）- 从原理到实践手把手带你搭建属于自己的个人博客_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">https://www.bilibili.com/video/BV1G84y1B7NH/?spm_id_from=333.788&amp;vd_source=3d5976f4f96ce120bf88891647d386cb</a></p><p><a href="https://www.fomal.cc/posts/e593433d.html">🐖抓到你啦～ (fomal.cc)</a></p><h1 id="重启操作教程"><a href="#重启操作教程" class="headerlink" title="重启操作教程"></a>重启操作教程</h1><p>熟能生巧</p><p>github 用git访问+chatgpt的回答帮助</p><ol><li>nodejs + nmp  安装+版本匹配<ol><li>删除nmp</li><li>更改下载源</li></ol></li><li>hexo 下载 单独安装配置up项目</li><li></li><li></li></ol><p><img src="../assets/image-20240906221418850.png" alt="image-20240906221418850"></p><h1 id="心路历程"><a href="#心路历程" class="headerlink" title="心路历程"></a>心路历程</h1><blockquote><p>2024年5月3日 gitee的page服务 被关闭，不在对外访问，导致自己的网站无法访问，也不能再继续写笔记和完成自己的博客l</p></blockquote><p>在2024年9.6日 重新为了封装简历，想着哪怕拿出一天的时间，全部花进去，用来把自己的博客后台迁移，从不能访问的gitee 迁移到github</p><p>花了一天的时间，从一开始7:00左右的信心满满，动力十足，到快到10：20左右，【在这期间成功的把之前的GitHub不能用git访问的问题解决了这个问题困扰了好久】然后跟我家美女一起改简历，搬到小教室里边，用自己的手机开热点，直到11：40左右，成功的能够跟之前一样，用hexo d 和hexo -g 完成数据往远端传输，搜着网页上说问题是网络不好</p><p>而且在用hexo d 传输的时候，蹦出一个网页，用来验证github的账号登录情况，这时候就知道能够完成数据传输了</p><p>当时是开着热点在小教室里，就没去吃饭，一直开着热点，给手机充电，期望能够成功，幸好最后成功传输上去了！！！</p><h4 id="但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs-和hexo-版本问题，导致hexo下载主题失败"><a href="#但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs-和hexo-版本问题，导致hexo下载主题失败" class="headerlink" title="但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs 和hexo 版本问题，导致hexo下载主题失败"></a>但是页面确实不能正常展示，估计是渲染主题的问题，也有可能是nodejs 和hexo 版本问题，导致hexo下载主题失败</h4><p>npm  与 node 匹配 </p><p>下载使用的node.js 是12.19.0版本</p><h6 id="nodejs和npm版本不匹配：ERROR-npm-is-known-not-to-run-on-Node-js-v10-16-2"><a href="#nodejs和npm版本不匹配：ERROR-npm-is-known-not-to-run-on-Node-js-v10-16-2" class="headerlink" title="nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2"></a>nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2</h6><p><a href="https://blog.csdn.net/qq_41992943/article/details/124925758">nodejs和npm版本不匹配：ERROR: npm is known not to run on Node.js v10.16.2-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_45056216/article/details/101163126">关于node js和 npm 版本不匹配的情况_linux中npm版本个nodejs版本不匹配解决方法-CSDN博客</a></p><p>删除node 重装</p><p>node 12.19.0 重装</p><p>更改 npd 源头</p><h6 id="npm报错：request-to-https-registry-npm-taobao-org-failed-reason-certificate-has-expired"><a href="#npm报错：request-to-https-registry-npm-taobao-org-failed-reason-certificate-has-expired" class="headerlink" title="npm报错：request to https://registry.npm.taobao.org failed, reason certificate has expired"></a>npm报错：request to <a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a> failed, reason certificate has expired</h6><p><a href="https://blog.csdn.net/maoge_666/article/details/136038003">npm报错：request to https://registry.npm.taobao.org failed, reason certificate has expired-CSDN博客</a></p><p>hexo  重装——直接在新文件夹中安装</p><h6 id="win10系统使用npm安装hexo失败"><a href="#win10系统使用npm安装hexo失败" class="headerlink" title="win10系统使用npm安装hexo失败!"></a>win10系统使用npm安装hexo失败!</h6><p><a href="https://github.com/hexojs/hexo/issues/2741">win10系统使用npm安装hexo失败! · Issue #2741 · hexojs/hexo (github.com)</a></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="selector-tag">i</span> -<span class="selector-tag">g</span> hexo-cli    </span><br><span class="line">sudo npm install gulp-cli -<span class="selector-tag">g</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://github.com/hexojs/hexo-cli/issues/23">hexo-cli 安装后失败 ·问题 #23 ·hexojs/hexo-cli 命令 (github.com)</a></p><p>git上传文件和文件夹</p><h6 id="Github如何上传项目-超详细小白教程"><a href="#Github如何上传项目-超详细小白教程" class="headerlink" title="Github如何上传项目(超详细小白教程)"></a>Github如何上传项目(超详细小白教程)</h6><p><a href="https://blog.csdn.net/KevinRay_0854/article/details/140408003">Github如何上传项目(超详细小白教程)_github上传项目-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_40413898/article/details/104287509#:~:text=git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤 1 1、在GitHub中新建一个仓库 在创建仓库是，一些配置说明如下： Repository name%3A 仓库名称 Description,. ... 5 5、最终结果 将本地仓库与远程仓库连接 再次要推送文件时，需要与远程仓库进行交互，使用git remote指令。">git基本操作：将指定文件夹内容或代码文件推送到GitHub中的基本步骤_github推送文件夹视图工具-CSDN博客</a></p><h4 id="验证登录框"><a href="#验证登录框" class="headerlink" title="验证登录框"></a>验证登录框</h4><blockquote><p>为了方便下次再碰到这种问题，记录一下蹦出的网页情况</p></blockquote><p>网页蹦出来是个小的登录界面，可以直接打开链接，然后验证与github账号的关联，同时确定能够跟这个电脑设备关联上。</p><p>打开链接，是一个github页面，然后输入设备登录码！！！很重要的成功标志</p><blockquote><p>幸好chatgpt帮助 解决 git 访问github</p><ol><li>配置对应的github库，需要设置好远程库与用户名一致，然后后边加上github.io</li><li>先将up的模板放到github库中，详见配置教程</li><li>再将自己的test文件上传到对应的目录中，在进行hexo d 上传</li></ol></blockquote><h1 id="秋招文件收集"><a href="#秋招文件收集" class="headerlink" title="秋招文件收集"></a>秋招文件收集</h1><p>数据库项目</p><p><a href="https://articles.zsxq.com/id_gsuoxn7qnvip.html">轮子实战项目：手写数据库 MYDB (zsxq.com)</a></p><p><a href="https://blog.csdn.net/qq_40856284/article/details/121580249">一起写个数据库 —— 0. 项目结构和一些不得不说的话-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_40856284/category_11504274.html">一起写个数据库_何人听我楚狂声的博客-CSDN博客</a></p><p>技术派</p><p><a href="https://paicoding.com/article/detail/472">如何将技术派写到简历上？-技术派 (paicoding.com)</a></p><p>PmHub</p><p><a href="https://www.yuque.com/canghe-u0ocv/laigeoffer-pmhub/mdoff4twogexi4ri">✅人人都是产品经理，打造竞争优势（🌟新人必看） (yuque.com)</a>   cgd9</p><p>八股学习</p><p><a href="https://javabetter.cn/sidebar/sanfene/nixi.html">面渣逆袭必看，面试题八股文Java基础、Java 集合框架、Java 并发编程、JVM、Spring、Redis、MyBatis、MySQL、操作系统、计算机网络、RocketMQ、分布式、微服务、设计模式、Linux👍 | 二哥的Java进阶之路 (javabetter.cn)</a></p><p>面试题整理</p><p><a href="https://top.interviewguide.cn/">InterviewGuide大厂面试真题</a></p><h2 id="面试经验"><a href="#面试经验" class="headerlink" title="面试经验"></a>面试经验</h2><blockquote><p>刷面经</p><p><a href="https://www.yuque.com/snailclimb/mf2z3k/guh0u9hb3pr70rtk">大厂四年，2024 阿里、字节、蚂蚁、小红书面试经历分享 (yuque.com)</a>      《面试指北》cnk4</p></blockquote><p><a href="https://interviewguide.cn/notes/05-xiustar/02-campus_prepare/04-01-互联网面试总结.html">⭐阿秀三个多月、50余场面试经验浓缩为经验和总结 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>简历修改</p><p>优质简历样例——<a href="https://articles.zsxq.com/id_5p3fcdmwpw4h.html">精选简历：基本信息、教育背景、专业技能、工作经验、项目经历、实习经历、荣誉证书写的比较好的例子 (zsxq.com)</a></p><p><a href="https://wx.zsxq.com/dweb2/index/topic_detail/182882241124882">知识星球 | 深度连接铁杆粉丝，运营高品质社群，知识变现的工具 (zsxq.com)</a></p><p>如何写好简历——<a href="https://www.yuque.com/itwanger/gykdzg/grvrtx?singleDoc=#e9486e82">如何写好简历？（完结） (yuque.com)</a>——王二</p><p>毒液   <a href="https://erdengk.top/archives/jian-li--mian-shi">简历、面试 | 我的知识海洋 (erdengk.top)</a></p><h2 id="知识星球常用"><a href="#知识星球常用" class="headerlink" title="知识星球常用"></a>知识星球常用</h2><p><a href="https://wx.zsxq.com/dweb2/index/favorites">收藏</a></p><p>秀哥资源——<a href="https://articles.zsxq.com/id_dmf00wjw9dtm.html">🏆资源沉淀 (zsxq.com)</a></p><p>秀哥问答——<a href="https://articles.zsxq.com/id_wm06pp9dnogj.html">📖知识图谱（星球问答文章精华） (zsxq.com)</a></p><p><a href="https://interviewguide.cn/notes/05-xiustar/03-resume/01-00-简历开篇词.html">简历远远比你想要的重要的多 | 阿秀的学习笔记 (interviewguide.cn)</a></p><p>公司列表  <a href="https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u">https://www.yuque.com/itwanger/gykdzg/yvq87dkkwtax2m3u</a> 面试指南 gqya</p><p>优质主题——<a href="https://articles.zsxq.com/id_v6xfrmw5637h.html">JavaGuide 知识星球优质主题汇总（2024-04-01） (zsxq.com)</a></p><h2 id="AI"><a href="#AI" class="headerlink" title="AI"></a>AI</h2><p><a href="https://www.coze.cn/">扣子 - AI 智能体开发平台 (coze.cn)</a></p><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><blockquote><p>测试八股文/测开八股文</p></blockquote><p>算法题目</p><p>数据库基础知识+ sql题目</p><p>项目梳理</p><p>java八股文</p>]]></content>
    
    
    <summary type="html">网站迁移，重新开始博客之路</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://www.fomal.cc/posts/0.html"/>
    <id>https://www.fomal.cc/posts/0.html</id>
    <published>2024-09-06T13:11:52.770Z</published>
    <updated>2024-03-06T02:12:54.729Z</updated>
    
    <content type="html"><![CDATA[<h2 id="社科人文认知突破"><a href="#社科人文认知突破" class="headerlink" title="社科人文认知突破"></a>社科人文认知突破</h2><h2 id="社会科学入门"><a href="#社会科学入门" class="headerlink" title="社会科学入门"></a>社会科学入门</h2><p>是这么回事，学习社会科学，学习文科，其实用处没你想得那么大。<br><strong>你如果想靠这个挣钱呢，那还是很难的。</strong></p><h2 id="学文科的好处"><a href="#学文科的好处" class="headerlink" title="学文科的好处"></a>学文科的好处</h2><p><strong>但是呢，学这玩意用处很大的，一个是陶冶自身的情操。另一个能避免被政府欺骗。</strong>    </p><p>学文科的话，其实最重要的是多读书，多积累，然后能学会万事万物的联系。</p><p>这个万事万物的联系，说的比较笼统，就是<strong>你读一本书，能想到别的书之间和这本书的联系。</strong></p><p>比如你读唐代的历史书，能联系到北周的柱国，再联系到皇帝的更迭， 那恭喜你，你已经学会一点了。</p><p><strong>学文科的话，最高的水平是能做预测分析，其次是做事后分析。</strong></p><p>做预测分析这玩意属于有点是占卜，没有固定的套路，但是做事后分析，寻找历史的真相，其实如果多积累，多思考，总是能找到路径的。<strong>但是呢，这个时间可能需要很多年的积累。</strong><br>学文科另一个方面是<strong>有利于你炒股</strong>，如果你将来会看工信部的数据，懂得如何分析走向，不敢说带你走向暴富，但是穷应该是可以避免。</p><h2 id="文科的坏处"><a href="#文科的坏处" class="headerlink" title="文科的坏处"></a>文科的坏处</h2><p>上面说了那么多学文科的好处，我也要说说学文科的坏处，一方面是要<strong>花费非常多的时间，</strong><br>另一方面是<strong>没有很明显的进步提示</strong>，他不像学数学，学了一本就有一本的知识，而是你要在庞大的积累后，<br>才能实现质的飞跃。</p><p>除此之外，另一个问题是<strong>容易走火入魔</strong>。关于走火入魔，比较明显的例子就是各种魔怔念经人，佐佐右右托派。</p><p>因为你读一些书，你就是和作者在产生交流和对话，作者会讲他的思考模式，然后把你说服，<br><strong>如果你能力不行，就很容易被他说服。</strong>      比较明显的例子就是哈耶克 和佛利德曼的书。</p><h2 id="怎么学习社科人文"><a href="#怎么学习社科人文" class="headerlink" title="怎么学习社科人文"></a>怎么学习社科人文</h2><p>好了，我要正视开始讲学文科的事情了。<br>首先呢，<strong>你需要积累一定的历史，</strong>（方便在了解一系列的东西之后，能够分析出其发生的背景以及对应的因果关系，方便结合时势分析）（其实也可以不积累，但是不积累的话，在看一些东西的时候，不能结合时代背景去认知）。<br>第二是<strong>根据自己的爱好，去决定学什么。</strong><br>文科里面，我个人首推的是<strong>逻辑学导论</strong>。这本书会教你如何去正确的思考，会让你避免一些基础的逻辑推导错误。<br>第二 我是推荐看<strong>经济历史</strong>。经济历史对于人的收获是，你能理解说一些事，是如何演化过来的。<br>比如说我们曾经是社会主义，为啥会90年代破产， 为啥会cr ，为啥到现在又重新国进民退。<br>第三，我推荐是看<strong>历史类的书</strong>，历史类的书，其实我个人推荐是看法国人还有英国人写的书，<br>这两个国家的特点就是有丰富的国外交流经验，是认真的理解国外的，英国的博物学家是一群很不可思议的人，<br>19世纪的英国探险者能穿越喜马拉雅山跑到拉萨和当地的活佛交流，20世纪的劳伦斯一个人就能推翻伊拉克帝国，<br>瓦解德国在阿拉伯地区的努力，而法国在非洲的殖民，甚至一度让美国人都感到绝望。</p><p>​    </p><h3 id="个人体验share"><a href="#个人体验share" class="headerlink" title="个人体验share"></a>个人体验share</h3><p><strong>哲学类的书我不推荐看，因为这个很耗时间，容易学不到东西。</strong><br>关于道德类的讨论，还有国学这一块，旧文化毫无疑问是死掉了，现在什么复兴国学啊，<br>无非是对着一具僵尸祈祷。如果你们想学这个呢，可以<strong>大概扫扫孔子孟子荀子加王阳明的传习录，</strong><br><strong>不过心学讲的更多的是人的解放。</strong>扫一扫就行，学深了没啥意义，这玩意本质上是一种道德教化，<br>但是实际上来说，真有用的还是看人能不能敢于斗争。。。。<br>学文科没啥固定的套路，<strong>有用就用，没用扔掉，别变成那只腐臭文人，指挥掉书袋。</strong><br>今天就暂时写那么多。欢迎大家一起互相讨论。</p><blockquote><p>天道有感</p></blockquote><ol><li>传统文化 强势文化 弱势文化 ==》演变成应该以政治文化为主，以道德和传统文化作为人文性情掌控的加锁</li><li>逐渐去学习现在很多事情的运行规律，多去尝试体会理解事物发展的本质规律，只有这样借势，慢慢去利用规律，才可能会帮助自己更好的认识本来的发展方向，更好的借力打力</li><li></li></ol><h3 id="政治cr是什么意思"><a href="#政治cr是什么意思" class="headerlink" title="政治cr是什么意思"></a><strong>政治cr是什么意思</strong></h3><p><a href="https://zh.wikipedia.org/wiki/CR">CR - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>极度濒危<br>极危<br>严重濒危<br>极度濒危物种</p><p><strong>政治cr是一种政治符号语言，CR是Community Relief（社区救济）的缩写，指政治上中左派团体对给弱势群体提供的各种救济措施的简称。这个术语是在网络上出现的，源自于社交媒体上用户对政治、文化、社会问题的讨论。这个术语主要出现在中国网络空间中，因为中国社交媒体的特殊环境，这个术语主要指代的是一种执政党在为人民办事的同时，也是在为自己、为党派、为官僚机构、为强势集团利益服务的政治现实。</strong><br><strong>政治CR的主要表现形式包括但不限于各种策划、活动、宣传、演讲、集会、网络言论、社区服务等。其中，社区服务在中左派政治圈子中被认为是最典型、最重要的CR方式。中左派倡导政治CR，不仅是为了弱势群体的合法权益，而且也是为了塑造中左派的良好形象和生存空间。当今中国的政治环境不容乐观，中左派圈子在应对复杂政治环境和外部压力时需要将社区服务和救济作为其政治理念和社区组织中的核心内容，以此来表明自己的政治态度和提高自己的公信力。</strong><br><strong>政治CR还有一层意义，它是在对传统CR概念的一个批判和拓展。传统CR是指在弱势群体受到自然灾害等不可抗因素影响时，社会团体或个人通过捐款、物资等方式向遭受不幸的群体提供救济的行为。这种救济行为受到了许多批评，人们认为它追求的仅仅是短期效应，而没有深入研究问题的根源。而政治CR则更注重于问题的根源研究，在提供救助的同时，也更注重于倡导相关的政治观点，从而更具有持续性和影响力。</strong><br><strong>总而言之，政治CR作为一种政治行动形式，在中国等一些国家有着广泛的意义。它是中左派的一种社会实践方式，在提供弱势群体救济的同时，也是在呼吁社会公正和推动社会进步。</strong></p><p>个人人生复盘</p><blockquote><p>个人角色</p><p>个人规划</p></blockquote><h1 id="润宇笔记"><a href="#润宇笔记" class="headerlink" title="润宇笔记"></a>润宇笔记</h1><p>拆解笔记，解读拆分   带货小时榜  咋没有知识付费或者教育类的呀  润宇老师<br>这个小时榜就代表了限时限刻的流行风<br>我今天也要调整我的带货顺序了[偷笑]<br>拆解他到底在买什么？  把每个直播间最撑着他直播销售的量的商品【100w中 有80w的商品】<br>10个商品 找出货盘，找出排名的前几个，哪些是主推的【人 货 场 】短视频的量？  投流来的量 还是转发来的</p><p>套路话术  看懂有什么过程  会逼单<br>如何卖？话术、时间循环、塑造价值、当下购买的理由<br>对哪些人   说那些话  卖什么货  产生了价值</p><p>先去看哪些有用  再去看他的逻辑【从消费者行为 去想为什么去买】<br>1、货盘、热卖的产品 2、引流品、信任品、利润品 3、流量是怎么来的：场观怎么来的<br>【ai 视频好 中华文化 疗愈  情绪价值 小红书【平台相关】  健康 】</p><p>普通人 ai 不是周鸿祎 则大可不必焦虑 </p><blockquote><p>ai只是辅助创作的手段，从来不觉得ai是风口</p></blockquote><p>对于刚刚起步的商业体   找机会的话，一定冲着一个目标去：离现金流一定要近</p><blockquote><p>离现金流近，就一定要离交易近【交易场景明确】</p></blockquote><p>一定是需求和供给已经贼明确或者自己可以发现的时候: 人 货 场 形成干柴烈火的状态，一定最近且一定会发生</p><p>卖 ai 课的人，对ai感到焦虑的需求，视频直播平台提供课程 卖ai课是三者清晰的</p><blockquote><p>技术开发者  消费者  应用者</p></blockquote><p>AI写作，产生内容呢?</p><p>李一舟的课程一环套一环的感觉</p><p>写视频文案</p><blockquote><blockquote><p>技术开发者，需要有先机，抢开发机会</p></blockquote><p>技术抢先机，会形成商业壁垒</p></blockquote><p>哈哈，我现在都用ai来接文案商单了</p><blockquote><p>ai消费者</p></blockquote><p>新能源汽车 </p><p>技术的应用者</p><blockquote><p> 比亚迪 dmi的电池技术  宁德时代的玻璃 和 电池</p></blockquote><p>好不好用，好不好看，是否有高性价比</p><blockquote><p>伺候好消费者 是所有消费者的义务</p></blockquote><p>消费者好用便宜，不关注技术</p><blockquote><p>最终的技术最后都会不断降低壁垒，是不用学的</p><p>最后都会推动到消费者</p></blockquote><p>好的东西是不用学的</p><blockquote><p>但是你提前学会了，就有信息差，也能够提高你的生产效率。</p></blockquote><p>应用者 光有信息差，只能卖课，也需要产生出其他的价值帮助别人</p><p>大部分想要赚钱的 或者是利用好产品赚钱的，是应用者</p><blockquote><p>光写出文案 ，没有流量。光有数字人替代直播，但是卖不出去货</p><p>光有生成的视频，但是不能有浏览量，也没什么用</p><p>商业的底层逻辑</p><p>光帮助了生成量产，但是不能扩宽边界，仍然</p></blockquote><p>极其理解用户的交易场景，制造了一个想像空间，让用户感受到他也能，他也行的想法<br>价格由供求关系来决定 不是你的数量来决定[东西稀缺才值钱]</p><blockquote><p>热辣滚烫  </p><p>用了一个很土的方法，100斤不稀缺，但是他在电影里真有的一个</p></blockquote><p>土是2024最大的稀缺<br>数字人没有情感与直播间的人不能产生共鸣。<br>手工写信，是因为你愿意花时间精力来干这样的事情，说明你很重要，被尊重，被重视，被需要<br>缺了真情实感<br>技术需要了解用户，靠应用+了解用户的需求才能产生money</p><p>核心竞争力，自己的优势到底是什么<br>应用者 利用信息差快速上位<br>了解客户，而不仅仅是了解技术；少去炫技，多去用感情+满足需求来挣钱<br>技术是应用者 利用信息差快速上位</p><p>应用者中赚到钱的一定是最理解用户的人，手里有客户 ，ai只是一个工具，所面对的用户仍然没有变过<br>铁打的用户，流水的技术</p><h3 id="孩子教育-是否要学ai"><a href="#孩子教育-是否要学ai" class="headerlink" title="孩子教育 是否要学ai"></a>孩子教育 是否要学ai</h3><p>为什么要学扑克？ 象棋 围棋  人还要跑步？而不是打车呢</p><p>会让使用和学习的门槛回归0</p><p>提升审美和能力</p><p>从音乐 ，商业 到编程 都会一点的东西</p><p>培养孩子的内心力量，他们真的需求是什么</p><p>孩子的创造力和想象力很珍贵，AI影响不了</p><p>训练孩子的思维</p><p>情绪力 和创造力 ，人天然是一个大模型，</p><p>人要跟生活 跟很多元认知来产生很多碰撞</p><p>锻炼思考能力、逻辑能力、创造力</p><p>==遇到人生巨大困惑，需要使用创造力来进行利用解决，知识自己一只忽略了他的价值==</p><h1 id="中国基层的基础框架"><a href="#中国基层的基础框架" class="headerlink" title="中国基层的基础框架"></a>中国基层的基础框架</h1><blockquote><p>政治经济学 组织经济学</p></blockquote><p>北上广深苏浙杭 VS 乡村 （90%人口）</p><p><img src="..\assets\image-20240222101953924.png" alt="image-20240222101953924"></p><p>知之深才能爱之切</p><p><img src="..\assets\image-20240222103121120.png" alt="image-20240222103121120"></p><blockquote><p>理解基层才能理解中国</p></blockquote><p><img src="..\assets\image-20240222103214019.png" alt="image-20240222103214019"></p><p>循序渐进，逐渐深入的关系</p><blockquote><p>主要研究政企关系，营商环境和制度经济学</p><p>三农问题和基层治理 并不强擅</p></blockquote><h3 id="怎么学？"><a href="#怎么学？" class="headerlink" title="怎么学？"></a>怎么学？</h3><p><img src="..\assets\image-20240222103441214.png" alt="image-20240222103441214"></p><blockquote><p>底层框架+冷冰冰的手术刀</p></blockquote><ol><li><p>构建思维框架，增强因果推断；</p></li><li><p>完成思维的培养和锻炼，在沟通中成长，在交流中进步</p></li></ol><p>==如切如磋如琢如磨==</p><h1 id="第一讲——理解中国的治理架构"><a href="#第一讲——理解中国的治理架构" class="headerlink" title="第一讲——理解中国的治理架构"></a>第一讲——理解中国的治理架构</h1><blockquote><p>治理架构也就是根本制度</p><p>掌握</p></blockquote><p><img src="..\assets\image-20240222103729013.png" alt="image-20240222103729013"></p><blockquote><p>元规则 支撑中国文明运行几千年的基本制度</p></blockquote><p>为什么要从制度或者底层治理框架上开始？</p><blockquote><p>只有先从制度角度理解制度背景，才能从宏观角度理解中国的基层治理，才能高屋建瓴的看透中国基层治理的现状、问题 和对策</p></blockquote><p>从制度角度看问题才能看的更透，更深 更远</p><p><img src="..\assets\image-20240222104937019.png" alt="image-20240222104937019"></p><p>不是就事论事 形成头痛医头 脚痛医脚的毛病</p><p>主要研究三个问题：</p><ol><li>为什么要演研究中国的治理架构？</li><li>什么是中国治理的基本架构</li><li>如何在中央—地方关系下，理解基层与上层的互动<ol><li>中央和地方之间到底是兄弟关系还是父子关系呢？</li></ol></li></ol><p><strong>主要目的是形成理解基层治理的底层框架和系统思维</strong></p><p><img src="..\assets\image-20240222105109455.png" alt="image-20240222105109455"></p><blockquote><p>万变不离其宗，掌握底层框架才算是掌握了知识的金钥匙，也就是武林秘籍中的 独孤九剑 </p><p>条块结合，以块为主的中央集权体制</p></blockquote><p><img src="..\assets\image-20240222105215896.png" alt="image-20240222105215896"></p><blockquote><p>纪委为什么成为强势单位？ 县委书记为什么不好当?</p><p>怎么就形成了上边千条线，下边一根针的基层治理困境</p><p> 大A  制度经济学</p></blockquote><p><img src="..\assets\image-20240222110039821.png" alt="image-20240222110039821"></p><p><img src="..\assets\image-20240222110129262.png" alt="image-20240222110129262"></p><blockquote><p>制度经济学 包括 政治制度 经济制度 法律 文化 以及政府政策</p></blockquote><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ol><li>实时，需要有<strong>较快</strong>的推理速度（高优先级）</li><li>便捷，需要该模型便于部署，特别是部署到Unity平台</li><li>精确，至少要让用户觉得数字人的口型可以和实际口型对应</li><li>风格化，无需针对某个特定的音色/角色微调模型，即<strong>one model for all user</strong></li><li>口型、avatar面部表情、动作同步；其中口型、表情都是用blendershape驱动，动作暂时是循环播放固定动作</li></ol><h2 id="实验计划"><a href="#实验计划" class="headerlink" title="实验计划"></a>实验计划</h2><p>优先参考现有的代码：</p><p><a href="https://github.com/yunik1004/SAiD.git">https://github.com/yunik1004/SAiD.git</a></p><p><a href="https://github.com/huailiang/LipSync">https://github.com/huailiang/LipSync</a></p><p><a href="https://github.com/zhongshaoyy/Audio2Face">https://github.com/zhongshaoyy/Audio2Face</a></p><p><a href="https://github.com/leventt/surat.git">https://github.com/leventt/surat.git</a></p><p><a href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p><p><a href="https://github.com/Rtyper/LipSync-Pro">https://github.com/Rtyper/LipSync-Pro</a></p><p><a href="https://github.com/guanjz20/StyleSync">https://github.com/guanjz20/StyleSync</a></p><h2 id="后续需要补的知识"><a href="#后续需要补的知识" class="headerlink" title="后续需要补的知识"></a>后续需要补的知识</h2><p>@李润一</p><p>Unity开发、部署相关知识Sentis的使用：<a href="https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4">https://www.notion.so/AI-model-To-Unity-b232f2c2acaa49d59e0c0fa251c329c6?pvs=4</a></p><h2 id="Audio2Face-expression更新"><a href="#Audio2Face-expression更新" class="headerlink" title="Audio2Face+expression更新"></a>Audio2Face+expression更新</h2><p>可以参考下列工作：</p><p><a href="https://github.com/FACEGOOD/FACEGOOD-Audio2Face/">https://github.com/FACEGOOD/FACEGOOD-Audio2Face/</a></p><p>更新：FaceGood无法在linux平台上测试，只能在windows上运行</p><p>（其余工作未使用blendershape的输出，可在下<a href="https://github.com/FACEGOOD/FACEGOOD-Audio2Face/列链接中查看：）">https://github.com/FACEGOOD/FACEGOOD-Audio2Face/列链接中查看：）</a></p><p><a href="https://xie.infoq.cn/article/2f414deaa57ca29783d6f873f">语音驱动嘴型与面部动画生成的现状和趋势<em>算法</em>行者AI_InfoQ写作社区</a></p><p><a href="https://www.gameres.com/895985.html">语音生成口型与表情技术的演进与未来 - GameRes游资网</a></p><p><a href="https://github.com/nowickam/facial-animation/tree/production">nowickam/facial-animation: Audio-driven facial animation generator with BiLSTM used for transcribing the speech and web interface displaying the avatar and the animation (github.com)</a></p><p><a href="https://www.jianshu.com/p/fbc5743152f0">[Paper Reading] Audio-Driven Facial Animation by Joint End-to-End Learning of Pose and Emotion - 简书 (jianshu.com)</a>（这篇工作就是nvidia开发的omnivetrse audio2face的基础）</p><h1 id="个人选择"><a href="#个人选择" class="headerlink" title="个人选择"></a>个人选择</h1><p>相对竞争优势 &amp;&amp; 个人的兴趣爱好</p><blockquote><p>根据对应的时代特征，来进行选择</p><p>有一个选择4象限</p></blockquote><p><img src="..\assets\image-20240301094224506.png" alt="image-20240301094224506"></p><p><strong>政府部门进行招商引资</strong></p><p>PPT 造车  <a href="https://zhuanlan.zhihu.com/p/642907927">PPT造车鼻祖倒下？烧光174亿，九年也没憋出一辆车 - 知乎 (zhihu.com)</a></p><p><a href="https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT%e9%80%a0%e8%bd%a6&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1">https://www.bing.com/ck/a?!&amp;&amp;p=f0f86f68578cf44fJmltdHM9MTcwOTE2NDgwMCZpZ3VpZD0xZjZkZWZkNy0zZTg5LTYyNmQtM2JiYS1mYmU2M2Y4MTYzNWYmaW5zaWQ9NTE4MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=1f6defd7-3e89-626d-3bba-fbe63f81635f&amp;psq=PPT%e9%80%a0%e8%bd%a6&amp;u=a1aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDI5MDc5Mjc&amp;ntb=1</a></p><h2 id="工具ChatGPT"><a href="#工具ChatGPT" class="headerlink" title="工具ChatGPT"></a>工具ChatGPT</h2><p>ChatGPT4的归档功能</p><p><a href="https://www.landiannews.com/archives/101487.html">OPENAI宣布ChatGPT现在可以归档会话喽 不删除会话的同时保持简洁 - 蓝点网 (landiannews.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;社科人文认知突破&quot;&gt;&lt;a href=&quot;#社科人文认知突破&quot; class=&quot;headerlink&quot; title=&quot;社科人文认知突破&quot;&gt;&lt;/a&gt;社科人文认知突破&lt;/h2&gt;&lt;h2 id=&quot;社会科学入门&quot;&gt;&lt;a href=&quot;#社会科学入门&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.fomal.cc/posts/4a17b156.html"/>
    <id>https://www.fomal.cc/posts/4a17b156.html</id>
    <published>2024-09-06T13:11:52.768Z</published>
    <updated>2024-09-06T05:01:06.893Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>思考的原点1.1</title>
    <link href="https://www.fomal.cc/posts/6c9e281a.html"/>
    <id>https://www.fomal.cc/posts/6c9e281a.html</id>
    <published>2024-03-19T11:59:12.000Z</published>
    <updated>2024-09-06T01:04:24.695Z</updated>
    
    <content type="html"><![CDATA[<p><img src="../assets/image-20240319200003990.png" alt="image-20240319200003990"></p><p><img src="../assets/image-20240319200138262.png" alt="image-20240319200138262"></p><p>具体影响:</p><p><img src="../assets/image-20240319200442568.png" alt="image-20240319200442568"></p><blockquote><p>民国时期的张爱玲 胡适  不超过10% </p><p>国民党的留学博士  20%  高智商的人 但是不一定明白</p></blockquote><p><img src="../assets/image-20240319200959489.png" alt="image-20240319200959489"></p><p>思考框架背后 </p><p><img src="../assets/image-20240319222056735.png" alt="image-20240319222056735"></p><blockquote><p>哈耶克 说英美 两种经验主义？？</p><p><strong>14-15分钟左右 没听懂</strong></p></blockquote><p><img src="../assets/image-20240319223401990.png" alt="image-20240319223401990"></p><p><img src="../assets/image-20240319223752011.png" alt="image-20240319223752011"></p><p>  人类积累了上万年的常识智慧 我们需要借鉴吗？</p><p>哈耶克认为 英国自由的保守主义 包括解决一战 和法国大革命 是以经验主义演变来的，马老师定义为经验的共识？</p><p><img src="../assets/image-20240319223950464.png" alt="image-20240319223950464"></p><blockquote><p>不言而喻 的东西 + 假设 形成自然科学</p></blockquote><p>社会科学 基于很多公理 在进行理性推理+假设</p><blockquote><p>先聊基本常识有哪些【多年积淀】</p><p>在聊到良知【希特勒 ，斯大林的大清洗 法国资产阶级大革命    】</p></blockquote><p><img src="../assets/image-20240319225127785.png" alt="image-20240319225127785"></p><blockquote><p>人是目的还是工具？</p><p>与人相处的时候什么是平等和公平 </p><p>个人权利和公众权力</p><p>平等和自由的时候哪个更重要 本质是左派和右派</p><p>权力和道德的关系 良知和正义的关系</p></blockquote><p>读写是最好的自我投资</p><p>经济持续放缓（5年之内）悲观的2%的商铺不会盈利，疫情导致大家消费欲望变低</p><blockquote><p>最难的反而是28-40 的中层危机，持续危机</p></blockquote><p><img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20240409221345478.png" alt="image-20240409221345478"></p><blockquote><p>读书是</p></blockquote><p>35岁 经验为导向的职业，努力不在撬动发展，需要持续做对选择来撬动个人发展</p><blockquote><p>生存问题变成发展问题</p></blockquote><p>π型人才，</p><blockquote><p>所有的变化和需求都需要持续的阅读，来保证个人的读写能力</p></blockquote><p>每个人都最容易触及的，都可以获得的，收益最大的都是读写能力，</p><p>读写只是自己的一种阅读输入输出的方式</p><blockquote><p>读写 在信息时代把数据吃进去再用更好的方式把数据吐出来，就永远都不会饿死</p></blockquote><p><img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20240409221646881.png" alt="image-20240409221646881"></p><blockquote><p>消费型的阅读或者时间投入</p><p>仅仅是方便体验和知道了一些知道的</p></blockquote><p>投资型的阅读和输入</p><blockquote><p>咖啡豆的示例</p></blockquote><p><strong>自己的投资型和消费型的比例是怎样的呢？是否需要控制呢（包括视频和读书方面，甚至是任何各种输入自己眼中、头脑中的数据和信息）</strong></p><p>10分的话，消费占多少？投资占多少？  </p><p><strong>思想变得浅薄</strong></p><p><img src="D:\2023.6.2hexo\test\source\assets\image-20240409224042781.png" alt="image-20240409224042781"></p><p>底层思维的书  怎样应用呢？【】</p><blockquote><p>复用率 ？ 有短期收益或者长期收益？？？</p><p>遇到任何书的时候，都要去重新去思考，对症下药去分析</p></blockquote><p><img src="D:\2023.6.2hexo\test\source\assets\image-20240409224331895.png" alt="image-20240409224331895"></p><p>当你把自己价值能完整的输出的时候，就是一件非常有价值的东西</p><ol><li>你比别人想得深</li><li>你能够宣传，有品牌</li></ol><p>拍卖效应 + 手电思维？灯塔效应</p><blockquote><p>持续放光</p><p>德国人的灵魂？？丁元英  灵魂<br>开启一个一个的小镇</p></blockquote><p>穷人/普通人最好的投资、最好的职业规划、最好的学习方法 、是最便宜最舒适让内心能够更宁静的一种旅游小镇的方式</p><p><img src="..\assets\image-20240409224953036.png" alt="image-20240409224953036"></p><h2 id="写卡感触"><a href="#写卡感触" class="headerlink" title="写卡感触"></a>写卡感触</h2><p>我最有感悟的古典⽼师的⼀句话是：古典老师关于阅读读书时候的考量因素（如何选书或者评价书是否适合自己去阅读）+对于书复用率的不同分类（经典的底层逻辑的书  实现某种功能的书  畅销书 ）+古典老师如何论证阅读的效益的递进方式（先背景介绍-&gt;环境要求-&gt;个人发展需要(投资数据支持、职业规划)-&gt;心灵上读书是必要条件）+读写是一个持续的东西，很多公众号断更/没坚持到最后，只有持续的读+写，形成闭环才能够好的坚持走下去【读写是最好的个人投资，而不仅仅是只读/只写（写一篇没用，读一篇也没用）】</p><p>阅读不是要做的唯一一件事情，写作一样，并不是孤立的，而应该两者形成整体的一个系统，作为输入和输出，同时去构建他，维护他们，并作为长期主义，持续的做下去。</p><p>我过去踩的坑是：没坚持下来阅读+写卡（也有各种原因，也一直没有开始去做）;阅读读书没有将该原则应用，还是老套路选书；自己是第二次看这个视频，之前也看过这个视频好几遍，甚至都写过关于这个视频的卡（第一次参与共读会+选书开始写卡的时候，都是看的这个视频），但自己只是知道并没有做到，这两者中间真的差了一个巨大的鸿沟。</p><p>我以后可以更好的⽅式是：</p><ol><li><p>尽量减少完美主义的影响，有时候一想起要把事情做得很好会碰到一些苦难，所以就会害怕或者懒得去开启行动；尽可能快的开始，言出努力去行，用行动来进行反馈，尽量只要收获一点点东西就enough。</p></li><li><p>昨天听了师兄师姐的会议分享，感触颇多，要多跟大家去同频共振，减少自己对于现实世界中困难的想象，联机学习，一起进步</p></li></ol><h1 id="4-10"><a href="#4-10" class="headerlink" title="4.10"></a>4.10</h1><ol><li>整理时间安排</li></ol><h2 id="读书读不懂？记不住？用不上？"><a href="#读书读不懂？记不住？用不上？" class="headerlink" title="读书读不懂？记不住？用不上？"></a>读书读不懂？记不住？用不上？</h2><p>读不懂</p><ol><li>读深入 读透作为自己的读书指标</li><li>读书 缺少体验，<ol><li>听话vs阅读理解？ 人的大脑临时组件是不能对阅读理解来实现天生组件完成该功能的，所以需要将听觉、视觉、大脑打猎的部分临时组合来实现阅读理解</li><li>缺少人生经历和体验（任何的抽象概念都无法不通过具体的体验去理解）——没有见过圆的人永远想象不到圆有多完整（没见过趋近于直线的人无法想象到无限延长的直线）</li></ol></li></ol><p>记不住</p><p>存难取易（偷钱挨打这件事 一辈子记住（一辈子记住自己不要丢三落四））</p><p>大脑 easy come easy go 5</p><p>一天听4个小时 什么都记不住</p><blockquote><p>给别人讲一遍</p></blockquote><p>最好的记录员？？（跟他们做朋友）</p><p>利于记忆，但不利于提取</p><p>缺少必要的知识管理工具</p><blockquote><p>缺少实践【】</p><p>缺少系统【老板上课】</p></blockquote><h3 id="个人脑子里记录的东西"><a href="#个人脑子里记录的东西" class="headerlink" title="个人脑子里记录的东西"></a>个人脑子里记录的东西</h3><blockquote><p>思维导图—— 笔记整理软件——写卡片</p><p>白天听课，晚上写  </p><p>？？难度</p></blockquote><p>整体的系统框架？？</p><p>读写镜像，关于知识树的构建和拆分是利用读写来完成的</p><p>读的话 就是输入，将个人读的信息从一维提取主题后形成二维大纲，然后放入到大脑的知识网中</p><p>写的话，是输出，将大脑中的网状知识结构，先提取出一个主题形成大纲，然后大纲每一个主题往外写，形成一维输出</p><blockquote><p>既是一讲写作课，也是一讲阅读课</p></blockquote><p> 写卡营第二讲体悟分享</p><p>【内容来源】写卡营第二讲课程古典老师分享（下）</p><p>【本卡主题】关于建立体系以及写卡的四个重要知识点</p><p>【原文摘录】<br>SS卡的实现体系是从①选书开始 ② 关注理解体验 ③深刻记忆内容④付出行动实践（各种训练营 打卡营 读书营） ⑤坚持30天不断积累卡片 ⑥形成卡片体系 ⑦卡片成文营训练 ⑧文章/作品 大纲⑨创造价值，不断发展<br>SS卡的四个重要知识点：①存难取易②最小单元（卡片） ③乐高式写作（搭建） ④读写镜像（读写同构）</p><p>【概念转述】<br>————————————————————<br>选书——选的是底层的，比较基础的且复用率比较高的书<br>理解——转述出来之后，用自己的话说一遍辅助加深理解<br>体验——记录那一瞬间的感触/体验/思考，并不断积累汇总<br>行动——行动与体验并行，才能帮助更好的融会贯通<br>其他的点是在上边的基础上不断的践行+积累，慢慢创造价值</p><p>————————————————————<br>存难取易——既能帮助深入理解，方便快速提取体验卡，也能实现系统搭建<br>最小单元——借用卡片的方式，每天复习，学习+联系<br>乐高式写作——反对模板化的方式，以阅读输入为起点，存下每块乐高，最终搭建出体系<br>读写镜像——读写不分离，读书—写作应该是同步进行，并且是一个相互可逆的过程<br>————————————————————<br>【个人体验/思考】</p><ol><li>形成整个读写体系应该是战略性的设定，然后再按照整个流程详细的展开，按照古典老师讲述的过程，不断积累，一点点去主动阅读并写下体验，深刻体会后并用行动去加深理解，想起了之前阅读《七个习惯》好像是一样的思路</li></ol><p>2.在自己一个月从158变成142的过程中，深刻体会到自己实践——体验——继续付出努力增强信心的整个闭环过程，现在想来应该也是得益于系统的搭建</p><p>3.读写同构这一点，让自己印象深刻；在读书的时候，是从1维度获取零散的知识，然后在笔记中进行罗列整理，形成2维度大纲，最后在脑中形成3维度网状知识体系；<br>书写的时候，是利用脑中的3维度网络知识图，将信息提取后，在纸面上形成2维度大纲，最后按照每个要点详细展开细节也即1维度的内容加工。</p><p>【行动指南】<br>写卡先，首先认识到，到知道，再到做到，感谢老师给了一个可行性比较强的抓手——写卡</p><p><a href="https://doc.weixin.qq.com/doc/w3_AdgAaQawAMceNWSNgcxRSWAS0TYZ0?scode=AFgAiwfiAGI7oggms0AdgAaQawAMc">软件打包 (qq.com)</a></p><p><a href="https://www.zhihu.com/column/c_1245860717607686144">Nuitka-Python打包exe - 知乎 (zhihu.com)</a></p><p><a href="https://github.com/erdengk">erdengk (尔等同学) · GitHub</a></p><p><a href="https://book.douban.com/subject/35050614/">我的二本学生 (豆瓣) (douban.com)</a></p><p><a href="https://avuucupcq6.feishu.cn/docx/EWYYdkYIeoL4XIxw97xcLqQnnRc">一册通.《锻炼》共读营 - Feishu Docs</a></p><p>请结合下边给出的5个Skeleton-based Action Recognition 介绍的内容，生成一个新的关于Skeleton-based Action Recognition的详细介绍，内容尽量新颖独特一点，用词合理且通顺，大概在500单词左右<br>1 Skeleton-based Action Recognition<br>With the huge advances of deep learning, recurrent neural net-<br>work (RNN)-based, convolutional neural network (CNN)-based,<br>graph convolutional network (GCN)-based, and transformer-based<br>methods are studied for skeleton-based action recognition. RNNs<br>have been widely used to model temporal dependencies and cap-<br>ture the motion features for skeleton-based action recognition. The<br>work in [9] uses RNN to tackle the skeleton as sequence data. Sub-<br>sequently, Song et al. [37, 38] proposed to utilize the attention<br>mechanism and multi-modal information to enhance the feature<br>representations. Some other works [15, 24] transform each skeleton<br>sequence into image-like representations and apply the CNN model<br>to extract spatial-temporal information. Recently, GCN-based meth-<br>ods have attracted more attention due to the natural topology struc-<br>ture of the human body. Many works [6, 35, 47] apply GCN to<br>the spatial and temporal dimension [47] and achieves remarkable<br>results in the supervised skeleton-based action recognition. Mean-<br>while, transformer models [29, 36] also show promising results,<br>owing to long-range temporal dependency learning by attention.</p><p>2 Skeleton-based action recognition<br>Generally, there are two ways to obtain skeleton data from<br>videos. Firstly, the position information of human joints is ob-<br>tained from RGB video frames by pose estimation methods, such<br>as Openpose [26]. Secondly, the 3D skeleton data of the human<br>body can be obtained directly through the depth sensor, e.g., Mi-<br>crosoft Kinect v.2. In this paper, the datasets we use includes two<br>forms, RGB videos and skeleton sequences. These two forms of<br>data are shown in Fig. 1.<br>Most early research on skeleton-based action recognition usu-<br>ally used handcrafted features to represent the human body.<br>Hussein et al. [10] used the covariance matrix for joint posi-<br>tions over time. In [11], rotations and translations were used to<br>model 3D geometric relationships between body parts. However,<br>these methods were complex in design processes and ignored the<br>correlations between specific human body parts.<br>With the further development of deep learning, CNN-based<br>and RNN-based methods were proposed to process the grid<br>data reconstructed from human skeleton data. CNN-based meth-<br>ods [27–29] convert the skeleton data into pseudo-images, then<br>attempt to learn action features. In [30], a skeleton-based trans-<br>former module was designed for action classification and recog-<br>nition. Kim et al. [31] proposed a 3D human action recognition<br>model named Temporal Convolutional Neural Networks. In [32],<br>an enhanced skeleton visualization method was presented to<br>deal with view variations and noisy data. Liu et al. [33] first<br>applied 3D convolution for the skeleton-based recognition task to<br>simultaneously capture motion correlations of spatial–temporal<br>dimensions. Huynh et al. [34] proposed a new encoding method,<br>Pose-Transition Feature to Image (PoT2I), which transforms high-<br>level features of skeleton data into color pixels. In recent works,<br>the skeleton-based data was formulated as sequences of grid-<br>shaped structures and then modeled with RNN-based methods [8,<br>35,36]. Shahroudy et al. [37] proposed an RNN-based structure<br>to model the long-term temporal correlation of motion features.<br>In [38], RNN was used to analyze the hidden sources of motion<br>relationships in both spatial and temporal dimensions. In [39],<br>an end-to-end spatial–temporal attention model was built with<br>Long Short-Term Memory (LSTM) to focus on discriminative<br>joints. In [40], a novel contrastive action learning paradigm called<br>AS-CAL with a momentum LSTM module was proposed to unsu-<br>pervised action recognition. However, the skeleton-based data are<br>embedded not in the form of vector sequences or 2D grids but<br>in the graph structure, so both RNNs and CNNs based methods<br>cannot represent the structure of skeleton-based data well.<br>GCN-based methods utilize graph convolution modules to ex-<br>tract motion dependencies from the graph structure of natural<br>human skeleton data, which has been demonstrated to be the<br>most expressive. Yan et al. [4] innovatively proposed the first<br>model with GCN, namely ST-GCN, to capture the balance between<br>spatial and temporal dependencies. In this model, the skeleton<br>is expanded into a graph structure, and the spatial–temporal<br>patterns are automatically extracted from skeleton-based graph<br>data. Subsequently, many studies [41–43] were proposed based<br>on ST-GCN. In [35], 2s-AGCN was presented with adaptive ad-<br>jacency matrices to capture the motion information from high-<br>order skeleton data. Liu et al. [44] proposed MS-G3D to conduct<br>multi-scale long-range modeling of spatial–temporal graphs. Sim-<br>ilarly, AS-GCN [21] inferred A-links from input data to capture<br>multi-scale actional relationships. Zhang et al. [45] investigated<br>high-level semantics information from skeleton-based data and<br>designed two modules to exploit the high-order correlation of<br>skeleton data.</p><p>3 Skeleton-based models: To find a more effective rep-<br>resentation of the dynamics of human actions, Johansson [24]<br>utilizes 3D skeleton sequences for action recognition, making<br>an obvious decrease of computational cost as well as a good<br>performance boost. Recently, with the rapid development of<br>deep learning techniques, skeleton-based action recognition<br>methods have attracted increasing attentions. Researchers have<br>proposed various models to improve the performance of action<br>recognition, which can be divided into three major categories.<br>The first category builds the models with convolutional net-<br>works. For example, Li et al. [16] propose a CNN-based<br>co-occurrence feature learning framework, which gradually<br>aggregates various levels of contextual information. Kim et<br>al. [15] build a temporal convolutional network to explicitly<br>learn readily interpretable spatio-temporal representations for<br>3D human action recognition.<br>Besides, for the second category, researchers concatenate all<br>joints in one frame into a single vector, then use sequential<br>models such as long short-term memory (LSTM) to explore<br>the temporal dynamics. Du et al. [25] design a hierarchical<br>bidirectional RNN to capture rich dependencies between dif-<br>ferent human body parts. The study in [9] employs a view<br>adaptive LSTM, which enables the network itself adaptive to<br>the most suitable observation viewpoints. Additionally, Song<br>et al. [26] firstly introduce attention modules into skeleton-<br>based action recognition.<br>Both CNN-based and RNN-based methods are still limited<br>to extract the spatial structure information among skeleton<br>joints, where the joints of different body parts are connected<br>as a skeleton graph. Instead, in the third category, graph-<br>based methods can be naturally utilized to deal with the<br>skeleton graph, which successfully captures the most infor-<br>mative features for various actions. Si et al. [13] use GNN to<br>model the relationships among five body parts. Yan et al. [10]<br>initially introduce GCN into skeleton-based action recognition,<br>and produce a baseline named ST-GCN for future research.<br>Based on the ST-GCN, many studies achieve continuous<br>improvements on skeleton-based action recognition [27], [28],<br>[29].<br>4Skeleton-based action recognition<br>The progress of deep learning [30–32] and the advancement of<br>human pose estimation algorithms have facilitated human action<br>research. Basically, skeleton-based action recognition deals with<br>temporal series, where the dynamics of human body poses over<br>time characterize human actions. Early methods generally used a<br>manual feature approach to capture human action [33–35] . How-<br>ever, this relies mainly on 3D rotations and translations between<br>joint points, so feature design is more complex and has average<br>performance.<br>Recently, deep learning methods have achieved favorable out-<br>comes in skeleton-based action recognition, which can be basi-<br>cally classified according to the network architecture: Convolu-<br>tional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),<br>and Graph Convolutional Networks (GCNs). CNN-based methods<br>convert human skeleton sequences into pseudo-images, which are<br>trained and then classified. Structurally, CNNs are constructed by<br>stacking convolutional layers, pooling layers, and activation func-<br>tions, etc. The CNN achieves network optimization by adjusting the<br>number of convolutional and pooling layers, the size of convolu-<br>tional kernel, and the step size of convolution. Li et al. [36] pro-<br>posed an end-to-end convolutional co-occurrence feature learn-<br>ing framework. The co-occurrence features are learned using a hi-<br>erarchical method with different levels of contextual information<br>gradually aggregated. Kim and Reiter [37] proposed a new model<br>called Temporal Convolutional Neural Network (TCN) for 3D hu-<br>man action recognition, it used one-dimensional residual CNNs and<br>based on directly connected joint coordinates to identify skeleton<br>sequences. Liu et al. [38] proposed an enhanced skeleton visual-<br>ization method, which visualizes the skeleton as a series of color<br>images and then implicitly encodes the spatio-temporal informa-<br>tion of the skeleton joints. Since CNN can only process regular grid<br>data in non-Euclidean space, it is not as good as other networks in<br>skeleton-based action recognition.<br>RNN-based approaches typically model skeleton sequences as<br>a series of coordinate vectors along spatial and temporal dimen-<br>sions, where the vectors represent the body’s joint points. Liu et al.<br>[39] proposed a tree-structure-based traversal method that uses<br>an RNN-based approach to model the spatio-temporal domain and<br>introduces a new gating mechanism in LSTM to learn the relia-<br>bility of sequential input data. Liu et al. [40] designed a Global<br>Context-Aware Attention L STM (GCA-L STM), which is able to se-<br>lectively focus on information joints in action sequences with the<br>assistance of global contextual information. Zhang et al. [41] de-<br>signed an attentional mechanism, it embeds a recurrent attentional<br>network that can explore the spatiotemporal relationships between<br>different local regions to focus on important regions. Zhu et al.<br>[42] proposed an end-to-end fully connected deep LSTM network<br>that takes the skeleton as input at each time point and introduces<br>a new regularization method to learn the co-occurrence features of<br>skeleton joints.<br>However, both CNNs and RNNs have difficulty in capturing the<br>natural graph structure of skeleton topological features. To better<br>capture human action features [16,43] , recent work has utilized<br>GCNs [44,45] for spatial and temporal modeling of actions</p><p>5 Skeleton-Based Action Recognition<br>Skeleton-based action recognition is a fundamental yet<br>challenging field in computer vision research. Previous<br>skeleton-based motion recognition methods are usually re-<br>alized with the geometric relationship of skeleton joints [7,<br>36, 37]. The latest methods pay more attention to deep net-<br>works. Du et al. [6] applied a hierarchical RNN to process<br>body keypoints. Attention-based methods are proposed to<br>automatically select important skeleton joints [28–30, 47]<br>and video frames [29, 30] to learn more adaptively about<br>the simultaneous appearance of skeleton joints. However,<br>recurrent neural networks often suffer from gradient van-<br>ishing [11], which may cause optimization problems. Re-<br>cently, graph convolution networks attract more attention<br>for skeleton-based action recognition. To extract both the<br>spatial and temporal structural features from skeleton data,<br>Yan et al. [40] proposed spatial-temporal graph convolution<br>networks. </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;../assets/image-20240319200003990.png&quot; alt=&quot;image-20240319200003990&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/image-20240319200138262.png&quot; a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>今日一心</title>
    <link href="https://www.fomal.cc/posts/3ccff96b.html"/>
    <id>https://www.fomal.cc/posts/3ccff96b.html</id>
    <published>2024-03-13T14:09:23.000Z</published>
    <updated>2024-09-06T01:04:24.692Z</updated>
    
    <content type="html"><![CDATA[<h1 id="聂辉华番外课程1该不该走选调生"><a href="#聂辉华番外课程1该不该走选调生" class="headerlink" title="聂辉华番外课程1该不该走选调生"></a>聂辉华番外课程1该不该走选调生</h1><h2 id="细节展开1"><a href="#细节展开1" class="headerlink" title="细节展开1."></a>细节展开1.</h2><p><img src="../assets/image-20240315191357916.png" alt="image-20240315191357916"></p><p><img src="../assets/image-20240315191542319.png" alt="image-20240315191542319"></p><p><img src="../assets/image-20240315191701845.png" alt="image-20240315191701845"></p><p><img src="../assets/image-20240315191804417.png" alt="image-20240315191804417"></p><p><img src="../assets/image-20240315191829363.png" alt="image-20240315191829363"></p><p>区别</p><p><img src="../assets/image-20240315191931043.png" alt="image-20240315191931043"></p><p><img src="../assets/image-20240315192049444.png" alt="image-20240315192049444"></p><p><img src="../assets/image-20240315192057670.png" alt="image-20240315192057670"></p><p>选调生 vs 统考生</p><p><img src="../assets/image-20240315192337006.png" alt="image-20240315192337006"></p><p><img src="../assets/image-20240315192406696.png" alt="image-20240315192406696"></p><h2 id="选调生、国考、省考"><a href="#选调生、国考、省考" class="headerlink" title="选调生、国考、省考"></a>选调生、国考、省考</h2><blockquote><p>资料汇总</p><p><a href="http://www.sdgwy.org/html/kszc/202403/2_81743.html">2025年山东公务员考试报名信息「提前收藏」 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/">山东公务员考试最新消息 -2024年山东公务员考试网上报名时间、考试大纲、历年真题 (sdgwy.org)</a></p><p><a href="http://www.sdgwy.org/html/zkgg/xds/index.html">招考公告/选调生 - 山东公务员考试最新消息 (sdgwy.org)</a></p><p><img src="../assets/image-20240324125043891.png" alt="image-20240324125043891"></p><p>时政学习</p><p><a href="https://www.chinagwy.org/html/ggjczs/mszt/202401/52_615293.html">公务员考试备考必看：2024年时事政治合集「持续更新」 - 国家公务员考试最新消息 (chinagwy.org)</a></p></blockquote><h3 id="选调生"><a href="#选调生" class="headerlink" title="选调生"></a>选调生</h3><p>23年毕业生有一个青岛的选调  但是24年毕业生没有，只面向了东南大学</p><p><strong>25年未知</strong></p><p><a href="http://www.sdgwy.org/html/gdzk/qd/202211/67_73792.html">2023年青岛市“青选计划”选调公告（山东师范大学） - 山东公务员考试最新消息 (sdgwy.org)</a></p><p>都是只能报考一个单位或者岗位。</p><blockquote><p>需要选调生的历年数据  </p><p><a href="https://zhuanlan.zhihu.com/p/645424990">山东定向&amp;常规选调最全报录比数据（2023年最新） - 知乎 (zhihu.com)</a></p><p> 详细政策和选择分析</p><p><a href="https://zhuanlan.zhihu.com/p/601525199">2023山东定向选调第一批——山东专额选调全面解读（附岗位指导） - 知乎 (zhihu.com)</a></p><p>专额选调 是指的第一批选调</p><p><a href="https://ah.huatu.com/zt/sdxdszwb/">2024年山东招录选调生考试职位表下载_岗位查询-山东公务员考试网 (huatu.com)</a> x</p><p>选调岗位  【下载下来的是22年的】</p><p>在济南或者说山东 是普通选调的县直 或者是乡镇</p></blockquote><h3 id="证监会-VS-金管局"><a href="#证监会-VS-金管局" class="headerlink" title="证监会 VS 金管局"></a>证监会 VS 金管局</h3><p><a href="https://mp.weixin.qq.com/s/_XW4D0jRiy-g0Q6ldQWGVA">获取国考证监会历年真题资料方式 (qq.com)</a></p><h1 id="载脑体-爱情第一讲"><a href="#载脑体-爱情第一讲" class="headerlink" title="载脑体 爱情第一讲"></a>载脑体 爱情第一讲</h1><blockquote><p>系统性，本源性 去剖析男女关系的本质</p><p>从可实践的角度分析如何遇到一个对的人</p></blockquote><h2 id="摆正观念"><a href="#摆正观念" class="headerlink" title="摆正观念"></a>摆正观念</h2><p>对于大多数人而言，人生中最重要的决定是跟什么样的人相伴一生</p><p><img src="../assets/image-20240320155836104.png" alt="image-20240320155836104"></p><p>其次才是选择行业，再差的行业都会给与自己一个安身立命之所，职业的周期性会贯穿个人的前半生，但在未来的AI时代，自己可能无法保证未来行业的连续性，且很多时候决定行业的趋势的往往是风口和周期率，而非自身所能左右</p><blockquote><p>选对一个好的伴侣不仅仅影响自己的金钱、时间，也会影响自己人生漫漫长路的质量，也是自己为数不多的能够抵抗世界周期率和波动的抉择之一</p></blockquote><p>好男儿不在乎儿女情长，但对于大多数人，尤其是男生，除了谋生赚钱的知识以及专业技能基础课之外，优先掌握如下知识：</p><p><img src="../assets/image-20240320160615680.png" alt="image-20240320160615680"></p><blockquote><p>每一条都事关个人一生真正的幸福</p></blockquote><p>其他事情可以感兴趣，但不能对于以上事情无知的情况下，去琢磨其他事情</p><h3 id="摆正对于婚姻的理解"><a href="#摆正对于婚姻的理解" class="headerlink" title="摆正对于婚姻的理解"></a>摆正对于婚姻的理解</h3><blockquote><p>职场雇佣关系，朋友关系，商业上的价值互换关系</p></blockquote><p><img src="../assets/image-20240320160800430.png" alt="image-20240320160800430"></p><p>婚姻的本质 是找一个战场上的一个战友关系，是一个相互爱慕的战友</p><p>首先是同目标，共进退。不抛弃，不放弃；</p><p>其次才是对方的技能有多强</p><p>==慕强的本质关系==</p><h3 id="亲密关系的无知"><a href="#亲密关系的无知" class="headerlink" title="亲密关系的无知"></a>亲密关系的无知</h3><p>大学教育里边却没有教育，自己家庭里边的空白，大多数事情里边都有一个专业，</p><p>而对于两性关系方面，大多数人是面对的确是自己没有自知之明的无知</p><h2 id="先进行自身建设"><a href="#先进行自身建设" class="headerlink" title="先进行自身建设"></a>先进行自身建设</h2><blockquote><p>爱人之前，先爱自己</p></blockquote><p><img src="../assets/image-20240320161516105.png" alt="image-20240320161516105"></p><p>==沉默成本解释外，更深层的理解==</p><blockquote><p>自我认可度不高的人，很容易会把恋爱中的甜蜜时刻，理解为对方是命运给与自己的恩惠和赏赐，从而会给对方添加滤镜。即使往往对方对自己并不好，也很难结束一段有毒的关系。</p><blockquote><p>能做到这一步很难，很多人要用一生来去治愈童年</p></blockquote><p>自我认可度不高的人很容易为了人与人之间当下的短暂快乐，而自愿去做明知没有结果的事情，去维持明知没有结果的关系，甚至会让自己主动去付出巨额代价。</p><p>会遇到人渣并让自己在一段感情中自我献祭</p></blockquote><p><img src="../assets/image-20240320161612688.png" alt="image-20240320161612688"></p><blockquote><p>异地女朋友 单独和男生吃饭后才主动告知，下次买票钱我来出？？</p></blockquote><h3 id="如何把握严于律己，宽以待人的分寸？"><a href="#如何把握严于律己，宽以待人的分寸？" class="headerlink" title="如何把握严于律己，宽以待人的分寸？"></a><strong>如何把握严于律己，宽以待人的分寸？</strong></h3><p>严于律己，宽于待人 要当做适用于对待特殊情况下的策略，而不是适用于任何情况下的美德</p><p>对方和你没有任何的利益关系，且对方的行为在长远期看，不会损伤你的个人利益</p><p>朋友关系，朋友圈点赞关系，甚至某些情况下 要采用“宽于律己，宽以待人”的策略</p><p><img src="../assets/image-20240320163117338.png" alt="image-20240320163117338"></p><p>对于对方如果跟你有直接利益关系，并且它的行为会长远期看，会影响你自己的个人利益的情况下，==尤其是涉及到人品这个特殊变量的情况下==那么就要去采取“严于律己，同以待人”的策略</p><p><img src="../assets/image-20240320163603054.png" alt="image-20240320163603054"></p><p>学会善用反问句，要让那些让你自己不舒服的人同样难看</p><p><img src="../assets/image-20240321102350839.png" alt="image-20240321102350839"></p><blockquote><p>不管是亲密关系，还是社交关系，要清楚别人时刻是在筛选自己而自己也要时刻学会去主动筛选别人</p></blockquote><p><img src="../assets/image-20240320163636818.png" alt="image-20240320163636818"></p><blockquote><p>不是所有人都会与你同行，在一段亲密关系里边，在你为对方去付出并且对方很享受在你这里的获得时候，对方也应该为你而建立一份契约，你自己也有筛选别人的资格，而非被筛选的资格。</p><p>等你学会如何不惧直面冲突，再去学习如何高情商。</p></blockquote><p><img src="../assets/image-20240320164821898.png" alt="image-20240320164821898"></p><blockquote><p>宜人和温和是强者能量溢出的特权，再身弱之人那里，只会演变成懦弱和委曲求全而不自知，并最终反向筛选出一批人渣主动留在自己的身边。</p><p>要做到以上，内核是自尊自爱；而不管自己有没有做成一些事情，现在自己要做的就是在所有的关系中，去无条件的提高自我认可度。</p><p>重要的不是对自己高看或者低看，而重要的是在爱人之前，先让自己变得足够强</p></blockquote><h3 id="爱人之前先变强的两个心法"><a href="#爱人之前先变强的两个心法" class="headerlink" title="爱人之前先变强的两个心法"></a>爱人之前先变强的两个心法</h3><ol><li><p>改变自己的关键是创造价值 </p><ol><li>学会用价值规律看问题，让自己变成一个有价值的人</li><li>改变自己的关键是发掘自己并不断的创造价值（创造视频，写文案）<ol><li>帮助别人的同时不断的成就自己</li><li>创造的价值必须是长久的</li></ol></li></ol><blockquote><p>价值够长久，才能够价值够大。同时价值足够大时候，才能够在任何关系中得到对等的回馈</p><p>当把视角“我想要”变成“我能给”的时候，很多浮躁妄念，就会马上消失。</p></blockquote></li></ol><ol><li>建立自己的”身份感”,不是指的是社会世俗意义上的身份，而是从心底建设开始，<strong>弄清楚自己我要成为一个怎样的人</strong></li></ol><p>无论想要成为什么样的人，都要建立自己的身份感，是一种从上到下的全方面塑造，远比以结果为导向从下至上的塑造更具力量。</p><p>如果认为自己强大，那就要有自己发自内心的驱动力，让自己去承受相应的痛苦和困难，不断蜕变。</p><blockquote><p><strong>男性在两性关系中必须接受的事实</strong></p><p>男女而言，通用的是对一个人有没有情欲是爱情能不能发生的一个大前提，而体面感是情欲重要的催化剂。表面上是爱情，其实真正在乎的是体面感。</p><p>体面表面上的表现形式是外貌，财富，而==真正的体面感会在后边展现==</p><blockquote><p>生物进化领域的常识： </p><ol><li>长得太丑的男性，几乎不会获得女性来自情欲所产生的爱情；但当长得丑的男性，如果获得良好的供养能力的时候，比如有钱、有地位，女性会不介意去和其发展长期关系；<strong>但</strong>依旧很难因为情欲而产生爱情。同时这个阶段如果出现一个长得帅的男性来追求这个女生，即便是这个男生没有钱，也没有社会地位，多数女生也不会抗拒和其发展短期关系</li><li>男性在生物学上也并不是多高尚，让人舒服的话，如果说的太多就会让人丧失接收真相的能力</li><li>剔除掉男女两性中很善良的人，也剔除掉很坏的人，剩下的广大7成男女中就基本都是这样子</li><li>进化心理学的观点也就是对于大多数人的进行一个特征的描述</li></ol></blockquote><ol><li><p>财富在两性关系中很重要,但是不要过于高估</p><ol><li>有钱则低调，没钱尽量找一个没钱时爱你的女人，比如爱你的性格，爱你的才学等等、</li></ol></li><li><p>平时保持一个基本的整洁也算是一个必须要做的事情，保持一个基本的精气神（长得好看（生气时候看到就不会生气）vs精神干净，男性好看的包容性）</p></li><li><p>遇到人的“运气和概率”</p><p>烟花0.5s的幸运，会让20多年的努力显得渺小。</p><p>每个人的人生的关键节点，都充满着这0.5s幸运的随机性。如果你碰到的人足够好，不一定是你自己有多么的优秀，而是因为你自己遇到了这0.5妙的随机性，<strong>0.5妙的概率幸运，让自己的所有的人品，包括20多年的努力都会显得相形见绌</strong></p></li></ol></blockquote><h4 id="人唯一能做的"><a href="#人唯一能做的" class="headerlink" title="人唯一能做的"></a>人唯一能做的</h4><p><img src="../assets/image-20240320170004561.png" alt="image-20240320170004561"></p><blockquote><p>而不是遇到对的人和爱自己的人时候，发现不了，识别不了，甚至将别人对自己的爱轻视甚至浪费掉，这样的人注定不会承接住好东西。</p></blockquote><p>提高自己在亲密关系里识别和抓住好东西能力重点在于<strong>祛魅</strong> ==第二节重点讲祛魅==</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="../assets/image-20240321102137401.png" alt="image-20240321102137401"></p><p><img src="../assets/image-20240321102225633.png" alt="image-20240321102225633"></p><blockquote><p>唯有自知，方有自爱；唯有自爱，方有人爱。</p></blockquote><p><img src="../assets/image-20240321105014986.png" alt="image-20240321105014986"></p><p><strong>一切从认识自我开始</strong></p><blockquote><p>关于幸运的思考:  自己高考 524  差最后一名 ；大学里边成绩搞得挺好，但是能够让自己去兼职+成功大二；考研扩招+读研</p><p>中文互联网越来越趋向于封闭，获取有效信息的成本越来越大；想要看到真正有价值的思考，或者“隐学”正变得越来越难，很多真正的好回答将再次变成“家学”。</p><p>不再花费时间去回答很多热点内容，摒弃掉所有没有长期价值的内容，执着于长期价值的回答和做事（未来两三年或者长期十几年后的仍然有用的东西），力求每一件事情都能够解决实际问题或者改变一个观念，消除一切浮躁的动机。</p></blockquote><p>推荐电影《背靠背 脸对脸》，B站就有很多介绍</p><p><img src="../assets/image-20240322204210917.png" alt="image-20240322204210917"></p><p><img src="../assets/image-20240322211957586.png" alt="image-20240322211957586"></p><blockquote><p>纪律 服从 权威<br>1.恰当微笑。不得罪人，给人留下好印象，仪容仪表，把领导同事当客户，端庄微笑，和蔼可亲<br>2.夸别人。忌讳说别人坏话，夸领导表示服从，夸自己，夸同事打下群众基础<br>3.为领导分担。错事错办，但不能不办<br>4.忍。忍批评，受委屈，吃小亏<br>5.谨慎。慎之又慎，不背后说人坏话，不犯原则性错误；事无巨细，谨小慎微，反复检查<br>6.混。不鼓励个人英雄主义，强调集体主义，和光同尘。<br>7.熬。延迟满足，曲折前进，自制力强。熬资历，熬时间（人生低潮时期），熬领导。你年轻，时间站在你这边，终究会熬出头的</p></blockquote><p><a href="https://mp.weixin.qq.com/s/B2eBdYGzXEJkq0woTGWKAQ">冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究 (qq.com)</a></p><h1 id="冯军旗：如何制定好的政策-——基于《不变的是原则，万变的是方法》的实证研究"><a href="#冯军旗：如何制定好的政策-——基于《不变的是原则，万变的是方法》的实证研究" class="headerlink" title="冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究"></a>冯军旗：如何制定好的政策 ——基于《不变的是原则，万变的是方法》的实证研究</h1><h1 id="善思会写更快乐219"><a href="#善思会写更快乐219" class="headerlink" title="善思会写更快乐219"></a>善思会写更快乐219</h1><p>节后复工第一天，感觉怎么样？</p><p>有一种说法我很喜欢，职业就像游戏打怪通关，倦怠意味着你在这一关地图都展开了，打到头了，而新一关的钥匙，就在某个你曾经视而不见的熟悉地方。</p><p>而古典老师说过的这4条职场认知，我觉得帮助很大，也分享给你。</p><p>1️⃣结果思维，过程不重要，结果才重要。</p><p>2️⃣贡献思维，怀才不遇是个伪命题。<br>现在的社会分工很细，就像一条流水线拆出来的内容。所以，专业人士有义务告诉上游和下游，你能干什么。换言之，有义务把自己的才华“卖出去”，变成“贡献”。</p><p>3️⃣得失思维，职场不谈对错，谈得失。</p><p>4️⃣灰度思维，所有的职业环境，都是多元视角妥协而成的产物。在职业世界中，不是非黑即白，任何事情都是灰色的、混色的。</p><p>以上这4点，你想清楚了，职业世界的事，无往而不胜。工作即道场，记得要升级哦！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;聂辉华番外课程1该不该走选调生&quot;&gt;&lt;a href=&quot;#聂辉华番外课程1该不该走选调生&quot; class=&quot;headerlink&quot; title=&quot;聂辉华番外课程1该不该走选调生&quot;&gt;&lt;/a&gt;聂辉华番外课程1该不该走选调生&lt;/h1&gt;&lt;h2 id=&quot;细节展开1&quot;&gt;&lt;a hre</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>LLm_learn</title>
    <link href="https://www.fomal.cc/posts/1908cf59.html"/>
    <id>https://www.fomal.cc/posts/1908cf59.html</id>
    <published>2024-02-16T10:48:56.000Z</published>
    <updated>2024-03-01T09:58:27.040Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Fine-Tuning-or-Retrieval-Comparing-Knowledge-Injection-in-LLMs？"><a href="#Fine-Tuning-or-Retrieval-Comparing-Knowledge-Injection-in-LLMs？" class="headerlink" title="Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs？"></a>Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs？</h1><blockquote><p>相比于封装好的相关的事实知识信息，利用外部数据能够扩展大模型的性能，并减少其对于训练数据依赖性；</p><blockquote><p>如何更好的使用外部数据集来合并新信息，或根据之前看到的信息改进llm的功能，是一个重大的挑战</p></blockquote></blockquote><p>比较了两种常见的扩展方法:<strong>无监督微调</strong>和<strong>检索增强生成(RAG)（retrieval-augmented generation (RAG)）</strong>。我们在不同主题的知识密集型任务上评估这两种方法。我们的发现表明，==尽管无监督的微调提供了一些改进，但RAG的性能始终优于它==，无论是在训练中遇到的现有知识还是全新的知识。此外，我们发现，llm很难通过无监督的微调来学习新的事实信息，而在训练期间让他们接触同一事实的众多变体可以缓解这个问题。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ol><li>大型语言模型（LLMs）能够捕获大量的事实信息（Petroni等，2019年；Cohen等，2023年；Hu等，2023年）。由于它们庞大的预训练数据集，LLMs在各个领域展示出了显著的知识水平。然而，这种知识存在两个显著的局限性。<strong>首先，它是静态的，不随时间更新。其次，它不具体，因此可能在特定领域缺乏细致的专业知识。虽然这是两个不同的问题，但它们深深相关，因为它们的解决方案是相同的：增强模型的知识。</strong></li><li>最近，将LLMs适应特定领域并更新其知识的想法变得越来越普遍（Yu等，2022年）。已经提出了各种模型来改进不同领域的事实知识和能力，例如医疗保健（Singhal等，2023a;b；Wu等，2023a）、金融（Wu等，2023b；Yang等，2023）和法律（Huang等，2023年；Nguyen，2023年）。</li><li>在这项工作中，我们<strong>专注于评估模型的知识及其记忆、理解和检索事实数据的能力。</strong>我们的目标是理解知识注入的概念（Wang等，2020年；Chen等，2022年；Liu等，2020年；Lauscher等，2020年）。<strong>在给定文本语料库形式相关某些知识库的情况下，找到如何教一个预训练模型这些知识的最佳方法</strong></li><li>向预训练模型添加知识的一种方法是通过微调。通过微调，我们继续模型的训练过程，并使用特定于任务的数据进行适应。通过让模型接触特定的知识库，我们期望模型的权重相应地适应。这个过程旨在优化模型以用于目标应用，提高其在专业领域的性能和上下文相关性。</li><li>另一种增强模型知识库的方法是通过使用上下文学习（ICL）（Chen等，2021年；Radford等，2019年；Min等，2021年；Lampinen等，2022年）。ICL背后的主要思想是通过修改模型的输入query而不直接改变模型的权重来提高预训练LLMs在新任务上的性能。<strong>ICL的一种形式是检索增强生成（RAG）</strong>（Lewis等，2020年；Neelakantan等，2022年）。<strong>RAG利用信息检索技术使LLMs能够从知识源中获取相关信息并将其合并到生成的文本中。</strong></li><li>本研究旨在通过比较微调和RAG评估LLMs的知识注入能力。为了说明原理，让我们使用一个类比。考虑三名大学生在特定主题的考试上。所有人都可以获取课堂材料，但事先不知道主题。第一名学生只在考试时有教材，第二名学生在考试之前有准备，并学习了，而第三名学生在考试通知发布后失去了准备时间。谁可能表现得更好？</li></ol><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><ol><li>背景：为了评估知识注入，我们必须首先了解对于LLMs而言知识意味着什么。知识和语言模型：定义知识是一个复杂的哲学任务，远远超出了本研究的范围。然而，我们可以考察在语言模型的背景下事实知识意味着什么。如果一个模型知道一个事实，它可以准确而一致地回答与之相关的问题。此外，它可以可靠地区分与该事实相关的真假陈述。然后，我们可以将这个定义扩展到整个知识库，而不仅仅是单个事实。数学上，设Q = {qn}N为N个多项选择事实问题的集合，其中每个问题有L个可能的答案和一个正确答案。设A = {(a1n，…，aLn)}N为相应的可能答案集合，C = {cn}N为正确的答案。设M是一个语言模型。我们用M(qn) ∈ {a1n, …，aLn}表示模型对第n个问题的预测答案。我们定义M关于Q的知识分数L为标准的准确度分数：LM,Q := #{qn| M(qn) = cn} / N。如果以下条件成立，我们说模型M在Q集合方面拥有任何知识：LM,Q &gt; 1 / L。简单地说，模型可以一致地给出正确答案，超过简单的随机猜测基线。自然地，如果模型M相对于另一个模型的知识得分LM,Q更高，那么我们断言前者相对于后者在Q方面更有知识。</li><li>先前见过的知识：一个重要的区分是模型在预训练期间曾接触过的知识与全新事实之间的区别。考虑到现代LLM训练集的规模，它们涵盖了通过网络文本获得的大量信息。因此，即使在利基领域，知识注入的目标不一定是教会模型全新的事实，而是通过向特定领域施加偏好来“刷新”其记忆。</li><li>知识和推理：我们强调，LLMs的这种知识评估框架是不完美的。重要的是，它并没有解决影响模型响应的其他质量指标。创建一个纯粹以知识为基础的数据集而不涉及某种程度的推理是具有挑战性的。因此，具有强大推理能力的模型可能通过在多项选择考试中进行“有根据的猜测”而在陌生的知识密集型任务上表现出色。因此，对LLMs中的知识的任何评估都应考虑到这一点，并将结果视为推理（Sakaguchi等，2021年）、阅读理解（Dua等，2019年）和一般语言能力（Srivastava等，2022年）的更广泛基准的一部分。然而，这个评估框架仍然强调事实信息高于其他一切。</li><li>事实错误的原因：模型无法准确回答事实问题有很多可能的原因。在Wang等人的研究中，Wang等人介绍了五个主要的模型级错误原因的分类：<ul><li>领域知识不足：语言模型可能在特定领域缺乏全面的专业知识，因为它没有接触过这个领域。例如，一个只在威廉·莎士比亚的文本上训练的模型在被问及马克·吐温的作品时表现会很差。</li><li>过时信息：LLMs的截止日期由它们的训练数据集确定。因此，任何在最后一次训练更新之后发生的事件、发现或变化，如果没有外部来源的访问，将不会在模型的知识范围内。</li><li>遗忘：有时，模型在训练过程中接触到知识，但没有保留下来。对于在训练数据集中很少出现的罕见事实，这一点尤为真实。</li><li>遗忘：语言模型在预训练阶段之后通常会进行额外的训练（微调）。在某些情况下，这可能会导致一种被称为灾难性遗忘的现象，其中模型会失去在微调过程之前具有的一些知识。</li><li>推理失败：在某些情况下，语言模型可能具有与事实相关的相关知识，但未能正确利用它。这在复杂的多步推理任务（Tan等，2023年）或当对同一事实提出不同问题时特别明显，导致不同的结果。</li></ul></li></ol><h2 id="3-Injecting-Knowledge-to-Language-Models"><a href="#3-Injecting-Knowledge-to-Language-Models" class="headerlink" title="3. Injecting Knowledge to Language Models"></a>3. Injecting Knowledge to Language Models</h2><p>根据第2节中给出的背景，很明显，对于许多知识密集型任务来说，一般的预先培训是不够的。为了解决这个问题，额外的后处理步骤对于增强预训练模型的知识至关重要。这一步通常被称为知识注入（Wang et al.，2020年; Chen等人，2022; Liu等人，2020; Lauscher等人，2020年）。<strong>在本节中，我们将研究两种广泛使用的知识注入框架：微调（FT）和检索增强生成（RAG）。我们开始制定知识注入问题，旨在解释这两种方法使用一致的术语。</strong></p><p>现在使用相同的术语将这个公式扩展到知识注入的问题。给定一组事实问题，存在一些包含与这些问题相关的信息的文本语料库。<strong>知识注入的核心假设是，如果能够完全访问该语料库，它可以作为辅助知识库，并提高模型在这组问题上的性能。</strong>在数学上，假设M是一个预先训练好的模型，Q是一组事实问题。现在，假设我们有一个相关的辅助知识库BQ。我们的目标是发现一个变换，记为F，当应用时，将增强关于Q：M′：= F（M，BQ）s. t的知识。LM′，Q &gt; LM，Q.(3)</p><h3 id="3-2-微调"><a href="#3-2-微调" class="headerlink" title="3.2. 微调"></a>3.2. 微调</h3><p>3.2. 微调：微调是调整预训练模型以增强其在特定、通常更狭窄的数据集或任务上性能的过程。在这里，重要的是要区分不同类型的微调。FT技术通常被分类为监督、无监督和<strong>基于强化学习（RL）的方法</strong>。我们接下来简要回顾这些方法及其与知识注入问题的关系。</p><ul><li>监督微调：监督微调（SFT）需要标记的输入-输出对。其中一种最常见的SFT方法是指导微调，它已成为提高模型性能的最强大方法之一。使用指导微调，输入是自然语言任务描述，输出是所需行为的示例。许多当前最先进的LLMs在预训练阶段之后经历了指导微调。</li><li>强化学习：FT的另一种形式依赖于RL或<strong>RL启发的优化策略来更好地调整模型</strong>。其中一些突出的例子是来自人类反馈的强化学习（RLHF）、直接偏好优化（DPO）和近端策略优化（PPO）。这些技术已被证明在与指导微调结合使用时非常有用。然而，与指导微调类似，这些方法<strong>侧重于响应的整体质量和其预期行为，而不一定关注其知识的广度。</strong></li><li>无监督微调：我们讨论的最后一种FT策略是无监督的，这意味着模型没有可用的标签可供学习。一个常见的无监督FT技术通常被称为持续预训练或非结构化FT。在这种方法中，FT过程被视为预训练阶段的直接延续。</li></ul><blockquote><p>启发式离散数据微调【基于强化学习的方法】</p></blockquote><h3 id="3-3-检索增强生成："><a href="#3-3-检索增强生成：" class="headerlink" title="3.3. 检索增强生成："></a>3.3. 检索增强生成：</h3><p>3.3. 检索增强生成：检索增强生成（RAG）<strong>是一种通过使用外部知识源扩展LLMs能力的技术，尤其是在知识密集型任务中。</strong>原始的制定涉及每个任务的额外训练，但已经证明，<strong>一个预训练的嵌入模型可以在没有额外训练的情况下获得改进的性能。</strong>该想法是在给定一个辅助知识库和一个输入查询的情况下，使用RAG架构找到与输入查询相似的文档。这些文档随后被添加到输入查询中，从而使模型对查询的主题有进一步的上下文理解。实践中，实现建议的架构相当简单：<strong>给定一个辅助知识库BQ和一个预训练的嵌入模型Me，我们为每个文档b ∈ BQ创建一个密集向量表示（嵌入），并将这些存储在向量存储器中。收到新查询q后，我们使用其嵌入Me(q)来检索与q的前K个最近邻，bq = {bk}K1，根据点积排名。然后，我们更新q为˜q = bq∥q，其中∥表示字符串连接。最后，我们返回M(˜q)作为模型的输出。</strong></p><h2 id="4-Knowledge-Base-Creation"><a href="#4-Knowledge-Base-Creation" class="headerlink" title="4. Knowledge Base Creation"></a>4. Knowledge Base Creation</h2><h3 id="4-1-任务选择和原理"><a href="#4-1-任务选择和原理" class="headerlink" title="4.1. 任务选择和原理"></a>4.1. 任务选择和原理</h3><p>MMLU基准：为了正确评估LLMs在知识密集型任务上的能力，我们从大规模多语言语言理解评估（MMLU）基准中选择了四个不同的任务，涵盖解剖学、天文学、大学生物学、大学化学和史前等主题。所选任务是基于它们对事实知识的重视以及对推理的最小依赖而选择的。作为<strong>一种启发式方法</strong>，我们<strong>选择了问题简短且不涉及上下文的任务</strong>。在实践中，我们选择了四个STEM学科以及一个人文学科，以确保评估不局限于某些领域。值得注意的是，史前涉及涵盖所有非现代历史的问题。这种方法旨在使我们能够测试LLM在理解和操纵信息方面的熟练程度，与其推理过程隔离开来。</p><p>时事任务：为了进一步孤立LLM学习新知识的能力，我们<strong>创建了一个包含关于当前事件的多项选择题的任务</strong>。该任务包括关于发生在各种模型训练数据截止日期之后的事件的多项选择题。具体来说，我们侧重于美国的“当前事件”，时间跨度为2023年8月至11月，这些事件包含在相关的维基百科索引中。这种方法使我们几乎可以保证模型没有接触过这些事实，因此能够直接测试知识注入的能力。</p><p>总结：在本节中，作者选择了四个来自MMLU基准的不同任务，涵盖了解剖学、天文学、大学生物学、大学化学和史前等主题，以评估LLMs在知识密集型任务上的能力。此外，为了测试LLM学习新知识的能力，还创建了一个关于当前事件的任务。这些任务的选择旨在确保测试能够涵盖各种领域，并在不同背景下评估LLM的能力。</p><h3 id="4-2-数据收集与预处理"><a href="#4-2-数据收集与预处理" class="headerlink" title="4.2. 数据收集与预处理"></a>4.2. 数据收集与预处理</h3><p>4.2. 数据收集与预处理 为了有效评估LLMs在这些知识密集型任务上的表现，通过从维基百科中针对每个主题抓取相关文章，收集了一个全面的辅助数据集。选择维基百科作为主要知识来源的原因是它对相关主题的广泛覆盖以及作为群众验证知识库的可靠性。通过识别每个主题的相关中心页面，通过官方维基百科API检索了所有与任务相关的文章。 随后，采用了严格的清理过程将数据从原始子部分转换为干净的块。此步骤使用了“wikiextractor”工具（Attardi，2015）。将数据分成小的、干净的块（例如，删除HTML、URL等）旨在增强LLMs在各种知识领域的理解，并帮助LLMs进行微调过程。</p><h3 id="4-3-当前事件任务创建"><a href="#4-3-当前事件任务创建" class="headerlink" title="4.3. 当前事件任务创建"></a>4.3. 当前事件任务创建</h3><p>4.3. 当前事件任务创建 在从维基百科收集到相关块之后，我们借助GPT-4（OpenAI，2023）创建了一个新的多项选择数据集。首先，我们移除了任何小块。对于语料库中的每个剩余块，要求GPT-4创建四个高度具体、高质量的只有一个正确答案的多项选择题。所谓具体，是指问题可以在不了解问题所指的上下文的情况下回答，并且具有最小的歧义性。接下来，要求GPT-4选择其中最具体的两个。然后进行手动评估和验证步骤。总共，这产生了910个新问题。</p><h3 id="4-4-释义生成"><a href="#4-4-释义生成" class="headerlink" title="4.4. 释义生成"></a>4.4. 释义生成</h3><p>4.4. 释义生成 在创建数据集之后，我们利用GPT-4生成数据集的增强版本。我们要求GPT-4提供输入数据的释义版本，这些版本完全保留信息但用不同措辞。为了确保多样性，每次释义迭代都使用不同的种子。我们随机选择了每个任务的240个块，并为每个块创建两个释义。这些被保留用于用于超参数调整的验证集。对于描述在第6节中的微调过程中使用的当前事件数据集，我们为每个块创建了十个释义。</p><h2 id="5-实验和结果"><a href="#5-实验和结果" class="headerlink" title="5.实验和结果"></a>5.实验和结果</h2><p>实验框架 我们使用了流行的LM-Evaluation-Harness（Gao等人，2021）存储库来评估LLMs在所选知识密集型任务上的性能。LM-Evaluation-Harness是一个强大的基准测试工具，目前是模型评估的行业标准，并且是HuggingFace排行榜的基础。利用这个平台确保了标准化的评估框架，并允许跨模型、方法和数据集进行一致的比较。更重要的是，通过使用行业标准进行评估，我们可以避免由提示工程和格式问题引起的任何差异，并复制每个模型的报告基准结果。</p><p>模型选择 我们选择了三个模型进行推理评估：Llama2-7B（Touvron等人，2023）、Mistral-7B（Jiang等人，2023）和Orca2-7B（Mitra等人，2023）。选择这些模型旨在代表最流行的开源基础模型和跨各种基线能力的经过指令调整的模型。此外，我们选择了bge-large-en（Xiao等人，2023）作为RAG组件的嵌入模型，并使用FAISS（Johnson等人，2019）作为其向量存储。根据HuggingFace MTEB排行榜，这个嵌入模型目前是开源嵌入模型的SOTA。</p><p>配置变化 我们的评估包括多个配置，并对它们进行了网格搜索，以便进行更全面的基准测试。首先，我们比较了基线和微调模型以及它们与RAG组件的性能。其次，我们探索了在RAG中添加到上下文中的文本块的最佳数量。具体来说，采用了不同的K值 ∈ {0, . . . , 5}来分析对模型性能的影响。最后，我们探讨了5-shot性能与0-shot性能之间的比较。</p><p>训练设置 我们使用了第3.2节中描述的无监督训练过程对所有模型进行训练。对于每个数据集，我们将辅助知识库分成大小为256的相等块，通过连接或分割原始块来实现。我们还添加了两个特殊标记，<BOS>和<EOS>，来标记原始块的开头和结尾，以保留文档的结构。模型的训练使用了介于1 × 10^−6和5 × 10^−5之间的学习率，这是通过超参数搜索找到的。所有模型都在4个NVIDIA A-100 GPU上进行了最多5个时期的训练，并且批量大小为64。</p><p>评估方法 所有评估都是通过将每个多项选择选项附加到问题后，然后通过模型传递连接以获得每个选项的对数概率分数来完成的。最高分被解释为模型的选择，并用于准确性计算。更正式地说，在方程（1）中，我们说如果M(qn) = cn： cn = arg max l {M(qn∥a1 n), . . . ,M(qn∥aL n)}, 其中M(qn∥al n) = log PM(qn∥al n)。</p><p>MMLU结果 对于每个任务和模型，我们比较了四种方法：仅使用基本模型、RAG、FT以及通过使用微调模型作为生成器来合并FT和RAG。此外，我们使用了0-shot和5-shot场景来测试MMLU任务。完整的结果显示在表1中。相对准确性增益的聚合，即 (LM′,Q − LM,Q)/LM,Q， 其中M是基本模型，M′是注入知识的模型，显示在图2中。在所有情况下，RAG的表现明显优于基本模型。此外，将RAG与基本模型作为生成器一起使用一直比仅进行微调更好。在某些情况下，将微调模型而不是基本模型用作RAG管道中的生成器进一步提高了结果。然而，这并不一致，因此显示了微调的固有不稳定性。此外，我们发现5-shot方法在大多数情况下都略微提高了结果，在所有不同方法中观察到了类似的趋势。</p><p>当前事件结果 当前事件任务的评估显示在表2中。由于问题和辅助数据集之间的一对一对应（请参阅第4.3节），RAG证明了其特别有效性。微调与RAG相比不具竞争力。然而，微调与多个释义仍然比基线提供了显著的改进。我们注意到，与仅使用RAG相比，将RAG与微调相结合的性能较差。</p><p>值得注意的是，虽然问题是基于模型在训练过程中未接触到的信息，但基本模型的结果超过了1/L = 0.25。这部分可以通过模型在回答不独立于过去信息的问题时使用推理和/或现有知识来解释。一些这方面的例子可以在附录C中找到。</p><p>微调与RAG： <strong>在MMLU和当前事件任务的结果中，RAG相对于微调的显着优势是明显的。</strong>虽然微调在大多数情况下改善了结果，但与RAG方法相比并不具有竞争力。可能有几个因素会导致这种行为。首先，<strong>RAG不仅向模型添加知识，还包含与问题相关的上下文，这是微调所缺乏的功能。</strong>此外，<strong>微调可能会影响模型的其他功能，因为微调程度可能导致灾难性遗忘。</strong>最后，未监督微调模型可能会通过监督或基于RL的微调进一步对齐，正如Orca2相对于基本的Llama2的性能大大提高所示。</p><h2 id="6-重复的重要性"><a href="#6-重复的重要性" class="headerlink" title="6.重复的重要性"></a>6.重复的重要性</h2><p>与其他任务不同，在其他任务中，模型已经在预训练期间暴露于与主题相关的方面，当前事件包括新信息。在这种情况下，标准的常规微调不仅没有提高Llama2的性能，而且还显着降低了它。为了改善微调结果，我们探索了使用释义的数据增强。</p><h2 id="7-结论和未来的工作"><a href="#7-结论和未来的工作" class="headerlink" title="7.结论和未来的工作"></a>7.结论和未来的工作</h2><p>大型语言模型拥有各种主题的大量知识。在这项工作中，我们测试了他们适应新知识的能力：包括专业知识和完全看不见的知识。这是第一个研究比较两个突出的方法在这一领域，即微调和检索增强生成。虽然微调对许多用例都很有用，但我们发现RAG是知识注入的更可靠选择。这项工作的某些方面仍需要进一步研究。例如，我们专注于无监督训练作为我们的主要微调方法，而不是预防性调整或基于RL的方法。研究各种技术与各种辅助知识库的组合可能会产生更好的结果。这种方法，结合我们在第6节中的假设，可以进一步增强我们对通过FT进行知识注入的理解。虽然我们相信这项工作进一步提高了我们对LLM知识的理解，但在这一领域还有很多工作要做。具体而言，需要更多的研究，特别是从理论的角度来看，关于在LLM知识表示的问题。最后，需要进一步努力衡量法学硕士的知识。虽然我们采用了公式（2）中描述的经验方法，但探索其他关于知识的定义和观点并扩展这项工作也很重要。</p><h2 id="8-Limitations"><a href="#8-Limitations" class="headerlink" title="8. Limitations"></a>8. Limitations</h2><p>在所有机器学习应用中，超参数的选择会显著影响结果。因此，我们强烈建议针对特定情况优化所有相关的超参数。我们通过在三个不同的模型上进行实验来支持我们的主张。然而，对其他LLM的推广应该彻底测试。例如，GPT-4对于某些MMLU任务实现了近乎完美的准确性（Nori等人，2023），因此不适用进一步的改进。最后，当我们为知识库选择各种主题时，我们所有的来源都来自维基百科。其他数据集可能会产生不同的结果，必须仔细评估。</p><blockquote><p>Knowledge Base Creation   创建启发式方法</p></blockquote><p>优化prompt </p><blockquote><p>基于公开的数据库 大家去做测试 </p><p>能不能做两个 一个获取更好的数据，另一个去做</p><p>带着地理信息 图片信息 怎样放到prompt 中</p></blockquote><h1 id="RLPROMPT-Optimizing-Discrete-Text-Prompts-with-Reinforcement-Learning"><a href="#RLPROMPT-Optimizing-Discrete-Text-Prompts-with-Reinforcement-Learning" class="headerlink" title="RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning"></a>RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning</h1><blockquote><p>强化学习+模型微调</p><p>自动为每个任务找到最佳prompt</p></blockquote><p>==<strong>现状</strong>==</p><p>现有的大多数工作都依赖于调优<strong>软提示</strong>(例如，嵌入式提示)，这些<strong>提示缺乏可解释性</strong>、跨模型之间的的可重用性以及当梯度不可访问时的适用性。另一方面，离散提示很难优化，通常是通过“枚举(例如，解释)-选择”启发式创建的，这种<strong>启发式不会系统地探索提示空间</strong>。</p><p>==提出了一种<strong>基于强化学习的离散提示优化方法RLPROMPT</strong>。RLPROMPT构建了一个参数高效的策略网络，在奖励训练后生成优化的离散提示。==  为了利用来自大LM环境的复杂和随机奖励信号，我们结合了有效的稳定奖励，大大提高了训练效率。RLPROMPT可以灵活地应用于不同类型的大模型，如mask(如BERT)和left-to-right模型(如GPTs)，用于分类和生成任务。在少镜头分类和无监督文本样式转换上的实验表明，在现有的各种微调或提示方法中，该方法具有优越的性能。有趣的是<strong>，优化后的提示往往是不合语法的乱码;令人惊讶的是，这些胡言乱语提示在不同的LM之间是可转移的，以保持显著的性能，这表明LM提示可能不遵循人类的语言模式。</strong></p><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>Prompting已经成为使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，包括从左到右的模型（例如GPTs）（Radford等人，2019年；Brown等人，2020年）和掩码LMs（例如BERT）（Devlin等人，2019年）、RoBERTa（Liu等人，2019年）等。与昂贵地为每个下游任务更新大量LM参数的传统微调相比，<strong>Prompting在输入中连接附加文本，指导LM生成所需的输出，从而降低了成本</strong>。Prompting的一个关键问题是如何找到最佳提示来提高LM在各种任务上的性能，通常只有很少的训练样本。</p><ul><li>Prompting是使用大型预训练语言模型（LMs）解决各种NLP问题的一种有前途的方法，与传统的微调相比，它在输入中连接额外的文本以指导LM生成所需的输出，从而降低了成本。</li><li>关于Prompting的一个关键问题是如何找到最佳提示以提高LM在各种任务上的性能，通常只有很少的训练样本。</li></ul><p>其中，最流行的方案之一是调整软提示（即连续嵌入向量），因为它们易于梯度下降。然而，由于其本质上来说，由于人类难以理解且无法与其他LMs一起使用，生成的提示对人类来说很难理解，并且不兼容。此外，LM内部梯度通常难以计算，或者仅对仅具有推理API（例如GPT-3）的LMs不可用。因此，<strong>使用由词汇表中的具体标记组成的离散提示通常是可取的</strong>。然而，它们的离散性质使优化变得非常困难。以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择。AutoPrompt使用梯度信息编辑提示标记，但受到训练不稳定性和与基于梯度的软提示相同的适用性问题的影响，在实践中的效果有限。</p><ul><li>一个流行的方案是调整软提示，因为它们易于梯度下降，但生成的提示难以理解，而且不兼容其他LMs。</li><li>离散提示通常由具体标记组成，但它们的离散性质使得优化变得非常困难。</li><li>以往的研究通常依赖于手动工程或从多个释义/生成的提示中进行选择，但这些方法在实践中的效果有限。</li></ul><p>本文提出了一种新的基于强化学习（RL）的离散提示优化方法RLPROMPT，该方法汇集了在不同任务和LMs上高效使用的广泛理想属性。与直接编辑离散标记不同，<strong>RLPROMPT训练一个生成所需提示的策略网络</strong>。离散提示优化实际上就是学习少量策略参数，我们将其设置为<strong>插入到冻结的紧凑模型中的MLP层</strong>，例如distilGPT-2。该公式还使我们能够<strong>使用现成的RL算法</strong>，这些算法<strong>可以使用任意奖励函数来学习策略，无论是使用可用数据（例如在少样本分类中）定义的还是在没有监督数据可用时（例如在可控文本生成中）使用的其他弱信号。</strong></p><ul><li>RLPROMPT是一种新的基于强化学习（RL）的离散提示优化方法，它训练一个生成所需提示的策略网络，并在不同任务和LMs上高效使用。</li><li>与直接编辑离散标记不同，RLPROMPT训练一个生成所需提示的策略网络，这使得离散提示的优化更加高效。</li></ul><p>然而，RL用于提示优化对学习效率提出了新的挑战：大型黑箱LM呈现出高度复杂的环境，给定提示（即操作）后，经过一系列复杂的转换（例如读取输入和推断输出）才能计算奖励。这使得奖励信号非常不稳定，很难学习。为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</p><ul><li>RL用于提示优化对学习效率提出了新的挑战，因为大型黑箱LM呈现出高度复杂的环境，奖励信号非常不稳定且难以学习。</li><li>为了克服这一困难，我们提出了两种简单但令人惊讶地有效的方法来稳定奖励并提高优化效率。</li></ul><p>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法（例如表1中描述的方法）上都有所改进，并且对不同的建模选择（例如分类中的verbalizers）都具有鲁棒性。所得到的离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。尤其是，尽管优化后的提示能够引起强大的任务性能，但倾向于是没有明确人类可理解含义的无意义文本，与最近的研究相呼应，即利用提示的LMs并不一定遵循人类语言模式。也许令人惊讶的是，学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</p><ul><li>在少样本分类和无监督文本风格转换的实验中，我们的方法在各种微调和提示方法上都有所改进，并且对不同的建模选择都具有鲁棒性。</li><li>离散提示还有助于进行丰富的解释和分析，以便深入了解LM提示的新见解。</li><li>学习了一个LM的那些无意义提示可以在其他LM中显著提高性能，这表明不同的预训练LM已经掌握了用于提示的共享结构。</li></ul><p><strong>我们将给出离散prompt optimization的RL公式(§2.1-2.2)。然后我们讨论我们的策略网络的设计(§2.3)。最后，我们描述了我们的奖励工程技术来提高RL培训(§2.4)。</strong></p><blockquote><p>第一篇文章的knowledge base 搭建 </p><p>学习了初步的在大模型上以及大模型之间的应用的微调策略以及增强式生成策略，并对比了相应方法的不同</p><p>第二篇文章 是对应与微调策略里边的强化学习提示网路模型，生成对应的网络提示</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Fine-Tuning-or-Retrieval-Comparing-Knowledge-Injection-in-LLMs？&quot;&gt;&lt;a href=&quot;#Fine-Tuning-or-Retrieval-Comparing-Knowledge-Injection-in</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>参与开源社区</title>
    <link href="https://www.fomal.cc/posts/48379df9.html"/>
    <id>https://www.fomal.cc/posts/48379df9.html</id>
    <published>2024-01-24T10:27:14.000Z</published>
    <updated>2024-03-01T09:58:27.037Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>尝试参与开源活动，整理一下自己的github主页</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;尝试参与开源活动，整理一下自己的github主页&lt;/p&gt;
&lt;/blockquote&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AI_Agent调研</title>
    <link href="https://www.fomal.cc/posts/7641cfbd.html"/>
    <id>https://www.fomal.cc/posts/7641cfbd.html</id>
    <published>2024-01-09T10:50:43.000Z</published>
    <updated>2024-01-18T14:49:36.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="24-1-9日WorkShop活动准备"><a href="#24-1-9日WorkShop活动准备" class="headerlink" title="24.1.9日WorkShop活动准备"></a>24.1.9日WorkShop活动准备</h1><blockquote><p>需要看的3D产品 链接是官网</p></blockquote><p><a href="https://www.tripo3d.ai/">Tripo AI (tripo3d.ai)</a></p><h2 id="活动前置须知"><a href="#活动前置须知" class="headerlink" title="活动前置须知"></a>活动前置须知</h2><h3 id="AGI领域"><a href="#AGI领域" class="headerlink" title="AGI领域"></a>AGI领域</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/607125649">知识科普：什么是AGI？ - 知乎 (zhihu.com)</a> </p></blockquote><p>AGI最近经常被提到，主要是因为ChatGPT的开发公司OpenAI将其写在了自己的企业使命中了，只要在介绍OpenAI的场景都会介绍到企业使命，而且在此时此刻，AGI作为OpenAI的企业使命中重要的关键词，也显得那样伟大。</p><p>AGI是<strong>Artificial General Intelligence</strong>的缩写，中文应该是“通用人工智能”，是指一种能够像人类一样思考、学习和执行多种任务的人工智能系统。我觉得这个词就是为了有别于“特定领域人工智能”而创造出来的。</p><blockquote><p>个人感觉有点像使用了AI赋予能力之后的智能机器人，当然这种智能机器人是主要应用在企业领域</p></blockquote><p>AGI可以视为一种更高级别的人工智能，是当前人工智能技术发展的一个重要方向和目标。但由于其在技术和理论方面的挑战，它仍然是一个较为遥远的目标。Chatgpt也还不能被称为AGI，至少它还不会画画。</p><blockquote><p>几个AGI的实例：</p><ol><li>王一快的写作猫【帮助生成写作内容和文档】 </li><li>AIGC的相关产品【AI 生成内容】 </li><li>最近刚刚开始用的微信智能日程提醒</li></ol></blockquote><h3 id="AGI的发展方向"><a href="#AGI的发展方向" class="headerlink" title="AGI的发展方向"></a>AGI的发展方向</h3><p><a href="https://m.huxiu.com/article/968593.html">AGI的大时代，我们可以做些什么？-虎嗅网 (huxiu.com)</a></p><ol><li><p><strong>通用的AGI平台</strong></p><ol><li>【商业公司】Google的Bard、百度的文心一言、Facebook的LLaMa、阿里、腾讯和字节的大语言模型</li><li>【个人或者社区可以自己搭建自己的AI平台】清华大学开源的GLM-130B 、Facebook开源出来的Alpaca<ol><li>可以从GitHub上pull代码，在自己的机器上快速地搭建起一个模型，提供服务</li><li><strong>大语言模型的平台已经白菜价</strong></li></ol></li><li>先从商业或者科研领域研发并推广，再到向企业和军方中广泛使用，最后在资金和风向的推导下为大众所熟知，再到个人领域的应用和不断完善【这其中估计要有3-5个月的延迟，类比于ChatGPT【自己12月份就已经开始用，但是到明年的3月底才推广】】</li><li><strong>现在的工程师投身通用AGI平台的建设，亲自去参与搭建平台，设计架构，做一位AI平台架构师和算法工程师，将毫无疑问成为最抢手的人才</strong></li></ol></li><li><p><strong>定制化专业垂直领域的AGI平台</strong></p><ol><li>金融行业、医疗行业、教育行业、汽车行业、制造行业等各种行业对应的，与专业知识结合的平台</li><li>赶紧在Github上pull GLM-130B，或者骆驼的代码，先学习起来</li><li>除了可以直接参与AGI平台的构建。其实也可以做平台之上的应用。<ol><li><strong>现有的软件通过AGI的能力进行提升，为用户或者客户提供增值的服务。</strong>最典型的就是微软的Office 365 集成GPT的能力以后，可以极大提高办公人员的工作效率；Adobe的Firefly集成AGI的能力以后，可以帮助设计人员快速设计更多的创意产品。</li><li><strong>基于GPT的插件体系**</strong>，在平台上根据自己的需要进行插件开发，完成自己细分领域的创新型产品。**</li></ol></li></ol></li><li><p><strong>软件服务商升级AI的能力</strong></p><ol><li><strong>在传统软件行业的各个垂直领域，实际上都存在着用AI去提升软件生产力的机会</strong><ol><li>英伟达的CEO黄仁勋在GTC大会上，就介绍了在芯片制造领域，利用AI的能力提升芯片制程和生产效率的案例。所以，在未来十几年，很可能AI能够极大地加速人类的科技和生产能力</li></ol></li><li>微软的<strong>Office 365</strong>。最近微软也发布了接入GPT-4模型能力的Office 365 Copilot。有了AI能力的加持，我们通过简单的自然语言对话，告诉Office我们的文档需要什么，Office Copilot就会自动帮助我们生成结果，例如：微软的office365 自动生成需要的文档和对应文档内容；【本质是利用的ai自动生成office所识别的VBA代码】</li><li>Adobe也演示了Firefly，利用AI的能力进行自动绘画功能，增强了Adobe软件的设计效率</li></ol></li><li><p><strong>基于通用大平台，开发细分领域的创新型应用</strong></p><ol><li>基于小数量的用户创新或者开发面向有特殊需求的专业化产品</li><li><strong>基于类似GPT-4这样的通用AGI平台，一定可以在各个细分行业涌现无数的创新应用出来</strong><ol><li>过去的各种插件开发 例如谷歌或者edge 的插件开发</li><li>苹果 或者 安卓的应用市场中各种应用开发</li><li>微信平台中的某些特殊功能的产品开发</li></ol></li></ol></li></ol><h2 id="活动内容"><a href="#活动内容" class="headerlink" title="活动内容"></a>活动内容</h2><blockquote><p>介绍产品</p><p>了解非3D从业者怎样上手玩转3D</p><p>Tripo产品和3D内容生成的未来发展趋势</p></blockquote><p><img src="../assets/image-20240109190152774.png" alt="image-20240109190152774"></p><h2 id="活动准则"><a href="#活动准则" class="headerlink" title="活动准则"></a>活动准则</h2><blockquote><p>准备自己的问题，询问AGI领域内的超集个体怎样应对</p><p>学习或者了解现在大家都在做的AGI产品或者用的技术</p></blockquote><p><img src="../assets/image-20240109190322701.png" alt="image-20240109190322701"></p><h1 id="开始主题"><a href="#开始主题" class="headerlink" title="开始主题"></a>开始主题</h1><p>Founder Park创业者社区    </p><ol><li>两个嘉宾分享 </li><li>大家有问题 评论区提问</li></ol><p>索尼  gta5  到gta6 的跨越</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;24-1-9日WorkShop活动准备&quot;&gt;&lt;a href=&quot;#24-1-9日WorkShop活动准备&quot; class=&quot;headerlink&quot; title=&quot;24.1.9日WorkShop活动准备&quot;&gt;&lt;/a&gt;24.1.9日WorkShop活动准备&lt;/h1&gt;&lt;bloc</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>图卷积算法应用落地——骨架数据行为识别系统</title>
    <link href="https://www.fomal.cc/posts/99a90e7a.html"/>
    <id>https://www.fomal.cc/posts/99a90e7a.html</id>
    <published>2024-01-09T06:39:26.000Z</published>
    <updated>2024-09-06T01:06:35.588Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图卷积算法应用——视频行为识别系统落地方案"><a href="#图卷积算法应用——视频行为识别系统落地方案" class="headerlink" title="图卷积算法应用——视频行为识别系统落地方案"></a>图卷积算法应用——视频行为识别系统落地方案</h1><h2 id="1-识别系统介绍"><a href="#1-识别系统介绍" class="headerlink" title="1.识别系统介绍"></a>1.识别系统介绍</h2><p>随着AI、web3、短视频的不断发展，现代生活对于图片的注意力开始转向视频数据，AI智能生成视频和各种主题短视频爆火等都是针对于视频数据，不断推动这视频内容的感知和创作。人作为视频中的最重要的一个主体，对视频内容的创作和生成有着至关重要的作用，所以对于视频中人体的动作进行识别变得越来越重要。近几年视频中人体动作识别领域已经成为热门领域，同时也有越来越多的产品开始应用该技术。</p><p>针对于视频中人体行为，最直观是将RGB视频直接作为视频数据或者间接裁剪为图片数据进行处理。利用CV中的算法将图片数据扩充上一个维度——时间帧维度，对于视频中人体的行为数据进行处理分析并最终输出一个类别概率数据。另外受限与视频数据处理量较大、视频数据噪音较多等因素，利用从视频中提取出的人体骨架信息能够更好的去除背景影响和其他噪音信息，重点关注与骨架动作和内容，提高识别准确率；因此针对于骨架数据的动作识别领域也不断兴起新技术，针对于从视频中提取出的骨架数据，不断提高对于骨架数据的模型性能和完善骨架数据动作识别技术生态有了至关重大的意义。</p><p>本篇文章针对于最近时间做的一项工作，针对于行为识别方面，尤其是视频和骨架数据方面，从视频采集到骨架数据库划定，再到算法设定和模型训练，完成整个动作视频实时处理和动作分类展示，完成了一项落地方案的实施，将内容进行输出和分享，希望跟大家讨论交流，不断改进方案的细节，精益求精。大家如果对于整套方案中所涉及到的某个技术或者实现过程感兴趣，或者能够提供相应的改进，欢迎在后台留言，与作者一起交流</p><h2 id="2-系统需求设定"><a href="#2-系统需求设定" class="headerlink" title="2.系统需求设定"></a>2.系统需求设定</h2><h3 id="1-骨架数据标准库"><a href="#1-骨架数据标准库" class="headerlink" title="1.骨架数据标准库"></a>1.骨架数据标准库</h3><p>对于骨架数据先进行动作识别，然后针对与完成的动作依据对应的动作标准进行评分，是非常常见的操作。根据日常的动作进行动作标准库选择，选择执行动作比较标准的数据，完成数据的采集、清洗、入库后，完成对应的数据标准库设定。</p><h3 id="2-数据存储格式指定"><a href="#2-数据存储格式指定" class="headerlink" title="2.数据存储格式指定"></a>2.数据存储格式指定</h3><p>对应于所处理的骨架数据，在完成数据采集的时候，应该设定好按照什么样的格式进行存储，同时将数据标准进行设定，方便后边其他操作应用数据的时候，使用相应的标准进行数据解析，方便下游任务的实施。</p><h3 id="学习与评价算法"><a href="#学习与评价算法" class="headerlink" title="学习与评价算法"></a>学习与评价算法</h3><p>针对于相应的视频或骨架数据，需要选择相应的学习算法和评价算法： </p><ol><li>学习算法完成对于数据集中视频的学习，并在推理阶段完成动作类别划定，完成动作分类</li><li>评价算法需要将测试的数据与标准库中的数据，进行动作比对，完成对于测试数据的分数划定</li></ol><h3 id="4-标准数据生产与模型训练流程"><a href="#4-标准数据生产与模型训练流程" class="headerlink" title="4.标准数据生产与模型训练流程"></a>4.标准数据生产与模型训练流程</h3><p>下边图片是设定的整个项目的流程和操作过程，从数据源的采集到标准库的设定，在宝库模型训练系统的完善。</p><p><img src="../assets/image-20240109153021375.png" alt="image-20240109153021375"></p><h2 id="3-数据和算法调研"><a href="#3-数据和算法调研" class="headerlink" title="3.数据和算法调研"></a>3.数据和算法调研</h2><h3 id="1-视频or骨架"><a href="#1-视频or骨架" class="headerlink" title="1.视频or骨架"></a>1.视频or骨架</h3><p>针对于视频数据和骨架数据进行深入的分析，将两种数据进行详细的优劣对比，发现有如下对比：</p><p><img src="../assets/image-20240109160548890.png" alt="image-20240109160548890"></p><p>视频相对更加的方便采集，可以直接从摄像头拍摄后完成数据获取；但是因为视频数据中包含内容比较丰富，同时数据量比较大，所以导致数据中会包含一些对于动作识别分析过程中不可用的噪音数据，对于后续的识别过程会带来一些麻烦。</p><p>相对于视频数据，骨架数据的优势便是主要针对的就是动作进行获取的骨架数据，能够完整的表示动作表示的同时不受到噪音数据的影响，同时能够很好的减少像光照、视频背景等信息的干扰。但对于骨架数据的获取可能会需要一些相应的设备，并需要提前做好数据转换。</p><p><img src="../assets/image-20240109160022176.png" alt="image-20240109160022176"></p><blockquote><p>视频数据骨架数据转换图</p></blockquote><h3 id="2-算法分析"><a href="#2-算法分析" class="headerlink" title="2.算法分析"></a>2.算法分析</h3><p>目前在基于视频动作识别和基于骨架数据动作识别方面主流方法准确率都能达到80%以上，主要基于深度学习方法包括但不限于（1）CNN，（2）GNN,（3）Transformer等主流方法</p><p><img src="../assets/image-20240109161059481.png" alt="image-20240109161059481"></p><p>​                                                                                视频动作识别算法发展脉络图</p><p><img src="../assets/image-20240109161202559.png" alt="image-20240109161202559"></p><p>​                                                                                骨架动作识别算法发展脉络图</p><h3 id="3-数据集选定"><a href="#3-数据集选定" class="headerlink" title="3.数据集选定"></a>3.数据集选定</h3><p>针对于所要完成的整套方案，需要从视频中完成数据采集后，在处理成骨架数据。相较而言，骨骼数据相比于RGB视频数据不易受背景影响，避免遮挡、光照等噪音影响搜集到数据集包括太极拳数据、Ntu数据集及相关研究。这里我们直接选择了对应的NTU60骨架数据集来作为我们的预定义数据集，并在后续的实现方案中完成所需要的全部操作。</p><p>在这里我们也对于其他的一些数据集和开源算法以及可以直接进行实操的项目进行了实操和测试，下图是我们进行过测试的项目罗列，如果对图中兴趣感兴趣，欢迎在后台留言和交流</p><p><img src="../assets/image-20240109161839748.png" alt="image-20240109161839748"></p><p>​                                                                                方案调研技术罗列</p><h2 id="4-NTU60标准库构建"><a href="#4-NTU60标准库构建" class="headerlink" title="4.NTU60标准库构建"></a>4.NTU60标准库构建</h2><p>在这里，我们会对与我们使用到的标准库进行介绍，并完成我们方案所需要的标准库的数据格式解析：</p><p><img src="../assets/image-20240109162158671.png" alt="image-20240109162158671"></p><h3 id="1-骨架数据格式设定"><a href="#1-骨架数据格式设定" class="headerlink" title="1. 骨架数据格式设定"></a>1. 骨架数据格式设定</h3><p>NTU-RGB+D60 数据集采集到的关节点为25个。数据集内包含若干“.<em>skeleton</em>”文件，每个文件代表一个样本。</p><p>文件命名方式如下：</p><p><img src="../assets/image-20240109162328756.png" alt="image-20240109162328756"></p><p>命名格式和相应实例</p><p><img src="../assets/image-20240109162353641.png" alt="image-20240109162353641"></p><p>这里为相机和不同的人物设定id 是为了下边在执行相同动作时候，能够划定不同的变量，保证数据集能够更加全面的展示相应的数据，并能够帮助模型学习同一个动作的不同表现格式。在模型完成动作识别后，会对于动作类别进行划定，输出的动作分类结果就是A后边相应的数字借以表示动作所属的类别</p><h3 id="2-骨架数据获取标准"><a href="#2-骨架数据获取标准" class="headerlink" title="2.骨架数据获取标准"></a>2.骨架数据获取标准</h3><p>这里介绍一下根据不同的相机角度拍摄结果来完成动作跨视角的获取和解析，以及根据不同人物id完成相应动作来区别跨主题分类</p><p><img src="../assets/image-20240109162916550.png" alt="image-20240109162916550"></p><p>​                                                                                                        <strong>原始数据图</strong></p><h4 id="1-跨视角"><a href="#1-跨视角" class="headerlink" title="1.跨视角"></a>1.跨视角</h4><p>将所有的3个摄像头获取的数据，根据摄像头的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同视角的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162951421.png" alt="image-20240109162951421"></p><p>​                                                                                                    处理后的可解析跨视角数据</p><h4 id="2-跨主题"><a href="#2-跨主题" class="headerlink" title="2.跨主题"></a>2.跨主题</h4><p>将所有的不同id的人物执行的相同动作获取的数据，根据人物的id 来进行区别后，利用数据处理程序将原始数据来进行处理，获取不同主题的可以直接解析的骨架数据</p><p><img src="../assets/image-20240109162957586.png" alt="image-20240109162957586"></p><p>​                                                                                                    <strong>处理后的可解析跨视角数据</strong></p><h3 id="3-动作类别介绍"><a href="#3-动作类别介绍" class="headerlink" title="3.动作类别介绍"></a>3.动作类别介绍</h3><p>  NTU-RGB+D60数据集包含60个种类的动作。数据集由Kinect v2传感器采集得到，使用三个不同角度的摄像机，采集的数据形式包括3D骨架信息、RGB视频等。</p><p><img src="../assets/image-20240109163500174.png" alt="image-20240109163500174"></p><h3 id="4-骨架数据内容标准"><a href="#4-骨架数据内容标准" class="headerlink" title="4.骨架数据内容标准"></a>4.骨架数据内容标准</h3><p>这里解析一个.skeleton数据内容:一个动作包含104帧 ,25是关节个数,紧接着的25行12列表示,25个关节点的12类别信息,后边还有类似的其他103组表征103帧的关节特征数据</p><p><img src="../assets/image-20240109163607368.png" alt="image-20240109163607368"></p><p>​                                                                                                 <strong>.skeleton数据格式解析</strong></p><p>其中的‘x’, ‘y’, ‘z’, 构建成骨架关节点特征信息，用来表示动作发生过程中的空间坐标信息，构建X∈[25,3] 维特征向量标识一个动作的特征信息，点线数据结构表征空间信息。</p><p><img src="../assets/image-20240109163944894.png" alt="image-20240109163944894"></p><p>​                                                                                           <strong>X Y Z三维数据格式解析</strong></p><h3 id="5-骨架数据构图"><a href="#5-骨架数据构图" class="headerlink" title="5.骨架数据构图"></a>5.骨架数据构图</h3><p>每个骨架都有25个骨骼点,使用自连接节点构建成数值为0/1 的[25，25]维邻接矩阵,数值为1表示节点之间有邻接关系,数值为0 表示节点间没有邻接关系</p><p>25个节点自身与自身之间具有自连接关系,利用25个节点选择不同的构图方式构建成为人体骨架模型</p><p><img src="../assets/image-20240109163803430.png" alt="image-20240109163803430"></p><h3 id="6-骨架数据可视化"><a href="#6-骨架数据可视化" class="headerlink" title="6.骨架数据可视化"></a>6.骨架数据可视化</h3><p>下边是一些骨架执行动作的2d可视化</p><p><img src="../assets/image-20240109164258750.png" alt="image-20240109164258750"></p><p>​                                                                                                              <strong>抢劫</strong></p><p><img src="../assets/image-20240109164438223.png" alt="image-20240109164438223"></p><p>​                                                                                                              <strong>坐下</strong></p><p><img src="../assets/image-20240109164332785.png" alt="image-20240109164332785"></p><p>​                                                                                                             <strong>扔东西</strong></p><p>​                                                                                            </p><p><img src="../assets/image-20240109164455067.png" alt="image-20240109164455067"></p><p>​                                                                                                             <strong>单手捡东西</strong></p><h2 id="5-落地方案展示"><a href="#5-落地方案展示" class="headerlink" title="5.落地方案展示"></a>5.落地方案展示</h2><h3 id="1-标准数据生产与模型训练流程"><a href="#1-标准数据生产与模型训练流程" class="headerlink" title="1.标准数据生产与模型训练流程"></a>1.标准数据生产与模型训练流程</h3><p>下边图片展示的是我们本套方案所计划应用的全部技术，以及已经完成的相应展示，在后边视频中已经展示了相应的实现过程</p><p><img src="../assets/image-20240109165738475.png" alt="image-20240109165738475"></p><h3 id="2-动作学习方案展示"><a href="#2-动作学习方案展示" class="headerlink" title="2.动作学习方案展示"></a>2.动作学习方案展示</h3><blockquote><p>视频插入</p></blockquote><p>视频中展示的是我们利用我们预训练后的模型学习完成整个NTU60动作后，对于骨架数据——扔东西这个动作先进行可视化，然后将测试数据输入给模型后，模型对于数据进行识别后将识别到的动作进行可视化并将测试结果写回到本地文件中并将模型的准确率进行计算并同步到数据文件中</p><h3 id="3-评价算法"><a href="#3-评价算法" class="headerlink" title="3.评价算法"></a>3.评价算法</h3><h4 id="1评价算法的模型图和实现过程"><a href="#1评价算法的模型图和实现过程" class="headerlink" title="1评价算法的模型图和实现过程"></a>1评价算法的模型图和实现过程</h4><ol><li>将两个样本分别送入到两个相同的深度神经网络中，两个特征提取网络的结构和参数相同;</li></ol><ol><li>使用损失函数计算两个输出结果的距离差，误差越小相似度越高;</li></ol><ol><li>损失函数根据输入数据和任务的不同可以使用对比损失、余弦损失、交叉熵损失等;</li></ol><p><img src="../assets/image-20240109171444116.png" alt="image-20240109171444116"></p><p><strong>评价算法的模型图</strong></p><h4 id="2-评价算法改进"><a href="#2-评价算法改进" class="headerlink" title="2.评价算法改进"></a>2.评价算法改进</h4><p>我们对于评价算法重新划定评价等级，设定了下边的改进计划</p><p><img src="../assets/image-20240109171817932.png" alt="image-20240109171817932"></p><p>​                                                                                                <strong>改进方案设定</strong></p><h4 id="3-方案前端界面"><a href="#3-方案前端界面" class="headerlink" title="3.方案前端界面"></a>3.方案前端界面</h4><p>方案设定的前端展示如下，可以对于某个视频中动作进行骨架展示并将对应的数据骨架评分进行展示</p><p><img src="../assets/image-20240109171929541.png" alt="image-20240109171929541"></p><p>​                                                                <strong>前端展示图</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图卷积算法应用——视频行为识别系统落地方案&quot;&gt;&lt;a href=&quot;#图卷积算法应用——视频行为识别系统落地方案&quot; class=&quot;headerlink&quot; title=&quot;图卷积算法应用——视频行为识别系统落地方案&quot;&gt;&lt;/a&gt;图卷积算法应用——视频行为识别系统落地方案&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解7骨架序列的目标表示自监督学习框架</title>
    <link href="https://www.fomal.cc/posts/f1740f63.html"/>
    <id>https://www.fomal.cc/posts/f1740f63.html</id>
    <published>2024-01-06T03:31:36.000Z</published>
    <updated>2024-01-06T03:52:23.373Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Skeleton2vec-A-Self-supervised-Learning-Framework-with-Contextualized-Target-Representations-for-Skeleton-Sequence骨架序列的目标表示自监督学习框架"><a href="#Skeleton2vec-A-Self-supervised-Learning-Framework-with-Contextualized-Target-Representations-for-Skeleton-Sequence骨架序列的目标表示自监督学习框架" class="headerlink" title="Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence骨架序列的目标表示自监督学习框架"></a>Skeleton2vec: A Self-supervised Learning Framework with Contextualized Target Representations for Skeleton Sequence骨架序列的目标表示自监督学习框架</h1><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下Abstract段落</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Abstract:<br>Self-supervised pre-training paradigms have been exten-<br>sively explored in the field of skeleton-based action recog-<br>nition. In particular, methods based on masked predic-<br>tion have pushed the performance of pre-training to a new<br>height. However, these methods take low-level features,<br>such as raw joint coordinates or temporal motion, as pre-<br>diction targets for the masked regions, which is subopti-<br>mal. In this paper, we show that using high-level contex-<br>tualized features as prediction targets can achieve supe-<br>rior performance. Specifically, we propose Skeleton2vec,<br>a simple and efficient self-supervised 3D action represen-<br>tation learning framework, which utilizes a transformer-<br>based teacher encoder taking unmasked training samples<br>as input to create latent contextualized representations as<br>prediction targets. Benefiting from the self-attention mech-<br>anism, the latent representations generated by the teacher<br>encoder can incorporate the global context of the entire<br>training samples, leading to a richer training task. Ad-<br>ditionally, considering the high temporal correlations in<br>skeleton sequences, we propose a motion-aware tube mask-<br>ing strategy which divides the skeleton sequence into sev-<br>eral tubes and performs persistent masking within each<br>tube based on motion priors, thus forcing the model to<br>build long-range spatio-temporal connections and focus on<br>action-semantic richer regions. Extensive experiments on<br>NTU-60, NTU-120, and PKU-MMD datasets demonstrate<br>that our proposed Skeleton2vec outperforms previous meth-<br>ods and achieves state-of-the-art results. The source code<br>ofSkeleton2vec is available at <a href="https://github.com/Ruizhuo-Xu/Skeleton2vec">https://github.com/Ruizhuo-Xu/Skeleton2vec</a>.</p><p>帮我用只展示中文的形式先翻译并根据输入内容总结一下1. Introduction段落</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Introduction:<br>Human action recognition has significant applications in the<br>real world, such as security, human-robot interaction, and<br>virtual reality. The development of depth sensors and ad-<br>vancements in pose estimation algorithms [4, 12, 41] have</p><p>propelled skeleton-based action recognition into a popular<br>research topic, owing to its computational efficiency, back-<br>ground robustness, and privacy preservation. A series of<br>fully-supervised skeleton-based human action recognition<br>methods have been developed using CNNs [10, 19], RNNs<br>[24, 46], and GCNs [5, 43]. Despite their promising per-<br>formance, these methods rely on large amounts of manu-<br>ally annotated data, which is expensive, labor-intensive, and<br>time-consuming to obtain. This circumstance motivates us<br>to explore self-supervised representation learning for 3D ac-<br>tions.<br>Earlier works [21, 29, 33, 47] have employed various<br>pretext tasks, such as motion prediction, jigsaw puzzle<br>recognition, and masked reconstruction, to learn 3D ac-<br>tion representations. Recently, contrastive learning meth-<br>ods [15, 22, 28, 30] have gained prominence. However,<br>these methods often require carefully designed data aug-<br>mentations and tend to encourage the encoder to learn more</p><p>global representations, thereby neglecting local spatiotem-<br>poral information. With the rise of transformer models [37],<br>self-supervised pre-training methods based on masked pre-<br>diction tasks have become mainstream in visual represen-<br>tation learning [15, 22, 28, 30]. Works like SkeletonMAE<br>[39, 42] and MAMP [27] have attempted to transfer MAE<br>[17] methods to the field of 3D action representation learn-<br>ing, achieving promising results. However, these MAE-like<br>methods inefficiently utilize model capacity by focusing on<br>low-level high-frequency details with raw joint coordinates<br>or temporal motion as learning targets, which is subopti-<br>mal for modeling high-level spatiotemporal structures. We<br>believe that using higher-level prediction targets will guide<br>the model to learn better representations and improve pre-<br>training performance.<br>Motivated by this idea, we propose Skeleton2vec, a sim-<br>ple and efficient self-supervised framework for 3D action<br>representation learning. Addressing the limitations of ex-<br>isting MAE-like methods, as illustrated in Fig. 1, Skele-<br>ton2vec leverages contextualized prediction targets. Fol-<br>lowing the work of data2vec [1, 2], we employ a teacher<br>encoder that takes unmasked training samples to generate<br>latent contextualized representations as targets. We then<br>use a student encoder, taking a masked version of the sam-<br>ple as input, combined with an asymmetric decoder to pre-<br>dict data representations at the masked positions. The entire<br>model is based on the vanilla transformer architecture. The<br>self-attention mechanism ensures that the constructed tar-<br>gets are contextualized, incorporating information from the<br>entire sample, making them richer than isolated targets (e.g.<br>raw joint coordinates) or targets based on local context (e.g.<br>temporal motion).<br>Additionally, considering the strong spatiotemporal cor-<br>relations in 3D skeleton sequences, we propose a motion-<br>aware tube masking strategy. Initially, we divide the in-<br>put skeleton sequence along the temporal axis into multiple<br>tubes, where frames within each tube share a masking map<br>to avoid information leakage from neighboring frames. This<br>forces the model to extract information from distant time<br>steps for better prediction. We then guide the sampling of<br>masked joints based on the spatial motion intensity of body<br>joints within each tube. Joints with higher motion inten-<br>sity will be masked with higher probability, allowing the<br>model to focus more on spatiotemporal regions with rich ac-<br>tion semantics. Compared to random masking, our method<br>better utilizes the spatiotemporal characteristics and motion<br>priors of 3D skeleton sequences, effectively improving pre-<br>training performance.<br>In summary, the main contributions of this work are<br>three-fold:<br>• We propose the Skeleton2vec framework, which uses<br>contextualized representations from a teacher encoder as<br>prediction targets, enabling the learned representations to<br>have stronger semantic associations.<br>• We introduce a motion-aware tube masking strategy that<br>performs persistent masking of joints within tubes based<br>on spatial motion intensity, forcing the model to build bet-<br>ter long-range spatiotemporal connections and focus on<br>more semantic-rich regions.<br>• We validate the effectiveness of our method on three<br>large-scale 3D skeleton-based action recognition datasets<br>and achieve state-of-the-art results.</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><h3 id="2-1-Self-supervised-Skeleton-based-Action-Recognition"><a href="#2-1-Self-supervised-Skeleton-based-Action-Recognition" class="headerlink" title="2.1. Self-supervised Skeleton-based Action Recognition"></a>2.1. Self-supervised Skeleton-based Action Recognition</h3><p>Previous studies [21, 33, 47] on self-supervised representa-<br>tion learning for skeleton-based action recognition utilize<br>various pretext tasks to capture motion context. For in-<br>stance, LongTGAN [47] leverages sequence reconstruction<br>to learn 3D action representations. P&amp;C [33] employs a<br>weak decoder to enhance representation learning. MS2L<br>[21] employs motion prediction and jigsaw puzzle tasks.<br>Yang et al. [44] introduce a skeleton cloud colorization<br>task. Contrastive learning methods have gained prominence<br>in 3D action representation learning [14–16, 22, 28, 30].<br>AS-CAL [30] and SkeletonCLR [20] utilize momentum<br>encoder and propose various data augmentation strategies.<br>AimCLR [15] introduces extreme augmentations. ActCLR<br>[22] performs adaptive action modeling on different body<br>parts. Despite their remarkable results, contrastive learning<br>methods often overlook local spatio-temporal information,<br>a crucial aspect for 3D action modeling.<br>The surge in popularity of transformers has led to the<br>mainstream adoption of self-supervised pretraining based<br>on masked visual modeling for visual representation learn-<br>ing [3, 17]. SkeletonMAE [39] and MAMP [27] apply the<br>Masked Autoencoder (MAE) approach to 3D action repre-<br>sentation learning. SkeletonMAE employs a skeleton-based<br>encoder-decoder transformer for spatial coordinate recon-<br>struction, while MAMP introduces Masked Motion Predic-<br>tion to explicitly model temporal motion. In this study, we<br>demonstrate that utilizing higher-level contextualized rep-<br>resentations as prediction targets for masked regions yields<br>superior performance compared to directly predicting raw<br>joint coordinates or temporal motion.</p><h3 id="2-2-Masked-Image-Modeling"><a href="#2-2-Masked-Image-Modeling" class="headerlink" title="2.2. Masked Image Modeling"></a>2.2. Masked Image Modeling</h3><p>BEiT [3] pioneered masked image modeling (MIM) for<br>self-supervised pretraining of visual models, aiming to re-<br>cover discrete visual tokens from masked patches. Subse-<br>quently, various prediction targets for MIM have been ex-<br>plored. MAE [17] and SimMIM [40] treat MIM as a de-<br>noising self-reconstruction task, utilizing raw pixels as the<br>prediction target. MaskFeat [38] replaces pixels with HOG</p><p>descriptors to enable more efficient training and achieve<br>superior results. PeCo [8] introduces a perceptual loss<br>during dVAE training to generate semantically richer dis-<br>crete visual tokens, surpassing BEiT. These works demon-<br>strate superior performance by utilizing higher-level and se-<br>mantically richer prediction targets in MIM. To further en-<br>hance performance, data2vec [1, 2] employs self-distillation<br>to leverage latent target representations from the teacher<br>model output at masked positions. Compared to isolated<br>targets like visual tokens or pixels, these contextualized rep-<br>resentations encompass relevant features from the entire im-<br>age, enabling improved performance.<br>In this research, we introduce the data2vec framework<br>into self-supervised pretraining of skeleton sequences, uti-<br>lizing latent contextualized target representations from the<br>teacher model to guide the student model in learning more<br>effective 3D action representations.</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3.Method"></a>3.Method</h2><h3 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1. Overview"></a>3.1. Overview</h3><p>3.1. Overview<br>The overall framework of Skeleton2vec is shown in Fig. 2.<br>It takes a skeleton sequence I ∈ RTs×V×Cs<br>as input, where<br>Ts is the the number of frames, V is the number of joints,<br>and Cs is the the coordinates of joints. Similar to most vi-<br>sual transformers [9], the skeleton sequence is first divided<br>into fixed-size patches and then linearly transformed into<br>patch embedding E ∈ RTe×V×Ce<br>. After that, we employ<br>the motion-aware tube masking strategy to guide the mask-<br>ing of joints. The teacher model constructs the full con-<br>textualized prediction targets using unmasked training sam-<br>ples, while the student model receives the masked version<br>of the samples and predicts corresponding representations<br>at the masked positions.<br>As our student model, we adopt an asymmetric encoder-<br>decoder architecture, where the encoder operates solely<br>on non-masked tokens. The lightweight decoder inserts<br>masked tokens into the latent representations outputted by<br>the encoder, forming a full set for predicting the targets. The<br>teacher encoder shares the same model structure as the stu-<br>dent. After accomplishing the aforementioned pre-training<br>task, the teacher encoder is retained for downstream task<br>fine-tuning.</p><h3 id="3-2-Model-Architecture"><a href="#3-2-Model-Architecture" class="headerlink" title="3.2. Model Architecture"></a>3.2. Model Architecture</h3><p>3.2. Model Architecture<br>Encoder: Following MAMP [27], we first divide the raw<br>skeleton sequence I ∈ RTs×V×Cs<br>into non-overlapping<br>segments I′ ∈ RTe×V×(l·Cs), where Te = Ts/l and l is<br>the length of each segment. A trainable linear projection is<br>then applied to each joint to obtain the embedding:</p><p>where Ce represents the dimension of the embedding. Tem-<br>poral positional embedding Et ∈ RTe×1×Ce<br>and spatial po-<br>sitional embedding Es ∈ R1×V×Ce<br>are then added to the<br>joint embedding to yield the final input:</p><p>For the teacher encoder, the entire set is flattened as input<br>ET ∈ RNT×Ce<br>, where NT = Te × V represents the total<br>number of tokens in the skeleton sequence. For the student<br>encoder, most tokens are masked, and only the unmasked<br>tokens are utilized as input, flattened as ES ∈ RNS×Ce<br>,<br>where NS = Te × V × (1 − m) denotes the number of<br>visible tokens, and m is the masking ratio. Subsequently,<br>Le layers of vanilla transformer blocks are applied to extract<br>latent representations. Each block comprises a multi-head<br>self-attention (MSA) module and a feed-forward network<br>(FFN) module. Residual connections are employed within<br>each module, followed by layer normalization (LN).<br>Decoder: The decoder input D ∈ RTe×V×Ce<br>contains the<br>full set of tokens, including the latent representations of vis-<br>ible encoded tokens ZS<br>e and the inserted masked tokens.<br>Each masked token is represented by a shared learnable<br>vector EM ∈ RCe<br>, indicating missing information to be<br>predicted at that position. Similar to the encoder, spatial<br>positional embedding E′<br>s and temporal positional embed-<br>ding E′<br>t are added to all tokens to assist masked tokens in<br>locating their positions. The decoder employs an additional<br>Ld layers of transformer blocks for masked prediction.</p><h3 id="3-3-Contextualized-Target-Prediction"><a href="#3-3-Contextualized-Target-Prediction" class="headerlink" title="3.3. Contextualized Target Prediction"></a>3.3. Contextualized Target Prediction</h3><p>Rather than relying on isolated raw joints or temporal mo-<br>tion with limited local context, we employ a transformer-<br>based teacher encoder to construct globally contextualized<br>prediction targets, thereby introducing a diverse training<br>task.<br>Contextualized Target Representations: We extract fea-<br>tures from the output of each FFN block in every layer of<br>the teacher encoder and average them to form our training<br>targets. Following data2vec 2.0 [2], the features from each<br>layer are normalized with instance normalization [36] be-<br>fore averaging. Finally, the averaged features are normal-<br>ized by layer normalization to serve as the prediction tar-<br>gets. Normalizing the targets helps prevent the model from<br>collapsing to a trivial solution, and also prevents any sin-<br>gle layer’s features from dominating. The generation of the<br>target representations can be formulated as:</p><p>where IN and LN refer to instance normalization and layer<br>normalization, respectively. ZT<br>l denotes the output of the<br>FFN block in the lth layer of the teacher encoder.</p><p>Target Prediction: Given the output Hd of the student<br>decoder, we employ an additional linear prediction head<br>to regress the contextualized target representations of the<br>teacher:</p><p>Finally, we adopt L2 loss as our learning objective, cal-<br>culating loss only for the masked positions:</p><p>whereM denotes the set of masked positions.<br>Teacher Parameterization: The student model weights θ<br>are updated through backpropagation on the loss gradients.<br>The teacher model weights ∆ are initialized to be the same<br>as the student weights and parameterized during training by<br>taking an exponentially moving average (EMA) of the stu-<br>dent weights:<br>∆ ← τ∆+ (1 − τ)θ, (6)<br>where τ is a hyperparameter controlling the update fre-<br>quency of the teacher weights using a linearly increasing<br>schedule, gradually increasing from an initial value τ0 to 1<br>throughout training.</p><h3 id="3-4-Motion-Aware-Tube-Masking"><a href="#3-4-Motion-Aware-Tube-Masking" class="headerlink" title="3.4. Motion-Aware Tube Masking"></a>3.4. Motion-Aware Tube Masking</h3><p>3.4. Motion-Aware Tube Masking<br>We propose the motion-aware tube masking strategy to ad-<br>dress the issue of high spatiotemporal correlations in skele-<br>ton sequences.<br>Tube Division: The tube masking strategy, initially intro-<br>duced by VideoMAE [35], considers the entire video se-<br>quence along the temporal axis as a single tube, sharing the<br>same masking map across different frames. This mitigates<br>the information leakage issue between adjacent frames. Al-<br>though the skeleton sequence is derived from the video, di-<br>rectly applying this single-tube masking strategy to skeleton<br>data is suboptimal due to the inherent structural differences.<br>In video data, the basic units for masking are image patches<br>in each frame. Due to scene motion or camera viewpoint<br>changes, a masked body part like the hand in the first frame<br>may find its correspondence in unmasked regions in later<br>frames far apart, which facilitates long-range dependency<br>modeling. In contrast, the basic units for masking in skele-<br>ton sequences are the joints in each skeleton frame, where<br>the same-order joints have explicit correspondence across<br>frames. As a result, a body part masked in the first skeleton<br>frame will remain masked in all frames, causing a complete<br>loss of information for that part, which makes the masked<br>prediction task overly difficult and harms the model’s learn-<br>ing capability. To address this, as illustrated in Fig. 2a, we<br>empirically divide the skeleton sequence along the time axis<br>into multiple tubes instead of one tube. Each tube shares the<br>same masking map to force the model to extract informa-<br>tion from farther time steps, while different tubes use differ-<br>ent masking maps to avoid joints being masked throughout.<br>The tube division can be represented as:<br>E′ = Reshape(E) ∈ RN×α×V×Ce  ,(7)<br>where α is tube length and N = Te α is number of tubes.<br>Motion-Aware Sampling: Regions with larger motion in-<br>tensity intuitively contain richer semantic information about<br>actions. Therefore, we utilize the spatial motion intensity of<br>each human body joint within a tube as empirical guidance<br>to generate the masking map.<br>Specifically, we first extract the corresponding motion</p><p>sequence M ∈ RTs×V×Cs<br>from the input skeleton se-<br>quence I ∈ RTs×V×Cs<br>by calculating temporal differ-<br>ences of corresponding joint coordinates between adjacent<br>frames:</p><p>Similar to joint embedding in the encoder, we reshape<br>M into non-overlapping segments M′ ∈ RTe×V×(l·Cs) to<br>match the shape of input sequence I′. We then calculate the<br>motion intensity of each joint within a segment as:</p><p>Afterwards, we compute the spatial motion intensity of<br>each body joint within a tube, normalizing it along the spa-<br>tial dimension:</p><p>Finally, we utilize the normalized spatial motion inten-<br>sity to generate a unique masking map for each tube:</p><p>where η is random noise drawn from a uniform distribu-<br>tion between 0 and 1, β is a hyperparameter controlling the<br>influence of spatial motion intensity on sampling, Mi is<br>the masking map for ith tube, K = V × (1 − m) is the<br>number of joints to be masked, and m is the masking ra-<br>tio. By customizing motion-aware masking maps for each<br>tube, the model is encouraged to focus more on seman-<br>tically richer regions, leading to improved spatiotemporal<br>representations.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Skeleton2vec-A-Self-supervised-Learning-Framework-with-Contextualized-Target-Representations-for-Skeleton-Sequence骨架序列的目标表示自监督学习框架&quot;&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解6利用三维热图量改善小规模人体动作识别性能</title>
    <link href="https://www.fomal.cc/posts/ba38a36f.html"/>
    <id>https://www.fomal.cc/posts/ba38a36f.html</id>
    <published>2024-01-03T09:00:35.000Z</published>
    <updated>2024-01-06T04:11:11.170Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Improving-Small-Scale-Human-Action-Recognition-Performance-Using-a-3D-Heatmap-Volume利用三维热图量改善小规模人体动作识别性能"><a href="#Improving-Small-Scale-Human-Action-Recognition-Performance-Using-a-3D-Heatmap-Volume利用三维热图量改善小规模人体动作识别性能" class="headerlink" title="Improving Small-Scale Human Action Recognition Performance Using a 3D Heatmap Volume利用三维热图量改善小规模人体动作识别性能"></a>Improving Small-Scale Human Action Recognition Performance Using a 3D Heatmap Volume利用三维热图量改善小规模人体动作识别性能</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>摘要：近年来，基于骨架的人体动作识别引起了广泛的研究关注，提出的识别或分割方法通常在大规模粗粒度动作数据集上进行验证。然而，在使用深度学习方法识别小规模细粒度人体动作方面仍存在研究不足，而这在实际应用中具有更大的实际意义。为填补这一空白，我们提出了一种基于热图伪视频的新方法，并采用适用于所有模态数据集的统一通用模型。利用人体测量学运动学作为先验信息，通过一个专门预训练的模型提取数据集之间的共同人体运动特征。为了克服关节不匹配问题，我们将人体骨架分为五个部分，这是一种简单而有效的信息共享技术。我们的方法在两个数据集上进行了评估，包括公共的护理活动数据集和我们自建的太极动作数据集。通过线性评估协议和微调评估，结果表明我们的预训练模型有效地捕捉了人体动作之间的共同运动特征，并在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。值得注意的是，我们的模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><p>近年来，骨架基础的人体动作识别备受关注，但主要集中在大规模的粗粒度动作数据集上。</p><p>尚缺乏关于使用深度学习方法识别小规模细粒度人体动作的研究，而这在实际应用中更为实际。</p><p>提出了一种新方法，基于热图伪视频和适用于所有模态数据集的统一通用模型。</p><p>利用人体测量学运动学作为先验信息，通过专门的预训练模型提取数据集之间的共同人体运动特征。</p><p>为了解决关节不匹配问题，将人体骨架分为五个部分，采用简单而有效的信息共享技术。</p><p>在两个数据集上进行评估，包括公共的护理活动数据集和自建的太极动作数据集。</p><p>通过线性评估协议和微调评估，结果表明预训练模型有效捕捉了人体动作之间的共同运动特征。</p><p>在所有训练设置下实现了稳定而准确的精度，同时减轻了网络过拟合。</p><p>模型在融合通道维度上的关节和肢体模态特征时，在识别准确度方面优于最先进的模型。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><strong>引言</strong></p><p>人体动作识别领域经历了广泛的研究，导致建立了几个包含各种数据模态的人体动作数据集，包括RGB视频、深度、雷达[1]、红外辐射（IR）和骨架坐标[2]。目前的研究主要集中在利用深度和复杂的神经网络来取得令人印象深刻的结果，这得益于构建了大规模的人体动作数据集，如包含13,000个样本的UCF-101 [3]和包含392,000个样本的Kinetics-600 [4]。然而，这些广泛的多模态人体动作数据集的潜力不应仅限于验证这些算法的性能。相反，必须广泛探索它们的应用价值，例如用于辅助小规模细粒度数据集的识别。</p><p>目前的人体动作数据集通常侧重于粗粒度的日常活动，显示出明显的类间差异，并且仅涉及某些身体部位的主要运动。相反，对特定细粒度动作的研究具有显著的实际价值，因为它们可以被视为粗粒度环境中的一般动作。例如，舞蹈可以包括芭蕾舞、萨尔萨舞和探戈等。一个高效的模型可以识别微妙的差异，并在互动任务中提供适当的反馈，同时在动作评估场景中评估运动表现。然而，由于取样条件有限或取样成本高昂，这类数据集中的样本数量明显少于主流数据集。因此，使用深度学习方法对具有有限训练样本的细粒度动作进行识别仍然是一项具有挑战性且值得关注的努力，这构成了本文的主要贡献。</p><p>在创建这些数据集之后，已经提出了许多与人体动作识别任务相关的方法。早期的方法依赖于手工制作的特征来区分不同的动作类别。然而，随着深度学习的出现，各种方法，如2D卷积神经网络（2D-CNN）、长短时记忆（LSTM）、3D卷积神经网络（3D-CNN）、图卷积网络（GCN）和Transformer，被用于识别样本，并且分类准确性稳步提高。目前，在基于骨架的人体动作识别中，研究人员将GCN或Transformer视为必不可少的工具，而在视频人体动作识别中，3D-CNN和TimeSformer [5]很受欢迎。</p><p>上述方法在特定样本模态方面都有优势和局限性。视频模态样本无法通过GCN的图节点表示有效地，而骨架数据不能直接用于CNN的输入。这种差异在视频和骨架动作之间形成了自然的边界。在骨架模态动作样本的情况下，局部关节可能不存在，并且可能发生多人互动动作。之前使用基于GCN的方法通过在剩余关节上执行线性插值来完成骨架结构 [6]，并将每个执行者的输出的平均值作为动作特征 [7]来解决这些情况。然而，补充的骨架结构可能包含不可靠的信息并引入干扰信息。此外，多人特征的平均化忽略了字符之间的互动信息。如果仅使用小规模的细粒度数据集来从头开始训练深度神经网络，网络容易过拟合。先前的研究在图像处理中采用了自适应锐度感知最小化[8]（ASAM）或使用ImageNet数据集 [9]中的预训练模型来解决这些挑战。基于预训练策略，我们的研究致力于利用公共动作数据集对网络进行预训练，并解决不同数据集之间的关节和模态不匹配问题。</p><p>在使用深度神经网络进行小规模人体动作识别时，一些问题，如缺失的骨架数据、互动样本和过拟合，尚未解决。我们提出了一个统一且通用的模型SSCLS用于小规模动作分类。图1显示了我们识别框架的网络结构。与主流基于骨架的人体动作识别任务不同，我们引入了3D热图体，也称为基于热图的伪视频，作为网络输入，而不是视频图像或骨架序列。来自两种不同格式的代表性热图帧在图2中显示。对于视频输入的任何帧，我们使用高分辨率网络（HRNet）[10]估计人体2D姿势。对于原始的3D骨架坐标，我们将（x，y）2D坐标在每个时间步转换成热图。通过这种方式，我们可以获得一个热图h ∈ RC×H×W，其中C、H和W分别指通道、高度和宽度。在计算每个单帧的热图之后，我们沿着时间维度连接这些热图以形成3D热图体H ∈ RC×T×H×W。如图1所示，公共的NTU RGB+D样本 [11]有17通道的输入，对应于17个骨架关节，而我们自建的太极动作 [12]有72个关节。为了解决在使用完整关节信息的样本时不同数据集之间的骨架关节不匹配问题，我们将骨架关节分为五个部分，包括四肢和一个躯干，并将样本均匀分成5通道输入，如图1中标记为红色、绿色、黑色、紫色和橙色，同时确保每个通道都保持其物理含义。当以3D热图体作为网络输入时，我们使用主流的PoseConv3D模型 [13]作为我们的模型骨干。在这项工作中，我们使用NTU RGB+D数据集进行网络预训练，小规模数据集仅用于最终分类头训练或微调预训练网络。</p><p>我们总结本文的贡献如下： • 一个统一的动作表示。我们引入基于热图的伪视频来统一动作输入的格式。这种格式可以将基于视频和基于骨架的样本转换为统一的输入，消除了这些格式之间的障碍。 • 人体测量学运动先验。我们提出并证明人体动作具有共同的运动特征，因此预训练模型骨干可以帮助提取人体特征，以克服网络过拟合并提高在小规模数据集上的识别性能。 • 一个统一的关节分割方法。我们根据人体结构将骨架关节分为五个部分，以充分利用不同数据集之间的数据信息，而不考虑动作数据的格式和采样器数量。 • 最先进的性能。我们在公共的Nursing Activities和我们自建的太极数据集上分别进行各种配置的实验。识别性能和提取特征的t-SNE [14]可视化表明，与先前的SOTA方法相比，我们的识别准确性得到了很大程度的提高，网络的过拟合现象得到了显著缓解。当融合从关节和肢体提取的特征时，识别准确性进一步提高，证明了我们提出的模型的普遍性。我们的实验记录和结果可在网站<a href="https://github.com/eunseo-v/SSCLS（于2023年5月10日访问）上找到。">https://github.com/eunseo-v/SSCLS（于2023年5月10日访问）上找到。</a></p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>本节旨在概述基于骨骼的人类动作识别研究的最新进展，包括相关数据集、识别算法、小尺度细粒度动作数据集、训练前策略和评估协议。我们还对我们提出的方法与其他相关工作进行了比较分析，包括先前对太极拳动作识别的研究。</p><h3 id="2-1-基于骨架的人体动作数据集"><a href="#2-1-基于骨架的人体动作数据集" class="headerlink" title="2.1. 基于骨架的人体动作数据集"></a>2.1. 基于骨架的人体动作数据集</h3><p>2.1. 基于骨架的人体动作数据集 本节重点介绍基于骨架的人体动作数据集及其获取方法，可以分为三组：基于深度的、基于可穿戴的和基于提取的。其中，Microsoft Kinect系统[15]广泛用于基于深度传感器的3D人体动作提取。两个大规模的人体日常活动数据集，即NTU RGB+D数据集 [11,16]和PKU-MMD数据集 [17]，包含由Kinect V2传感器收集的骨架坐标，并可以使用姿势估计算法提取多个人体动作。然而，这些数据集的准确性受到光强和背景等因素的影响。另一种类型的数据集是使用可穿戴运动捕捉系统获取的，例如Perception Neuron [18]，它利用惯性传感器测量穿戴者的方向和加速度。我们自建的太极动作数据集 [19] 使用Perception Neuron系统收集，仅包含3D骨架坐标。尽管这种方法可以精确捕捉骨架坐标，但仅限于单个采样器的动作，并且测量精度可能受到惯性传感器漂移的影响。</p><p>在基于提取的数据集领域，姿势估计算法已被广泛用于从视频中检测和估计2D人体姿势，实现从动作视频剪辑直接获取骨架坐标。与3D姿势估计器相比，估计的2D骨架更准确且更稳健。值得注意的是，高分辨率网络（HRNet）[10]被提出作为一种预测更精确和空间准确的关键点热图的手段，而MediaPipe [20]则作为一个跨平台的机器学习（ML）框架，可以从视频中提取人体姿势，如MediaPipe Hands [21]，一个实时的设备上手部跟踪解决方案，可以预测手的骨架。这些方法为调整网络输入格式提供了一种方便的方式，也有效地丰富了人体动作数据集。</p><p>在我们的研究中，我们利用NTU RGB+D数据集的骨架坐标作为我们的预训练数据集，该数据集包含60个跨足日常、相互和健康相关动作的动作类别，采集了三个摄像头视角和众多主体。鉴于我们的假设，即人类活动共享共同特征，我们预计预训练数据将包含丰富的动作信息，并因此有助于我们的小规模动作识别任务。</p><p><strong>要点总结：</strong></p><ol><li>介绍了基于骨架的人体动作数据集，分为深度传感器、可穿戴系统和提取方法三类。</li><li>提到Microsoft Kinect系统广泛用于3D人体动作提取，NTU RGB+D和PKU-MMD数据集是两个使用该系统的大规模数据集。</li><li>另一类数据集使用可穿戴运动捕捉系统，如Perception Neuron，但受到单一采样器和传感器漂移的限制。</li><li>在基于提取的数据集中，介绍了姿势估计算法在从视频中获取2D人体姿势方面的应用，以及使用高分辨率网络（HRNet）和MediaPipe的方法。</li><li>本研究使用NTU RGB+D数据集的骨架坐标作为预训练数据，认为预训练数据将有助于小规模动作识别任务。</li></ol><h3 id="2-2-人体动作识别方法"><a href="#2-2-人体动作识别方法" class="headerlink" title="2.2. 人体动作识别方法"></a>2.2. 人体动作识别方法</h3><p>2.2. 人体动作识别方法 </p><p>由于方法部分不是本文的主要焦点，我们将简要介绍人体动作识别。先前的方法依赖于手工制作的描述符来表示人体动作，成功程度各异。例如，Wang等人[22]尝试通过将摄像机运动纳入其方法来改进基于运动的描述符。与此同时，Oreifej和Liu [23]开发了一种称为4D定向梯度直方图（HON4D）的新型描述符，它创建了时间和空间的4D投影仪。这些描述符是专门设计的，对所有数据集的泛化性较差。近年来，基于深度学习的方法在动作识别研究中变得流行[24]。这些方法主要集中在将基于骨架的序列作为序列问题使用，提出了基于LSTM的模型[25–28]，用于构建序列的不同空间对齐的时间关系。 研究人员[29–32]还探讨了使用伪图像表示3D骨架序列的方法，这允许使用预训练的主流CNN模型进行特征提取，而无需从头开始训练。例如，Wang和Li [31]提出将3D骨架序列转换为关节轨迹图，然后使用CNN核进行分类。然而，这些方法因分解骨架序列的物理结构且缺乏时空动态而受到批评。 为了解决这些局限性，Kipf等人[33]开发了一种有效的卷积神经网络（CNNs）变体，可直接在图上运行，用于处理图结构化数据。该方法将CNNs推广到任意结构的图上，使其非常适用于表示3D骨架序列，因为这可以自然地看作是图数据。Yan等人[34]随后提出了一种称为时空图卷积网络（ST-GCN）的新模型，该模型基于Kipf的方法，并可以自动学习关节的空间配置和时间动态。该模型已成为后续动作识别研究的基准比较。后续基于GCN的工作主要包括自适应空间图卷积核或双向骨架序列[35–38]，以及论文[35]中的Ck核计算每帧中任意两个关节的相似度，类似于注意机制。Qin和Liu [39]在原始输入的通道维度中引入了角度编码，以融合高阶特征。尽管具有自注意模块的Transformer架构[40]在自然语言处理（NLP）任务中已成为事实上的标准，但其结构已在包含计算机视觉[42]和长序列时间序列预测[43]的各种研究领域中得到广泛应用。在基于骨架的人体动作识别领域，研究人员提出了一些方法，例如时空Transformer网络（ST-TR）[7]和群体活动识别网络GroupFormer [44]。这些方法利用Transformer的自注意机制来模拟关节之间的依赖关系，并共同捕获时空上下文信息。观察到与使用骨架坐标作为输入的基于GCN的方法相比，基于Transformer的方法在性能上表现出色。 视频理解是另一个使用视频剪辑作为输入的人体动作识别领域[45]。这些视频数据集中的剪辑通常来自涵盖广泛人体动作相关类别的互联网视频。Karpathy等人[46]收集了Sports-1M数据集，并提出了一种具有上下文流的多分辨率CNN架构，该流模型低分辨率图像，而视觉流模型高分辨率中心裁剪图像。Simonyan和Zisserman [47]提取了光流并提出了一种两流ConvNet，用于使用单帧图像和多帧光流捕获空间和时间信息。Jing等人[48]试图在复杂场景中识别动作。具体而言，他们提出了一个联合损失的时空神经网络模型，用于在视频中识别人体动作，并使用提取光流和外观的两流网络来捕获空间特征，以及使用LSTM网络来建模时态动态。Tran等人[49]将3×3 2D扩展到3×3×3 3D卷积核，并学习由简单线性分类器对齐的C3D特征用于视频理解。由于视频稀缺，3D CNN很难训练。Carreira和Zisserman [4]扩展了2D核并提出了一种两流扩张3D ConvNet（I3D）。I3D模型可以使用在大图像数据集上预训练的相应2D CNN网络初始化，有助于网络训练。由于从视频中提取光流耗时，一些工作[50,51]尝试应用基于幻觉的方法来学习扩充网络输入。例如，Wang等人[50]试图通过一种幻觉步骤将I3D特征图的输出转换为Fisher向量表示。唐等人[51]提出了一种从外观输入中想象光流特征的网络，以节省计算成本。Feichtenhofer等人[52]提出了一个SlowFast网络框架，其中慢路径具有低帧速率和更高通道以捕获空间特征，而快路径可以学习具有高帧速率和更轻通道的时态嵌入。3D CNN核，如C3D和I3D，可以填充到该框架中以进行识别任务。随着注意机制的流行，工作[53]将时空非局部操作应用于视频识别任务，而论文[5,54]的作者则将标准Transformer架构调整到不同技能的大规模视频数据集识别中。 在我们之前的工作[12]中，我们使用3D骨架坐标作为太极动作识别的输入。然而，由于预训练的NTU RGB+D数据集与太极数据集之间的数据格式和收集系统差异，即使在网络管道中应用了批归一化，识别性能仍然相对较差。为了解决这个问题，Duan等人[13]提出使用3D热图体代替传统的图序列来表示人体骨架，这比3D骨架坐标具有几个优势。例如，由于测量可能在不同数据集之间有所不同，使用3D骨架坐标可能导致相同动作的显著差异，需要在数据集之间进行归一化。使用3D热图体可以直接解决这种归一化问题，同时允许应用视频处理技巧，如居中和裁剪，以丰富训练数据集。此外，多人动作骨架可以投影到单个热图上，我们可以利用视频理解领域的主流方法，以在不增加额外计算成本的情况下实现稳定和更好的性能。然而，GCN的计算随着一个动作中采样器数量的增加而呈线性扩展。在这项工作中，我们的目标是将基于图的骨架坐标转换为3D热图体，以实现在不同数据集之间共享人体动作特征，并缓解数据格式不匹配的问题。对于特征提取，我们将使用PoseConv3D网络作为我们的模型骨干，该网络在性能上表现优越，超过了先前的动作识别方法。</p><p><strong>要点总结：</strong></p><ol><li>介绍了人体动作识别的方法，强调了先前依赖手工制作描述符的方法和近年来基于深度学习的方法。</li><li>概述了基于骨架的序列的传统方法，如使用LSTM模型，以及对3D骨架序列使用伪图像的方法，以便利用预训练的CNN模型进行特征提取。</li><li>引入了基于图的CNN变体，特别是ST-GCN模型，以及后续工作中基于GCN和Transformer的方法，这些方法在处理骨架坐标作为输入时表现出优越性能。</li><li>概述了基于视频的动作识别方法，包括对视频理解领域的关键工作，如I3D模型和SlowFast网络。</li><li>提到先前的工作中使用3D骨架坐标的问题，引出了使用3D热图体的解决方案，以便在不同数据集之间共享人体动作特征，并介绍了PoseConv3D网络作为特征提取的模型骨干。</li></ol><h3 id="2-3-小规模细粒度数据集研究"><a href="#2-3-小规模细粒度数据集研究" class="headerlink" title="2.3. 小规模细粒度数据集研究"></a>2.3. 小规模细粒度数据集研究</h3><p>2.3. 小规模细粒度数据集研究 研究人员致力于通过新颖的神经网络提高在流行的大规模粗粒度数据集上的识别性能。创建特定的小规模细粒度数据集可能更为有益。Weinland等人[55]创建了一个包含来自多摄像头的11个动作的IXMAS数据集。Nicora等人[56]建立了MoCA数据集，该数据集包含每个活动的3个相机视图中的20个细粒度烹饪动作。此外，MoCA是一个双模态数据集，收集了烹饪场景中的运动捕捉数据和视频序列。这些特定数据集可以证明所提出模型的细粒度识别能力，有助于在特定场景中的应用。Gu等人[57]发布了一个包含注释篮球比赛视频的细粒度篮球动作数据集。他们提出了一个集成了NTS-Net的两流网络，用于提取其细粒度数据集的判别特征。 Wu和Shao [58]提出了一种多最大间隔支持向量机（MMM-SVM），以提高IXMAS数据集的准确性，采用了多视图系统。Wang等人[59]使用内部迁移学习策略增强了小规模数据集的性能。这种策略未使用其他数据集，需要从候选模型列表中选择最佳模型，这在应用中可能需要更复杂。Shen等人[60]提出了一种自动数据增强模型，称为Imaginative Generative Adversarial Network，它可以从学习的数据集分布中采样新数据。增强的数据集可以提高使用相同神经网络的分类准确性。这种方法需要相对较大规模的数据集，而由于样本不足，小规模数据集仍然难以训练。Ijaz等人[6]提出了一种多模态基于Transformer的网络，用于提取和融合骨骼关节和加速度数据的特征信息，以提高小规模细粒度护理活动[61]的识别性能。然而，他们没有使用NTU-RGB+D数据集对网络进行预训练，只引入了自适应锐度感知最小化（ASAM）[8]来收敛他们的Transformer模型。他们还在结论中推断，可以探索预训练骨架分支以进一步提高模型的收敛性，这在我们的工作中已经实施，并陈述了预训练管道的优越性。Goyal等人[62]试图利用大规模预训练表示，假设它们隐含地包含小规模数据集任务的相关线索。该论文还应用了视图智能批量归一化以最小化跨视图动作识别任务的内部协变量转移。这种方法对可以提取为运动特征的哪一层输出进行了实验性分析，并没有详细描述人体数据集之间的隐含相关线索，这些都将在我们的论文中解决。 与普遍的粗粒度动作不同，太极动作可以视为专业的动作类别，每个太极动作由多个与身体运动协调的元动作组成。与日常人体动作相比，需要关注不同人体部位在不同阶段的运动特征。在太极动作中，由于太极动作类别之间的相似性，识别是具有挑战性的。由于太极已成为亚洲运动会的项目，有效的识别网络可以帮助初学者评估他们的活动。在太极相关的动作识别工作中，Lin等人[63]提出了一个包含17个类别的大深度包含人体动作（DHA）视频数据集。他们将所有太极动作视为相同的类别。Sun等人[64]提出了一个包含58个太极动作的细粒度太极数据集，总共有2772个样本。所有视频样本都来自具有动态背景的网站。他们还应用了改进的密集轨迹特征和Fisher向量表示进行识别，并取得了51.39%的识别准确度。Dong等人[65]还提出了一个名为“Sub-Tai chi”的太极数据集，包含15个动作，并应用了带有注意模块的结构LSTM进行识别；他们在自己的数据集上达到了79%的识别准确度。Liu等人[66]在他们自己的太极数据集上应用了ST-GCN模型，取得了89.22%的识别准确度。所有这些研究都创建了自己的太极数据集，并且不能在网站上进行比较。他们提出了具体的或应用了流行的深度学习方法进行识别，对于细粒度动作识别而言并不通用。 我们之前的工作在太极动作上进行了初步研究。我们首先创建了太极动作数据集，并提出了一种使用节点轨迹特征的太极动作识别算法[19]。我们提取了手工制作的特征，并且仅使用了一个单一节点进行特征提取，在小规模训练集下表现不佳，通用性可能更好。我们引入了深度学习方法，并提出了一种用于太极动作识别的空间变换网络[12]。我们使用了NTU RGB+D数据集对模型进行预训练，以解决过拟合网络问题。太极训练样本仅负责使用冻结的模型骨干参数进行最终分类头培训。该算法使用了人体骨架的24个关节点，并相对于第一个传统方法取得了改进。然而，它仅部分利用了太极数据集的72个骨架关节，导致了我们的细粒度太极动作缺失信息。网络识别还需要进行复杂的数据预处理。此外，小规模训练集下的性能仍需要改进，并且结果的分析较少呈现给读者。我们的论文包含准确性提高和消融研究，将解决这些弱点。 我们打算在我们的工作中为这些小规模细粒度动作数据集提供一个通用框架。我们假设样本在人体动作数据集之间共享共同的运动特征，并提出了一个简单但有效的模型框架来解决小规模细粒度动作识别问题，该问题已在公共护理活动数据集和我们自建的太极数据集上进行了评估。该模型框架不引入对抗网络，可以识别小规模细粒度动作数据集。</p><p><strong>要点总结:</strong></p><ol><li>研究人员致力于改善大规模粗粒度数据集上的识别性能，特别关注了小规模细粒度数据集的建立。</li><li>提到了不同研究中创建的小规模细粒度数据集，如IXMAS、MoCA、篮球动作数据集等，以及它们的用途和特点。</li><li>讨论了先前研究采用的一些方法，包括支持向量机、迁移学习、数据增强、多模态Transformer等，以提高小规模细粒度数据集的性能。</li><li>强调太极动作作为细粒度动作类别的挑战性质，以及太极数据集的创建和其他研究。</li><li>总结了先前关于太极动作的研究，包括使用节点轨迹特征和空间变换网络进行识别，指出了它们的局限性和需要改进的地方。</li><li>提出了在工作中旨在为小规模细粒度动作数据集提供通用框架，假设样本在不同数据集之间共享运动特征。</li></ol><h3 id="2-4-预训练策略和评估协议"><a href="#2-4-预训练策略和评估协议" class="headerlink" title="2.4. 预训练策略和评估协议"></a>2.4. 预训练策略和评估协议</h3><p>2.4. 预训练策略和评估协议 在计算机视觉和视频理解领域，许多研究人员通过线性评估协议或微调评估验证他们的方法。在线性评估协议中，我们冻结预训练的网络参数，仅对目标数据集训练最终分类器。此外，在微调评估中，所有网络参数都与新数据集进行微调。例如，MoCo-V1 [67]首先在ImageNet-1M [9]数据集上进行无监督预训练，然后冻结参数并重新训练一个监督线性分类器。这种无监督学习的主要目标是学习可转移的特征。这种策略可以证明提取特征的有效性。Simonyan和Zisserman [47]使用了三种评估方法来衡量其空间流ConvNet的性能，包括在目标UCF-101数据集上从头开始训练，基于ILSVRC-2012数据集的预训练线性评估协议，以及在UCF-101上训练分类器并进行微调。I3D模型 [4]提出了充气的3D CNN核，可以在2D ImageNet模型上进行预训练，并将相关的2D CNN核引导到3D CNN核以进行初始化。I3D模型还通过首先在Kinetics上进行预训练，然后在HMDB-51 [68]和UCF-101数据集上进行微调来评估其出色的迁移学习能力。 如上所述，预训练策略总是与评估协议相配。在计算机视觉领域的下游任务中，如目标检测 [69]、语义分割 [70] 和人体姿态估计 [71]，研究人员将在ImageNet数据集上预训练的网络模型用于微调其下游任务。在视频理解领域，I3D模型使用在ImageNet数据集上预训练的2D卷积网络的参数来初始化其3D卷积网络的权重参数，从而加速网络的收敛。我们观察到，这些下游任务的数据集都是自然图像或视频样本。此外，在ImageNet上预训练的模型具有相似的现象，即从层次网络的第一层到最后一层的提取特征逐渐从通用性变化为任务特定性 [72]。具体而言，前几层可以捕获样本的低级特征，例如图像的颜色斑点特征，而最后一层的输出则是高级和任务特定的。因此，在ImageNet上预训练的模型骨干可以提取自然图像的低级边缘和纹理特征，并加速在其他自然图像或视频数据集的下游任务中的模型收敛，以实现与从头开始训练相比更好的性能。 在本文中，我们打算使用SlowFast构建一个通用且准确的模型框架。与基于骨架的人体动作识别方法不同，我们将基于骨架的动作序列转换为基于热图的伪视频。我们利用大规模数据集提取热图的常见低级运动特征，这有助于改善我们的小规模细粒度动作数据集上的识别性能。以前的基于骨架的动作识别任务只是使用大规模基于骨架的动作数据集NTU RGB+D从头开始训练其提出的模型。对于足够大规模的数据集，从头开始训练的模型也可以展现出令人印象深刻的性能 [7,13]。</p><p>然而，在小规模数据集的下游任务中，预训练策略是提高识别性能的有效方法 [73]。因此，我们基于预训练策略的模型框架对于小规模动作识别任务是必要的，这在先前的工作中从未得到解决 [6,57,59,60]。我们还通过直观的t-SNE [14]可视化给出了完整的分析，以展示预训练策略的有效性。 我们的动作研究使用了两种不同目的的评估。线性评估协议表明我们的预训练模型可以通过基于热图的伪视频样本学习共同的人体运动特征，而微调评估实验证明了我们框架的迁移学习能力，并与从头开始训练网络相比展现出卓越的动作识别性能。</p><ol><li><strong>评估协议：</strong><ul><li>计算机视觉和视频理解领域的研究者使用线性评估协议或微调评估来验证方法。</li><li>线性评估协议中，冻结预训练网络参数，只训练目标数据集的最终分类器。</li><li>微调评估中，对新数据集进行所有网络参数的微调。</li><li>以MoCo-V1为例，首先在ImageNet-1M数据集上进行无监督预训练，然后通过线性评估协议验证特征可转移性。</li></ul></li><li><strong>预训练策略的有效性：</strong><ul><li>在计算机视觉领域的下游任务中，如目标检测、语义分割和人体姿态估计，使用ImageNet上预训练的模型可加速收敛。</li><li>模型在ImageNet上的预训练使其能够在其他自然图像或视频数据集的下游任务中表现更好。</li></ul></li><li><strong>本文模型框架：</strong><ul><li>使用SlowFast构建通用准确的模型框架。</li><li>不同于骨架方法，将基于骨架的动作序列转换为基于热图的伪视频。</li><li>利用大规模数据集提取热图的共同低级运动特征，以改善小规模细粒度动作数据集上的性能。</li></ul></li><li><strong>小规模数据集的挑战：</strong><ul><li>在小规模数据集的下游任务中，预训练策略对提高识别性能是有效的。</li><li>本文模型框架基于预训练策略，专注于解决小规模动作识别任务，这是以往工作未解决的问题。</li></ul></li><li><strong>两种评估方法：</strong><ul><li>线性评估协议展示预训练模型学到共同人体运动特征的能力。</li><li>微调评估实验展示框架的迁移学习能力，相较于从头开始训练，表现出更优越的动作识别性能。</li></ul></li></ol><h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>我们在3.1节中详细描述了从三维骨骼关节坐标到三维热图体积的动作序列转换。然后我们在第3.2和3.3节中介绍了我们的模型框架和培训策略。</p><h3 id="3-1-从关节坐标到3D热图体积"><a href="#3-1-从关节坐标到3D热图体积" class="headerlink" title="3.1. 从关节坐标到3D热图体积"></a>3.1. 从关节坐标到3D热图体积</h3><ol><li><strong>PoseConv3D方法：</strong><ul><li>对于PoseConv3D方法，动作数据应转换为3D热图体积。</li><li>在PoseConv3D方法[13]中，使用在COCO-keypoints上训练的2D姿势估计器HRNet [10]直接从动作视频剪辑中提取2D人体姿势。</li><li>Nursing Activities和Tai Chi动作数据集仅包含帧内的3D骨架坐标，需要转换为3D热图体积。</li></ul></li><li><strong>坐标转换及数据格式处理：</strong><ul><li>不采用PoseConv3D建议的将3D骨架（x，y，z）分成三个2D骨架的建议，而是使用（x，y）、（y，z）和（x，z）分别，因为有些数据集可能直接由相机收集，3D坐标提取不可行或噪声干扰，影响预训练或微调模型。</li><li>数据格式（y，z）和（x，z）因未固定采样器到相机的距离而缺乏物理含义，因此仅使用2D坐标（x，y）形成3D热图体积，z轴坐标可用于通过沿y轴旋转进行数据增强。</li></ul></li><li><strong>多视角和数据增强：</strong><ul><li>每个动作从三个固定相机视角同时捕捉，可获取相同动作的多视角样本。</li><li>还可以通过沿y轴旋转进行透视变换访问多视角样本，如图3b所示。</li><li>3D骨架坐标序列X ∈ RT×V×C，其中T，V，C表示帧数、关节数和坐标数，包含运动捕捉系统收集的V个骨架关节的三个坐标。</li><li>为了充分利用收集到的3D骨架序列，对每个小规模训练样本执行旋转和剪切操作。</li></ul></li><li><strong>2D热图构建：</strong><ul><li>对于每个2D关节坐标（xk，yk），在方程（3）中，Hkij是像素（i，j）处的关键点热图值，ck是（xk，yk）的置信度分数，σ控制了高斯图的方差；Hvij是肢状热图中像素（i，j）的值。</li><li>利用方程（4）计算所有这些像素的热图值，其中d2_ab是从像素到骨ab的投影距离，因此当像素位于肢上时，该值始终为1。</li><li>计算所有关节和骨的热图值后，像素（i，j）的最终热图值是这些V个热图值的最大值，如方程（5）所示。</li><li>图4说明了原始3D关节坐标和太极动作样本在一个帧上的最终2D关键点和肢状热图。</li></ul></li><li><strong>3D热图体积的处理：</strong><ul><li>完成2D热图构建后，将它们组合成跨所有帧的3D热图体积。</li><li>由于网络的输入是两个具有不同关节数的数据集，需要重新构建热图通道。</li><li>采用两种策略来关联两个数据集的信息：一种是从72个太极关节中选择与NTU RGB+D数据集匹配的17个关节；第二种是将整个骨架分为五个部分 - 四肢和一个主干。</li><li>将所有2D热图沿时间维度连接，构建3D热图体积H ∈ RC×T×H×W。</li><li>进行一系列的数据处理策略，将体积固定为48，采用均匀采样。</li><li>遵循中心裁剪策略[13]，找到包围所有帧的2D姿势的最小框，进行随机裁剪，这是视频理解中的一种流行处理方法。</li><li>热图体积将调整为56×56的分辨率，并有50％的概率翻转骨架的右侧和左侧以丰富数据集。</li><li>与其他识别方法相比，我们的预处理数据方法更为简洁，没有零填充[34]或数据插值策略[12]的限制，对采样器数量也没有限制。</li></ul></li></ol><h3 id="3-2-模型架构"><a href="#3-2-模型架构" class="headerlink" title="3.2. 模型架构"></a>3.2. 模型架构</h3><p>我们的3D热图体积可视为一种特殊的视频格式。我们将关节分割后的3D热图体积作为网络输入，如图1所示。我们的网络骨干显示在图1的中间模型骨干部分，遵循PoseConv3D [13]的配置，详细信息见表1。他们采用了SlowOnly [52]方法，将ResNet层直接从2D扩展到3D的最后两个阶段作为其模型骨干。中间列出了每个残差块的组成部分，右侧说明了相应块之后的输出特征大小。核的尺寸用{T× S2, C}表示，分别代表时间、空间和通道的大小。非退化的时间滤波器（时间核尺寸 &gt; 1）用下划线标示。残差块用括号表示，实例化的骨干是ResNet-50 [76]。从图1中可以看出，在时间维度上没有应用下采样操作。最后，动作嵌入将通过线性分类头传递，其中包含全局平均池化（GAP）层和全连接（FC）层，以生成预测的动作类别。</p><p>相对于ResNet-50模型，我们从模型骨干中移除了ResNet2块，因为我们已经使用骨架坐标从视频中提取了姿势特征。我们在第2.4节详细说明了我们的分层网络逐渐从通用特征提取到任务特定特征。由于基于热图的伪视频样本缺乏背景信息，不包含任何低层特征。因为我们的输入相对于自然图像已经是中层特征，所以具有浅层和窄通道的网络模型更加适用。与X3D [77]和I3D模型 [4]相比，我们的模型骨干更轻便，提高了基于热图的伪视频输入的特征提取效率。我们计划使用每个输入样本中均匀采样的48帧更密集的时间输入，使我们能够捕捉更多的时间动态。在模型框架中，沿时间维度没有下采样操作，其他操作遵循ResNet-50结构以提高模型骨干的鲁棒性。</p><h3 id="3-3-训练策略"><a href="#3-3-训练策略" class="headerlink" title="3.3. 训练策略"></a>3.3. 训练策略</h3><p>在NTU RGB+D动作识别研究中，段等人[13]在基于骨架的跨视角（CV）和跨主体（CS）设置中取得了最先进的准确性。然而，他们使用从视频中提取的2D骨架坐标数据，并丢弃了原始数据集提供的3D骨架坐标，而这些坐标在其他识别方法中被使用。图7a显示了原始3D骨架坐标的可视化，图7b是从RGB样本视频中提取的2D坐标的热图。对于这个喝水的动作，采样环境中有两个人，只有一个人在喝水，另一个是干扰采样者。原始3D骨架坐标有两个采样器，数据过于模糊，难以直观识别。先前的工作筛选了原始数据，并利用去噪后的数据进行动作识别。这可以从采样器中提取正确的采样器，但采样环境总是影响数据的一致性。当他们无法从原始样本中提取正确的采样器时，他们必须将两个采样器的数据输入到网络中，并计算提取特征的平均分数。NTU RGB+D数据集中有交互样本，因此采样器的数量根据动作类别而异。先前的工作必须将单人动作样本的第二个人填充为零值，这对于批量计算很方便。图7b反映了一个更准确的喝水动作，视频格式没有零填充问题，这对应用来说是简洁的。在我们的预训练阶段，我们继续使用从NTU RGB+D视频数据集中提取的2D坐标，这有助于更好地学习普通人类动作特征，并减少网络的过拟合。在训练过程中，如图1所示，我们首先使用NTU RGB+D数据集对模型骨干进行预训练。对于小规模细粒度动作识别，我们分别尝试线性评估协议和冻结所有预训练骨干参数或微调整个网络的微调评估，以验证提出的模型框架的有效性。由于我们可以生成关节和肢体格式的3D热图体积，我们还尝试了伪热图及其融合策略，详细内容在第4节讨论。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Improving-Small-Scale-Human-Action-Recognition-Performance-Using-a-3D-Heatmap-Volume利用三维热图量改善小规模人体动作识别性能&quot;&gt;&lt;a href=&quot;#Improving-Small-</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>备战1月</title>
    <link href="https://www.fomal.cc/posts/7a7d65fb.html"/>
    <id>https://www.fomal.cc/posts/7a7d65fb.html</id>
    <published>2024-01-02T02:56:51.000Z</published>
    <updated>2024-02-22T02:22:46.648Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>如何快速精通一个领域</p><blockquote><p>尝试更新一下自己的个人学习系统</p><p>自己之前都是过多关注于刷了多少课，却没有关注学习效果 同时导致学的太多 累了之后懒得去复习和反馈应用，应该关注学到的知识和应用</p></blockquote></blockquote><p><img src="../assets/image-20240102105700646.png" alt="image-20240102105700646"></p><h1 id="备赛"><a href="#备赛" class="headerlink" title="备赛"></a>备赛</h1><blockquote><p>先练手 开始刷题 </p><p>在深入学习对应的不会的知识点</p></blockquote><p>java 公开课<a href="https://www.bilibili.com/video/BV1MA4116793?p=28&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">试题一：指数计算_哔哩哔哩_bilibili</a></p><p><a href="https://pan.baidu.com/s/1lELNX9iYDW6FN5b1pOX4Gg?pwd=6fo6">2020年蓝桥杯JavaC组省考题目.pdf_免费高速下载|百度网盘-分享无限制 (baidu.com)</a></p><ul><li>[ ] 19年老视频<a href="https://www.bilibili.com/video/BV1Lb4y1k7K3?p=23&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">22-动态规划入门视频讲解_哔哩哔哩_bilibili</a></li></ul><p>java备考方法论</p><blockquote><p>主要是关于对应的算法题目的一些拿分技巧 建议后期掌握一下</p><p><a href="https://www.bilibili.com/video/BV1NZ421z7WV/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">Java组经验分享| 蓝桥杯拿分技巧_哔哩哔哩_bilibili</a></p><p>下边方法来源于<a href="https://www.bilibili.com/video/BV1Hb4y1W7Pn/?spm_id_from=333.880.my_history.page.click">【蓝桥杯】蓝桥杯备赛攻略/算法学习路线/经验分享_哔哩哔哩_bilibili</a></p><p> 排序算法 查找算法</p><blockquote><p>八大排序 + 主要练习归并+ 快速排序</p></blockquote><p>难度低的通过率高的题目</p><p>二分查找+双指针+ 练习</p><p>递归+暴力学习+练习【蓝桥杯的基础】</p><p>贪心算法</p><p>最后学习完算法之后在学习一边数据结构 加强能理解 </p><p>搜索【深度搜索【暴力基础 打蓝桥杯最重要的算法，可以骗到很多的分数】+ 广度搜索（练习几道题即可）】</p><p>动态规划杯</p></blockquote><p>暴力 递归 暴力搜索这些练好刷题之后 直接刷蓝桥杯真题</p><p>学习路线<a href="https://www.bilibili.com/video/BV1u84y1w7xt/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3d5976f4f96ce120bf88891647d386cb">（新生必看）小白或新手如何准备蓝桥杯Java组，以及如何避坑_哔哩哔哩_bilibili</a></p><blockquote><p>java 可以减少一些底层的大佬，好好备赛，应该能拿奖</p><blockquote></blockquote></blockquote><p><img src="D:\2023.6.2hexo\test\source\assets\image-20240218100118769.png" alt="image-20240218100118769"></p><blockquote><p>先用洛谷刷题  去练习基础题目</p><p>再用对应真题刷题系统来刷真题</p></blockquote><p><img src="D:\2023.6.2hexo\test\source\assets\image-20240218100129245.png" alt="image-20240218100129245"></p><p><img src="D:\2023.6.2hexo\test\source\assets\image-20240218100103648.png" alt="，"></p><blockquote><p>必须要用Main 函数 来实现所有的代码提交，不能写package 包</p></blockquote><p>OA 赛制   有步骤分，也有技巧</p><p>日期计算器  excel 表格 写代码</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;如何快速精通一个领域&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;尝试更新一下自己的个人学习系统&lt;/p&gt;
&lt;p&gt;自己之前都是过多关注于刷了多少课，却没有关注学习效果 同时导致学的太多 累了之后懒得去复习和反馈应用，应该关注学到的知识和应用&lt;/p&gt;
&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>2024.1月</title>
    <link href="https://www.fomal.cc/posts/668a32f0.html"/>
    <id>https://www.fomal.cc/posts/668a32f0.html</id>
    <published>2024-01-02T01:15:04.000Z</published>
    <updated>2024-01-18T07:39:58.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="12月末尾"><a href="#12月末尾" class="headerlink" title="12月末尾"></a>12月末尾</h2><p>🗓️2023.12.30【日知日进】日历<br>📚日知书目：《人是如何学习的》</p><p>✨日进思考：“普通人和高手的认知差距在哪里?”</p><pre><code> 🔹普通人和高手之间的差距在于知识迁移能力的高下，高手总是能够更有效地迁移知识。是否了解原理决定了能否进行知识的迁移，而高手总是乐于探究事物运行背后的原理。高手往往也具备“元认知”的能力，元认知即“对自己认知的认知”，这种能力相当重要。</code></pre><p>💫共读推荐：<br>     🔹《知识大迁移》<br>     🔷学习者必须掌握足够多的事实，才能对自己掌握的知识及欠缺部分心中有数。</p><p><img src="../assets/image-20240102220712895.png" alt="image-20240102220701767"></p><p>今天是2023年最后一天啦！<br>年初的目标是否已经完成？未来的日子，又有哪些梦想要实现？</p><p>凡是过往，皆为序章。<br>每个终点也是新的起点，只要拼搏过，时光会看见。<br>2023最后一天，早安！</p><p>今日简报</p><p>12月31日   星期日</p><p>1、阿根廷正式拒绝加入金砖国家。</p><p>2、董宇辉宣布由东方甄选100%控股的新账号1月9日开播。</p><p>3、直播PK输了给对方洗脚？线下约架签“生死状”？长沙警方整治网红乱象。</p><p>4、央行同意支付宝变更为无实际控制人。</p><p>5、元旦假期首日跨省游占55%，中国香港成最火跨年境外目的地。</p><p>6、北京城市轨道交通运营总里程达836公里，超越上海跃居全国第一。</p><p>7、因假冒大凉山农特产品，凉山包括赵灵儿、曲布等多名百万级网红被判刑！</p><p>8、易建联宏远9号球衣正式退役！传奇生涯不会落幕。</p><p>9、一眼望不到头！也门首都大规模集会声援巴勒斯坦。</p><p>10、2024年起USB-C将成为欧盟电子设备通用标准。</p><p>11、马斯克强势回归领跑2023年全球富豪榜。</p><p>12、哈马斯官员称停火前不再交换被扣押人员。</p><p>✨此心不动，随机而动。<br>       来帆书APP 听《心学的诞生》</p><p>🎈今天是三末:<br>周末，月末，年末。<br>把过去所有的遗憾、烦恼、病痛通通抹去！</p><p>明天是三新:<br>日新，月新，年新。<br>让喜乐、梦想、健康都新新启程！<br>让2023的一切不顺都变成2024惊喜的铺垫！</p><h1 id="2024年1月"><a href="#2024年1月" class="headerlink" title="2024年1月"></a>2024年1月</h1><h2 id="1-1日周一"><a href="#1-1日周一" class="headerlink" title="1.1日周一"></a>1.1日周一</h2><h3 id="英语角"><a href="#英语角" class="headerlink" title="英语角"></a>英语角</h3><p>无</p><h3 id="1-1晨读"><a href="#1-1晨读" class="headerlink" title="1.1晨读"></a>1.1晨读</h3><p>无</p><h3 id="今日简报"><a href="#今日简报" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月1日，星期一，元旦快乐</p><p>1、国家主席习近平发表二〇二四年新年贺词。</p><p>2、家用神车桑塔纳月销仅1辆，又一代经典车型将退出历史舞台。</p><p>3、爱国主义教育法2024元旦起施行：将爱国主义教育贯穿学校教育全过程。</p><p>4、我国调整部分商品进出口关税，对部分抗癌药实施零关税。</p><p>5、国家统计局：2022年全国女性就业人员3.2亿人，占全部就业人员的比重为43.2%。</p><p>6、段永平向浙大再捐10亿+，支持母校“双一流”建设。</p><p>7、经济日报：摘牌倒逼景区提质升级。</p><p>8、7500米深，日本发现大地震海底断层悬崖，此次发现的断层有助于提高地震、海啸等灾害预测的准确度。</p><p>9、印度法院下令释放vivo两高管。</p><p>10、国家统计局：2022年中小学生《国家学生体质健康标准》达到优良的比例为55.1%。</p><p>11、以总理称加沙地带军事行动将持续数月。</p><p>12、美军研发无线能量“网”为无人机空中充电。</p><p>✨以自己的想法安排人生，即便在别人眼里有些离经叛道，至少还有机会活出自我。——《不安的哲学》</p><h3 id="名人语录"><a href="#名人语录" class="headerlink" title="名人语录"></a>名人语录</h3><p>🗓️2024.01.01漫画国学👣<br>『元旦佳节』</p><p>💫【诗歌欣赏】</p><p>《今日歌》<br>【明】文嘉<br>今日复今日，今日何其少。<br>今日又不为，此事何时了。<br>人生百年几今日，<br>今日不为真可惜。<br>若言姑待明朝至，<br>明朝又有明朝事。<br>为君聊赋今日诗，<br>努力请从今日始。</p><p>💫【诗歌鉴赏】<br>这是一首劝勉诗。语言朴实无华，道理浅显易懂，就是叫人珍惜光阴，努力奋进。因为我们整个人生就是由每一个今日组成，每一天都做好今日事，“明日”才不会因为虚度了“昨日”而感到遗憾，那么，由无数个“今日”组成的今生，才会有所作为。</p><h3 id="日知日进"><a href="#日知日进" class="headerlink" title="日知日进"></a>日知日进</h3><p><img src="../assets/image-20240102220932012.png" alt="image-20240102220932012"></p><h2 id="1-2日周二"><a href="#1-2日周二" class="headerlink" title="1.2日周二"></a>1.2日周二</h2><p>每个人的花期不同，不必因为有人提前拥有而焦虑。重要的是，当你选择了你要的方式，那就坚定地走下去。</p><h3 id="英语角-1"><a href="#英语角-1" class="headerlink" title="英语角"></a>英语角</h3><h3 id="今日晨读"><a href="#今日晨读" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>🍁2024.1.2晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。<br>——来自帆书《十分钟冥想》</p><p>👉【精彩选段】<br>冥想不是思考！它所做的不过是将一道明亮的光照耀在你的心灵上，所以你才更清楚地看到一切。</p><p>👉【作者之语】<br>无论我们感受如何，心灵的深层本质就像蓝色的天空一样，并没有发生变化。</p><p>👉【樊老师之语】<br>本书作者用生动故事和精妙比喻帮你破除误区，理解冥想精髓，带领你学习一些简单易学且非常有效的冥想方法，将其融入你的生活、工作和内心。每天给自己十分钟，拥抱清醒、平和与快乐，让我们的生活变得更加丰富而细腻。</p><h3 id="今日简报-1"><a href="#今日简报-1" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月2日，星期二</p><p>1、百城二手住宅平均价格去年累计下跌3.53%，今年市场有望逐渐筑底企稳。</p><p>2、各地推动开展根治欠薪冬季专项行动，让农民工安“薪”过节。</p><p>3、新疆三文鱼火出圈，尼勒克县年产6000吨三文鱼。</p><p>4、中美两国领导人互致贺信庆祝两国建交45周年。</p><p>5、南方人涌入哈市洗浴店爆满，有游客排队搓澡要等两三小时。</p><p>6、雪乡住宿2床1炕一晚3000元？价格被指“高攀不起”，当地回应雪乡住宿价格被指过高。</p><p>7、多地降温降雪低温津贴引关注，多名户外劳动者称“没听说过”。</p><p>8、2023全国电影票房超549亿元。</p><p>9、国家移民局：去年12月6国超11万人次免签入境中国，观光休闲占七成。</p><p>10、日本本州西岸近海发生7.4级地震。</p><p>11、2023年泰勒巡演带动50亿美元，消费者支出泰勒巡演经济效益或超50个国家GDP。</p><p>12、丹麦女王玛格丽特二世宣布退位，王位将传给儿子。</p><p>✨最重要的是，你可以想象心中有一片始终宁静、始终澄澈的区域。——《十分钟冥想》</p><h3 id="名人语录-1"><a href="#名人语录-1" class="headerlink" title="名人语录"></a>名人语录</h3><p>💞拉卡拉创始人孙陶然曾说：<br>“我所有的成就，<br>一半来源于天资，<br>一半来源于复盘。”<br>可见复盘对人生的重要性。</p><p>人生没有白走的路，每一步走过的路，每一件经历过的事，都是人生的财富。拉卡拉创始人孙陶然曾说：“我所有的成就，一半来源于天资，一半来源于复盘。”</p><p>反思自己的不足之处，总结出过往的经验。好的方面继续保持，坏的方面及时改正。</p><p><img src="../assets/image-20240102221129250.png" alt="image-20240102221129250"></p><p><img src="../assets/image-20240102221119189.png" alt="image-20240102221119189"></p><blockquote><p> ✍🏻如果用五到十年坚持做一件事，也许生活会发生巨变。</p></blockquote><h3 id="日知日进-1"><a href="#日知日进-1" class="headerlink" title="日知日进"></a>日知日进</h3><p>🗓️2024.01.02漫画国学👣</p><p>🔹生活小困扰：朋友误会我了，该怎么办?</p><p>💫国学大智慧：人不知，而不愠，不亦君子乎?</p><p>🔸译文诠释：别人不了解我，我不怨恨(恼怒)，不也是君子吗?</p><p>✨迁移阅读：参差多态，乃幸福之源。（《不抱怨的世界》）</p><p><img src="../assets/image-20240102221307687.png" alt="image-20240102221307687"></p><p>农夫种地以前有一位农夫，做事总爱瞻前顾后。春天的时候，有人问农夫：“春天到了，你种麦子了吗？”农夫回道：“没，我担心天不下雨。”那人又问：“那你种棉花没？”农夫又回道：“没，我担心虫子吃了棉花。”那人再问：“那你种了什么？”农夫说：“什么也没种，我要确保安全。”</p><p>感悟：我们总是想的太多而做的太少。顾虑重重只会束手束脚，一事无成。生活永远充满不确定，有时放手一搏，大胆尝试，方能有所收获。</p><h2 id="1-3日周三"><a href="#1-3日周三" class="headerlink" title="1.3日周三"></a>1.3日周三</h2><h3 id="英语角-2"><a href="#英语角-2" class="headerlink" title="英语角"></a>英语角</h3><p>Life is too short for long-term grudges.<br>人生短暂，何必长期心怀怨念。</p><h3 id="今日晨读-1"><a href="#今日晨读-1" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>🍁2024.1.3晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生中最大的发现就是发现自己。<br>——来自帆书《心态》</p><p>👉【精彩选段】<br>我们无须思考我们是否“有成功特质”和“无成功特质”，那是固定型心态所关注的。一个更健康、有效的成长型观念是，“如果成功没有如期而至，那只是因为我没有付出足够的努力”。</p><p>👉【作者之语】<br>解决问题的方法永远都在问题之外。</p><p>👉【樊老师之语】<br>只有真正了解自己的心态，你才能更好地驾驶人生航船。听了这本书，你将从4个维度了解自己的心态，并收获一套科学改善心态的行动方案，从而更积极地面对工作与生活。</p><h3 id="今日简报-2"><a href="#今日简报-2" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月3日星期三</p><p>1、中泰将从3月起永久互免对方公民签证。</p><p>2、日本石川县能登地区7.6级地震已致48人死亡。</p><p>3、印度人口76年增长近11亿！印度成世界人口第一大国。</p><p>4、东航第四架C919大型客机交付入列。</p><p>5、日媒：客机与海上保安厅飞机相撞，保安厅飞机死亡人数增至5人。</p><p>6、韩媒：袭击李在明嫌疑人为60多岁男性，供述中承认有谋杀意图。</p><p>7、乌称基辅等地遭袭，俄方对此无回应。</p><p>8、因反垄断未获批，百度宣布终止收购YY直播。</p><p>9、驻美大使馆回应中国留学生疑似遭绑架：系遭遇“虚拟绑架”电信诈骗。</p><p>10、强震造成日本多个核电站燃料池水溢出。</p><p>11、广东一地公示8名躺平休闲人员，当事人称不便回应，专家称躺平者标准并无法律支撑：应谨慎。</p><p>12、古茗向港交所提交上市申请。</p><p>✨人生中最大的发现就是发现自己。——《心态》</p><h3 id="名人语录-2"><a href="#名人语录-2" class="headerlink" title="名人语录"></a>名人语录</h3><p>生活就像一面镜子，你对它微笑，它便会为你绽放欢颜。<br>从今天开始，用乐观代替沮丧，用行动代替懒散。<br>调节好自己的心态，成功和幸福才会不请自来。</p><p>你人生的起点并不是那么重要<br>重要的是你最后抵达了哪里<br>———— 巴菲特</p><h3 id="漫画国学"><a href="#漫画国学" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.03漫画国学👣</p><p>🔹生活小困扰：对新事物，我总是不敢尝试，怎么办?    </p><p>💫国学大智慧：司马牛问君子。子曰：“君子不忧不惧。”</p><p>🔸译文诠释：司马牛问怎样才是君子。孔子说：“君子不忧愁，不恐惧。”</p><p>✨迁移阅读：儒家学说的终极目标不是建立道德规范来束缚人，而是提供一种让人美好、艺术地生活的指南。</p><p>📔推荐书目：《孔子：人能弘道》</p><p><img src="../assets/image-20240103143404860.png" alt="image-20240103143404860"></p><hr><h2 id="1-4日周四"><a href="#1-4日周四" class="headerlink" title="1.4日周四"></a>1.4日周四</h2><h3 id="英语角-3"><a href="#英语角-3" class="headerlink" title="英语角"></a>英语角</h3><p>It’s no use going back to yesterday, because I was a different person then.<br>回到昨天毫无用处，因为今天的我已和过去有所不同。</p><h3 id="今日晨读-2"><a href="#今日晨读-2" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.01.04晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>只要人活得高兴，穷也不怕。<br>——来自帆书·李蕾讲经典《活着（上）》</p><p>👉【精彩选段】<br>我看着那条弯曲着通向城里的小路，听不到我儿子赤脚跑来的声音，月光照在路上，像是撒满了盐。</p><p>👉【作者之语】<br>人是为活着本身而活着，而不是为了活着之外的任何事物所活着。</p><p>👉【李蕾老师之语】<br>《活着》是一本真实的书，同时也很坚韧，它讲了一个人和他命运的复杂友情。不管遭遇什么，活着是不该也不能放弃的事情。</p><p><img src="../assets/image-20240105081848787.png" alt="image-20240105081848787"></p><h3 id="今日简报-3"><a href="#今日简报-3" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月4日星期四</p><p>1、日本地震死亡升至73人。</p><p>2、成品油价迎来2024年首次上调，加满一箱92号汽油将多花8元。</p><p>3、科技部发文规范AI使用，禁用生成式人工智能直接生成申报材料。</p><p>4、12306客户端推出购票需求预填及起售提醒订阅。</p><p>5、日本撞机前通话记录公布，日本海上保安厅飞机未被允许进跑道。</p><p>6、俄罗斯钻石不再被允许进入欧盟。</p><p>7、美国国债总额首次达到34万亿美元。</p><p>8、伊朗克尔曼市发生爆炸致多人死伤。</p><p>9、无人机未经实名登记飞行最高罚2万。</p><p>10、哈佛历史上首位黑人校长辞职，卷入抄袭丑闻和校园反犹争议。</p><p>11、303名抖音主播被关闭收礼物权限，抖音直播新版健康分处罚正式生效。</p><p>12、哈尔滨市政府提醒宾馆酒店珍惜“出圈”机遇：不盲目调整价格。</p><p>✨只要人活得高兴，穷也不怕。——《活着（上）》</p><h3 id="名人语录-3"><a href="#名人语录-3" class="headerlink" title="名人语录"></a>名人语录</h3><p>越是优秀的人，越是知道这世间没有任何捷径可走。<br>把每一件事情坚持做、用心做，才可能厚积薄发，收获意料之外的惊喜！</p><h3 id="漫画国学-1"><a href="#漫画国学-1" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.04漫画国学👣</p><p>🔸生活小困扰：能说会道和踏实肯干，究竟哪个比较重要？</p><p>💫国学大智慧：子贡问君子。子曰：“先行其言而后从之。”</p><p>🔅译文诠释：子贡问怎样才能做一个君子。孔子说：“对于自己要说的话，要先实行了，然后再说出来，这样就够算是一个君子了。”</p><p>✨迁移阅读：曾国藩之于后人最大的意义是，他以自己的实践证明，一个中人，通过“陶冶变化”，可以成为超人。</p><p>📔推荐书目：《曾国藩的正面和侧面》</p><p><img src="../assets/image-20240105082045243.png" alt="image-20240105082045243"></p><h3 id="日知日进-2"><a href="#日知日进-2" class="headerlink" title="日知日进"></a>日知日进</h3><p>📯你是否也有这样的情况？<br>想和家人好好说话，<br>可一开口就管不住脾气。<br>想换一份工作，<br>但一直没有勇气辞职。<br>在一段感情里内耗许久，<br>却又舍不得放手。</p><p>🌟为什么我们都清楚需要改变，却没法做到呢？<br>如果你也有类似这样的困境，<br>想要改变却不知道如何开始。<br>不妨读读这本《5%的改变》。<br>试着从5%的改变里，<br>去感受自己的变化，<br>去发现生活的不同。</p><p>萧伯纳曾说：很多时候，完成比完美更重要，在一次次完成中迭代，就是进步。</p><p>苛求完美，像是一场与自己的无穷斗争，只会搞得精疲力竭。而真正的蜕变，是以轻松的心态去接纳原来的我，以5%的改变去迎接崭新的我</p><h2 id="1-5日周五"><a href="#1-5日周五" class="headerlink" title="1.5日周五"></a>1.5日周五</h2><h3 id="英语角-4"><a href="#英语角-4" class="headerlink" title="英语角"></a>英语角</h3><p>Nothing is impossible, the word itself says “I’m possible”!<br>没有不可能，连“不可能”这个词自己都说：“不，可能！”</p><h3 id="今日晨读-3"><a href="#今日晨读-3" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.1.5晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>我们不能仅仅依靠善心和意志力就成为更好的人。<br>——来自帆书·非凡精读《善恶之源》</p><p>👉【精彩选段】<br>由共情直接激发出的助人行为，往往会有“只见树木不见森林”的效果，它把我们的注意力引导到一个个案上，但往往没法兼顾公平。</p><p>👉【作者之语】<br>我们创造出来的社会环境，可以把一个只拥有部分道德意识的婴儿，变成一个拥有强烈道德感的成年人。</p><p>👉【主讲老师之语】<br>这本书帮助我们认识到天生的道德直觉并不完美。道德本能很强大，我们需要好好理解它。个人的道德观、社会的道德规范都是从这些道德本能里发芽生长出来的。</p><h3 id="今日简报-4"><a href="#今日简报-4" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月5日星期五</p><p>1、2024年购买新能源车免征购置税。</p><p>2、我国自研电动飞机成功首飞：最大平飞速度218km/h，航程1100km。</p><p>3、外交部回应中国留学生被美方强制遣返：敦促美方停止打压限制中国留学生。</p><p>4、我国将开展“信号升格”专项行动。</p><p>5、多家日媒：日本石川县震后失联人数已增至179人。</p><p>6、首个石墨烯制成的功能半导体问世，天津大学团队承担主要研究与攻关。</p><p>7、圆通、韵达和京东物流违规寄递烟花被约谈。</p><p>8、伊朗克尔曼省发生两起爆炸已致84人死亡。</p><p>9、四部门：大力推广智能有序充电设施，原则上新建充电桩统一采用智能有序充电桩。</p><p>10、美国多个州议会大厦收到炸弹威胁。</p><p>11、高校严控重复设置过热专业，2035年全国或有近200万教师过剩。</p><p>12、湖南将保障支付农民工工资切实纳入对市州人民政府考核。</p><p>✨我们不能仅仅依靠善心和意志力就成为更好的人。——《善恶之源》</p><h3 id="名人语录-4"><a href="#名人语录-4" class="headerlink" title="名人语录"></a>名人语录</h3><p>总是充满期待的人，每一天都是全新的开始~</p><p>山不却垒土之功，故能成其高；海不避涓涓细流，故能成其大。这世上从来就没有一步登天的神话，有的只是日积月累、滴水穿石。</p><h3 id="漫画国学-2"><a href="#漫画国学-2" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.05漫画国学👣</p><p>🔸生活小困扰：我们如何获得他人的尊敬?</p><p>💫国学大智慧：君子以仁存心，以礼存心。</p><p>🔅译文诠释：君子心里存在着仁爱，存在着礼义。</p><p>✨迁移阅读：我们希望对方以何种方式对待我们，我们也就必须以同样的方式对待他们——己所不欲，亦施于人。</p><p>📔推荐书目：《人性中的善良天使》</p><p><img src="../assets/image-20240105111556361.png" alt="image-20240105111556361"></p><h3 id="日知日进-3"><a href="#日知日进-3" class="headerlink" title="日知日进"></a>日知日进</h3><blockquote><p>糟糕的坚持 也是一种坚持，也要比轻易的放弃要好</p><p>两次放弃坚持的东西   就是一种坏习惯的开始了</p><p>糟糕的坚持  也能够帮你保持一种身份，帮助你对抗这种世界外部的无序</p><blockquote><p>跑5分钟 走一圈 因为今天实在是不想动，那就溜一圈，也比放弃要好，你要对自己宽容一点，</p><p>至少你现在所做的一切还能帮助你保持一种身份</p><p>我们宁肯要糟糕的坚持，也要对抗这种轻易的放弃</p></blockquote></blockquote><p>🌻你的孩子在外遇到难题了，<br>你会是他第一个求助的人吗？<br>你的孩子犯错了，<br>他会有勇气跟你说真话吗？<br>你的孩子会主动跟你袒露自己的脆弱吗？</p><p>:sun_with_face:前几天，看到这样一则故事：<br>“有个7岁的小男孩和朋友打篮球时摔倒了，<br>把手给摔折了。他害怕被父母骂，<br>生生地忍着疼痛，没跟大人说。”<br>直到后面忍不了了，被父母发现了，<br>才送往医院治疗。</p><p>:full_moon:到底是什么原因，<br>让孩子有事宁愿自己扛，<br>也不愿告诉父母？</p><p>📚分享内容｜简单人生<br>📆分享递增｜Day251<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.5</p><p>禅宗思想里，有一个“看山理论”。它将人生分为了三重境界：看山是山，看山不是山，看山还是山。第一重境界的人，心思单纯，没啥城府，看见什么便是什么；第二种境界的人，经历了世事的摧残，觉得生活复杂，看什么都是雾里看花；而最高境界的人，饱经沧桑后，依旧能淡看人生起落，把一切看得简单而自然。人生最好的状态，是在复杂的世界里，做一个简单的人。的确，人生短短三万天，学会看轻得失，看淡纷扰，将复杂的生活简单过，就是最好的处世之道！</p><h2 id="1-6日周六"><a href="#1-6日周六" class="headerlink" title="1.6日周六"></a>1.6日周六</h2><h3 id="英语角-5"><a href="#英语角-5" class="headerlink" title="英语角"></a>英语角</h3><p>would die for you.<br>But I won’t live for you.<br>我愿意为你赴死，但不会指望你而活。</p><h3 id="今日晨读-4"><a href="#今日晨读-4" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>☃️2024.1.6晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>除了吃饭，生命还有更重要的事!<br>        ——来自帆书·李蕾讲经典《海鸥乔纳森》</p><p>👉【精彩选段】<br>天堂不是一个地点，也不是一段时间。天堂是一种完美的状态。</p><p>👉【作者之语】<br>不要只相信你眼睛看到的东西。要用你的悟性去看，理解你已经知道的东西，然后你会发现飞翔的真理。</p><p>👉【李蕾老师之语】<br> 《海鸥乔纳森》是一部关于梦想与追求的寓言故事，故事里的海鸥乔纳森是个“异类”。对于他来说，飞翔远比吃饭更重要。因此在命运转折的时候，大多数海鸥选择安于现状，而他选择成为想要成为的自己。</p><h3 id="今日简报-5"><a href="#今日简报-5" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月6日，星期六</p><p>1、市场监管总局：特斯拉召回超160万辆存在安全隐患电动汽车。</p><p>2、央视曝河南距今2300年的赵长城被拦腰截断。</p><p>3、IS宣布对伊朗克尔曼市的连环爆炸案负责。</p><p>4、无锡全面施行经常居住地登记户口制度。</p><p>5、快递新规禁止未经同意擅用快递柜，未经同意代收货最高罚3万元。</p><p>6、韩方称朝鲜发射200多发海岸炮，外交部：正密切关注形势发展。</p><p>7、美媒：美军一架B-1轰炸机试图降落时坠毁，事发时正执行训练任务。</p><p>8、深圳拟出新规推进城中村改造，涉全市约40%建面。</p><p>9、阿根廷法院发布禁制令，暂停阿根廷总统米莱提出的涉及300多项经济改革中的劳工法令。</p><p>10、浙江将组织千名科学家任中小学校科学副校长。</p><p>11、福彩回应网曝福彩主持人提前播报中奖球号：视频系拼凑嫁接，已报警。</p><p>12、广西警方通报“女大学生摆摊卖糖葫芦被壮汉威胁”：行拘3人。</p><p>✨人生是一场无人可替代的修行，生活则是修炼心性的真经。——《给孩子一生的安全感》</p><h3 id="名人语录-5"><a href="#名人语录-5" class="headerlink" title="名人语录"></a>名人语录</h3><p>有时候，成长就如种子发芽一般，需要积蓄足够多的力量，才能冲破土壤。在人生的跑道上，一旦认准目标，就只管努力，总有一天会惊艳所有人。</p><h3 id="漫画国学-3"><a href="#漫画国学-3" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.06❄️漫画国学👣</p><p>『小寒已至』</p><p>❄️小寒，是踏雪赏梅的好时节，也标志着一年中最寒冷的日子就要来了。出门游玩的时候，小朋友们一定要做好保暖工作，日常也要多食用一些温热的食物来滋补身体。相传，古时的南京人对小寒颇为重视，有吃菜饭的习俗。但每家每户的菜饭并不相同，比如有用青菜、咸肉片、生姜粒和糯米一起煮的，咸香可口。大家可以在家里邀请爸爸妈妈一起来制作哦!</p><p>📔小寒谚语:<br>🔸小寒暖，立春雪。<br>🔸小寒不寒，清明泥潭。<br>🔸小寒蒙蒙雨，雨水还冻秧。<br>🔸小寒一场白，来年收小麦。<br>🔸小寒胜大寒，常见不稀罕。<br>🔸小寒暖立春雪，小寒不寒立春雪。<br>🔸小寒时处二三九，天寒地冻冷到抖。<br>🔸小寒不寒大寒寒，大寒不寒倒春寒。</p><p><img src="../assets/image-20240106210118345.png" alt="image-20240106210118345"></p><h2 id="1-7日周天"><a href="#1-7日周天" class="headerlink" title="1.7日周天"></a>1.7日周天</h2><h3 id="英语角-6"><a href="#英语角-6" class="headerlink" title="英语角"></a>英语角</h3><p>Years fly by, but the heart stays in the same place.<br>时光飞逝，我心依旧。</p><h3 id="今日晨读-5"><a href="#今日晨读-5" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.1.7晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>生活本有千万可能，别让过去束缚了你。<br>   ——来自帆书·非凡精读《拥抱可能》</p><p>👉【精彩选段】<br>被动就是让别人替你做决定，争强好胜就是为别人做决定，坚定自信就是为自己做决定。</p><p>👉【作者之语】<br>生命中，所有能让你惊喜若狂的事情都来自于内心。没有人能把你心里的东西拿走。</p><p>👉【推荐之语】<br>《拥抱可能》是埃格尔对自己一生的回望，记录了坚韧、治愈、有尊严地生存、充满勇气地拥抱自由和幸福。</p><h3 id="今日简报-6"><a href="#今日简报-6" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月7日，星期日</p><p>1、首张房票已发放！广州打响一线城市房票安置“第一枪”。</p><p>2、境外机构免费送航空爱好者设备窃取我国信息。</p><p>3、江苏新发现1例小p血型，目前我国仅有9人。</p><p>4、世界最大冰雪主题乐园！哈尔滨冰雪大世界拿下吉尼斯世界纪录。</p><p>5、不分节假日、不限身份，山西21所高校向社会开放。</p><p>6、学历入户放宽至大专！成都户籍新政来了，2月20日起施行。</p><p>7、中美联合研制全球首个由石墨烯材料制成的功能性半导体。</p><p>8、上海与嘉兴实现公共交通乘车码互联互通，后续将扩大至长三角11城。</p><p>9、美国阿拉斯加航空公司宣布：暂时停飞65架波音737 MAX 9飞机。</p><p>10、日本羽田机场撞机事故影响波及逾15万人，跑道预计8日重启。</p><p>11、美邦服饰“二代”接任7年亏损超32亿，创始人重新被提名董事。</p><p>12、英国多地洪水，气象局连发三百多个预警。</p><h3 id="名人语录-6"><a href="#名人语录-6" class="headerlink" title="名人语录"></a>名人语录</h3><p>✨大脑的疲劳和压力都来自过去和未来：对过去的事情心有不甘，对未来的事情充满不安。——《高效休息法》</p><p>生命是一棵长满可能的树 ————米兰·昆德拉</p><p>每一天都是新的旅程，<br>不需要去改变任何人，<br>只需要不断的提升自己，<br>自然有同频人与你同行。</p><h3 id="漫画国学-4"><a href="#漫画国学-4" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.07漫画国学👣</p><p>📖一周复习：</p><pre><code>🔸人不知，而不愠，不亦君子乎?     ——《论语·学而》🔸司马牛问君子。子曰:“君子不忧不惧。”     ——《论语·颜渊》🔸子贡问君子。子曰:“先行其言而后从之。”    ——《论语·为政》🔸君子以仁存心，以礼存心。     ——《孟子·离娄下》</code></pre><p><img src="../assets/image-20240108193200012.png" alt="image-20240108193200012"></p><p>💞美好的亲子共读时光开始啦<br>🎤今日共读内容<br>    大禹治水</p><p>很久很久以前，<br>洪水经常泛滥。<br>大水淹没了田地，<br>冲毁了房屋，<br>毒蛇猛兽到处伤害百姓和牲畜，<br>人们的生活痛苦极了。<br>洪水给百姓带来了无数的灾难，<br>必须治好它。</p><p>当时，<br>一个名叫鲧(gǔn)的人领着大家治水。<br>他只知道筑坝挡水，<br>九年过去了，<br>洪水仍然没有消退。</p><p>他的儿子禹继续治水。<br>禹离开了家乡，<br>一去就是十三年。<br>这十三年里，<br>他到处奔走，<br>曾经多次路过自己家门口。<br>可是他认为治水要紧，<br>一次也没有走进家门看一看。</p><p>禹吸取了鲧(gǔn)治水失败的教训，<br>采用疏导的办法治水。<br>他和大家一起，<br>疏通了很多河道，<br>让洪水通过河道，<br>最后流到大海里去。<br>洪水终于退了，<br>毒蛇猛兽被驱赶走了，<br>人们把家搬了回来。</p><p>大家在被水淹过的土地上耕种，<br>农业生产渐渐恢复了，<br>百姓安居乐业，<br>重新过上了幸福的生活。<br>由于禹为人们作出了很大的贡献，<br>大家便把他称为大禹。</p><h2 id="1-8日周一"><a href="#1-8日周一" class="headerlink" title="1.8日周一"></a>1.8日周一</h2><h3 id="英语角-7"><a href="#英语角-7" class="headerlink" title="英语角"></a>英语角</h3><p>May we be together till forever.<br>朝暮与岁月并往，愿你我共至光年。</p><h3 id="今日晨读-6"><a href="#今日晨读-6" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.1.8晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>人生在世，做事只求尽心，而成败不必强求。<br>——来自帆书《战安庆》</p><p>👉【精彩选段】<br>所谓中年危机，无非就是人到中年因事业无成而产生的焦虑和惶恐，一旦跳出功名利禄的圈子，从宇宙的维度来看待世事，以恬淡冲融的态度来重新看待人生，危机带来的抑郁也就消泯了。</p><p>👉【作者之语】<br>乱极时站得定，才是有用之学。</p><p>👉【樊老师之语】<br>打开本期新书，为你还原曾国藩那段最艰难的时光，看他如何用谦虚自省的心态，战胜中年危机，走向完满的人生境界。</p><h3 id="今日简报-7"><a href="#今日简报-7" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月8日   星期一</p><p>1、教育部：网传义务教育教学改革实验区“取消中考”等说法不实。</p><p>2、上海税务：个人通过网络买卖虚拟货币需要缴纳个人所得税。</p><p>3、外交部：中方决定对5家美国军工企业实施制裁。</p><p>4、广西河池回礼东北老铁：16个景区免首道门票。</p><p>5、今年首批银行业罚单开出，3家银行合计被罚千万！去年千万以上罚单超20张。</p><p>6、陕西省定边县一住户发生煤炭炉取暖中毒，致4人死亡。</p><p>7、预计最快10天可找到马航MH370！最新研究指向未经搜索区域。</p><p>8、日本气象厅：1日以来能登地区已发生1000余次地震。</p><p>9、腾讯微信团队致歉：私密朋友圈bug已彻底修复。</p><p>10、流感病例数激增，西班牙多地医疗系统超负荷！部分地区重启口罩强制令。</p><p>11、OpenAI和微软遭集体诉讼，被指控“窃取”他人作品训练AI模型。</p><p>12、中国在极地布放首个生态潜标。</p><p>✨人生在世，做事只求尽心，而成败不必强求。</p><h3 id="名人语录-7"><a href="#名人语录-7" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-5"><a href="#漫画国学-5" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.08漫画国学👣</p><p>🔸生活小困扰：别人总是看不到我的优点，我该怎么办?</p><p>💫国学大智慧：君子病无能焉，不病人之不己知也。</p><p>🔅译文诠释：君子只会惭愧自己没有能力，不怨恨别人不了解自己。</p><p>✨迁移阅读：问题不能在产生问题的维度得到解决，升一个维度，才能解决问题。</p><p>📔推荐书目：《升维》</p><p><img src="../assets/image-20240108193443684.png" alt="image-20240108193443684"></p><h2 id="1-9日周二"><a href="#1-9日周二" class="headerlink" title="1.9日周二"></a>1.9日周二</h2><h3 id="英语角-8"><a href="#英语角-8" class="headerlink" title="英语角"></a>英语角</h3><p>Through the crowd to you.<br>穿越人群，奔赴于你。</p><h3 id="今日晨读-7"><a href="#今日晨读-7" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.1.9晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>——来自帆书《减压生活》</p><p>👉【精彩选段】<br>无论是我们自己，还是身边的人，在改变自己的时候，都要学会借助外力和改变认知。改变，并没有使我们失去什么，而是获得了一个更好的自己。</p><p>👉【作者之语】<br>健康的四大基石：乐观的心情、均衡的饮食、充足的睡眠、适度的运动。</p><p>👉【樊老师之语】<br>本书作者从神经外科医生的视角提出减压的根本是通过改变生活习惯，来干预和调节自主神经，给神经“松松绑”，让你由内而外，击退压力。更从四大维度提出十余种减压实操法，让你告别疲劳、焦虑，重新回归愉悦的生活。</p><h3 id="今日简报-8"><a href="#今日简报-8" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月9日   星期二</p><p>1、多国停飞波音737MAX9型客机。</p><p>2、广西龙门大桥顺利合龙。</p><p>3、贵州村超新赛季打响！村超带动旅游综合收入近60亿元。</p><p>4、东航C919机型9日起开始执飞京沪航线。</p><p>5、深圳欢乐谷过山车碰撞事故调查报告公布，9人被追责问责。</p><p>6、字节与腾讯谈判出售多款游戏。</p><p>7、日本一核电站因地震漏约1.98万升油入海。</p><p>8、黎巴嫩真主党一高级指挥官在黎南部被以方打死。</p><p>9、我国破获一起英国秘密情报局（MI6）间谍案，MI6利用第三国人员从事对华间谍活动。</p><p>10、巴基斯坦发生爆炸袭击事件，致5名警察死亡。</p><p>11、广州“房票安置”政策细化：广州房票全市通用可转让。</p><p>12、北京新增设集成电路专业职称，放宽个人企业参评限制。</p><p>✨整理环境也是在整理思绪，一个人的生活环境其实是他内心的显现。<br>  ——来帆书APP 听《减压生活》</p><h3 id="名人语录-8"><a href="#名人语录-8" class="headerlink" title="名人语录"></a>名人语录</h3><p>无论年纪，只有不断提升自己，眼界才会愈加开阔，也才能更有底气、更加从容地面对人生。<br>保持学习、持续向上，就是我们给生活最好的回馈</p><p>与其千方百计去改变别人，让自己陷入麻烦，不如专注于对方的优点，放过别人，也放过自己。</p><p>德国哲学家康德说：请接受任何一个独立灵魂的存在，哪怕有些你并不认可，但也尽可能试着去理解。</p><p>允许对方有不同于我们的喜恶，生活才不会只有一种颜色。</p><h3 id="漫画国学-6"><a href="#漫画国学-6" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.09漫画国学👣</p><p>🔸生活小困扰：我有些偏科，只喜欢学数学，这样不可以吗?</p><p>💫国学大智慧：君子不器。</p><p>🔅译文诠释：君子不能像器皿一样(只有一种用途)。</p><p>✨迁移阅读：风会熄灭蜡烛，却会使火越燃越旺。对随机性、不定性和混沌也是一样：你要利用它们，而不是躲避它们。你要成为风，渴望得到风的吹拂。</p><p>📔推荐书目：《反脆弱》</p><p><img src="../assets/image-20240109111933885.png" alt="image-20240109111933885"></p><h2 id="1-10日周二"><a href="#1-10日周二" class="headerlink" title="1.10日周二"></a>1.10日周二</h2><h3 id="英语角-9"><a href="#英语角-9" class="headerlink" title="英语角"></a>英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-8"><a href="#今日晨读-8" class="headerlink" title="今日晨读"></a>今日晨读</h3><h3 id="今日简报-9"><a href="#今日简报-9" class="headerlink" title="今日简报"></a>今日简报</h3><h3 id="名人语录-9"><a href="#名人语录-9" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-7"><a href="#漫画国学-7" class="headerlink" title="漫画国学"></a>漫画国学</h3><h2 id="1-11日周二"><a href="#1-11日周二" class="headerlink" title="1.11日周二"></a>1.11日周二</h2><h3 id="英语角-10"><a href="#英语角-10" class="headerlink" title="英语角"></a>英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-9"><a href="#今日晨读-9" class="headerlink" title="今日晨读"></a>今日晨读</h3><h3 id="今日简报-10"><a href="#今日简报-10" class="headerlink" title="今日简报"></a>今日简报</h3><h3 id="名人语录-10"><a href="#名人语录-10" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-8"><a href="#漫画国学-8" class="headerlink" title="漫画国学"></a>漫画国学</h3><h2 id="1-12日周二"><a href="#1-12日周二" class="headerlink" title="1.12日周二"></a>1.12日周二</h2><h3 id="英语角-11"><a href="#英语角-11" class="headerlink" title="英语角"></a>英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-10"><a href="#今日晨读-10" class="headerlink" title="今日晨读"></a>今日晨读</h3><h3 id="今日简报-11"><a href="#今日简报-11" class="headerlink" title="今日简报"></a>今日简报</h3><h3 id="名人语录-11"><a href="#名人语录-11" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-9"><a href="#漫画国学-9" class="headerlink" title="漫画国学"></a>漫画国学</h3><h2 id="1-13日周二"><a href="#1-13日周二" class="headerlink" title="1.13日周二"></a>1.13日周二</h2><h3 id="英语角-12"><a href="#英语角-12" class="headerlink" title="英语角"></a>英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-11"><a href="#今日晨读-11" class="headerlink" title="今日晨读"></a>今日晨读</h3><h3 id="今日简报-12"><a href="#今日简报-12" class="headerlink" title="今日简报"></a>今日简报</h3><h3 id="名人语录-12"><a href="#名人语录-12" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-10"><a href="#漫画国学-10" class="headerlink" title="漫画国学"></a>漫画国学</h3><h2 id="1-14日周二"><a href="#1-14日周二" class="headerlink" title="1.14日周二"></a>1.14日周二</h2><h3 id="英语角-13"><a href="#英语角-13" class="headerlink" title="英语角"></a>英语角</h3><p>Live up to the tender years<br>不负人间温柔岁月。</p><h3 id="今日晨读-12"><a href="#今日晨读-12" class="headerlink" title="今日晨读"></a>今日晨读</h3><p>❄️2024.1.14晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>四大运动指南 ，教你如何正确锻炼。<br>          ——来自帆书·非凡精读《锻炼》</p><p>👉【精彩选段】<br>人体哲学与人生哲学同样重要。在健康面前，每个人都不是一座孤岛，我们良好的健康状况是可以相互影响的。</p><p>👉【作者之语】<br>不管是快走、短跑，还是跳舞，挑一个适合自己、自己喜欢的动起来，我们的身体将会因此受益无穷。</p><p>👉【馆长推荐】<br>本书从人类学和进化学的角度探讨我们的身体，为我们解答关于锻炼的种种问题。</p><h3 id="今日简报-13"><a href="#今日简报-13" class="headerlink" title="今日简报"></a>今日简报</h3><p>今日简报</p><p>1月14日    星期日</p><p>1、发改委推出第七批重大外资项目，计划总投资超150亿美元。</p><p>2、六部门明确：进一步加强适老化无障碍出行服务，优先给老年人安排下铺。</p><p>3、气象台：北方大部地区将迎来大范围雨雪天气，局地降温可达15℃。</p><p>4、多地探索教师退出机制，以转变岗位、待岗培训、解聘为主要途径。</p><p>5、全球新冠加速蔓延！感染科专家：春运将至护好“脆弱人群”。</p><p>6、2024年台湾地区领导人和民意代表选举结果揭晓：民进党候选人赖清德、萧美琴当选台湾地区正副领导人。</p><p>7、上海放松限购：青浦、奉贤两区取消单身限购，社保3年即可买房。</p><p>8、河南平顶山煤矿事故已确认10人遇难，6人失联，其余人员全部脱险。</p><p>9、2023中国城市海外网络传播力前三名为上海、北京、杭州。</p><p>10、2.89万亿美元！微软超越苹果重新成为全球市值最高的公司。</p><p>11、加沙地带逾2.3万名巴勒斯坦人死亡，190万平民流离失所。</p><p>12、美军再次空袭胡塞武装，拜登政府考虑重新将也门胡塞武装列为恐怖组织。</p><p>✨审视自己的想法，而不是根据自己的想法来看问题。<br>  ——来帆书APP 听《跳出头脑，融入生活》</p><h3 id="名人语录-13"><a href="#名人语录-13" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-11"><a href="#漫画国学-11" class="headerlink" title="漫画国学"></a>漫画国学</h3><p>🗓️2024.01.14☀漫画国学👣</p><p>🔸国学小课堂：孔子推崇的同时代君子有哪些?</p><pre><code>在周朝是老子，在卫国是蘧伯玉，在齐国是晏子，在楚国是老莱子，在郑国是子产，在鲁国是孟公绰。</code></pre><p>✨迁移阅读：看2000年前的圣人如何练成自我。</p><p>📔推荐书目：《孔子如来》</p><p><img src="../assets/image-20240114172254529.png" alt="image-20240114172254529"></p><h3 id="故事角"><a href="#故事角" class="headerlink" title="故事角"></a>故事角</h3><p>三国时期，<br>孙权送给曹操一只大象。<br>曹操带领大臣们和小儿子曹冲，<br>一同去看。</p><p>大象又高又大，<br>人站在它跟前，<br>只能够到它的肚子，<br>曹操对大臣们说：<br>“这只大象真是大，<br>可是到底有多重呢？<br>你们哪个有办法称一称？”<br>大臣们纷纷议论起来，<br>一个大臣说：<br>“只有造一杆很大很大的秤来称。”<br>而另一个大臣说：<br>“这可要造多大的一杆秤呀！<br>我看只有把它宰了，切成块儿称。”<br>大臣们想了许多办法,<br>但都行不通。</p><p>这时，<br>曹冲对曹操说:“我有一个办法。”<br>曹操就让他说来听听。<br>曹冲把办法说了，<br>曹操一听连连叫好。<br>对大臣们说:“走！咱们到河边看称象去!</p><p>众大臣跟随曹操来到河边，<br>河里停着一只大船，<br>曹冲叫人把大象牵到船上，<br>等船身稳定了，<br>在船舷上齐水面的地方，<br>刻了一条线。 </p><p>他再叫人把大象牵到岸上来，<br>把石头往船上装，<br>船身就一点儿一点儿往下沉。<br>等船身沉到刚才刻线的地方，<br>曹冲就叫人停止装石头。<br>大臣们起先还摸不清是怎么回事，<br>看到这里不由得赞叹不已：”好办法!好办法！”<br>现在谁都明白了，<br>只要把船里的石头分别称一下，<br>再把重量加起来，<br>就知道大象有多重了。</p><h2 id="1-13日周二-1"><a href="#1-13日周二-1" class="headerlink" title="1.13日周二"></a>1.13日周二</h2><h3 id="英语角-14"><a href="#英语角-14" class="headerlink" title="英语角"></a>英语角</h3><p>🌤𝒢𝑜𝑜𝒹 𝑀𝑜𝓇𝓃𝒾𝓃𝑔早安˗ˎˏˋ❸<br>The world is inferior to you.<br>世人万千，都不及你。</p><h3 id="今日晨读-13"><a href="#今日晨读-13" class="headerlink" title="今日晨读"></a>今日晨读</h3><h3 id="今日简报-14"><a href="#今日简报-14" class="headerlink" title="今日简报"></a>今日简报</h3><h3 id="名人语录-14"><a href="#名人语录-14" class="headerlink" title="名人语录"></a>名人语录</h3><h3 id="漫画国学-12"><a href="#漫画国学-12" class="headerlink" title="漫画国学"></a>漫画国学</h3><h1 id="独白记录"><a href="#独白记录" class="headerlink" title="独白记录"></a>独白记录</h1><h3 id="1-1日独白"><a href="#1-1日独白" class="headerlink" title="1.1日独白"></a>1.1日独白</h3><blockquote><p>从家里回来</p><p>累并快乐着</p><p>从姨家里吃晚饭 回家 回济南</p></blockquote><h3 id="1-2日独白"><a href="#1-2日独白" class="headerlink" title="1.2日独白"></a>1.2日独白</h3><p>成大事不在于力量的大小而在于能坚持多久？ 五到十年之内如果你坚持做一些事情 你的生活会发生巨变！ 就是我们日常生活当中的一点点 微小的改善 </p><p>这个让我想到了坚持晨读和夜读的力量</p><blockquote><p>早上果然不能看 各种的  会想一天</p><p>炮  </p></blockquote><p>正念冥想应对胡思乱想</p><p>把想法当成天空的云，看着它，来来去去，时而乌云密布，时而蓬松轻柔</p><h3 id="1-3日独白"><a href="#1-3日独白" class="headerlink" title="1.3日独白"></a>1.3日独白</h3><blockquote><p>关于习惯</p></blockquote><p><img src="../assets/image-20240103190120380.png" alt="image-20240103190120380"></p><blockquote><p>关于产品价值</p></blockquote><p>情绪，是下一个赛道。<br>【原文引用】<br>一个产品有三种价值。一是资产价值，二是功能价值，三是情绪价值。<br>【核心概念转述】<br>每一个产品都可以挖掘它三个方面的价值。第一方面是资产价值，资产价值也就是这个产品的天然属性。二是功能价值，也就是这个产品所带来的实用性。三是情绪价值，也就是为了获得某种情绪或感受而愿意支付的价值。这个产品的增值价值、附加价值。<br>【个人体验】<br>我想用几个例子来进一步理解如何去看待一个产品的三种价值，比如说一杯奶茶。它的售价是15块钱，这就是它的资产价值，它的价值是由它的人工成本，它的配料来确定的。同时一杯奶茶，它可以解渴，也可以饱腹，可以当早餐，也可以做下午茶，这就是它的功能价值。那么如果一杯奶茶是在冬天的清晨，由你的男朋友专门为你点送的，他就具有了情绪价值，是温暖，是爱，是无价的。<br>这样我们就比较好理解，为什么所有迪士尼的产品都会有溢价，再比如说像去年的冰墩墩大火大卖，溢价不止十倍以上。这也是我们常说的，要讲好一个故事，把历史，把文化，把情感带进去，也是现在IP打造的核心内涵。<br>我们经常会强调客户的满意度，客户的满意度也分几种。第一种不达预期的，那一定是不满意，第二种符合预期。其实只能算接受、认可。只有超出预期给客户带来惊喜才能是真正的满意。海底捞的服务为什么被奉为教科书式的经典，同样是因为被赋予了超值的情绪价值。<br>从银行的角度来看，产品的价格越来越低。各种的减费让利尽可能多的惠及客户。 同时也通过各种活动，各种权益来吸引客户。价格战已经卷到不能再卷。而产品的功能属性不断的在开发迭代，基本上是属于同质化的。谁能为客户带来超预期的情感体验，才是能够吸引客户，稳定客户，获取最大份额的利器。<br>【行动指南】<br>直营经理作为线上经营的主力。从产品的价格、产品的功能上和线下的营业机构都是同质化的。但是又缺乏和客户面对面的交流，客户的信任感天然是有差距的。<br>直营经理的人设打造，不断注入有情感，有温度的服务。将是需要我们不断去思考，去尝试，改进，迭代的方向。<br><a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=255360&amp;tag_name=刘润&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#刘润 </a>/ <a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=240257&amp;tag_name=营销&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#营销 </a></p><blockquote><p>关于怎样选择 看书 和选择一个是否要做的东西</p><blockquote><p>跃迁知识体系+ 读书投资</p></blockquote><p>关于笔记</p><blockquote><p>探索下浮墨卡片 + 成段的笔记</p></blockquote></blockquote><p><img src="../assets/image-20240103190109567.png" alt="image-20240103190109567"></p><blockquote><p>精选出所有的点赞量大的文章 进行写文+ 应用+ 反馈【社区里边的】</p></blockquote><p>认同共读的三大原则：<br>1、成年人原则<br>2、自助餐原则<br>3、以自己为圆心<br>先让自己快乐起来，再去体验：善思会写的快乐！</p><p>长期主义，乐观主义</p><blockquote><p>关于金钱</p></blockquote><p>金钱心理学：财富的转机和钱给你带来什么？<a href="https://appty2gguua5789.h5.xiaoeknow.com/xe.community.community_service/v2/tagList?community_id=c_61415f12dbf6f_4AXgbUYr8758&amp;id=254525&amp;tag_name=金钱心理学&amp;S2V2aW5ZYW5=nIGF1dGhvcmVkIDE4IGhvdXJzIGFnbw">#金钱心理学 </a><br>个人转述：什么时候你会感觉到财富转机的出现，不一定是说你需要拥有多少钱？而是你开始拥有的能力或技能（不是体力或时间），能轻松地换取到满足你生存所需的钱，这代表财富转机的出现。而钱最终能给人带来什么？一是允许你能自由控制自己的时间，二是可以不用为了生存做讨厌的事，接触不喜欢的事和人。概括两字就是“选择”的底气。<br>个人体验：刚从体制内辞职的一两年，我会有一种失重感，这种失重是彷徨和迷茫，满腔热血但迷茫，深耕哪个行业哪个岗位？广告行业、商务拓展、政务公关？每个我都细细思量过，适不适合自己，有没有前途，容不容易被替代？直到在财富管理领域保险行业获取一批客户，以及自己的公司进入平稳后，知道自己的财富转机已经到来，我不再需要再找工作，不再面临中年危机。我的转型第一步目标已经实现。感谢过去支持信任的朋友们，感恩！<br>行动指南：基础的架构已经搭建，那就要去做有意义和扩大规模的事。要再持续输入和输出，在亲子教育方面，不断积累，输出一本客观、理性，分享当父母的书和课。在财务规划团队建设方面，2024要新上台阶，更加珍惜每个合作伙伴，当个好教练，而不仅仅是好人！</p><p>📚分享内容｜活出心花怒放的人生<br>📆分享递增｜Day249<br>✍🏻分享书友｜远古精灵<br>日期：2024.1.3</p><p>活出心花怒放的人生的有几点收获：<br>1 人需要从多个维度来认识自己的生活。健康、愉悦、良好的人际关系、一定程度的社会认可，这是很重要的。对于人来讲，这样的需要才是我们真正的需要。<br>2 你的行动反过来会影响你的想法。多则惑，简单的生活更容易快乐；闻香和深呼吸是调整情绪的最快方法！<br>3 找到工作的意义，除了收入之外，它的价值，对自己的帮助，对社会的帮助，好的团队关系。工作的幸福感，很重要。<br>4 爱需要你去感受才能够有。感谢亲人的付出，望着他们的眼睛，主动拥抱他们！<br>5 孩子是爱的结晶，爱的寄托。不是自我超越的工具。培养孩子的审美、创造和同理心。千万别和别的孩子比。<br>6 少而精才是好，比如孩子的玩具等。现在结婚恋爱的欲望下降，也是看到的美女太多了。<br>7 不开心的时候要立马做点事。嗅觉是让自己开心最快的方法，立竿见影。母亲闻孩子的味道也能感受幸福。副交感神经调动会让人开心，如深呼吸。<br>8 最后借用彭凯平教授的话“再好的方法你不学，你更不信，一点作用都没有的。”共勉！</p><h3 id="1-4日独白"><a href="#1-4日独白" class="headerlink" title="1.4日独白"></a>1.4日独白</h3><blockquote><p>跟广阔合作 完成项目可视化 和 对应的代码框架 【不得不说 跟人合作 做项目确实会比较高效】</p></blockquote><h3 id="1-5日独白"><a href="#1-5日独白" class="headerlink" title="1.5日独白"></a>1.5日独白</h3><blockquote><p>项目推进 做完ppt</p><p>零零散散 做了一天</p><p>也是不知道咋做，也是不知道怎样面对 </p></blockquote><h3 id="1-8日独白"><a href="#1-8日独白" class="headerlink" title="1.8日独白"></a>1.8日独白</h3><p><img src="../assets/image-20240108221205689.png" alt="image-20240108221205689"></p><h3 id="1-12日独白"><a href="#1-12日独白" class="headerlink" title="1.12日独白"></a>1.12日独白</h3><h4 id="积极主动，操作在我"><a href="#积极主动，操作在我" class="headerlink" title="积极主动，操作在我"></a>积极主动，操作在我</h4><p>在自己的选择和回应之间 不断抓住自己的自由度和空间，给与主动的、建设性的回应，并不断的为所有的选择做出积极的行动，不断的把自己的掌控权来进行扩大【方法论包括三个圈的应对方法】来不断的实现自己的人生。</p><p>同时观察并察觉自己的负面言论，不断的去停下来思考这些言论背后的原因和解决这些负面语言的可能性，并问自己能为这些可能性去做些什么，一旦问出这些话就能够不断的扩宽自己人生的可能性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;12月末尾&quot;&gt;&lt;a href=&quot;#12月末尾&quot; class=&quot;headerlink&quot; title=&quot;12月末尾&quot;&gt;&lt;/a&gt;12月末尾&lt;/h2&gt;&lt;p&gt;🗓️2023.12.30【日知日进】日历&lt;br&gt;📚日知书目：《人是如何学习的》&lt;/p&gt;
&lt;p&gt;✨日进思考：“普通</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解5基于图扩散卷积网络的二人动作语义识别</title>
    <link href="https://www.fomal.cc/posts/d937b92.html"/>
    <id>https://www.fomal.cc/posts/d937b92.html</id>
    <published>2023-12-28T09:44:39.000Z</published>
    <updated>2023-12-29T09:48:50.126Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Graph-Diffusion-Convolutional-Network-for-Skeleton-Based-Semantic-Recognition-of-Two-Person-Actions基于图扩散卷积网络的二人动作语义识别"><a href="#Graph-Diffusion-Convolutional-Network-for-Skeleton-Based-Semantic-Recognition-of-Two-Person-Actions基于图扩散卷积网络的二人动作语义识别" class="headerlink" title="Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition of Two-Person Actions基于图扩散卷积网络的二人动作语义识别"></a>Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition of Two-Person Actions基于图扩散卷积网络的二人动作语义识别</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​         Graph Convolutional Networks (GCNs) have successfully boosted skeleton-based human action recognition. However, existing GCN-based methods mostly cast the problem as separated person’s action recognition while ignoring the interaction between the action initiator and the action responder, especially for the fundamental two-person interactive action recognition. It is still challenging to effectively take into account the intrinsic local-global clues of the two-person activity. Additionally, message passing in GCN depends on adjacency matrix, but skeleton-based human action recognition methods tend to calculate the adjacency matrix with the fixed natural skeleton connectivity. It means that messages can only travel along a fixed path at different layers of the network or in different actions, which greatly reduces the flexibility of the network. To this end, we propose a novel graph diffusion convolutional network for skeleton based semantic recognition of two-person actions by embedding the graph diffusion into GCNs. At technical fronts, we dynamically construct the adjacency matrix based on practical action information, so that we can guide the message propagation in a more meaningful way. Simultaneously, we introduce the frame importance calculation module to conduct dynamic convolution, so that we can avoid the negative effect caused by the traditional convolution, wherein the shared weights may fail to capture key frames or be affected by noisy frames. Besides, we comprehensively leverage the multidimensional features related to joints’ local visual appearances, global spatial relationship and temporal coherency, and for different features, different metrics are designed to measure the similarity underlying the corresponding real physical law of the motions. Moreover, extensive experiments and comprehensive evaluations on four public large-scale datasets (NTU-RGB+D 60, NTU-RGB+D 120, Kinetics-Skeleton 400,SBU-Interaction) demonstrate that our method outperforms the state-of-the-art methods.</p><p><strong>摘要:</strong></p><p>图卷积网络（GCNs）已成功提升基于骨架的人体动作识别。然而，现有的基于GCN的方法大多将问题视为单独的人物动作识别，忽视了动作发起者和动作响应者之间的相互作用，尤其是对于基本的双人互动动作识别。有效考虑双人活动的固有局部-全局线索仍然具有挑战性。此外，GCN中的消息传递依赖于邻接矩阵，但基于骨架的人体动作识别方法往往使用固定的自然骨架连接性计算邻接矩阵。这意味着消息只能沿着网络的不同层或不同动作的固定路径传递，这极大地降低了网络的灵活性。为此，我们提出了一种新颖的图扩散卷积网络，通过将图扩散嵌入GCN，用于双人动作的语义识别。在技术上，我们基于实际动作信息动态构建邻接矩阵，以便以更有意义的方式引导消息传播。同时，我们引入了帧重要性计算模块进行动态卷积，以避免传统卷积引起的负面影响，其中共享权重可能无法捕捉关键帧或受到嘈杂帧的影响。此外，我们全面利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并针对不同特征设计不同的度量标准，以衡量相应运动的真实物理法则的相似性。此外，对四个公共大规模数据集（NTU-RGB+D 60、NTU-RGB+D 120、Kinetics-Skeleton 400、SBU-Interaction）进行了广泛的实验证明和全面的评估，证明我们的方法优于现有方法。</p><p><strong>总结:</strong></p><p>本研究针对基于骨架的人体动作识别中双人互动的问题，提出了一种新颖的图扩散卷积网络（GCNs）方法。与现有方法不同的是，该方法考虑了动作发起者和动作响应者之间的相互作用，并通过在GCNs中嵌入图扩散来更有效地利用双人活动的局部-全局线索。在技术上，研究动态构建邻接矩阵以引导有意义的消息传播，并引入帧重要性计算模块进行动态卷积，以避免传统卷积的负面影响。综合利用与关节的局部视觉外观、全局空间关系和时间一致性相关的多维特征，并设计不同的度量标准以测量相应运动的相似性。实验证明，在四个大规模数据集上，该方法在双人互动动作识别方面优于现有方法。</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p><strong>I. 引言</strong></p><p>近年来，随着计算机视觉的快速发展，人体动作识别引起了越来越多的关注，这对于自然人机交互、动态行为分析和安全监控至关重要。由于动作理解依赖于信息丰富的表示，视频中的人体动作识别可以利用各种特征，如颜色、深度和骨架。许多方法尝试通过从原始视频帧中提取时间上的视觉运动线索来分析动作。然而，这些方法很容易受到复杂的背景、间歇性遮挡、自遮挡、视角变化和计算成本的挑战。</p><p>随着3D深度摄像机和先进的人体姿势估计算法的快速发展，获取动态3D骨架序列变得方便。骨架基动作识别方法在过去十年中取得了显著的进展，具有对复杂背景和动态环境的灵活适应性，以及计算和存储的高效性。然而，现有的基于骨架的人体动作识别方法通常侧重于单个人的动作识别。在现实生活中，群体活动和多人互动对于理解人类行为更为重要。作为群体活动的基础，双人互动识别成为本文的技术焦点。</p><p>近年来，基于深度学习的方法取得了显著的性能提升。通常，循环神经网络（RNN）通常学习骨架序列的时间上下文，而卷积神经网络（CNN）更倾向于空间线索。然而，同时考虑时间和空间信息仍然很难。因此，提出了在骨架结构上进行深度学习，通过这种方式，网络可以同时集成时间和空间信息。例如，时空图卷积网络（ST-GCN）已经成为骨架识别的最先进骨干。ST-GCN连接具有自然空间连接性的人体关节，并在连续帧中建立相同关节之间的时间连接。其邻接矩阵是通过构建的图形建立的，然后分为三部分以捕获三种运动形式：向心、静态和离心。自那时以来，基于各种ST-GCN的工作提供了一种新的语义动作识别方向，并取得了令人印象深刻的成果，但仍存在一些局限性。</p><p>首先，ST-GCN及其变体的骨架结构是固定的，仅源自身体的自然连接性，在某些需要关节之间进行长距离消息传递的动作（例如“拍手”）时，信息将大大减弱。其次，对于双人行为，ST-GCN及其变体将分成两个独立的样本进行处理，忽略了人体关节之间的连接。第三，在时间上，上述方法中每一帧的关键性相同，这使得难以处理现实中多个连续动作和“小动作”的影响。第四，ST-GCN仅使用关节作为特征，其变体添加了骨骼和运动特征，但所有上述方法都使用了距离导出的高斯相似性作为相似性测量。这很难捕捉物理运动中的相对位置关系。第五，对于ST-GCN的改进往往引入了注意机制以提取用于识别的关键区域，或者设计复杂的网络以捕获关键信息。这些可学习的方法意味着更复杂的网络结构。</p><p>为了缓解上述问题，如图1所示，我们提出了一种用于基于骨架的双人动作语义识别的图扩散卷积网络。在本文中，引入图扩散的思想以建立任意两个关节之间的连接，使得图结构不仅连接同一身体的远距离关节，还连接两个互动人的关节。使用固定的图结构作为先验知识来引导网络学习，并使用通过图扩散获得的图结构作为学到的知识来引导网络快速收敛。通过将关键关节的轨迹叠加来计算帧的重要性，使得时间维度上的卷积具有动态性，并且动态搜索时间扩散的源帧可以促进识别关键动作并屏蔽小动作。对于连续动作，此操作可以自适应地对不同动作进行识别而无需预先分割，这接近于现实生活中对视频的语义理解。</p><p>同时，受2s-AGCN的启发，我们引入“骨”特征，为了与真实物理运动保持一致，我们考虑3D坐标的更高阶导数，如速度、加速度。通过大量实验证明，更高阶导数会引入噪声，而平滑的轨迹会大大降低区分度。因此，我们使用3D坐标、骨骼、速度作为三个特征，并使用余弦相似性以及高斯相似性来测量肢体的相对位置，以获得更具物理意义的结果。</p><p>值得一提的是，空间和时间均以手工方式设计，也就是说，我们没有引入复杂的网络，而是通过统计分析以数学方式提取关键信息。特别是，本文的显著贡献可总结如下：</p><ul><li>我们通过在基于自然人体骨架连接性构建的先验骨架图上进行扩散来提取扩散图结构。先验骨架图用于使网络收敛到全局最优解，而扩散图用于加速收敛和提高灵活性。</li><li>我们提出了基于图扩散的动态卷积，通过在远距离关节和不同身体之间建立直接连接，使GCN的消息传递超过直接邻居。</li><li>我们为时间扩散的源帧引入了动态搜索方案，既可以屏蔽由小动作引起的噪声，又可以促进对连续多个动作的自适应分割，这与现实生活中对视频的语义理解接近。</li><li>我们设计了不同的相似性度量来挖掘与不同特征相关的不同潜在空间，使特征融合更符合动作的相关物理规律。</li></ul><p><strong>引言总结：</strong></p><p>本文介绍了对于实际骨架动作识别的一个新的子领域，并提出了基于信息理论的原则，引入了实际图卷积网络（RW-GCNs）来在NTU-RGB-D-120数据集上实现94.16%的最新准确度，相较于ST-GCN基线具有3.02倍的低延迟。此外，RW-GCNs可以在基线实现的基础上实现10倍的低延迟，并且准确度仅降低了3.8%。对Northwestern UCLA数据集的评估显示，RW-GCNs可以实现90.4%的准确度，比基线ST-GCNs低32.5倍的延迟。这是尽管在验证和训练中存在空间关键点噪声。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，其在Northwestern UCLA数据集上比基线ST-GCNs低32.5倍的延迟，并在保持71.8%准确度的同时。这一切都是通过隐私感知和可扩展的边缘计算为中心的方法实现的，在这种方法中，每个节点的系统成本可以降低10倍，同时仍然保持在场景复杂性（15.6至5.5 ApS）范围内的吞吐量。这项工作标志着实际骨架动作识别子领域的开始。通过设计RW-GCNs，本文希望促进以前不可行的新边缘计算应用的设计和创建。然而，我们认为这个新兴领域仍然存在许多挑战，如时间变化、环境特定场景动态和进一步的应用特定约束。</p><h2 id="II-RELATED-WORKS"><a href="#II-RELATED-WORKS" class="headerlink" title="II. RELATED WORKS"></a>II. RELATED WORKS</h2><h3 id="A-Skeleton-Based-Action-Recognition"><a href="#A-Skeleton-Based-Action-Recognition" class="headerlink" title="A. Skeleton-Based Action Recognition"></a>A. Skeleton-Based Action Recognition</h3><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别</strong></p><p>早期的基于骨架的人体动作识别方法通常采用手工设计的特征来捕捉关节的运动规律[12]，然而，这种手工设计的特征往往不够丰富，因此结果通常令人不满意。实际上，动作识别强烈依赖于人体的表示和分析[13]。随着深度神经网络的发展[14]，数据驱动的方法引起了广泛关注。例如，基于RNN的方法使用坐标作为序列数据来表示人体关节[15]。长短时记忆网络由于其在建模序列数据的依赖性和动态性方面的优势，展现出良好的性能[16][17]，用于提取长期语义信息[18]。将3D坐标映射到基于CNN的伪图像上[19]，而伪图像的第三维用于表示时间动态。</p><p>图卷积的引入使得动作识别更加专注于基于骨架的方法，从而不受复杂背景的干扰，并且极大地提高了识别准确性。2s-AGCN[8]提出了一种两流自适应图卷积网络，并通过注意模块以数据驱动的方式学习新的拓扑结构，增强了网络的灵活性。AS-GCN[7]采用编码器-解码器结构捕获特定于动作的潜在依赖性，建立了无自然连接性的关节之间的连接，形成广义骨架图以获取更丰富的信息。ST-TR[5]引入Transformer[20]处理长时依赖关系，同时在时空中引入自注意机制以提取关键信息。FDGCN[21]设计了一个聚焦扩散图卷积网络，充分探索时空上下文。MS-G3D[22]提出了一个强大的特征提取器，捕获了先前未被注意到的多尺度时空特征。DC-GCN[10]构建了一个解耦图卷积以增强骨架动作识别的图建模能力，并引入了注意引导的DropGraph模块，有效缓解了图卷积网络中的过拟合问题。AGC-LSTM[23]提出了一个增强型注意图卷积LSTM网络，不仅可以捕获空间配置和时间动态中的判别特征，还可以探索时空域之间的共同关系。STGR[24]设计了一种新颖的时空图路由方案，用于骨架动作识别，自适应地学习物理上相距较远的骨架关节的高阶连接关系。</p><p>人与人之间的相互动作识别是人体活动分析的重要研究分支[25]，但由于相互遮挡的原因仍然具有挑战性。在人体动作识别中，如果将动作划分为单个人进行识别，将缺乏对人体各关节之间关联的关注。许多文献已经证明，单个人体的远端关节对于不同动作具有不同的重要性。同样，在两人互动中，需要更多关注两者关节之间的关系。关节的时间运动轨迹应是区分相似动作的关键因素，而大多数先前的方法未能充分利用视频段之间的时空关系。基于图的方法可以以灵活的方式处理骨架数据，以探索骨架关节之间的关系。时空图卷积网络（ST-GCN）[4]是第一个使用图卷积进行动作识别的模型，由于骨架数据的简单性和高效性，ST-GCN实现了最先进的准确性。因此，在本文中，我们采用ST-GCN作为基准模型。</p><p><strong>A. Skeleton-Based Action Recognition 骨架基础动作识别总结</strong></p><p>在骨架基础动作识别领域，传统方法采用手工设计的特征，但这些特征往往信息不足，效果不佳。随着深度神经网络的兴起，数据驱动的方法备受关注，如RNN和CNN等。引入图卷积后，骨架方法更专注于动作识别，避免了复杂背景的干扰，并大幅提高了准确性。</p><p>具体模型包括2s-AGCN、AS-GCN、ST-TR、FDGCN、MS-G3D、DC-GCN等，它们通过自适应卷积、注意机制、编码器-解码器结构等方式增强网络灵活性，捕获更丰富的时空特征。这些模型在骨架动作识别中取得显著成果，但也存在一些局限性，如固定的骨架结构、对两人互动关系的忽略等。</p><p>为解决这些问题，文章提出了基于图扩散的动态卷积网络，引入图扩散建立关节连接，通过动态卷积适应不同距离和不同身体之间的连接，提高信息传递效率。同时，通过关键关节轨迹的叠加计算帧的重要性，使时间维度的卷积具有动态性，能够识别关键动作并抑制小动作的噪音。该方法综合利用了多维特征，通过不同相似度度量捕捉特征之间的物理关系。总体而言，该方法通过图扩散和动态卷积有效地应对了骨架动作识别中的挑战。</p><h3 id="B-Graph-Neural-Network"><a href="#B-Graph-Neural-Network" class="headerlink" title="B. Graph Neural Network"></a>B. Graph Neural Network</h3><p><strong>B. 图神经网络</strong></p><p>图神经网络[31]使用节点表示对象，使用边表示对象之间的关系，并在图上应用图卷积。图卷积网络的构建通常遵循两个原则：空间视角[32]和频谱视角[33]。基于频谱理论的方法计算图拉普拉斯矩阵[34]的特征值和特征向量，并借助图傅里叶变换在频域进行图卷积，而无需从图中提取局部区域的连接。与频谱理论相反，基于空间理论的方法需要知道节点及其邻居以执行卷积操作[34]。在本文中，采用空间理论构建图卷积核，同时引入图扩散使我们的模型在频谱视角上更具可解释性。</p><p>图卷积是图神经网络的核心，通常采用直接邻居进行消息传播[37]。动作识别任务的最关键因素在于关节共存的帧内表示和骨架时间演变的帧间表示[38]。关节之间的连接不仅限于直接邻居，还包括更本质相关的关节。例如，在拍手动作中，两只手没有自然连接，但却密切相关[5]。因此，直接邻居的传播将削弱远离但密切相关关节之间的消息[6]。图扩散卷积（GDC）[37]支持超出直接邻居的消息传递，可以有效提取关键区域的不同动作，削弱无用的边。同时，GDC与频谱理论密切相关，可以结合空间理论和频谱理论的优势，突破直接邻居消息传递的局限性。因此，我们将采用图扩散卷积的思想，自适应地构建不同动作的骨架图，以提高动作识别的准确性。</p><p><strong>总结：</strong> 图神经网络是通过节点和边表示对象及其关系，并应用图卷积进行信息处理的模型。构建图卷积网络可基于空间或频谱视角，本文选择空间理论。图卷积是核心，通常使用直接邻居进行消息传播，但对于关节间的复杂关系，引入图扩散卷积（GDC）支持超出直接邻居的消息传递，提高了对不同动作关键区域的识别能力。GDC结合了空间和频谱理论的优势，通过自适应构建不同动作的骨架图，有效突破了传统直接邻居消息传递的限制，提高了动作识别的准确性。</p><h2 id="III-PRIOR-SKELETON-GRAPH"><a href="#III-PRIOR-SKELETON-GRAPH" class="headerlink" title="III. PRIOR SKELETON GRAPH"></a>III. PRIOR SKELETON GRAPH</h2><p>我们的先验骨架图以关节为图节点，以人体的自然连通性为空间边，相邻框架中相同的关节连接为时间边。除了关节特征外，我们还使用了骨骼数据的二阶信息(骨骼的长度和方向)，这自然对动作识别[8]更具信息和鉴别性。为了引入物理运动方向的大小，我们添加了速度特征，所以涉及到的特征是关节(3d坐标)，骨骼(骨骼数据)和速度。</p><h3 id="A-Skeleton-Graph-Construction"><a href="#A-Skeleton-Graph-Construction" class="headerlink" title="A. Skeleton Graph Construction"></a>A. Skeleton Graph Construction</h3><p>构建的两人交互骨架模型分为两部分:一部分是来自人体自然连接的先验知识，另一部分是通过数据驱动的方式获得的个性化模型。先验模型主要用于利用先验知识引导网络学习，而个性化模型产生动态骨架，通过图扩散获得动态骨架，主要用于引导网络更快地收敛，同时增强固定骨架的远距离消息传递。图2为两人动作的先验骨架图，其中棕色实心圆表示人体关节，棕色线表示人体自然连通性，蓝色线表示相同关节的时间轨迹。</p><h3 id="B-Feature-Selection"><a href="#B-Feature-Selection" class="headerlink" title="B. Feature Selection"></a>B. Feature Selection</h3><p><strong>B. 特征选择</strong></p><p>动作特征元素通常包括轨迹、速度、方向等。我们将其分解为关节、速度和骨骼。</p><p>关节特征通常是人体动作识别中最常用的特征之一[4]，[6]，[8]。关节之间的位置关系反映了运动的幅度和轨迹。在空间维度上，关节的相对位置可以有效区分不同的运动，而在时间维度上，关节的轨迹反映了运动的周期和幅度。</p><p>速度特征被引入以反映实际物理运动中力的重要性，它是一个带有方向和大小的矢量。在一些文献中[6]，[39]，位置、速度和加速度经常被用作动作识别的三个特征。基于大量实验，作为二阶导数的加速度可能会受到噪音的影响（例如，小动作），如果在时间上平滑，加速度的区分能力将会减弱，因此我们舍弃了加速度特征。与空间点的位置相比，作为3D坐标的一阶导数，速度更关注力的方向和大小，并且与人体运动学更密切相关。大量实验证明，速度的区分能力不逊于3D坐标，并且通过考虑不同的注意力隐藏空间后，可以显著提高其区分能力。</p><p>骨骼特征。实际上，骨骼和关节将两人互动动作的骨架模型分为两部分，如图3所示，这也是骨架动作识别中通常使用的两个基本特征[6]，[8]，[9]，[11]，[40]。骨骼是人体关节之间的自然连接的反应，是人体运动的最小单位，也是运动中姿势的重要载体。骨骼特征在人体动作识别中具有很强的区分能力。特别是在两人互动中，骨骼的相对关系比其他特征具有更多的物理意义，并且导致更高的识别准确性。</p><p><strong>总结：</strong> 本节介绍了动作特征的选择，主要包括关节、速度和骨骼。关节特征通过反映运动的幅度和轨迹在空间和时间维度上进行区分。速度特征引入了对实际物理运动中力的重要性的考虑，通过矢量表示方向和大小。骨骼特征将骨架模型分为关节和骨骼两个基本部分，对于人体动作识别，尤其是在两人互动中，骨骼的相对关系具有更多的物理意义，导致更高的识别准确性。</p><h3 id="C-Feature-Similarity-Measurement"><a href="#C-Feature-Similarity-Measurement" class="headerlink" title="C. Feature Similarity Measurement"></a>C. Feature Similarity Measurement</h3><p><strong>C. 特征相似度测量</strong></p><p>不同的特征具有不同的关注特性，使不同的特征在其专业领域发挥出较大的区分能力非常重要。在本文中，为不同的特征引入了不同的相似度度量方法。实验证明，可以使用高斯相似度测量3D位置坐标，而速度更关注余弦相似度。动作识别实际上涉及对人体每个骨骼的力进行分析。这种力不仅具有大小，还具有方向。我们可以使用高斯相似度进行幅度的相似性分析，使用余弦相似度来计算方向的相似性。通过这种方式，我们可以更好地捕捉与不同特征相对应的不同隐藏空间。</p><p>高斯相似度测量多维空间中每个点的绝对距离。为了将欧几里德距离映射到0-1范围，我们使用高斯核函数。当两点之间的欧几里德距离为0时，高斯核函数的值接近1；当距离无穷大时，它接近于0。参数σ用于调整下降速度。σ越小，变化越剧烈。</p><p>余弦相似度计算两个向量之间的夹角的余弦值：余弦值可用于表示两个向量的相似性。夹角越小，余弦值越接近1，它们的方向越匹配，相似性越高。余弦相似度与向量的大小无关，只与向量的方向有关。</p><p><strong>总结：</strong> 本节介绍了针对不同特征引入的不同相似度度量方法。通过使用高斯相似度和余弦相似度分别进行幅度和方向的相似性分析，可以更好地捕捉不同特征对应的隐藏空间，提高动作识别的准确性。</p><h2 id="IV-GRAPH-DIFFUSION-CONVOLUTIONAL-NETWORK"><a href="#IV-GRAPH-DIFFUSION-CONVOLUTIONAL-NETWORK" class="headerlink" title="IV. GRAPH DIFFUSION CONVOLUTIONAL NETWORK"></a>IV. GRAPH DIFFUSION CONVOLUTIONAL NETWORK</h2><p>我们的图扩散卷积神经网络以STGCN为基线。我们首先简要回顾了ST-GCN，然后详细介绍了我们在ST-GCN上的改进。ST-GCN将图形卷积应用于基于骨骼的人体姿态识别。对于单帧，有areNjointsVt, joint对sh = {(i, j)|i, j∈N}，其连接是根据人体关节的自然连度定义的，因此，骨架边集可以定义为Et = {(vti,vtj)|t∈τ， (i, j)∈H}，其中τ为视频帧总数。如图1所示，整个网络采用端到端训练方法，并通过反向传播进行优化。网络的前三层用于自学习，第四层和第五层用于以数据驱动的方式引导网络达到全局最优，后两层用于收敛，最后通过标准的SoftMax分类器对每个类别进行关联</p><h3 id="A-Generalized-Graph-Diffusion-and-Graph-Diffusion-Convolution"><a href="#A-Generalized-Graph-Diffusion-and-Graph-Diffusion-Convolution" class="headerlink" title="A. Generalized Graph Diffusion and Graph Diffusion Convolution"></a>A. Generalized Graph Diffusion and Graph Diffusion Convolution</h3><p><strong>A. 广义图扩散和图扩散卷积</strong></p><p>对于单帧，根据上述定义，我们现在有一个无向图 G = (V，E)，其中 V 是点集，E 是边集，广义图扩散的扩散矩阵定义如下：</p><p>其中 θk 是权重系数，T 是广义转移矩阵。为了确保收敛性，限制必须满足 ∑（从 k=0 到 ∞）θk = 1，θk ∈ [0, 1]，且T的特征值被限制在 λ ∈ [0, 1]。</p><p>常用作转移矩阵的随机游走转移矩阵 Trw = AD^(-1) 和对称转移矩阵 Tsym = D^(-1/2)AD^(-1/2)，其中 D 是节点度矩阵，A 代表邻接矩阵。为了确保转移矩阵列随机，我们在转移矩阵中添加了具有权重 wloop 的自环 IN。现在，随机游走的转移矩阵可以定义为：</p><p>实际上，图扩散卷积将原始邻接矩阵 A 替换为广义图扩散矩阵 S，这也可以理解为在图扩散之后，我们构建了一个新图用于后续网络卷积。</p><p><strong>总结：</strong> 本节介绍了广义图扩散和图扩散卷积的定义。对于单帧，通过定义广义图扩散矩阵，构建了一个无向图，并引入了相应的转移矩阵。图扩散卷积通过将原始邻接矩阵替换为广义图扩散矩阵，实际上是在图扩散后为后续网络卷积构建了一个新的图。</p><h3 id="B-Spatial-Graph-Diffusion-Convolution-Neural-Network"><a href="#B-Spatial-Graph-Diffusion-Convolution-Neural-Network" class="headerlink" title="B. Spatial Graph Diffusion Convolution Neural Network"></a>B. Spatial Graph Diffusion Convolution Neural Network</h3><p><strong>B. 空间图扩散卷积神经网络</strong></p><p>图卷积的最重要部分是采样函数和权重函数的定义。在ST-GCN中，采样函数获取与关节点vti相隔一跳的直接邻居。邻居集合可以表示为B(vti) = {vtj | d(vti, vtj) ≤ 1}，其中d(vti, vtj)表示vti和vtj之间的最短距离。</p><p>由于图中节点的邻居与图像相比没有顺序，合理的划分策略和权重设置的定义成为图卷积的关键。根据人体运动学，ST-GCN将节点vti的邻居分为三类，如图4所示。节点本身被划分为静态运动集。靠近骨架重心的节点被划分为向心运动集，远离中心的节点被划分为离心运动集。划分策略可表示为：</p><p>其中ri是训练集中所有帧中从重心到关节点i的平均距离。根据划分策略，权重函数定义为：</p><p>因此，ST-GCN中的空间图卷积定义为：</p><p>其中Zti(vtj)=|{vtk | lti(vtk)= lti(vtj)}|是归一化项，表示相应子集的基数。 根据上述公式，我们的图扩散卷积网络需要设计两个部分，采样函数和权重函数。</p><p>我们仍然采用ST-GCN采用的空间配置划分方法。实际上，由于ST-GCN中的固定骨架结构，不同帧的权重函数是固定的，这意味着不同关节具有相同的重要性，与不同部位协同执行不同行为的思想相悖。因此，我们将引入图扩散到空间图卷积，以便不同行为采用不同的权重函数。</p><p>图扩散的示例包括个性化PageRank（PPR），热核等，我们采用了PPR，它使用具有转移矩阵T = Trw和系数θPPR k = α(1 − α)k（我们设置alpha = 0.05）的随机游走转移矩阵。在复杂社交网络挖掘中通常采用PPR，通常数据漫游不是随机的，而是具有用户偏好。因此，我们应考虑如何将PPR应用于两人互动识别模型。</p><p>按照这个思路，我们将节点传输过程转化为基于前述节点相似性计算的数据驱动过程。这意味着节点vi在行走过程中停留在原地的概率是wloop/Di，而权重wloop可以由我们的相似性矩阵定义。</p><p>通过图扩散，我们可以将骨架图转化为任意两个关节之间的关系矩阵。这种关系不是通过相似性计算直接获得的，而是在整个骨架图上使用相似性矩阵进行扩散。消息传递到整个图的每个关节，甚至传递到另一个人的关节。这导致了一个新的骨架结构，包含了在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。图5显示了我们的扩散结果。</p><p><strong>总结：</strong> 本节介绍了空间图扩散卷积神经网络的关键概念，包括采样函数和权重函数的定义。在空间图卷积中，通过引入图扩散，改变了原始的图结构，使不同行为采用不同的权重函数。图扩散通过在整个骨架图上进行相似性矩阵扩散，获得了不同关节之间的关系，包括在单个人中没有直接连接的关节之间的关系，以及在两人互动模型中两个人的关节之间的关系。</p><h3 id="C-Temporal-Dynamic-Convolution-Neural-Network"><a href="#C-Temporal-Dynamic-Convolution-Neural-Network" class="headerlink" title="C. Temporal Dynamic Convolution Neural Network"></a>C. Temporal Dynamic Convolution Neural Network</h3><p><strong>C. 时间动态卷积神经网络</strong></p><p>在一系列视频帧中，通常会存在一些无关的动作[41]。例如，在“拥抱”动作中，“接近”动作首先执行，然后是“拥抱”。因此，我们希望将振幅最大的动作标识为扩散和识别的焦点。同时，在一些动作中，会有一些“小动作”作为插曲，例如，有些人会习惯性地“拿眼镜”、“拿耳朵”或“摇腿”等。这些小动作不应成为识别的重点。而且，有些动作非常相似，振幅略有不同，例如“挥手”和“打排球”等。在实际场景中，动作通常是连续的，因此在分割后识别动作具有重要的实际意义。</p><p>在许多相关研究中，通常通过简单的卷积进行知识融合，并将关节的变化抽象为时间维度的隐藏空间，但忽略了每帧的重要性的差异[42]。由于卷积操作共享卷积核的特性，扩散的源帧与其他帧没有差异。然而，对于一些相似的动作，动作的振幅成为识别的关键。因为大多数动作是由身体部位执行的，而身体部位具有简单的图结构，对于动作来说，主要的区别在于哪些部位适合在一起以及每个部位的振幅。在前一种情况下，我们通过扩散在空间维度的组件之间建立了空间连接。后者是时间维度的问题。因此，我们通过在时间维度中寻找扩散的源帧来放大扩散过程中源帧的权重，使模型收敛更快、更准确，提高相似动作的识别能力。</p><p>在ST-GCN中，时间卷积操作是通过考虑周围9帧来更新当前帧的关节。由于卷积过程的局部感知和参数共享特性，由于对不同帧具有相同的感知强度，很难捕捉关键帧。</p><p>因此，我们研究了关节的时间活动轨迹，以改变不同帧的重要性，这意味着我们的时间卷积是动态的。时间的帧重要性计算可以表示为：</p><p>在这里，我们选择了所有关节进行重要性计算，并采用了原始未处理的特征。在后续的消融实验中，我们将讨论一系列更有意义的堆叠方法。现在，我们的时间卷积可以表述为：</p><p>准确捕捉源帧还将抑制小动作和噪音帧的影响。同时，帧重要性计算可以以细粒度捕捉人体运动趋势的变化，例如从“前进”到“后退”，从“外”到“后”等，这些常常在运动轨迹中呈现为拐点。通过这些拐点，我们可以分割多个连续的动作，以处理现实中复杂而连续的视频。</p><p><strong>C. 时间动态卷积神经网络总结</strong></p><p>在时间动态卷积神经网络中，通过识别动作的振幅最大的源帧来作为扩散和识别的焦点，以区分动作中的“小动作”和相似动作。传统卷积操作的问题在于对不同帧的感知相同，而时间动态卷积通过考虑关节的时间活动轨迹，使模型能够动态地调整每帧的重要性。通过在时间维度中寻找源帧，扩散的源帧在卷积过程中的权重得以放大，提高了模型的收敛速度和准确性，同时消除了小动作和噪音的影响。这种动态的卷积操作能够更好地捕捉人体运动趋势的变化，为处理连续和复杂的实际视频提供了更有效的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Graph-Diffusion-Convolutional-Network-for-Skeleton-Based-Semantic-Recognition-of-Two-Person-Actions基于图扩散卷积网络的二人动作语义识别&quot;&gt;&lt;a href=&quot;#Gra</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>论文理解4真实世界的图卷积网络用于智能视频监控中的动作识别</title>
    <link href="https://www.fomal.cc/posts/69357615.html"/>
    <id>https://www.fomal.cc/posts/69357615.html</id>
    <published>2023-12-28T03:08:41.000Z</published>
    <updated>2023-12-29T09:48:50.121Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Real-World-Graph-Convolution-Networks-RW-GCNs-for-Action-Recognition-in-Smart-Video-Surveillance"><a href="#Real-World-Graph-Convolution-Networks-RW-GCNs-for-Action-Recognition-in-Smart-Video-Surveillance" class="headerlink" title="Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance"></a>Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance</h1><h1 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h1><p>摘要： 动作识别是新兴的边缘智能视频监控和安全系统中的关键算法部分。基于骨架的动作识别是一种吸引人的方法，它不使用RGB像素数据，而是依赖于人体姿势信息来分类适当的动作。==然而，现有的算法通常假设理想条件，这些条件不代表真实世界的限制，如嘈杂的输入、延迟要求和边缘资源约束。==</p><p>为了解决现有方法的局限性，本文提出了Real-World Graph Convolution Networks（RW-GCNs）<strong>，这是一个面向真实世界骨架动作识别域约束的架构级解决方案。</strong>受人类视觉皮层中反馈连接的启发，RW-GCNs利用对现有近最先进（SotA）的时空图卷积网络（ST-GCNs）<strong>进行专注反馈增强</strong>。ST-GCNs的设计选择<strong>源于信息理论中心的原则，以解决端到端实时和边缘智能视频系统中通常遇到的空间和时间噪声</strong>。我们的结果表明，RW-GCNs能够通<strong>过在NTU-RGB-D-120数据集上实现新的SotA准确性（94.1%）</strong>来服务这些应用，并在存在空间关键点噪声的情况下，<strong>在Northwestern UCLA数据集上实现90.4%的准确性</strong>，比基线ST-GCN应用的延迟低32倍。RW-GCNs通过在成本效益高达10倍的NVIDIA Jetson Nano上运行（而不是NVIDIA Xavier NX），同时在资源受限的设备上保持可观的吞吐量范围（15.6到5.5个动作每秒），进一步展示了系统的可扩展性。代码可在此处获取：<a href="https://github.com/TeCSAR-UNCC/RW-GCN。">https://github.com/TeCSAR-UNCC/RW-GCN。</a></p><p>总结： 本文介绍了一种名为Real-World Graph Convolution Networks（RW-GCNs）的架构，用于解决真实世界骨架动作识别中的各种约束。受人类视觉皮层中反馈连接的启发，RW-GCNs通过对现有时空图卷积网络（ST-GCNs）进行专注反馈增强，==以应对噪声、延迟和资源限制等挑战==。实验证明RW-GCNs在NTU-RGB-D-120数据集上==实现了新的最先进准确性==，==同时在较低的延迟下仍保持了高准确性==。RW-GCNs还通过在成本效益高的边缘设备上运行，展示了系统的可扩展性。</p><h1 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h1><p>动作识别，即在视频片段中识别演员执行的动作，是计算机视觉领域一个具有挑战性且繁荣的领域。在边缘实时应用的约束条件下，这一挑战变得更加严峻。边缘计算是许多旨在维护隐私和可扩展性的应用的要求。深度神经网络（DNNs）的最新进展使该领域迎来了一场复兴，将模型的准确性推动到远远超出以往可能的水平。这在基于骨架的动作识别中尤为显著，该方法不使用RGB像素数据，而是依赖于人体姿态信息来分类适当的动作。特别是，源自图卷积网络（GCNs）[22]，具体来说是时空图卷积网络[64]（ST-GCNs）的方法，将最先进（SotA）的动作识别推向了新的高度。目前，即使对于相对简单的基线ST-GCN算法，也可以实现近SotA的准确性，达到了87%的准确性。有了这么高的准确性，现在可以在理解人们动作的基础上实现依赖于复杂计算机视觉的应用，例如视频监视、自动驾驶车辆、患者监测等。然而，许多这些应用都具有尚未得到适当解决的实际约束条件，例如需要在边缘端到端系统中满足隐私和可扩展性要求以及实时延迟的约束条件。这些现实世界的约束条件阻碍了现有基于骨架的动作识别工作的部署。</p><p>现有致力于基于骨架的动作识别的工作往往假设并非真实世界的理想条件。通常，网络使用的姿势信息被假定为完美的，并且通常基于手动注释的数据。然而，在实际系统中，骨架姿势信息将来自于不完美的人体姿势估计器，导致不完美的姿势数据。即使是最先进的SotA姿势估计器，具有数千万参数和数百亿次操作[4, 37]，也无法产生无瑕疵的姿势数据。这些姿势估计器的准确性受到边缘设备资源约束的进一步限制。此外，这些工作通常忽视了在边缘设备上满足这些现实世界应用的潜在延迟约束，这明显限制了它们在实际世界中的适用性。</p><p>虽然大多数作品主要关注实现高准确性，但也有一些尝试解决实际条件的特定方面，特别是关于噪声数据和端到端边缘系统的实时延迟约束。噪声数据可以看作是由于场景遮挡引起的不完整的人体骨架，或由于ID混淆导致多人设置中缺失的骨架。通过在Kinetics-Skeleton数据集[21]上进行评估，可以看出模拟实际分析的主要方法。这是通过使用OpenPose[4]从原始RGB图像中提取人体姿势信息而导出的流行Kinetics动作数据集的增强。然而，对于该特定数据集，准确性往往较低，SotA低于50%。部分原因可以通过使用不完美的姿势估计器引入的噪声来解释，但主要因素是对于该特定数据集，姿势数据不包括许多动作相关信息。这些原因包括带有移动摄像机的场景、第一人称视角的互动以及依赖于场景背景的信息。</p><p>总体而言，现有方法未能解决包括延迟约束和噪声数据在内的现实世界动作识别约束。这一失败阻碍了在具有边缘和实时要求的实际应用中的实现。这需要一种新的设计思维方式、形式化和领域规范，以应对甚至解决这些挑战，并同时保持强大的准确性。本文提出了实际图卷积网络（RW-GCNs）作为解决方案，以应对实际动作识别的固有约束。通过这样做，我们正在实现在边缘设备上部署实际动作识别。我们关注深度学习的信息瓶颈理论[58]，并受到神经科学的启发[24]，从而提出了关于处理边缘设备上的实时延迟和关键点噪声的注意反馈增强作为解决方案。RW-GCNs的反馈通过存储和重复使用过去的特征以及对重要特征的隐式时间采样，解决了在边缘设备上操作时保持准确性的问题。关注性反馈重新审视过去的特征并增强模型的区分能力。我们进一步推导了RW-GCNs的两个反馈变体：（1）语义反馈和（2）控制反馈。</p><p>为了全面评估与边缘实际约束条件相关的问题，我们为基于骨架的实际动作识别确定了一个现实的边缘系统，该系统满足现有云系统无法满足的隐私和可扩展性约束。此外，我们引入了两个新的计算性能指标：每秒动作（ApS），一种分析在多个延迟约束下的吞吐量的方法，以及动作产品延迟（APD），一种端到端系统延迟度量。然后，我们定量评估了RW-GCNs，包括语义反馈和控制反馈，以应对在实际边缘系统引起的延迟约束和噪声输入条件下的现实世界动作识别挑战。我们的结果显示，RW-GCNs在NTU-RGB-D-120数据集[30]上实现了94.16%的新SotA准确性，其延迟比ST-GCN基线少3.02倍。对Northwestern UCLA数据集[61]的评估显示，RW-GCNs可以在验证和训练中存在空间关键点噪声的情况下，以32.5倍的较低延迟实现90.4%的准确性。总的来说，RW-GCNs能够在完全端到端系统噪声存在的情况下以32.5倍的较低延迟运行，同时在Northwestern UCLA数据集上保持71.8%的准确性。总之，这项提议的工作做出了以下贡献：</p><ul><li>我们定义了领域约束，并引入了针对实际世界基于骨架的动作识别领域的新评估方法。</li><li>引入RW-GCNs作为解决同时处理延迟和噪声数据的实际约束的解决方案，通过基于信息理论的关注反馈架构增强。</li><li>在与现有实际边缘设备上的真实无约束工作的系统性能之间进行系统评估。</li><li>对端到端动作识别系统以及视频监控导向数据集上的紧急系统噪声进行了割离研究。</li></ul><p>接下来，第2节回顾相关工作。第3节介绍我们的领域约束和评估指标，以及与实际端到端边缘系统及其相关的新兴系统噪声。第4节提出了RW-GCNs作为在边缘和实时情况下启用基于骨架的动作识别的解决方案，并解释了语义和控制反馈的RW-GCNs的理论基础和实施细节。第5节呈现了该工作的结果、分析和评估。第6节包含我们的结论性意见。</p><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2 Related Works"></a>2 Related Works</h1><p>动作识别通常可以通过直接从视频信息中进行 [28]，或者通过时间利用人体姿势信息来完成。前一种方法可以称为基于像素的动作识别，并已在大型数据集上进行了广泛探讨 [21]。基于像素的动作识别方法必须对个体演员的空间外观特征具有鲁棒性，同时捕捉演员的时间动态，正如江等人在 [20] 中所做的那样。由于空间域中的噪声，这变得更加具有挑战性。现有作品在转移到动作数据集之前，利用大型图像数据集进行预训练，正如Tran等人在 [59] 中所示。动作识别的时间动态也需要被考虑，正如Liu等人在视频片段中以多个粒度查看的情况 [32]。</p><p>基于像素的动作识别的另一种选择是基于骨架的动作识别，从直观上来看，它与RGB视频流相比具有更少的噪声输入。正确提取该信息并对特定动作进行分类一直是一项持续的研究挑战。先前的方法依赖于基于循环神经网络（RNN）的模型的表达能力来处理图结构化输入 [25]。图卷积网络（GCNs），如Kipf等人的工作中所见 [22]，利用对非欧几里德图进行卷积，而不是在图像中找到的规则图。这个想法被应用于人体姿势图，用于动作识别，如Yu等人的工作中所见 [64]。最终的结果是ST-GCNs [64]。RW-GCNs可以被看作是RNN和基于ST-GCN的方法之间的中间体，类似于AGC-LSTMs [54]。</p><p>基于骨架的动作识别领域的当前最先进方法主要源自（或至少与）Yang等人的ST-GCN [64] 的工作有关。这些作品在ST-GCNs的基础上进行了多方面的创新。Liu等人的工作 [33] 解决了基线ST-GCN公式的现有偏见。第一个偏见是针对表示远程关节关系的。由于邻接矩阵中的冗余连接在ST-GCN的特征聚合中占主导地位，导致了这种偏见。提出的解决方案是删除现有的冗余连接。第二个偏见是针对复杂的时空特征传播的。这是由于将时空图卷积的因子分解为单独的层。这项工作另外提出了一个统一的运算符，同时处理数据的空间和时间方面。</p><p>一些作品没有将手工设计的设计原则编码到网络中，而是利用自适应图结构，使网络能够学习导致更稳健表示的设计原则。Plizzari等人的工作 [44] 将变压器引入，如自然语言处理领域所见 [60]。变压器的灵活性，最终的核心组件自我关注，使ST-GCN能够动态建模关节连接并学习依赖于数据的重要连接。虽然Plizzari等人 [44] 利用自我关注来克服基线ST-GCN公式的局限性，但仍然利用了图卷积形成（即在不规则图上进行显式卷积）。Shi等人的工作 [52] 超越了这一点，并完全用解耦的自我关注（解耦的自我关注在空间和时间上分解）取代了GCNs。通过利用位置编码，这项工作依赖于自我关注隐式模拟GCNs的能力。与增加图结构的灵活性不同，一些其他方法通过使输入表示更具信息性来改进输入表示 [26, 50, 51]。这些作品通过预处理现有的关节信息，进一步以骨骼和运动的方式明确定义输入。</p><p>尽管这些创新有助于更健壮的泛化和在关键数据集上实现整体更高的准确性，但很少有作品尝试解决在现实世界中部署ST-GCNs时面临的现实约束和困难。因此，大多数现有作品未能实现现实世界的部署，特别是考虑到某些应用所需的边缘设备的计算限制。Song等人的工作 [55] 是一个例外，该工作改进了输入表示，类似于 [26, 50, 51]，然后构建了一个专注于部分注意力的高效架构。Cheng等人的工作 [7] 也通过利用时间移位模块（TSMs）[28]来提高实时性能，作为时序卷积的有效替代方案。Yang等人的工作 [65] 利用反馈机制增加了时序采样策略的灵活性，并进一步减少了分类所需的帧数。最后，Yu等人的工作解决了噪声关键点的真实世界效应，并根据使用人工噪声进行训练的类型提出了解决方案 [67]。</p><p>与考虑现实世界部署要求时的工作方向不同。与Cheng等人和Song等人的工作不同 [7, 55]，我们没有为高效模型做出明确的贡献。相反，我们专注于输入数据依赖性，并在满足延迟约束的同时将我们的模型与大量输入要求解耦，类似于Yang等人 [65]。然而，我们并非利用密集连接层来控制必要的反馈，而是提出了一种基于注意机制的反馈机制，类似于Plizzaris等人 [44] 中发现的机制。此外，我们还考虑与Yu等人 [67] 类似的关键点噪声，尽管我们是在现实端到端动作识别系统的背景下这样做。通过这样做，我们希望实现在实时边缘应用中部署基于骨架的动作检测。</p><h1 id="3-Real-World-Action-Recognition-Domain-Constraints-Gaps-and-Methods"><a href="#3-Real-World-Action-Recognition-Domain-Constraints-Gaps-and-Methods" class="headerlink" title="3 Real-World Action Recognition: Domain Constraints, Gaps, and Methods"></a>3 Real-World Action Recognition: Domain Constraints, Gaps, and Methods</h1><p>虽然基于骨骼的动作识别领域取得了越来越多的成功，但与现实世界的约束有关的算法性能在很大程度上还未被探索。我们的工作希望能够启动现实世界中基于骨骼的动作识别的探索，这将使基于骨骼的动作识别能够部署在边缘上。在本节中，我们将分析诸如患者监控和安全等应用程序的实际部署限制，并定义子领域真实世界的骨骼动作识别。首先，我们简要回顾了现实世界的行动识别约束和边缘计算范式的必要性。然后，我们引入两个新的评估指标来捕获延迟约束。最后，我们将深入研究边缘视频监控应用中完整的端到端视觉处理管道背景下的噪声数据的细节。</p><h2 id="3-1-Real-World-Constraints"><a href="#3-1-Real-World-Constraints" class="headerlink" title="3.1 Real-World Constraints"></a>3.1 Real-World Constraints</h2><p>嘈杂输入：这个新的子领域的第一个主要约束是现实世界的数据。基于骨架的动作识别高度依赖以关键点形式呈现的人体姿势数据。通常，该领域中发布的作品假设提供了完美的人体姿势数据。这种完美的数据通常是通过耗时的手动标注或在实验室环境中使用昂贵且高度准确的传感器获得的。在实际部署中，姿势数据通常需要由专用的机器学习算法即时生成，导致数据不完美。这被称为噪声数据，在本文中我们假设这种噪声是由基于深度神经网络的多人姿势估计器生成的。我们采用了他们关于空间噪声的定义，即应该在帧中但未提供的人体关键点关节。然而，我们对他们对于时序噪声的定义进行了扩展，将其分为两个不同的类别。第一类是帧级时序噪声。当个体的数据完全缺失时，就会发生这种情况，无论是由于未能检测到一个人，还是由于在多人环境中误认为他们。第二类是感受野级时序噪声。这是一种隐式的噪声，定义为网络在能够查看整个动作剪辑的持续时间方面的能力的限制。</p><p>延迟约束：现实世界基于骨架的动作识别的第二个主要约束是延迟。虽然一些现有的作品关注基于骨架的动作识别的计算复杂性，但许多忽视了执行单个动作识别所需的帧数。通常，现有作品中用于分类单个动作（即剪辑大小或T）的帧数为300。如果我们假设神经姿势估计器以FPSin = 30运行（这取决于硬件，是一项非常庞大的成就），那么300帧的剪辑对应于10秒的延迟。这对于需要实时响应的应用，如患者监测、公共安全和一般视频监控而言，是无法接受的昂贵。</p><p>边缘计算约束：最后，由于考虑到现实世界基于骨架的动作识别需要额外的隐私和可扩展性考虑，边缘计算范式变得必不可少。大多数现有的视频监控实现从边缘的本地摄像头流式传输图像数据到云服务器，云服务器执行大部分计算。这使用户隐私无法得到保障，并限制了可扩展性。隐私对于启用智能边缘视频应用至关重要，其他作品为确保用户隐私而专门致力于使用计算昂贵的深度神经网络 [57]。在我们的工作中，靠近传感器的低功耗边缘设备用于进行姿势估计和动作识别的计算。通过直接在边缘处理视频流，传送到网络的唯一数据是经过预处理的，不包含个人身份信息。</p><p>这从根本上解决了通过云端传输个人数据的不安问题。此外，使用边缘设备来处理大部分计算正面影响系统的可扩展性。在基于云计算的系统中，可扩展性受云服务器的处理能力和网络吞吐量的限制。增加摄像头会导致对服务器的更多请求，这可能导致要么昂贵的服务器升级，要么由于无法满足系统计算需求而增加系统范围的延迟。当系统实现成本太高时，它会限制智能视频应用在边缘的推广，以及整体人工智能的普及。通过减少昂贵的云服务器，采用更便宜的本地边缘计算解决了这个问题，并已经在现有的深度神经网络服务平台中使用 [34, 35]。然而，边缘解决方案通常受到资源限制，这导致其他实现使用混合边缘/云解决方案 [38]，或者优化DNN以提高性能 [40, 45]。</p><p><strong>总结：3.1 现实世界约束</strong></p><p><strong>1. 噪杂输入：</strong></p><ul><li>真实世界的数据对于基于骨架的动作识别构成首要约束，依赖于以关键点形式呈现的人体姿势数据。</li><li>大多数文献中假设提供完美的人体姿势数据，但在实际部署中，姿势数据往往由专用机器学习算法即时生成，导致数据不完美，即噪声数据。</li><li>噪声数据主要由基于深度神经网络的多人姿势估计器生成，包括空间噪声和时序噪声（分为帧级和感受野级）。</li></ul><p><strong>2. 延迟约束：</strong></p><ul><li>实际世界基于骨架的动作识别的第二个主要约束是延迟。</li><li>许多现有作品关注计算复杂性，但忽略了执行单个动作识别所需的帧数。</li><li>典型情况下，现有作品的帧数为300，而这将导致10秒的延迟，对于需要实时响应的应用来说是昂贵的。</li></ul><p><strong>3. 边缘计算约束：</strong></p><ul><li>由于隐私和可扩展性的考虑，现实世界基于骨架的动作识别需要采用边缘计算范式。</li><li>大多数现有视频监控实现通过从边缘传输图像数据到云服务器进行计算，这违反了用户隐私且限制了可扩展性。</li><li>我们的工作使用低功耗边缘设备进行计算，解决了云端传输个人数据的问题，同时提高了系统的可扩展性。</li></ul><h2 id="3-2-Real-World-Evaluation-Metrics"><a href="#3-2-Real-World-Evaluation-Metrics" class="headerlink" title="3.2 Real-World Evaluation Metrics"></a>3.2 Real-World Evaluation Metrics</h2><p>为了解决资源受限边缘系统的延迟约束，我们定义了两个新的指标，用于分析和研究动作识别算法在现实世界中的部署能力。第一个指标是每秒动作数（ApS）。</p><p>该指标表示模型在单个秒内能够完成多少工作（即吞吐量）。其中，T是剪辑大小，W是窗口大小，CpSin是每秒剪辑数，或者是完整剪辑被输入到网络的速率。这个指标是为了将受到延迟约束的作品（如我们的作品）与无约束的作品进行比较而需要的，因为它们在单个剪辑中完成了不同数量的工作。虽然模型接受的理论输入吞吐量（FPST）对于像自动驾驶汽车这样的快速应用非常重要，但视频监控领域的要求要宽松得多（对于人体运动为30 FPS）。相反，第二个指标动作产品延迟（APD）更具预测性，特别适用于视频监控等领域的实际部署性能。</p><p>APD是输入帧速率FPSin的倒数与网络窗口大小W之间的乘积。该指标表示处理单个动作所需的延迟量。在监测患者或危险工作环境等应用中，检测事故（例如摔倒、绊倒）的响应时间至关重要，其下限即为APD。</p><p><strong>总结：3.2 现实世界评估指标</strong></p><p><strong>1. 每秒动作数 (ApS)：</strong></p><ul><li>用于衡量模型在单秒内完成的工作量，即吞吐量。</li><li>考虑剪辑大小（T）、窗口大小（W）和每秒剪辑数（CpSin）。</li><li>用于比较延迟受限作品与无约束作品，因为它们在单个剪辑中执行不同数量的工作。</li><li>对于视频监控等领域，与模型接受的理论输入吞吐量（FPST）相比，具有更宽松的要求。</li></ul><p><strong>2. 动作产品延迟 (APD)：</strong></p><ul><li>是输入帧速率（FPSin）的倒数与网络窗口大小（W）之间的乘积。</li><li>表示处理单个动作所需的延迟量。</li><li>在监测患者或危险工作环境等应用中，检测事故的响应时间至关重要，其下限为APD。</li><li>更具预测性，特别适用于视频监控等领域的实际部署性能评估。</li></ul><h2 id="3-3-End-to-End-Video-Surveillance-and-Noisy-Data"><a href="#3-3-End-to-End-Video-Surveillance-and-Noisy-Data" class="headerlink" title="3.3 End-to-End Video Surveillance and Noisy Data"></a>3.3 End-to-End Video Surveillance and Noisy Data</h2><p>在现实世界的环境中，基于骨骼的动作识别将成为更大端到端视觉管道的一部分。图1展示了智能视频监控应用的典型视觉管道。该图受到[36]和[23]中提出的现有视觉管道框架的启发。如图1所示，这些框架通常包括三个概念阶段。我们还增加了一个额外的阶段，其中包含我们的RW-GCN网络。 RW-GCN的详细描述可以在第4.2节中找到。第一个阶段是检测阶段。多人体姿势估计器用于检测人体关键点并从输入视频的单个帧中生成边界框。接下来，在提取阶段，使用先前阶段的边界框生成人体图像裁剪。这些人体裁剪通过特征提取网络，为每个人体图像生成嵌入式特征表示。这些边界框还用于计算每个个体在与先前帧相比的帧中的时间位置。在匹配阶段，匹配算法测量当前帧特征与先前帧特征之间的欧几里得外观相似性，并考虑先前计算的时间位置，确定要分配给哪个人的ID。</p><p>这一端到端的视觉管道完全在靠近摄像机的边缘节点上运行，通过从多个节点聚合非敏感信息到边缘服务器，实现了[36]中所做的操作。</p><p><strong>空间噪声：</strong> 基于骨骼的动作识别需要人体姿势信息，以关键点骨架的形式作为输入。在封闭环境中的动作识别中，假定这些骨架是演员场景的无缺陷、完美的表示。然而，在现实世界中，由人体姿势估计器提供给网络的骨架通常存在缺陷。即使是最先进的方法也不是完美的，有时会出现错误，要么是误检测关键点，要么完全未检测到。这些错误在整体上可能是不可避免的，但当应用环境与姿势估计器训练的数据集不同时，错误会进一步加剧，而在现实世界中几乎总是这种情况。对于需要考虑硬件资源的系统，或者存在实时约束的系统，使用大型、高精度的最先进模型可能不是一个选择。在这些情况下，需要做出权衡，使用轻量级的姿势估计器，并经常牺牲准确性以满足应用要求，这可能进一步增加这些错误的频率。</p><p>这些缺失和错误检测的关键点在系统中形成了一种空间噪声，如图2所示。当传播到动作识别时，这种噪声可能会带来很大问题。图2上半部分的演员正在弯下身子将物品放入桶中。这对于人眼来说是清晰的。然而，用于动作识别的关键点骨架在这个场景中存在明显错误，特别是涉及演员右臂的部分。除了在检测方面通常不一致之外，在图中最右边的图像中，演员的右手被错误地检测到了他的脚踝。如果动作识别系统被设计为假定接收到的数据是完美的，这种类型的错误很容易使网络混淆，并导致不正确的动作分类。因此，在现实世界的动作识别中需要考虑这种类型的噪声。</p><p>为了在准确性和实时约束之间取得平衡，我们使用EfficientHRNet [37]来获取人体姿势骨架。</p><p><strong>时间噪声：</strong> 现实世界动作识别中常见的另一类噪声是时间噪声。时间噪声可以通过两种方式之一发生。第一种是演员的骨架在视频片段的一个或多个帧中丢失。通常，这是因为姿势估计器没有检测到骨架，或者因为演员的骨架在这一帧中没有与演员正确匹配。在这两种情况下，这导致动作识别网络在应该有骨架信息的情况下没有该演员的骨架信息。在封闭环境中的动作识别中，其中提供了完美的骨架和ID数据，这不是问题，但在现实世界的动作检测中，使用不完美的算法生成关键点和ID，这种类型的错误相当普遍。</p><p>第二种形式的时间噪声发生在边缘系统没有同时将动作的所有帧馈送到网络中。虽然这对于封闭环境的动作识别不是问题，在现实世界中，在延迟约束 dictate 下，只能一次处理少数帧的应用程序中，这种情况经常会遇到。</p><p>图2说明了第一类时间噪声的一个例子；关键点骨架丢失。虽然图2下半部分的演员在第一帧中被正确识别，但中间两帧的关键点骨架变灰，表明它们与演员没有正确匹配。这是由重新识别算法引起的错误。此外，在最后一帧中，根本没有检测到关键点骨架；这是神经姿势估计器的失败。凭借仅有的一帧姿势数据，动作识别网络将无法理解演员的运动和位置情况。直观地说，这将使得对于场景的动作识别非常具有挑战性，设计现实世界的动作识别时必须考虑这些情况。</p><p>我们选择使用与[36]中相似的Re-ID方法，这是一种不依赖于面部识别的现实解决方案。</p><p><strong>总结：</strong></p><p>这一段描述了基于骨架的动作识别在现实世界环境中的应用，作为更大端到端视觉管道的一部分。该视觉管道包括检测、提取、匹配等阶段，其中使用多人体姿势估计器进行关键点检测，并通过边缘节点进行端到端的实时处理。文章指出了两种主要噪声：空间噪声和时间噪声。空间噪声源于人体姿势估计的不完美，包括关键点缺失和错误检测，可能导致动作识别错误。为了平衡准确性和实时性，文章采用了EfficientHRNet等轻量级姿势估计器。时间噪声涉及骨架在视频中的丢失，以及边缘系统无法同时处理所有帧的情况。这种噪声使得动作识别在现实场景中更具挑战性，需要针对性的解决方案，如采用Re-ID方法。最后，介绍了Real-World GCNs（RW-GCNs）作为一种解决方案，通过端到端处理空间和时间噪声，提高对于现实世界骨架动作识别的适用性，尤其是在边缘计算领域的应用。</p><h1 id="4-Real-World-GCNs-RW-GCNs-through-Attentive-Feedback-Augmentation"><a href="#4-Real-World-GCNs-RW-GCNs-through-Attentive-Feedback-Augmentation" class="headerlink" title="4 Real-World-GCNs (RW-GCNs) through Attentive Feedback Augmentation"></a>4 Real-World-GCNs (RW-GCNs) through Attentive Feedback Augmentation</h1><p>为了满足现实世界骨骼动作识别的领域约束，本节介绍了实际世界GCNs（RW-GCNs）。在RW-GCN中，我们利用一个具有预定义结构的静态图，模仿了COCO数据集关键点格式[29]，用于空间图表示。时间组件由每个顶点通过相邻帧之间的一对一连接组成。这个时空人体姿势图被网络视为一个张量X ∈ RN×T×C×M，其中N是节点数，T是帧数，C是通道数，M是整个剪辑中单帧中分开的人体姿势的最大数量。此外，网络使用的邻接矩阵A ∈ RN×N用于定义图上的卷积。</p><p>我们基于ST-GCN的网络的空间组件是通过在输入缓冲区的单个帧上执行1×P标准的2D卷积来实现的，其中P决定了权重矩阵和邻接矩阵的分区数。然后，结果与一组归一化的P邻接矩阵相乘。根据定义的分区策略，权重矩阵和邻接矩阵可能被分割成子集，并按方程（3）中所示相加，该方程在[7]中进一步解释。RW-GCN利用了在[64]中定义的空间分割策略。RW-GCN通过细致的反馈增强了现有的ST-GCN架构。反馈可能增强ST-GCN在处理嘈杂输入方面的能力。馈送回架构的特征将包括更高级别的语义，如果通过时间缺乏变化，则可能是冗余的。如果神经姿势估计器的噪声与其估计相关，这些在时间上冗余的特征可能在空间上产生协同作用，这在定性上是成立的。总体而言，细致的反馈增强使RW-GCN能够应对各种噪声，特别是由现实世界的延迟约束引起的时间噪声。因此，RW-GCN为边缘计算领域的基于骨骼的动作识别应用打开了可能性。对于大多数计算在接近传感器边缘节点的系统，例如REVAMP2T [36]，RW-GCNs添加了一个灵活的网络，可以根据其他网络的计算强度使用不同数量的帧。对于像EdgeEye [31]这样将所有数据发送到本地边缘服务器的系统，RW-GCNs增强了边缘系统在处理网络带宽和传感器存储限制方面的灵活性。最后，对于更分布式的边缘系统，例如VideoPipe [46]，RW-GCNs相对于本地存储容量和网络带宽的灵活性将能够实现许多不同的流水线实现，每个都专门针对特定的应用。</p><p><strong>总结:</strong></p><p>上述段落介绍了Real-World GCNs（RW-GCNs），这是为了满足现实世界骨骼动作识别领域的约束而提出的一种方法。RW-GCN采用静态图和预定义结构，模拟了COCO数据集的关键点格式，用于构建空间图表示。时间组件由相邻帧之间的一对一连接组成。网络将这个时空人体姿势图视为张量，其中包含节点数、帧数、通道数和单帧中人体姿势的最大数量。空间组件通过在输入缓冲区的单个帧上执行标准的2D卷积来实现，并结合了一组归一化的邻接矩阵。RW-GCN采用了一种空间分割策略，并引入了有意义的反馈，以增强网络对嘈杂输入的处理能力。这种反馈包括高级语义特征，有助于处理时间上的冗余。最终，RW-GCN通过处理各种噪声，尤其是由于现实世界延迟引起的时间噪声，为边缘计算领域的基于骨骼的动作识别应用提供了可能性。这对于不同计算强度和网络带宽的系统提供了灵活性，为实时应用和边缘设备上的动作识别提供了解决方案。</p><h2 id="4-1-Theoretical-Aspects"><a href="#4-1-Theoretical-Aspects" class="headerlink" title="4.1 Theoretical Aspects"></a>4.1 Theoretical Aspects</h2><p><strong>4.1 理论方面</strong></p><p>我们关注的自注意力反馈增强的理论基础基于深度神经网络的信息瓶颈理论，由Tishby等人提出[58]。</p><p>虽然Saxe等人的工作[48]最初对这一理论提出了质疑，但Noshad等人的工作[41]随后证实了该理论，指出存在对互信息的计算问题，因为使用了较差的互信息估计器，Nosahd等人提出了他们自己的互信息估计器。此外，还有其他尝试验证信息瓶颈理论在更复杂架构上的应用[9]。尽管直接将信息瓶颈应用于深度学习存在困难，但其他研究已经表明，基于信息瓶颈理论的解决方案在开发时在泛化方面取得了明显的改进[19, 43]。我们的工作采用相同的方法，基于信息瓶颈理论的基础，开发了面向现实世界实时动作识别的体系结构级解决方案。</p><p>信息瓶颈理论假设深度神经网络可以被表示为马尔可夫链： X → T1 → … → Tz → Y 每一层的输出在马尔可夫链中由一个随机变量表示。例如，X是输入特征，T1是第一层的隐藏表示，Tz是第Z层的隐藏表示，Y是输出随机变量。</p><p>应用于深度学习的信息瓶颈理论认为学习等效于最小化以下拉格朗日函数：</p><p>我们可以解释方程（4）为学习具有两个明显且独立的阶段，通过最小化−I(T; Y)（我们可以将其视为最大化其倒数）和最小化I(X; T)。拟合阶段是第一阶段，如方程（5）所示： max I(T; Y) = H(T) − H(T|Y) （5） 拟合阶段由β调节，通过最大化隐藏特征的熵H(T)来学习信息性特征，使隐藏表示更具信息性。此外，拟合阶段最小化H(T|Y)，这最小化了当已知标签信息时隐藏表示的信息。从直观上看，这相当于删除与隐藏表示无关的特征。</p><p>学习的第二阶段是压缩阶段，如方程（6）所示： min I(X; T) = H(X) − H(X|T) （6） 压缩阶段最小化H(X)，即源数据的熵，同时最大化隐藏表示对输入的信息量H(X|T)。这导致了一个模型，它对输入信息的依赖较少。我们的工作假设利用具有助于拟合或压缩学习阶段的特性的解决方案，可以导致类似于Jeon等人[19]的强大算法解决方案。</p><p>RW-GCNs的第一个体系结构级解决方案基于自注意力。Bloem等人的工作[2]指出，注意力使网络能够通过捕获数据相关的不变性来学习和建模。了解信息瓶颈可以被看作是查找最小充分统计量的框架[53]，我们假设注意力使模型更有效地捕获数据集的最小充分统计量，这直接有助于模型与信息瓶颈框架相关的方面。这进一步得到了支持，因为注意力可以作为一种形式的预训练压缩[2]。我们此外强调了早停止（仅在验证准确性不再增加时停止训练）通常用于训练深度神经网络。其他研究发现，早停止通常（但不总是）在压缩阶段之前停止训练，暗示压缩阶段可能导致过拟合[43, 63]。鉴于此，可以将压缩阶段解释为学习要关注的信息。我们认为自注意力是一种足够灵活的归纳偏置（由于模仿和扩展归纳偏置的卷积的能力[1, 8]），可以实现非过拟合的压缩阶段。</p><p>反馈还可以协助模型的泛化，不是通过压缩阶段，而是通过拟合阶段。事实上，RW-GCNs的主要前提是保持良好的表示或最大化H(T)，尽管存在感受野级别的时间噪声（由延迟约束引起）。在考虑到这一前提的基础上，我们将目光投向拟合阶段，max I(T; Y)，将其视为一个经典的信道编码问题，其中我们的网络既是编码器又是信道本身，信道容量是互信息I(T; Y)。我们理论上认为，如果反馈可以协助经典通信信道中的信道编码，那么它可以通过改善神经网络训练的拟合阶段来协助深度神经网络。</p><p>已知，当反馈应用于具有不相关高斯噪声信道的传统通信通道时，信道容量不会增加[12]。但是，当噪声与输入相关（即带有记忆的信道）时，反馈可以导致信道容量的小幅提高。此外，反馈还可以显著降低信道编码器和解码器（即神经网络）的复杂性[6]，通过降低传统通信通道中接近容量所需的平均块长度。我们认为这相当于减少深度神经网络在单个时间步中需要看到的特征数量。我们将这与卷积神经网络层具有具有受限感受野的神经元（即更少的空间特征）以及网络深度作为一种模仿反馈能力以用更简化的复杂性实现合理准确性的机制相联系。已知，由于深度神经网络通常具有高度的层间权重共享[3, 47]，深度可以模仿反馈。</p><p>反馈还可以在无记忆多路访问信道（MACs）的容量中提供附加增益，其中多个信号传递到同一信道[15]。这可以看作是神经网络处理多个特征。通过反馈的能力增强信号的合作，可以实现容量的提高，使神经网络更好地共同优化多个竞争特征。当仅希望部分信息的多个信号时，可以实现信道容量的乘法增益[56]。我们将语义反馈视为一种试图通过利用高级语义信息（例如深度神经网络特征）来最大化H(T)或等效地增加特征表示的信息的机制，如在CliqueNets[66]中所见。我们认为这种高级语义信息通过反馈的能力降低编码器（即网络）的复杂性，从而允许我们使用更小的时间感受野。</p><p>此外，我们将控制反馈视为一种使统计信息提取与特定随机变量相关但与其他变量无关的机制。这种类型的反馈专注于最小化H(T|Y)，从而在已知目标分类的某些信息时减少隐藏特征的冗余。这可以看作是在传统多路访问通信信道中协调多个信号的反馈的类似物。此外，已知信息瓶颈在有条件地优化适用于不同输出随机变量的模型时具有交替公式[5]。根据反馈的不同，可能可以学习专注于不同输出随机变量的条件模型。直观地说，如果我们假设一个具有树、狗和人类类别的深度神经网络模型，最佳模型将不会将树的高级特征暴露给人类类别的分类。控制反馈可能有助于实现这种条件信息流。</p><h2 id="4-2-Implementation-details"><a href="#4-2-Implementation-details" class="headerlink" title="4.2 Implementation details"></a>4.2 Implementation details</h2><p>以下是段落的逐句翻译：</p><p><strong>4.2 实施细节</strong></p><p>为了解决第3节中讨论的领域限制，我们使用注意反馈增强了现有的ST-GCNs，创建了Real-World Graph Convolutional Networks（RW-GCNs）。在所有变种的RW-GCNs中，我们假设存在一种窗口大小W的延迟约束。当W等于动作片段T中的总帧数时，RW-GCN充当通用的ST-GCN。对于RW-GCN的基线实现，将包含T帧的片段分解为W个窗口，每个窗口包含T/W帧，并对每个窗口生成一个动作分类。RW-GCN采用滑动窗口操作，其步幅等于T/W。</p><p>这个有限的窗口大小导致了有限的时间感受野。为了克服这一问题，RW-GCN在处理当前窗口的同时，将先前窗口的信息传递到网络中。为此，我们探讨了两种专门的反馈形式，即（1）语义反馈和（2）控制反馈用于RW-GCNs。每种公式都使用过去的特征作为输入（除了当前特征），并实现两种专门的注意力形式（语义注意力和通道注意力）。接下来，我们详细讨论每种形式。</p><p><strong>总结：</strong></p><p>在这一部分，作者介绍了为了解决第3节中提到的领域限制而引入的Real-World Graph Convolutional Networks（RW-GCNs）的实施细节。为了处理潜在的延迟约束，他们采用注意反馈来增强现有的ST-GCNs。RW-GCNs的不同变体在窗口大小W的情况下操作，可以作为通用ST-GCN，但作者也介绍了基线实现，其中通过滑动窗口操作处理T帧片段，以克服有限的时间感受野。为了更好地处理这一问题，他们引入了两种专门的反馈形式：语义反馈和控制反馈。每个形式都使用过去的特征作为输入，并实现了两种专门的注意力形式。这些细节旨在提高RW-GCNs在处理实际世界骨架动作识别时的性能。</p><h3 id="4-2-1-Semantic-Feedback"><a href="#4-2-1-Semantic-Feedback" class="headerlink" title="4.2.1 Semantic Feedback:"></a>4.2.1 Semantic Feedback:</h3><p><strong>4.2.1 语义反馈:</strong></p><p>我们的语义反馈设计有两个目标。第一个目标是在考虑延迟约束的同时解决空间和时间噪声。通过首先仅在输入的一个窗口（或时间切片）上操作，并保留上一个窗口的高级语义特征以供下一个窗口的处理，实现了这一目标。然后，我们将这些特征馈送到我们的语义空间-时间注意块（语义注意块）中，如图3所示。</p><p>语义注意块与原始的自注意机制有显著的不同，后者通常以方程（7）的形式存在。 自注意(<em>Q</em>,<em>K</em>,<em>V</em>)=<em>so<strong>f</strong>t<strong>ma</strong>x</em>(<em>Q<strong>K</strong>T</em>/√<em>d**k</em>)×<em>V</em> (7) 这个实现是通过过去的特征和当前特征之间的点积来实现的，而不仅仅是当前特征。这是一种交叉关注。如果X中的时间信息高度冗余，那么我们的语义关注公式几乎等同于简化的自关注[11]。这可以为模型提供捕获更强大的空间特征的机会，并在理论上处理空间噪声。然而，在存在多样化的时间特征（即运动非常快）的情况下，语义关注块使得可以捕获跨帧的成对交互，从而通过最大化H(T)实现更精细的特征表示。</p><p>此外，我们的语义关注旨在具有计算效率。通过简化方程（7）的公式，我们的语义关注块将键和值的计算合并为X的单一投影，而不是两个[11, 16]。我们还受到[11]中的工作的启发，不对输出进行规范化。当我们将X的输入之一替换为我们的过去特征FB时，这使得我们的语义关注块能够捕获语义反馈。</p><p>为了进一步提高计算效率，我们将过去的特征压缩成1X32张量，然后应用语义关注。尽管关注机制的计算复杂度呈二次扩展，但这限制了计算成本。最后，我们通过1×1卷积加上以零初始化的批归一化，以及在注意特征上带有乘法门的残差，添加了门，有助于网络的收敛。我们反馈增强的语义关注块的最终公式如图4所示，这个公式使我们的ST-GCN能够实现语义反馈。 语义关注块放置在标准ST-GCN之前，如图3所示。在这项工作中，我们保留了Yan等人的相同基础架构[64]，如表1所示。重要的是要注意，ST-GCN块由GCN层和TCN层组成。GCN层的工作与本节前面描述的相同。时态卷积只是通过时间维度进行的1×9卷积，具有可变的步幅。</p><p><strong>总结：</strong></p><p>本节介绍了语义反馈的实现细节。语义反馈旨在解决空间和时间噪声，并考虑了延迟约束。该机制通过在输入的窗口上操作并保留上一个窗口的高级语义特征，然后将这些特征馈送到语义空间-时间注意块中。与原始的自注意机制不同，语义关注块采用了一种交叉关注的实现，其中点积考虑了过去和当前的特征。这允许模型更好地捕获空间特征，并理论上处理时间噪声。此外，语义关注块设计为计算效率高，通过将过去的特征压缩为1x32张量并添加门来进一步提高效率。最终，该机制被放置在标准ST-GCN之前，以实现语义反馈。</p><h3 id="4-2-2-Control-Feedback"><a href="#4-2-2-Control-Feedback" class="headerlink" title="4.2.2 Control Feedback"></a>4.2.2 Control Feedback</h3><p><strong>4.2.2 控制反馈：</strong></p><p>与实施语义反馈不同，RW-GCN 也可以实施控制反馈。控制反馈仅使用图4中所示的语义关注公式的压缩反馈特征。与通过时间和空间对特征进行成对交互的建模不同，我们的控制反馈实现使用过去的时空特征来主动加权通道，并有效地控制哪些特征传播更强。我们通过利用简单高效的通道关注[62]来实现这一点。这可以解释为特征选择，并允许整个网络在理论上根据过去的信号调整其信息流，以预期未来的信号（即最小化H(T|Y)或等效地减少冗余特征）。</p><p><strong>总结：</strong></p><p>本节介绍了控制反馈的实现。与语义反馈不同，控制反馈仅使用语义关注公式的压缩反馈特征。控制反馈利用过去的时空特征，通过通道关注主动加权通道，从而有效地控制特征的传播。这实际上是一种特征选择，允许整个网络根据过去的信号理论上调整其信息流，以预测未来的信号。</p><h1 id="5-Experimental-Results-and-Evaluation"><a href="#5-Experimental-Results-and-Evaluation" class="headerlink" title="5 Experimental Results and Evaluation"></a>5 Experimental Results and Evaluation</h1><p>这项工作评估了我们提出的RW-GCN在现实世界骨架动作识别领域的几个变体，重点是视频监控应用。为此，我们将RW-GCN分为三种变体:(1)基线，(2)受控反馈，(3)语义反馈。RW-GCN的基线实现不使用语义或控制反馈，而是简单地将过去的框架窗口与当前的框架窗口平均起来。我们称这种变化为共识RW-GCN。相反，我们的语义反馈变异被称为RW-GCN-SF，我们的受控反馈被称为RW-GCN-CF。所有实验都在Nvidia-V100 GPU上进行训练，使用Nesterov加速随机梯度下降，动量项为M = .9，权值衰减为WD = 10−4。除非另有说明，否则基础开始学习速率是lr = .01，并有10的衰减。此外，当训练任何反馈增强的RW-GCNs(不是共识)时，我们在特定时期实现了一个不断增长的架构策略。我们取基础RW-GCN的权重，对语义反馈和控制反馈分别“增长”语义注意和有效渠道注意。我们相信，通过更好地捕捉数据的不变性，这些注意力模型可以导致更鲁棒的压缩阶段[2,63]。</p><h2 id="5-1-Latency-Constraint-Evaluation-and-Analysis"><a href="#5-1-Latency-Constraint-Evaluation-and-Analysis" class="headerlink" title="5.1 Latency Constraint Evaluation and Analysis"></a>5.1 Latency Constraint Evaluation and Analysis</h2><p><strong>5.1 延迟约束评估和分析：</strong></p><p>对于延迟受限准确性实验，我们关注 NTU-RGB-D-120 [30] 数据集。NTU-RGB-D-120 数据集是当前最大的姿势 RGB+D 人体动作识别数据集。该数据集包含 120 种不同的室内动作类别，是评估骨架动作识别作品的当前事实标准。每个样本片段在 30 FPS 下最多有 300 帧。此外，该数据集在单个帧中最多有 2 人。数据集由 106 个独特的主体组成，配有 32 种可能的设置。总体而言，它包含超过 114,000 个视频样本和 8 百万帧。原始 NTU-RGB-D-120 数据集的姿势格式包含 3D 坐标空间中的 25 个节点 (x, y, z)。我们忽略深度维度，并将节点数量减少到 18，以与 REVAMP2T 等端到端系统兼容。 此次训练的具体超参数使用了 N = 32 的批处理大小。基线 RW-GCN 模型在第 30 和第 60 个时期进行了学习速率衰减（10^(-1)）。对于该数据集，我们仅关注第一个骨架序列，即使场景中可能有两个演员。RW-GCN-SF 变体在第 40 个时期采用学习率重新启动，并训练 45 个时期，总共 85 个时期。所有共识模型均在 60 个时期内进行了训练。我们没有对此数据集进行 RW-GCN-CFs 的测试。在验证结果时，我们使用交叉设置和交叉主体验证策略[30]。 我们分析的第一个实验结果是训练基线 RW-GCN 共识模型的准确性曲线与使用语义反馈的 RW-GCN-SF 相比。由于用于训练反馈增强网络的时期较多，有必要将训练超参数的贡献与我们的关注反馈增强的贡献分开。图 5 显示了当共识模型的训练延长到相似数量的时期时，其准确性停滞在其现有准确性。这些学习曲线使用 50 和 90 时期的学习率重新启动，并且没有使用增长的训练策略。我们发现，当使用增长的反馈增强注意力进行训练时，RW-GCN 可以在更少的时期内达到相似的准确性，避免过拟合。 接着，我们将受延迟约束的 RW-GCN 的准确性与第 2 节中先前提到的多个现有作品进行比较。这不包括具有额外模态的作品，如 RGB 补丁。表 2 显示，仅通过利用 COCO 格式的关键点，RW-GCN 就能够实现 87.7% 的跨主体准确性，比常规 NTU-RGB-D 骨架基线高出 16.4%。这是在没有延迟约束的情况下完成的，距离之前的 SotA 作品 DSTA-net [52] 仅有 1.6% 的差距。我们认为性能提高是由于输入的维度减少所致。通过将节点数量从 25 降低到 18 并去除深度通道，我们将输入的维度降低了 48%。直观地说，这些维度被解释为冗余的，使模型能够专注于学习而不是降维和特征选择。 跨主体的结果进一步显示，我们具有延迟约束的 RW-GCN 在 W = 30 的情况下表现为 70.6%，比 SotA 低了 17% 以上。这是在拥有 3 倍较少帧的情况下完成的，因此在动作分类之间有 3 倍较少的延迟。事实上，假设这项工作将基于 NTU-RGB-D-120 数据集的 30 FPS 输入，SotA（以及除 FGCN [65] 外的所有现有作品）将具有相当于 10 秒延迟的动作产品延迟，这在实时视频监控等真实应用中可能是 prohibitively expensive。RW-GCN 具有 1 秒的延迟，我们将其分类为次实时。当使用语义反馈时，RW-GCN-SF 实现了 86.4% 的准确性，距离之前的 SotA 仅有 1.3% 的差距。这显著缩小了与 SotA 之间的差距，同时满足次实时延迟约束，从而使新兴的边缘应用得以实现。当放宽延迟约束以允许 3.33 秒的延迟时，我们成为新的 SotA（就交叉主体验证而言），准确性为 94.16%。这表明，我们的方法不仅满足了实际骨架动作识别的子领域，而且为基于骨架的动作识别作出了贡献。在进行交叉设置验证时，我们看到我们的模型并没有定义新的 SotA，而是仅比 SotA 少 0.4%。交叉主体和交叉设置结果之间的这种差异可能可以通过两个因素来解释。第一个因素是缺乏深度。由于我们依赖于 COCO 关键点，由于训练集和验证集之间的视点不同（以及缺乏特征来通知模型样本视点），我们在域中经历了更多的转变。第二个因素是跨主体的时间方差，这也导致验证和训练集样本分布之间的差异。由于我们的注意力反馈增强具有更大的灵活性来适应时间变化，因此可以学会对个体主体时间变化不变，最小化训练集和验证集之间的分布转移。这种解释在基线 ST-GCN 在交叉设置验证中的更好性能方面受到轻微质疑。这表明在存在时间接收场噪声的情况下，缺乏深度只是一个问题。</p><p>我们相信这种准确性是由于语义关注块在时间信息高度冗余或无关时（即窗口内的帧受到接收场噪声的影响较小）能够编码空间成对特征交互的能力，稀疏的时间特征交互需要对表示进行编码。因此，我们认为在时间噪声较低时，我们的模型编码了更有区别性的空间特征，从而实现了高准确性。我们还分析了其他作品在延迟约束方面与我们的性能匹配的能力，如表 3 所示。对于这个分析，我们没有训练这些现有方法，而是使用基本的共识反馈对它们进行验证。我们发现，如果没有训练，这些作品无法在真实世界的领域中执行，最好的作品仅达到 10.12%。 我们在 NTU-RGB-D-120 数据集上的最后一个延迟约束分析是对专注语义反馈增强到 ST-GCN 上的概括改进的消融研究。这种消融允许我们看到语义反馈执行两种操作模式。在 W = 30 的第一个延迟约束中，语义反馈发挥其预期功能，通过关注成对的特征交互保留模型的过去特征。W = 30 的延迟约束可能缺乏时间冗余，并隐含地强制执行类似于 FGCN [65] 的稀疏采样策略。因此，模型保留了其在长程时间特征交互方面的能力，并且准确性仅比其基线无约束 ST-GCN 实现低 1.3%，如表 2 所示。 当受到 W = 60 的窗口约束时，ST-GCN 的操作模式开始转变。它记录的特征更具有时间冗余性，而模型自然具有存储更长程时间相互作用的能力。因此，该模型改善了无约束基线 +2.3%。我们认为，这种窗口约束平衡了提高其空间判别能力与保持暂时性特征保持能力之间的任务。由于这两项任务的复杂性，与 W = 30 测试中的 16.4% 改进相比，模型在共识模型的准确性改善方面仅有 6.4%。最后，使用 W = 100 的约束，语义关注块可以保持最少量的时间特征表征力量，并专注于更有效地编码空间特征。由于共识模型无法提高 ST-GCN 的空间表征能力，共识模型的准确性停滞不前。</p><p><strong>总结：</strong></p><p>在延迟约束评估和分析章节中，研究侧重于使用 NTU-RGB-D-120 数据集进行实验。该数据集是目前最大的骨架动作识别数据集之一。实验中采用了 RW-GCN 和 RW-GCN-SF 变体，其中 RW-GCN-SF 引入了语义反馈机制。实验结果显示，通过利用 COCO 格式的关键点，RW-GCN 在无延迟约束情况下就能够实现较高准确性，较之前的 SotA 作品有显著提升。在具有延迟约束的情况下，RW-GCN 在较少的帧数下仍能取得较高的准确性，相比于其他作品具有更低的延迟。语义反馈机制的引入进一步提高了模型的准确性，特别是在满足次实时延迟约束的情况下。此外，与其他现有方法相比，该模型在没有进行训练的情况下也能够在真实世界领域表现出色。最后，通过对语义反馈的消融研究，显示了其在不同延迟约束条件下的作用，从而揭示了其对模型性能的影响。</p><h2 id="5-2-End-to-End-System-Error-Propagation-Analysis"><a href="#5-2-End-to-End-System-Error-Propagation-Analysis" class="headerlink" title="5.2 End-to-End System Error Propagation Analysis"></a>5.2 End-to-End System Error Propagation Analysis</h2><p><strong>5.2 端到端系统误差传播分析</strong></p><p>端到端系统噪声分析超越了延迟约束分析，着眼于现实端到端边缘系统的新兴系统噪声。使用的数据集是 Northwestern-UCLA 数据集，也是一个骨架动作识别数据集。它包含 1494 个视频片段，每个片段平均包含 18.5 帧，标准差为 ±13.25。该数据集有 10 个可能的动作类别，非常适用于视频监控应用。采用了 [65] 使用的 Cross-View 验证策略，其中略去最后一个摄像头视图。</p><p>为了真正评估适用于视频监控应用的端到端系统噪声，我们将该数据集的 RGB 图像通过 REVAMP2T 处理，使用 EfficientHRNet-H0 [37] 和 REVAMP2T 论文 [36] 中使用的 MobileNetV2 特征提取器。这导致由于神经姿势估计器引入的关键点的空间噪声，以及由 REVAMP2T 的 Re-ID 模型带来的帧级时间噪声，另外还有极端的延迟约束 W = 10。</p><p>由于 Re-ID 改变了检测到的人数以及数据集的自然时间变化，输入张量 X ∈ RN×T×C×M 具有广泛变化的 T 和 M 维度。我们利用动态批处理步骤来处理这一问题，首先使用重复剪辑填充 T 维度，直到批处理中最大剪辑的长度。张量的 M 维表示多个人。由于 Re-ID 的误差传播到动作识别，我们可能在 M 维度中以不连续和不规则的方式中有单人剪辑。为解决这个问题，动态批处理会自适应地将输入求和到较小的 M = λ。如果 λ = 1，则称为 ID 无关场景，此时我们不再建模来自 Re-ID 的误差传播。在样本的 M &lt; λ 的情况下，我们将张量填充为 M = λ。</p><p>对于训练，所有共识模型最初训练了 30 个 epoch，然后在 epoch 30 时在共识模型的权重上添加了反馈模型，并进行额外的 40 个 epoch 的训练。由于动态批处理引入了时间噪声，我们使用批量大小为 8。验证批处理大小设置为 1，以最小化动态批处理带来的时间噪声。这组实验还利用了附加到每个 FB-Augmented 注意力的可学习乘性门，从零开始初始化。这使得反馈增强模型能够顺利地集成到共识模型中。</p><p>表 4 显示了在 NW-UCLA 动作数据集上的噪声输入下的准确性结果。它还将我们的方法与 FGCN [65] 进行了比较，后者是一个同样引入反馈的现有作品，作为 SotA。在没有空间关键点噪声和 Re-ID 噪声的情况下，我们超越了 SotA 1.3%。此外，结果表明，尽管 RW-GCN-SF 具有 W = 10 的延迟约束、EfficientHRNet-H0 的嘈杂关键点和 REVAMP2T Re-ID 带来的帧级时间噪声，但我们的方法仍能够与 FGCN 架构竞争。我们报告了 90.4% 的准确性，仅比 FGCN 的 SotA 低 4.9%。此外，Yang 等人 [65] 报告了他们的准确性，考虑到一个 64 帧的延迟约束（与我们的 10 帧相比），并随机采样。</p><p>我们相信，尽管存在上述噪声，我们能够在超过 90% 的准确性下进行动作识别，这对于现实世界的骨架动作识别和智能边缘视频监控应用至关重要。这一点在表 3 的结果中得到了进一步的加强，表明现有作品无法适应延迟约束的感受场噪声。此外，我们继续分析控制反馈和语义反馈模型之间准确性的差异。表 2 强调了控制反馈的表现甚至不如基线共识模型，而且无法与语义反馈一起使用。我们的主要假设是，由于 RW-GCN-SF 建模了成对的双线性特征交互，而 RW-GCN-CF 变体仅建模了线性交互，因此控制反馈的效果不如语义反馈。由于特征的极端延迟约束导致缺乏时间冗余，这一情况更加恶化。此外，通过分析动态批处理注入的时间噪声（通过将验证批处理 N &gt; 1），控制反馈方法达到了 84%，而语义反馈为 83.1%。我们的逻辑推测是，动态批处理过程中特征的不规则复制模拟了嘈杂的循环反馈，即通过相同的网络权重反复推送相同的特征，并且这充当了信息处理的形式。这种信息处理方式更类似于传统循环神经网络处理时间信息的方式。可以说，我们的 RW-GCNs 存在于一个模型家族中，其中网络深度和循环处理的依赖因每个变体而异。AGC-LSTM [54] 模型将是与我们的模型相反的变体，它在循环和注意力上具有较大的依赖，但对网络深度的依赖较小。对于像 AGC-LSTM 这样的模型，控制反馈可能比语义反馈更可靠，因为它控制信息传播。</p><p>Northwestern-UCLA 数据集的最后一项分析是关于 Re-ID 误差传播效应的消融研究。我们在两种情况下测试了 RW-GCN 架构。第一种是 ID 无关场景，其中 RW-GCN 试图通过对输入的 M 维度求和来人为地消除时间噪声，将被剪辑分开的时间特征重新连接成一个单一的 1 × T × V × C 张量。尽管这在人多、执行不同动作的拥挤场景中理论上可能效果较差，但 Northwestern-UCLA 数据集 [61] 的这类样本很少。我们看到 RW-GCN-SF 的最佳性能达到了 90.4% 的动作识别准确性。在第二种情况下，我们分析了与 Re-ID 噪声相关的真实动作识别。我们变化了衡量时间 IoU 过滤器与外观特征距离的超参数。我们发现 RW-GCN-SF 在 71.8% 的准确性上最为弹性。我们结果的一个有趣方面显示，以外观为中心的 Noise 1 对 RW-GCNs 的抑制更为明显。虽然这可能只是数据集的一个方面，但我们认为这值得未来进一步探讨。</p><p>总的来说，我们得出结论，通过沿 M 维度对输入特征进行零填充会引入太多噪声，从而降低了所有模型在真实场景中的准确性。预期基于 IoU 的 Re-ID 将优于基于外观的 Re-ID，但从外观噪声测试的顶级网络中仅获得了较小的改进。</p><p><strong>总结：</strong></p><p>本节继续对提出的骨架动作识别模型进行详细分析，特别是在端到端系统的噪声和误差传播方面进行了探讨。使用 Northwestern-UCLA 数据集，作者模拟了真实世界的视频监控应用场景，通过 REVAMP2T 处理 RGB 图像，并引入了空间关键点和 Re-ID 模型的噪声。在不同的延迟约束下进行了实验，并使用动态批处理处理由 Re-ID 引入的时间噪声。</p><p>实验结果表明，在实际场景中，即使存在大量噪声，提出的 RW-GCN-SF 模型仍能实现超过 90% 的动作识别准确性，超越了同类模型 FGCN。此外，作者分析了控制反馈和语义反馈模型之间的性能差异，发现语义反馈相对于控制反馈更为有效，特别是在极端的延迟约束下。最后，通过消融研究，作者进一步验证了模型对 Re-ID 误差传播的鲁棒性，并提出了对动作识别系统设计的有价值见解。</p><p>总体而言，提出的模型在复杂实际场景下表现出色，为骨架动作识别在视频监控等领域的应用提供了强大的性能和鲁棒性。</p><h2 id="5-3-Real-Time-Performance-Analysis"><a href="#5-3-Real-Time-Performance-Analysis" class="headerlink" title="5.3 Real-Time Performance Analysis"></a>5.3 Real-Time Performance Analysis</h2><p>本节对模型在边缘设备上的实时性能进行了交叉分析，使用的是 Nvidia Jetson Xavier NX 设备。为了公平评估实时性能，引入了两个度量标准：每秒动作数（ApS），由方程（1）计算，以及动作产品延迟（APD），由方程（2）计算。值得注意的是，ApS是特定于设备的，而APD是端到端系统的。</p><p>该模型直观地支持更高的ApS，因为它在一个剪辑中产生多个动作分类。虽然在考虑实际场景时可能会贬低每个剪辑的多个输出的必要性，但要承认动作执行的速度既未知又依赖于数据。此外，动作识别模型通常不知道输入的帧率（除非经过训练，大多数模型没有），这导致了需要支持比理想训练场景中更大的ApS，并为将来训练模型以对不同输入帧率进行不可知处理提供了潜在可能性（如果不加以控制，可能导致准确性下降）。RW-GCN以 41.7 ApS 报告了最高性能，比现有模型高出 1.6 倍。从性能的角度来看，语义反馈的开销在ApS和参数方面都很小。正如在第4节中提到的，语义反馈被设计成轻量级，只在32个通道上工作。Shift-GCN 是参数效率最高的模型，因为它是专门设计用于轻量级和高效推断的，通过利用时间步长之间的数据移动。这种模型设计与 RW-GCNs 不是相斥的，在未来可以进一步探讨。</p><p>值得注意的是，RW-GCN 模型的 ApS 随着帧大小的变化而变化很大。这是由基于 GPU 的系统的执行偏差导致的。当我们减小窗口大小时，RW-GCNs 的数据依赖性被序列化并失去了性能效率。另一方面，当窗口大小增加时，由于需要更多帧来计算单个动作，我们面临与其他模型相同的问题。APD 的计算依赖于 EfficientHRNet 的 H0 FPS 为 22.95。RW-GCN 实现了最低的 APD，为 0.4 秒，这是从 Northwestern-UCLA 数据集得出的极端延迟结果。</p><p>最后，我们分析了 RW-GCNs 在轻量级和廉价边缘设备上的可扩展性。我们关注 Nvidia Jetson Nano，而不是之前在表6中展示的 Nvidia Xavier NX。截至本文撰写时，Jetson Nano 的价格几乎是 Xavier NX 的 10 倍。价格是确定系统成本可扩展性的重要因素，实际上通过负担得起的价格推动了人工智能的民主化。然而，Nano 的资源更为有限，只有一个 128 核的 Maxwell GPU，而 Xavier NX 则有一个 384 核的 Volta GPU 和 48 个 Tensor 核。</p><p>这是在更大的 RAM 带宽（约2倍）和更大的内存容量（约4倍）的基础上。我们使用 ApS 衡量我们的计算如何映射到这样一个受限环境。我们使用剪辑大小为 30 和窗口大小为 10 来计算 ApS。当一个场景中只有两个人时，我们看到 ApS 下降了 2.67 倍。当扩展到 8 人时，这一差距增加到 7.6 倍的 ApS 减少。我们认为这对于实时动作检测来说已经足够了，实际上我们相信可以进一步牺牲 ApS 来支持更多人数的场景。从理论上讲，我们可以顺序地跨越与我们的滑动窗口解决方案相似数量的人，以较大的时间框架处理场景。这可能允许我们以 1 的 ApS 处理包含 36 人的场景。未来的方向可以通过将更深的层级卸载到类似于[39]的服务器中来进一步优化。通过选择卸载深层次，可以保护隐私。另一个方向是利用输入自适应计算，类似于 Fang 等人的工作。【13】</p><p><strong>5.3 实时性能分析总结：</strong></p><p>本节详细分析了模型在边缘设备上的实时性能。使用 Nvidia Jetson Xavier NX 进行测量，并引入了两个度量标准：每秒动作数（ApS）和动作产品延迟（APD）。结果显示，RW-GCN 在 ApS 方面表现最佳，达到 41.7，比现有模型高出 1.6 倍。在考虑语义反馈的开销时，性能仍然相对较高。Shift-GCN 在参数效率上领先，专为轻量级和高效推断设计。值得注意的是，RW-GCN 模型的 ApS 随帧大小变化而变化较大，这与 GPU 执行的特性有关。最终，RW-GCN 在动作产品延迟方面取得了最佳结果，为 0.4 秒，这是在 Northwestern-UCLA 数据集上得出的。</p><p>最后，对于在轻量级和廉价边缘设备上的可扩展性，使用 Nvidia Jetson Nano 进行测试。虽然 Nano 相对于 Xavier NX 更便宜，但其资源更有限。在不同人数的场景下，ApS下降了，但仍然足够支持实时动作检测。此外，对于将模型部署到更大范围的场景，可以通过进一步优化和利用服务器进行深层次卸载来实现。</p><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 Conclusion"></a>6 Conclusion</h1><p>总的来说，本文定义了真实世界骨架动作识别的子领域，并利用基于信息理论的原理引入了Real-World Graph Convolutional Networks（RW-GCNs），在NTU-RGB-D-120数据集上取得了新的SotA准确率，达到94.16%。相比ST-GCN基准，RW-GCNs的延迟降低了3.02倍。此外，RW-GCNs在仅有3.8%准确率损失的情况下，可以比基准实现降低10倍的延迟。 通过对Northwestern UCLA数据集的评估显示，RW-GCNs在验证和训练中存在空间关键点噪声的情况下，可以达到90.4%的准确率，延迟比基准ST-GCN降低了32.5倍。最后，RW-GCNs可以在完全端到端系统噪声的情况下运行，包括时间Re-ID噪声，在Northwestern UCLA数据集上的延迟比基准ST-GCN降低了32.5倍，并保持71.8%的准确率。所有这些都是通过一种注重隐私和可扩展的边缘计算方法实现的，其中系统成本每个节点可以降低10倍，同时仍然保持了一定范围的吞吐量，具体取决于场景的复杂性（从15.6到5.5 ApS）。 这项工作标志着真实世界骨架动作识别子领域的开始。通过设计RW-GCNs，本研究希望促进先前不可行的新边缘计算应用程序的设计和创建。然而，我们认为这一新兴领域仍然存在许多挑战，如时间变化、特定环境场景动态和进一步的应用特定约束。 </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Real-World-Graph-Convolution-Networks-RW-GCNs-for-Action-Recognition-in-Smart-Video-Surveillance&quot;&gt;&lt;a href=&quot;#Real-World-Graph-Convolu</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mzl_视频动作识别调研</title>
    <link href="https://www.fomal.cc/posts/9e598b04.html"/>
    <id>https://www.fomal.cc/posts/9e598b04.html</id>
    <published>2023-12-24T07:55:09.000Z</published>
    <updated>2024-01-07T11:52:41.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="视频动作识别调研"><a href="#视频动作识别调研" class="headerlink" title="视频动作识别调研"></a>视频动作识别调研</h1><blockquote><p>视频动作分类技术【视频分类任务】</p><p>学习视频中时间和空间特征【Temporal feature时间特征 视频中人和物来进行区别 】【Spatial feature 空间特征辨别人怎么运动 】</p><p>落地过程</p></blockquote><p><img src="../assets/1%20(6" alt="1 (6)">.png)</p><h2 id="1-open-pose"><a href="#1-open-pose" class="headerlink" title="1. open pose"></a>1. open pose</h2><p>OpenPose是<strong>开源==人体姿态估计==系统</strong>，它能够从==图像或视频中====准确地检测并估计人体的关键点位置==。 这使得<strong>OpenPose DW</strong>能够更好地捕捉人体姿态中的微妙细节，并对复杂动作进行更准确的分析。关键点识别的精度提升OpenPose DW通过深度和宽度的优化，取得了关键点识别精度方面的显著提升。</p><blockquote><p>输入视频或者图片，输出原有图片+关键点展示，关键点数据存储文件</p></blockquote><p><img src="..\assets\200z51apve.gif" alt="img"></p><blockquote><p> 参考链接 </p><p> <a href="https://cloud.tencent.com/developer/article/1373539?from=15425"> Github开源人体姿态识别项目OpenPose中文文档</a></p><p> <a href="https://zhuanlan.zhihu.com/p/503396298">人体姿势估计openpose简单理解和应用</a></p><p> <a href="https://cloud.tencent.com/developer/article/1775853?from=15425">实现人体姿态估计（人体关键点检测)</a></p></blockquote><p>简要介绍<a href="https://blog.csdn.net/qq_43258953/article/details/104441286">人体姿态估计算法之open pose-CSDN博客</a></p><p>项目源码<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">CMU-Perceptual-Computing-Lab/openpose: OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation (github.com)</a> </p><h2 id="2-openPose-DW"><a href="#2-openPose-DW" class="headerlink" title="2.openPose DW"></a>2.openPose DW</h2><p>OpenPose DW作为<strong>一种精细的人体姿态识别系统</strong>，通过==深度和宽度优化==取得了==关键点识别==精度方面的显著提升。 它具有广泛的应用前景，在人机交互、虚拟现实、体育分析等领域都能发挥重要作用。 然而，仍然需要进一步的研究和改进，以提高OpenPose DW的准确性和实时性，以满足实际应用的需求。</p><p>深度优化：OpenPose DW引入了==更深层次的卷积神经网络==（Convolutional Neural Network, CNN），通过增加网络的深度，可以提取更高级别的特征表示。这使得OpenPose DW能够==更好地捕捉人体姿态中的微妙细节==，并对复杂动作进行更准确的分析。</p><p>宽度优化：OpenPose DW通过增加网络的宽度，即==增加每个卷积层的通道数==，从而增加了==网络的容量和感受野==。这种宽度优化可以提高网络的鲁棒性和泛化能力，使得OpenPose DW在不同场景下都能稳定地进行姿态估计。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">官网：<span class="attr">https</span>:<span class="comment">//github.com/IDEA-Research/DWPose论文地址：https://arxiv.org/abs/2307.15880</span></span><br><span class="line">配套骨骼图下载：<span class="attr">https</span>:<span class="comment">//pan.quark.cn/s/b73b2531675f</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>输入</strong>：图像、视频、网络摄像头、Flir/Point Grey、IP 摄像头，并支持添加您自己的自定义输入源（例如，深度摄像头）。</p><p><strong>输出</strong>：基本图像+关键点显示/保存（PNG、JPG、AVI等），关键点保存（JSON、XML、YML等），KEY点作为数组类，并支持添加自己的自定义输出代码（例如，一些花哨的UI）。</p><p>参考链接 </p><p><a href="https://cloud.tencent.com/developer/article/2358119?areaId=106001">更加精细的OpenPose DW Openpose(23)</a></p><p><a href="https://cloud.tencent.com/developer/article/2335410">更加精细的OpenPose DW Openpose</a></p></blockquote><h2 id="3-model-scope-开源识别模型"><a href="#3-model-scope-开源识别模型" class="headerlink" title="3.model scope 开源识别模型"></a>3.model scope 开源识别模型</h2><h3 id="1-Patch-Shift-Transformers"><a href="#1-Patch-Shift-Transformers" class="headerlink" title="1.Patch Shift Transformers"></a>1.Patch Shift Transformers</h3><p>魔搭社区中有一个开源的识别模型</p><p>Patch Shift Transformers(PST) 是在2D Swin-Transformer的基础上，增加temporal建模能力，使网络具备视频时空特征学习能力。而这一操作几乎不增加额外参数。具体地，通过shift不同帧之间的patch, 然后在每帧内部分别进行self-attention 运算，这样使用2D的self-attention计算量来进行视频的时空特征建模，论文原文<a href="https://readpaper.com/paper/4650578659522920449">链接</a>。</p><p><img src="../assets/1%20(5" alt="1 (5)">.png)</p><blockquote><p><a href="https://www.modelscope.cn/models/damo/cv_pathshift_action-recognition/summary">PST动作识别模型-tiny · 模型库 (modelscope.cn)</a></p><p><a href="https://github.com/MartinXM/TPS">MartinXM/TPS: A simple but efficient transformer model for video action recognition (github.com)</a></p></blockquote><h3 id="2-日常动作检测"><a href="#2-日常动作检测" class="headerlink" title="2.日常动作检测"></a>2.日常动作检测</h3><p>输入视频文件，输出该段时间内视频所包含的动作。算法内部每两秒均匀采样4帧输入到动作检测模型中，然后按设定时间步长滑动对整个视频的动作进行检测并返回结果。CUDA和CPU运行环境均支持。</p><p><img src="../assets/1%20(6" alt="1 (6)">.png)</p><blockquote><p><a href="https://modelscope.cn/models/damo/cv_ResNetC3D_action-detection_detection2d/summary">日常动作检测 · 模型库 (modelscope.cn)</a></p><p><a href="https://detectron2.readthedocs.io/en/latest/tutorials/install.html">https://detectron2.readthedocs.io/en/latest/tutorials/install.html</a></p></blockquote><p>阿里云社区带的日常动作监测库教程</p><p><a href="https://detectron2.readthedocs.io/en/latest/index.html">Welcome to detectron2’s documentation! — detectron2 0.6 documentation</a></p><h2 id="4-mmaction-2"><a href="#4-mmaction-2" class="headerlink" title="4.mmaction 2"></a>4.mmaction 2</h2><p>MMAction2 是一款基于 PyTorch 的视频理解开源工具箱，是 <a href="https://gitee.com/link?target=https%3A%2F%2Fopenmmlab.com%2F">OpenMMLab</a> 项目的成员之一</p><blockquote><p>开源的数据仓库，有对应的安装和使用文档，以及对应的数据集测试过程等</p><p><a href="https://gitee.com/open-mmlab/mmaction2">mmaction2: 基于 PyTorch 和 MMCV 的视频理解工具库，支持动作识别、动作定位、时空动作检测和骨骼动作识别等多种任务。 (gitee.com)</a></p></blockquote><p><img src="../assets/1%20(1" alt="1 (1)">.gif)</p><blockquote><p><a href="https://gitee.com/open-mmlab/mmaction2">mmaction2: 基于 PyTorch 和 MMCV 的视频理解工具库，支持动作识别、动作定位、时空动作检测和骨骼动作识别等多种任务。 (gitee.com)</a></p></blockquote><p><img src="../assets/1%20(1" alt="1 (1)">-1703393662178.png)</p><blockquote><p>中文教程<a href="https://mmaction2.readthedocs.io/zh-cn/latest/index.html">MMAction2 中文教程! — MMAction2 1.2.0 文档</a> </p><p><a href="https://github.com/open-mmlab/mmaction2">open-mmlab/mmaction2: OpenMMLab’s Next Generation Video Understanding Toolbox and Benchmark (github.com)</a></p></blockquote><p>MMPose 是一款基于 PyTorch 的姿态分析的开源工具箱，是 <a href="https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Fopen-mmlab">OpenMMLab</a> 项目的成员之一。</p><p><img src="https://user-images.githubusercontent.com/26127467/226655503-3cee746e-6e42-40be-82ae-6e7cae2a4c7e.jpg" alt="yolox-pose_intro"></p><blockquote><p><a href="https://gitee.com/open-mmlab/mmpose">mmpose: 基于 PyTorch 的姿态估计算法库，支持人体、人手、人脸、动物、服装等多类物体的 2D/3D 姿态估计。 (gitee.com)</a></p></blockquote><h2 id="5-slow-fast"><a href="#5-slow-fast" class="headerlink" title="5.slow fast"></a>5.slow fast</h2><blockquote><p>CNN 领域 比较强的一个模型</p><p>使用双流 + 帧内空间特征和帧时间序列特征学习</p><p><strong>提出了一种新的快慢网络SlowFast架构，来实现两个分支分别对时间与空间维度进行处理分析</strong>。</p><ul><li><strong>Slow分支</strong>：较少的帧数以及较大的通道数学习空间语义信息。</li><li><strong>Fast分支</strong>：较大的帧数以及较少的通道数学习运动信息</li></ul></blockquote><p><img src="../assets/1%20(4" alt="1 (4)">.png)</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/457854899">深度学习视频理解（分类识别）算法SlowFast原理解读 </a></p></blockquote><p><img src="../assets/1%20(2" alt="1 (2)">.gif)</p><blockquote><p><a href="https://github.com/facebookresearch/slowfast">facebookresearch/SlowFast: PySlowFast: video understanding codebase from FAIR for reproducing state-of-the-art video models. (github.com)</a></p></blockquote><h2 id="6-Multi-person-Real-time-Action-Recognition-Based-on-Human-Skeleton"><a href="#6-Multi-person-Real-time-Action-Recognition-Based-on-Human-Skeleton" class="headerlink" title="6.Multi-person Real-time Action Recognition Based-on Human Skeleton"></a>6.Multi-person Real-time Action Recognition Based-on Human Skeleton</h2><p><img src="../assets/1%20(3" alt="1 (3)">.gif)</p><p><a href="https://github.com/felixchenfy/Realtime-Action-Recognition?tab=readme-ov-file">felixchenfy/Realtime-Action-Recognition：将 ML 应用于 OpenPose 中的骨架动作识别（警告：很抱歉，这只适用于课程演示，不适用于现实世界的应用程序!!那些非常困难!!） (github.com)</a></p><p><img src="../assets/1%20(3" alt="1 (3)">.png)</p><h2 id="资源汇总"><a href="#资源汇总" class="headerlink" title="资源汇总"></a>资源汇总</h2><p><a href="https://developer.aliyun.com/live/251465">达摩院-开放视觉智能实验室 | OpenVI Tech Talk No.5：视频动作识别前沿技术介绍-云视频-阿里云开发者社区 (aliyun.com)</a></p><blockquote><p> 论文list</p></blockquote><p><a href="https://github.com/jinwchoi/awesome-action-recognition">https://github.com/jinwchoi/awesome-action-recognition</a></p><blockquote><p> 阿里云开放平台</p><p> <a href="https://help.aliyun.com/zh/viapi/developer-reference/api-action-recognition?spm=a2c4g.11186623.0.0.65cf2ccfIO6Mgu">动作行为识别功能介绍及使用方法 (aliyun.com)</a></p></blockquote><p>视频动作识别分类</p><blockquote><p>cnn 卷积算法</p><p>transformer （vision transformer  video swin transformer  shift transformer ）</p><p>self -supervised方法（video MAE  (Encoder-Decoder 模型)    Masked方法）</p><p>多模态（视频 文本 图片  /clip ）</p><p><a href="https://zhuanlan.zhihu.com/p/639351055">视频动作识别前沿技术介绍)</a></p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/456817021">深度学习视频理解（分类识别）算法梳理</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;视频动作识别调研&quot;&gt;&lt;a href=&quot;#视频动作识别调研&quot; class=&quot;headerlink&quot; title=&quot;视频动作识别调研&quot;&gt;&lt;/a&gt;视频动作识别调研&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;视频动作分类技术【视频分类任务】&lt;/p&gt;
&lt;p&gt;学习视频中时间和空</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mzl_古典写作训练营</title>
    <link href="https://www.fomal.cc/posts/bf4a1918.html"/>
    <id>https://www.fomal.cc/posts/bf4a1918.html</id>
    <published>2023-12-24T07:54:03.000Z</published>
    <updated>2023-12-28T09:28:54.728Z</updated>
    
    <content type="html"><![CDATA[<h1 id="古典老师共读会"><a href="#古典老师共读会" class="headerlink" title="古典老师共读会"></a>古典老师共读会</h1><blockquote><p>了解古典老师，老师有一个职业规划相关的课程，感觉很打动自己，自己一直在思考 到底是什么东西的缺失——其中一样是职业规划上没有做好</p><p><a href="https://zhuanlan.zhihu.com/p/129650975">古典老师职业规划公开课（7000字全） - 知乎 (zhihu.com)</a></p><p>首先是没有这个意识，再次是没有这个机会，也丝毫从来没想过</p></blockquote><p>古典老师  带领写文章，完成读书与知识系统搭建的全套流程</p><blockquote><p> 读书不是为了读书而读书，而是为了内化知识，让自己变得更加智慧</p></blockquote><ol><li>书中并没有知识，而有的只是信息，只有经过读书人内化之后，与自己的过去经历和思考之后的东西形成链接之后才会叫知识；<code>这一点很打动我</code></li><li>新知与旧概念的关系</li><li><strong>知识就是能够降低生活中所有的不确定性</strong></li></ol><h2 id="day0-开营"><a href="#day0-开营" class="headerlink" title="day0 开营"></a>day0 开营</h2><blockquote><p> 开营汇总链接 ——课程安排和介绍</p><p><a href="https://dxpqyi2j4o.feishu.cn/docx/EFGpdFZfZo1pb8xtxVDckCtXnRb">Day0-共读会体验营开营整理 - Feishu Docs</a></p></blockquote><p>古典老师</p><p>第一部分：3天的课程安排<br>第二部分：【个人发展共读会】是什么？<br>第三部分：【个人发展共读会】包含哪些内容</p><p>学习感悟是将信息内化成自己的知识的过程，很重要</p><p>但是你真的问他，那你简单复述一下，读过的书的要点？你觉得和自己生活对应起来，有什么感悟？你是怎样用到自己生活工作里去的？</p><p>所谓经典，就是在一个领域里，能够把整个知识从头到尾串起来的作品。经典是碎片知识收纳器，是领域知识地图，是牛人思维复刻机。具体可以直接看我之前的文章<br>读经典 且包含整个系列的经典书籍</p><p><img src="../assets/image-20231224124510014.png" alt="image-20231224124510014"></p><p>从高空到地面到边界，点亮「个人发展全景图」。</p><p><img src="../assets/image-20231224124558054.png" alt="image-20231224124558054"></p><p>这是明天的早读链接大家提前预习，期待看到大家的感悟输出</p><h2 id="day1早"><a href="#day1早" class="headerlink" title="day1早"></a>day1早</h2><blockquote><p>早7:00 第一节课 <strong>如何真正读透一本书</strong></p></blockquote><p>古典老师02年新东方到23年一直在做的事情</p><p>为什么做个人发展共读会？以及怎样做的 或者说是做的形式是什么？</p><p>08年-23年一直在做 <strong>生涯发展</strong> <strong>个人发展共读</strong></p><blockquote><p><strong>拆掉思维的墙  10年再版</strong></p></blockquote><p><img src="../assets/image-20231224124226255.png" alt="image-20231224124226255"></p><p>为什么读了很多书 没改变？</p><blockquote><ol><li>读了 没理解<ol><li>读了用的时候想不起来</li></ol></li><li>读的书太杂，并不系统，来源不清晰稳定</li></ol></blockquote><p>第一期的小伙伴 在读跃迁   读跃迁书的内容很重要，但是更重要的是跃迁实现的方式 和这个算法的怎样造成影响以及怎样用这个算法到自己生活中 更重要</p><p>怎样跟业内的 高手交流 沟通的 请教的 以及有什么方式 把自己这么多年的积累 去应用</p><p>以及怎样从写作的角度 来进行反推  该怎样读书</p><p>两个部分</p><ol><li>怎样选书 <ol><li>个人发展共读会怎样去选书</li></ol></li><li>怎样读书 以及应用<ol><li>怎样读透一本书 怎样用卡片的方式去读书</li></ol></li></ol><p>知识分子  一辈子 做6件事 <strong>输入和输出</strong></p><p><img src="../assets/image-20231224124241336.png" alt="image-20231224124241336"></p><blockquote><p>算上教材编写270w字  2kw收入</p><p>每天听本书 kindle 微信读书</p></blockquote><ol><li><p>听别人演讲的时候  把自己读的书用上了 说出来 自己却不会用</p></li><li><p>出租车司机知识 —— <strong>小故事</strong>【相对论知识  司机蒙住了】——糟糕的知识，只会转述，只会引述，并不会用它解决问题</p><ol><li>出租车知识的承载者</li></ol></li><li><p>能讲出来 但是写不出来文章 <strong>写自己的感想写不出来</strong></p></li><li>大部头书 读不懂，看不完<ol><li><strong>社会心理学，发展心理学  追忆似水年华，百年孤独 毛泽东传 原则 人类简史</strong></li></ol></li></ol><blockquote><p>知识 ——&gt; 应用  读书≠知识</p></blockquote><p><img src="../assets/image-20231224124301394.png" alt="image-20231224124301394"></p><p>信息论发明者 克劳德·香农</p><p><img src="../assets/image-20231224124315596.png" alt="image-20231224124315596"></p><blockquote><p>品酒师  喝一口就能品出来  喝酒收钱</p><p>知识 vs 信息  （瓜）</p><p>越学越乱，并不能说明学到了知识</p></blockquote><h3 id="评估阅读深度"><a href="#评估阅读深度" class="headerlink" title="评估阅读深度"></a>评估阅读深度</h3><p><img src="../assets/image-20231224124330609.png" alt="image-20231224124330609"></p><blockquote><p>data 数据  information 信息【分门别类 图上颜色 分清好坏了 】  knowledge 知识（与过去有链接，并不单纯是一个点）【信息能够互相连接 】</p><p>恍然大悟insight【洞察】 形成了见解  wisdom 面对事情 【洞察形成一片 】能够打通形成智慧    impact 洞见【单点洞穿一片 】击穿 第一性原理，基础原理  底层原理  道</p><p>好的文章 都是一些洞察  写文</p></blockquote><h3 id="阅读的有多深，输出就有多好"><a href="#阅读的有多深，输出就有多好" class="headerlink" title="阅读的有多深，输出就有多好"></a>阅读的有多深，输出就有多好</h3><p>发圈—— 刷过</p><p>好奇——读懂了之后，引发好奇</p><p>应用到生活中——学会了某个知识，与过去形成链接</p><p>两个好的知识点以一种独特的形式链接在一起 就是写文——洞见</p><p>有一个很好的打通 —— 写书</p><p>经典——一个点贯穿系统，击穿人类知识体系</p><blockquote><p>反脆弱这个概念 击穿了所有文章</p><p>原则 用人像机器一样进化  击穿了所有的算法</p><p>人类简史 把讲故事 击穿了整个人类过程</p><p>相对论 用光速不变 击穿了所有时间进化</p></blockquote><p><img src="../assets/image-20231224124340520.png" alt="image-20231224124340520"></p><h3 id="读书从来不是要记忆，重要的是要理解和链接"><a href="#读书从来不是要记忆，重要的是要理解和链接" class="headerlink" title="读书从来不是要记忆，重要的是要理解和链接"></a>读书从来不是要记忆，重要的是要理解和链接</h3><blockquote><p>新知识永远都是新东西和新概念链接到旧的只是上来获得的</p></blockquote><p><img src="../assets/image-20231224124402340.png" alt="image-20231224124402340"></p><p>书单 是自己个人过去的链接，以及与未来相关的一个发展方向展示</p><blockquote><p>有过去</p><p>写故事 编剧  戏剧的书 用戏剧方式表达出来</p><p>短视频时代，每个人都要学会借助人的视觉化思维，要用故事化的方式进行表达</p><p>书评 了解 书评是别人嚼过的口香糖</p><p>自己能把他复述一遍，遇到问题可能还不能用</p></blockquote><h3 id="知识存储方式"><a href="#知识存储方式" class="headerlink" title="知识存储方式"></a>知识存储方式</h3><blockquote><p>错误的存储方式</p></blockquote><p><img src="../assets/image-20231224124412324.png" alt="image-20231224124412324"></p><blockquote><p>真实的存储结构</p></blockquote><p><img src="../assets/image-20231224124443032.png" alt="image-20231224124443032"></p><p>多去读增量知识，而不是延展概念</p><blockquote><p>关键知识和原知识，能够吸引很多其他的小知识到一起</p><p>这种知识应该多去读</p></blockquote><h3 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h3><p><img src="../assets/image-20231224124454161.png" alt="image-20231224124454161"></p><blockquote><p>读书这件事不重要，有链接和理解才是最重要的</p><p>不断内化+ 不断应用+ 复习深入理解</p></blockquote><h4 id="个人复述"><a href="#个人复述" class="headerlink" title="个人复述"></a>个人复述</h4><blockquote><p>读书 和阅读深度</p><p>数据  信息  知识  洞见  智慧 击穿 一套体系</p><p>复述并不断与旧的只是形成链接</p><p>原知识 扩展成现在的思维知识体系</p><p>知识的概念是能够不断的降低生活中的不确定性</p></blockquote><h3 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h3><p>day1作业： </p><ol><li>你之前是如何读一本书的？效果怎么样？<ol><li>这段时间会去图书馆读之前想读的书，或者之前偶然间碰到的书，凭借兴趣和想法，还有一些偶尔读到的书去延伸去读，包括专业方面（计算机专业方面）和非专业方面（社科，小说，传记）</li><li>听书，参加了樊登读书： 买了樊登读书的会员，每天偶尔会在耳边听一些书，或者随缘听一些想读的书</li></ol></li></ol><p>2.听完古典老师的课程，接下来你准备怎么读书？ </p><ol><li>整理下自己已经读过或者了解的书目，包括所有已知道的知识，尝试搭建一下个人读书系统</li><li>有目的的去读，思考下现阶段要做的事情以及会碰到的问题，或者未来想要成为什么样的人或者做什么样的事情，去读一些相关的书目并进行复述和理解，增强个人知识的链接并尝试不断应用</li></ol><blockquote><p>晚课汇总链接_——为什么读写是最好的个人投资</p><p> <a href="https://dxpqyi2j4o.feishu.cn/docx/JUc4drgTRodfNLxBSs1crdqanbe">共读会体验Day1晚课整理 - Feishu Docs</a></p></blockquote><h2 id="day2-早"><a href="#day2-早" class="headerlink" title="day2 早"></a>day2 早</h2><blockquote><p>卡片是什么 该怎么写 写多长时间 写多少字 400-500字左右</p></blockquote><h3 id="作业-1"><a href="#作业-1" class="headerlink" title="作业"></a>作业</h3><p>补day2作业:<br>1.学完今天的课程，卡片格式是什么？你对这几个部分是怎么理解的？ </p><p>卡片格式，一共分为三部分，包括核心概念，个人体验和行动指南;</p><ol><li>核心概念:【不是…而是..】首先可以先将原文抄录下来，然后用自己话进行总结复述一遍</li><li>个人体验：【我有一个经验、感悟、发现等，来加深体验】 ；应用实例、联系案例，更深更生动理解和呈现出概念的理解</li><li>行动指南：  【当……时候，要……做，具体落实】把书中对应的概念落实到生活，作为自己的行动指南顺便也可以给SS卡做一个标签和命名。             </li></ol><p>核心概念转述——【复述并记忆】</p><p>个人体验——【链接过去，加深理解】从以前的观念升级到另一个层面或是深度的理解</p><p>行动指南——【写下行动计划】作为自己的原则，生活中再碰到问题的时候翻阅卡片去应对</p><p>理解： 通过知识卡片，可以将自己学到的知识存起来，既可加深理解，又能偶尔翻阅，需要时也能及时找到。<br>是一个很好用的学习方法。 </p><p>2.你觉得卡片在你接下来的工作和学习中能帮你做些什么？</p><ol><li><p>思维的改变——转述+链接+ 应用</p></li><li><p>学习方法【各种知识的一个方法改变】</p></li><li>读书方法【应对读书时候的一种新的体验】</li><li>尝试往工作和其他方面去应用</li></ol><h3 id="新概念学习"><a href="#新概念学习" class="headerlink" title="新概念学习"></a>新概念学习</h3><p><strong>集装箱原理，存入的越难，提取就容易</strong><br>【我的理解】卡片可以帮助我们把知识内化成自己的知识体系，用自己的语言转述是建立自己对知识的理解和吸收上，把新知和自己的知识经验勾连起来，情绪体验越深越好，有了深度理解这个知识才记得更深。<br>在阅读时遇到自己感兴趣或者有触动的地方，可以停下来，针对这个部分进行理解用自己的语言转述，并与过去自己的经验和认知感悟进行连接发散，这样就可以记得更牢更深。<br>最后通过行动指引，把知识学以致用，通过行动指引存一个心锚，为未来遇到相关问题时提供行动指南。<br>我好像更明白为什么自己读了很多书，但心里依然觉得很空虚，很迷茫，解决问题的思维和能力没有得到提升。很关键的就是过去的读书只是在扫描信息，而没有把知识与过去勾连，也没有用知识来解决问题，破除不确定性，真的是存入越简单，提取就越难。卡片笔记让学习真正与自己有关，让知识更有深度和宽度</p><p>卡片可以帮助我更好的理解知识，让阅读留下思考、理解、运用的痕迹，通过攒卡片建立有体感的知识宫殿，提高自己的认知深度和运用知识的能力，为知识输出打下坚实的基础。</p><blockquote><p>卡片制作难，难存入但是将知识内化效率高，易输出。真正的能帮助深入链接、思考，形成知识网络。可以帮助我梳理文案，助于书写报告，已经短视频。慢慢的写，持续的写，未来自己的思考深度，写作能力一定会有变化！期待。</p></blockquote><p>输入的越难，提取的越容易。这是一种思维，能让我初步理解看网课和看书没用的原因，都是被动输入且没有思考，在做题和处理事件的时候才会没什么效果<br>（一百个人中就有一百个哈姆雷特，让这一切贴近自己）</p><blockquote><p> 能够有助于我建立自己的知识体系，当遇到有用的知识时，将其加上自己的理解记录下来，并做好标签，加深理解的同时也建立起自己的知识库，做到既能输入又能输出。</p></blockquote><p>1.学完今天的课程，卡片格式是什么？你对这几个部分是怎么理解的？<br>卡片格式是：原文摘录、概念转述、个人体验、行动指引。<br>原文摘录：就是在看书、文章、视频等过程中，发现有意思、有触动、有启发、有感想的内容，直接原文摘录下来。<br>概念转述：就是把原文用自己的话重新说一遍，也可以说是“翻译”一下。也可以说用其它的词重新组织一下，把这个意思表达出来。<br>个人体验：就是结合自己的一些经验、感触、也或是一些思考，对这个有意思的知识点再重新组织描述一下。<br>行动指引：有了思考，有了和自己经验的结合，如何再以后的工作学习中“实践”，对自己以后有哪些指引。<br>2.你觉得卡片在你接下来的工作和学习中能帮你做些什么？<br>找到一个领域或问题，进行有目的的选择一些书来阅读，然后用ss卡片法对书进行深度阅读，积累卡片逐步解决问题，加深理解，直至解决问题，成为行家或专家。</p><blockquote><p>晚课汇总链接——为什么普通人要有自己的作品</p><p>  <a href="https://dxpqyi2j4o.feishu.cn/docx/KqEId690LoUnkex6VzscclLvnqb">共读会体验营Day2晚课整理 - Feishu Docs</a></p></blockquote><h2 id="day3"><a href="#day3" class="headerlink" title="day3"></a>day3</h2><h3 id="作业-2"><a href="#作业-2" class="headerlink" title="作业"></a>作业</h3><p>古典将书分为3类</p><ol><li>修身养性的书【陶冶情操类】——边城</li><li>经典的书【架构性数目】——动机心理学</li><li>功能性数目【技术类】——秒赞 </li></ol><p>day3作业：<br>1.你觉得接下来应该读哪些书？<br>我觉得古典老师提到的三类书籍都可以读。经典书籍要保证，其它两类看自己的爱好和需要即可。<br>2.你觉得选书和掌握一套善思会写的方法哪个更帮到你？为什么？<br>两个都有用。选好书读，会少走很多弯路，选好书相当于你可以上高速路，免得在乡间小路奔波。而善思会写的方法会帮我们读得更深甚至完成一部作品，它相当于给了你一个清晰的导航或者地图，否则很难或者永远也来不到目的地。</p><h3 id="直播"><a href="#直播" class="headerlink" title="直播"></a>直播</h3><blockquote><p>弟弟在直播间中每一个明星 打造了每个ip  需要回答2000 多个问题 </p></blockquote><p>读写的好处</p><ol><li>管理情绪 拖延 焦虑 抑郁</li></ol><blockquote><p>在书中 找到关于情绪 的各种东西   </p><p>比如抑郁  焦虑  拖延  </p><p>将小拖延 整理下来 放到大的拖延前面去</p></blockquote><p>管理拖延</p><p>读书就像串门一样，每一本书 都是一个人，</p><p>读书好比隐身的串门儿，要参见钦佩的老师或拜谒有名的学者，不必事前打招呼求见，也不怕搅扰主人。<br>——杨绛</p><p>正念书写 </p><p><img src="../assets/image-20231224124706929.png" alt="image-20231224124706929"></p><p><strong>1，焦虑 恐惧  —— 不确定的恐惧</strong></p><p>写下你的情绪 和 焦虑，写着写着就变好了，书写能够舒缓你的焦虑</p><p>2.情绪 心情 的即刻获得，与身边朋友 老师 家人产生化学反应，能够获得以后有钱后才能获得的那种感觉</p><blockquote><p>比如他说的 下边那一种都能给自己带来回报的（姐姐和妹妹之间表达感情 书写感情）</p></blockquote><p><img src="../assets/image-20231224124717756.png" alt="image-20231224124717756"></p><blockquote><p>所有的东西 都是文字</p><p>鼓励每个人给自己的家庭写一份信，</p><p>记录父母的人生，父母看的时候 会很心安</p></blockquote><p><img src="../assets/image-20231224124730405.png" alt="image-20231224124730405"></p><p>功利化的 对待读书 这件事情</p><ol><li>改变生活，知识改变命运</li></ol><blockquote><p> 怎样摆脱一般人</p><p> 读一些一般人不会读的书，总结一些一般人不会总结的</p></blockquote><p><img src="../assets/image-20231224124739418.png" alt="image-20231224124739418"></p><p><img src="../assets/image-20231224124750688.png" alt="image-20231224124750688"></p><blockquote><p>3.6倍溢价</p></blockquote><p><img src="../assets/image-20231224124807078.png" alt="image-20231224124807078"></p><p><img src="../assets/image-20231224124817439.png" alt="image-20231224124817439"></p><h1 id="参与读写共读会"><a href="#参与读写共读会" class="headerlink" title="参与读写共读会"></a>参与读写共读会</h1><blockquote><p> 目标： 成为推广大使【成为带课班班】+参加线下活动认识 + 认识很多新朋友，感觉好久没有新朋友认识了</p><p> 自律就是找到自己的节奏，成长就是找到自己的方式</p></blockquote><p>12.30号  面基  </p><p><img src="../assets/image-20231224124828513.png" alt="image-20231224124828513"></p><blockquote><p>家书的力量</p><p>带领孩子 父母 一起写起来</p><p>写作是跨越时空的</p></blockquote><p>已更新25本书直接听</p><p>1.每个月有8节解读课<br>2.初阶写作营（可延后学）【21天】<br>3.高阶写作营（可延后学）【28天】<br>4.AI写作营（可延后学）</p><p>3门成长课（可以永久回看，慢慢看就行）<br>6.已经更新的24本书的解读课（选修，看不看都行，或挑自己喜欢的看也可以）<img src="../assets/image-20231224124840904.png" alt="image-20231224124840904"></p><p>5个人发展大地图邮寄</p><blockquote><p>樊登老师 每周六更新一节课 </p><p>先把上周的看了</p><p>越早完成21 天读写应 越好 + 进阶的课程</p><p>养成读写思维习惯</p></blockquote><h1 id="赚回学费"><a href="#赚回学费" class="headerlink" title="赚回学费"></a>赚回学费</h1><h3 id="1-稍微小赚"><a href="#1-稍微小赚" class="headerlink" title="1.稍微小赚"></a>1.稍微小赚</h3><blockquote><p>读3-5本书 +践行一个方法，改变自己的生活方式</p><p>刷视频的时间 转成刷同学的卡片+ 同学联机的行动，获得N多个不同的视角读书体验和收获，杜绝长期刷视频的习惯</p></blockquote><h3 id="2-200-到无穷"><a href="#2-200-到无穷" class="headerlink" title="2.200%到无穷"></a>2.200%到无穷</h3><blockquote><p>深度读6本书 + 生活工作中真实践行 + 提升自己不同维度的认知</p><p> 完成21 天和28天  一套好的读写思维+掌握好文章的底层方法</p><p>一本书写5-10张卡片，养成习惯后 积累自己的人生体验，形成自己转述的独特人生素材</p><p>利他者，影响着 成为志愿者：支持他人，以教带学</p><blockquote><p>内部成为志愿者，组长 助教 带教（有偿） 发起活动</p><p>外部 分享经历，影响更多人，申请共读大使，获得副业收入</p></blockquote></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;古典老师共读会&quot;&gt;&lt;a href=&quot;#古典老师共读会&quot; class=&quot;headerlink&quot; title=&quot;古典老师共读会&quot;&gt;&lt;/a&gt;古典老师共读会&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;了解古典老师，老师有一个职业规划相关的课程，感觉很打动自己，自己一直在思考</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>晨读日更</title>
    <link href="https://www.fomal.cc/posts/3e014ae4.html"/>
    <id>https://www.fomal.cc/posts/3e014ae4.html</id>
    <published>2023-12-22T01:33:26.000Z</published>
    <updated>2024-01-02T14:18:21.961Z</updated>
    
    <content type="html"><![CDATA[<h1 id="12-22日冬至-周五"><a href="#12-22日冬至-周五" class="headerlink" title="12.22日冬至 周五"></a>12.22日冬至 周五</h1><p><img src="../assets/image-20231224124904345.png" alt="image-20231224124904345"></p><h2 id="英语角"><a href="#英语角" class="headerlink" title="英语角"></a>英语角</h2><p>Sometimes <strong>accompanied</strong> ，sometimes alone,stay <strong>awesome</strong> all the time.<br>聚散终有时，潇洒走一回。(有时陪伴，有时独自一人，时刻保持精彩。)</p><blockquote><p> awesome 令人敬畏的;令人惊叹的;令人惊惧的, 很好的；了不起的。</p></blockquote><p>His actions were accompanied by a rambling monologue<br>他的行动伴随着一串冗长含糊的独白。</p><blockquote><p> rambling 杂乱无章的;冗长芜杂的;漫无边际的, 延伸, 向四处延伸的;蔓延到很大区域的;蔓生的, 散步, （尤指在乡间）漫步，漫游，闲逛, 话／文字。</p><p> monologue 独白 （一个人的）滔滔不绝的话，长篇大论, 独角戏。</p></blockquote><p>She still had a mind of her own, which is awesome.<br>但是她还是有自己的思想的，她其实是个很好的女人。</p><h2 id="2023-12-22晨读"><a href="#2023-12-22晨读" class="headerlink" title="2023.12.22晨读"></a>2023.12.22晨读</h2><p>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>俯瞰3000年儒学，读懂中华文明之根。<br>——来自帆书《中国儒学三千年》</p><p>👉【精彩选段】<br>“儒”这一名词最早记载见于《论语·雍也篇》。孔子在这里告诫他的学生子夏说：要当就当“君子儒”，千万不要当“小人儒”。</p><p>👉【作者之语】<br>对儒家精神的理解应从常识入手，不必人为赋予其某种神秘意味，==只要以平常心对人对己==，就能把握儒学的真谛。</p><p>👉【樊老师之语】<br>这是每个中国人都应该补的一堂文化课。作者从大历史的视角，逐一介绍各个时代的儒学大师的主要成就以及儒家流派、思想流派。一部中国史，就在3000年儒学史里，让我们一同走进3000年儒学中，感受中华文化的跳动脉搏。</p><h2 id="名人语录"><a href="#名人语录" class="headerlink" title="名人语录"></a>名人语录</h2><p><strong>能量加毅力可以征服一切 ———- 富兰克林</strong></p><p>在平凡的生活中，只要你肯付出、肯努力，总有一天，你会站在最亮的地方，活成自己渴望的模样。</p><h2 id="12-22日独白"><a href="#12-22日独白" class="headerlink" title="12.22日独白"></a>12.22日独白</h2><p>今天是冬至,但是是周五，要面临开组会了，今天早上看到老师985的车，看来组会一定要开了</p><blockquote><p>下边一段是读书会关于冬至的总结</p><p>&gt;</p><blockquote><p>『冬至已至』</p><p>💨【诗歌欣赏】</p><pre><code>      《满江红·冬至》          南宋·范成大    寒谷春生，熏叶气、玉筒吹谷。    新阳后、便占新岁，吉云清穆。</code></pre><p>💨【诗歌译文】</p><p>🔹寒冬季节，山谷里却早一步有了春意，蕙草初生新叶的香气就像袅袅的笛音若有若无地在山谷里弥漫开来。明早的太阳升起后就可占候新一年的年景了，肯定是丽日纤云、天气清和的好年景。</p><p>💨【关于冬至】</p><p>🔹古时有“日短”或“日短至”之称，这一天，北半球各地白昼最短，黑夜最长，越往北白昼越短。古人认为，冬至是指阴气到达极点，此后阳气渐渐回升～</p></blockquote><p>关于冬至新的理解</p><p><a href="https://zhuanlan.zhihu.com/p/448355529">冬至快乐！一起来了解一下冬至吧 - 知乎 (zhihu.com)</a></p></blockquote><p>冬至过后，来到最寒冷的时候，数九开始了；</p><p>太阳直射南回归线，北半球白昼最短，黑夜最长，出现极夜现象</p><h2 id="日知日进"><a href="#日知日进" class="headerlink" title="日知日进"></a>日知日进</h2><blockquote><p>今天听书 听了樊登老师跟作者 胡老师讲的《恰如其分的孤独》</p><p>这本书有点绕，但是作者跟樊老师的解读比较接地气，需要重新听两遍+好好理解理解</p><p>今年八月份新上的书</p><blockquote><p>恰如其分的孤独 豆瓣读书信息介绍: <a href="https://book.douban.com/subject/36522232/?_dtcc=1">恰如其分的孤独 (豆瓣) (douban.com)</a></p></blockquote><p>孤独作者给分了3类  最后给了5个生活中常用的原则</p><p>从自己的孤独 推论到个人的自尊水平 再到怎样在人际关系中处理孤独情绪，实现更好的独处</p></blockquote><p><strong>提到的其他讲过的书目《恰如其分的自尊》</strong></p><p>孤独 VS 独处 </p><blockquote><p> 孤独不是一种情绪，而是一种状态，然而当我们使用这个词的时候，他可能指的是事实意外的一种东西，也就是感觉——通常是伤心或者无聊： 比如你感受到孤独，那就去找朋友去玩，去嗨，去开心，但反而在那种环境下可能会让你更加孤独——这个人可能需要的是 陪伴，是理解，是爱，或者是喜欢这样的东西</p><p>一个人并不意味着孤独或者孤单，因为一个人独处也可以是快乐的</p><p>换言之，孤独和孤单的表达总是意味着缺失或者剥夺感</p></blockquote><p>在关系互动中，当我们的<strong>渴望无法满足</strong>，抑或<strong>关系的发展与期待有所差距时，人就会感到孤独。</strong></p><p>我们都能接纳孤独，享受孤独，学会与自己独处，因为孤独是生命的本色，也是人类最简单最真诚最长久的伴侣，接纳与觉察才是通透。</p><p>冬至 很舒服 当天晚上跟研一的两位小朋友聊期末考试，聊论文 给他们指导意见</p><p>同时也更新一下电子文档 </p><h1 id="12-23日考研日-周六"><a href="#12-23日考研日-周六" class="headerlink" title="12.23日考研日 周六"></a>12.23日考研日 周六</h1><h2 id="英语角-1"><a href="#英语角-1" class="headerlink" title="英语角"></a>英语角</h2><p>Trees and flowers are what you want</p><p>满树繁花，你是心之所向。</p><h2 id="23-12-23晨读"><a href="#23-12-23晨读" class="headerlink" title="23.12.23晨读"></a>23.12.23晨读</h2><p>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>面朝大海，春暖花开。<br>        ——来自帆书·李蕾讲经典《海子的诗》</p><p>👉【精彩选句】<br>✨风后面是风，天空上面是天空，道路前面还是道路。</p><p>✨雨是一生的过错，雨是悲欢离合。</p><p>✨陌生人，我也为你祝福：愿你有一个灿烂的前程 ，愿你有情人终成眷属，愿你在尘世获得幸福，我也愿面朝大海，春暖花开。</p><p>👉【李蕾老师之语】<br>海子是一个传奇，这要靠时间来证明。海子不幸福，做一个幸福的人，是明天才会发生的一件事。海子的诗，每个人一生至少要读一次。悟透诗意的内核是“身在井隅却心向星光”的人生韧劲。</p><h2 id="今日简报"><a href="#今日简报" class="headerlink" title="今日简报"></a>今日简报</h2><p>12月23日星期六</p><p>1、网游不得设置“每日登录、首次充值”等诱导性奖励！国家新闻出版署公开征求意见。</p><p>2、积石山6.2级地震造成甘肃117人遇难，确认无失联人员！现场搜救已结束。</p><p>3、医生手术时捶打患者，爱尔眼科：贵港爱尔CEO免职、院长停职。</p><p>4、公安部：依法严打网红大V造谣传谣等乱象。</p><p>5、北大燕南园获联合国文化遗产保护奖。</p><p>6、红海航线7500亿元货物紧急改道，红海危机危冲击全球贸易。</p><p>7、特斯拉上海储能超级工厂项目正式启动，年产能高达1万台。</p><p>8、明年起，我国对部分抗癌药、罕见病药实施零关税。</p><p>9、游戏整改征求意见引圈内巨震：网易股价跌近 30%，腾讯跌超 15%。</p><p>10、无锡辟出首批150余个避寒场所，热水、空调24小时不间断。</p><p>11、安理会再次==推迟巴以问题==最新决议草案表决。</p><p>12、日本政府决定向美国提供“爱国者”防空导弹。</p><h2 id="名人语录-1"><a href="#名人语录-1" class="headerlink" title="名人语录"></a>名人语录</h2><p>✨我们应该遵循自己内心的良知，而不是遵循来自外部的赞扬或谴责。——《身份的焦虑》</p><p>你人生的起点并不是那么重要<br>重要的是你最后抵达了哪里 </p><p>—- 巴菲特</p><h2 id="12-24日独白"><a href="#12-24日独白" class="headerlink" title="12.24日独白"></a>12.24日独白</h2><p>永远都不要给自己的人生设限，要时刻相信你的潜力。要对所有美好事物充满好奇心，要不断学习去充盈自己，要相信你的潜能无限。只要用心去做，一切皆有可能。新的一天，早安！</p><blockquote><p>lyq 去了重庆，自己早上跟德玉商量30号的读书会线下活动</p><p>独享实验室——略感孤独的同时，最近信息太杂乱了，有点摸不清头绪</p><p>周六上午 做调研 研究视频动作相关的开源工具，同时也为周天晚上开会做准备</p><p>下午被龟给咬了哈哈哈</p></blockquote><p>晚上去槐荫区吃饭 真的以后不能去吃自助了，会把胃给吃坏的</p><p>平时控制太严苛，一下子吃这么多 真的会把胃给搞坏</p><blockquote><p>不过确实很值，自己一个人吃这么多，然后还轻轻松松吃回本</p><p>就是唯一一个缺点就是太远，来回跑太冷 太远了，晚上等车这么久 去的时候 4:30 出发 一开始导航还导错了  到了6.30了</p><p>吃完饭7点多 到学校 9点了 正好买完鸡蛋 回来</p></blockquote><p>==找地方 还蠢蠢的去问别人 哎找老大爷 大妈去问路，却不自己质疑自己是错误的==</p><h2 id="日知日进-1"><a href="#日知日进-1" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.23【日知日进】日历<br>📚日知书目：《跨越不可能》</p><p>✨日进思考：“想要快速入门一个领域，该如何选择合适的图书?”</p><pre><code> 🔹尝试按照以下标准，通读五本书。第一本书：关于这个话题最流行、最畅销的书，目的是激发兴趣。第二本书：一本同样很受欢迎，但通常更专业、与主题相关性更强的书。第三本书：关于这个主题的半技术性读物——这本书仍然颇具可读性，也很有趣，但可能略有些枯燥。第四本书：关于这个主题的第一本真正深奥的专业书，开始正式学习。第五本书：关于这个主题发展方向、前沿信息的书，目的是了解未来趋势。</code></pre><p>💫共读推荐：<br>     🔹《意志力》<br>     🔷我们大部分人的问题不是没有目标，而是目标太多。</p><h4 id="2023年12月23日星期六"><a href="#2023年12月23日星期六" class="headerlink" title="2023年12月23日星期六"></a>2023年12月23日星期六</h4><p>读书分享：《说文解字十二讲》<br>分享书友：会淇<br>距离新年还有8</p><p>我们“姓氏”姓名离我们最近，谁都关心与自己最亲近的东西。尽管每个人的姓名跟了自己多年，却很少有人知道它们原本是什么意思。名太多，不便一一指认，暂且置之不论。而姓是有限的，可以一一道来，如<strong>百家姓里的“赵钱孙李周吴郑王”</strong>，等等。那么这些姓是从哪来的？它们原始的造字意图又是什么？要回答这些问题，还得从“姓氏”二字说起。<br>“姓者，统于上者也；氏者，别于下者也。”“姓”比“氏”的涵盖范围大，且出现得早一些。《左传·隐公八年》：“天子建德，因生以赐姓，胙之土而命之氏。”后来合称“姓氏”。殷商时代，帝王还无姓氏。甲骨文中，帝王、后妃是用天干（甲乙丙丁等）作称谓，又男称祖，女称妣，或别以文武、大小，如祖乙、祖辛、祖丁、大甲、中丁、小乙、武丁、文丁、妣甲、妣丙、妣庚等。<strong>周秦时代有了“姓”，</strong>开始主要是氏族、部落的徽号标记，与后世的姓氏多有不同。<br>     <strong>姓，原本指的是母亲这一系的血缘关系；氏，指的是父亲这一系的血缘关系。所以在先秦时期的同姓不婚，是有着科学依据的</strong>现在我们基本都知道了，姓，又称姓氏，即名字中的第一个字，各个家族都有自己特定的姓氏，这是更改不了的。名就是名称，用来区分每一个个体。相比于姓氏，名的范围要大的多，这样就容易区分每一个个体</p><h3 id="故事会"><a href="#故事会" class="headerlink" title="故事会"></a>故事会</h3><p>相传， 仓颉“始作书契， 以代结绳。”<br>在此以前，人们结绳记事，<br>即大事打一大结，<br>小事打一小结，<br>相连的事打一连环结。<br>后又发展到用刀子在木竹上刻以符号作为记事。</p><p>随着历史的发展，<br>文明渐进， 事情繁杂，<br>名物繁多， 用结和刻木的方法，<br>远不能适应需要，<br>这就有创造文字的迫切要求。</p><p>有一年，<br>仓颉到南方巡狩，<br>以“羊马蹄印”为源灵感。</p><p>仓颉日思夜想，到处观察，<br>看尽了天上星宿的分布情况、<br>地上山川脉络的样子、<br>鸟兽虫鱼的痕迹、<br>草木器具的形状，</p><p>描幕绘写，<br>造出种种不同的符号，<br>并且定下了每个符号所代表的意义。</p><p>他按自己的心意用符号拼凑成几段，<br>拿给人看，经他解说，<br>倒也看得明白。<br>仓颉把这种符号叫做“字”。</p><p><img src="../assets/image-20231225175319418.png" alt="image-20231225175319418"></p><p><img src="../assets/image-20231225175324818.png" alt="image-20231225175324818"></p><h1 id="12-24日考研第二日-周天"><a href="#12-24日考研第二日-周天" class="headerlink" title="12.24日考研第二日 周天"></a>12.24日考研第二日 周天</h1><p>📚12/18—12/24一周书目单</p><p>1.《不抱怨的世界》<br>2.《列奥纳多·达·芬奇传》<br>3.《你就是孩子最好的玩具》<br>4.《鸢屋经营哲学》<br>5.《魔术师时代：哲学的黄金十年》<br>6.《沃顿商学院最受欢迎的谈判课》<br>7.《帮孩子摆脱焦虑》<br>8.《为未知而教，为未来而学》<br>9.《中国儒学三千年》<br>10.《海子的诗》<br>11.《跨越不可能》<br>12.《苦乐参半》<br>13.《不被父母控制的人生》</p><h2 id="英语角-2"><a href="#英语角-2" class="headerlink" title="英语角"></a>英语角</h2><p>Meet you like the wind<br>遇见你，温澜如风。</p><h2 id="23-12-24晨读"><a href="#23-12-24晨读" class="headerlink" title="23.12.24晨读"></a>23.12.24晨读</h2><p>☃️2023.12.24晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>生活中既有喜剧，也有悲剧。<br>      ——来自帆书·非凡精读《苦乐参半》</p><p>👉【精彩选段】<br>苦乐参半其实是无处不在的，而且我们作为人，无一幸免。</p><p>有痛苦、有快乐、有获得、有失去、有爱、有悲伤才是生活的本来面目。</p><p>👉【作者之语】<br>不是有了痛苦就能创作艺术，而是有了创造力才能够正视痛苦。</p><p>👉【推荐之语】<br>生活中有甜蜜的日子，也有悲痛落泪的日子，苦乐参半其实是人生的常态。有时，痛苦会让我们的人生更完整，也更富有创造力。 </p><h2 id="今日简报-1"><a href="#今日简报-1" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月24日，星期日</p><p>1、<strong>人民币超过日元成为国际支付第四大货币！创最高纪录。</strong></p><p><strong>2、联合国通过决议将春节作为联合国假日。</strong></p><p>3、清华“铊中毒案”受害者朱令去世，刚过50岁生日!</p><p>4、甘肃省积石山县中小学开始有序复课。</p><p>5、2024年考研昨日开考，438万考生奔赴考场。</p><p>6、国有大行年内第三轮下调存款利率，助力消费和需求回升。</p><p>7、冰冻周即将收场！本周末中东部大部气温陆续回升。</p><p>8、公安机关依法行政处罚编造发布地震谣言网民11人、批评教育105人。</p><p>9、泰国宣布控枪措施，停发私人持枪证一年。</p><p>10、我国未成年网民突破1.93亿，九成未成年人拥有属于自己的上网设备。</p><p>11、国产首艘大型邮轮启航，开启试运营航次。</p><p>12、美国商业地产雷声滚滚，洛杉矶第三高楼折价近半卖出。</p><h2 id="名人语录-2"><a href="#名人语录-2" class="headerlink" title="名人语录"></a>名人语录</h2><p>✨人的天性就是这样不完美的！就连最明亮的星球上也会有黑点。——《简˙爱（上）》</p><p>人有了物质才能生存<br>有了理想才谈得上生活<br>———- 雨果</p><h2 id="12-24日独白-1"><a href="#12-24日独白-1" class="headerlink" title="12.24日独白"></a>12.24日独白</h2><p>有人给予你帮助，那是幸运；<br>没人给予你帮助，那是命运。<br>在幸运青睐自己的时候学会感恩，<br>在命运磨练自己的时候学会坚韧。<br>新的一天，早安！</p><blockquote><p>早上弄龟  佛系 睡到自然醒  </p><p>为晚上会议做准备 </p></blockquote><p>博客崩溃的一天，重新整理  原来最后是不知道为什么，就只有那两个文件出错误，真懵逼了</p><blockquote><p>视频理解动作识别调研， 古典老师阅读整理 </p><p>晚上八点会议 </p><p>在龟身上画了太长时间了，弄得自己周报也没好好写，到周一早上早起来写的，结果周一早上来了实验室 还是弄龟 真的下头</p></blockquote><h2 id="日知日进-2"><a href="#日知日进-2" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.24【日知日进】日历<br>📚日知书目：《不被父母控制的人生》</p><p>✨日进思考：“和父母同住，如何和谐相处?”</p><pre><code> 🔹与父母同住是把“双刃剑”，和谐相处的关键是建立恰当的边界感。慢下来，控制好自己的情绪，与其说“你必须”“你赶紧”，不如说“给我点儿时间，让我考虑一下”，让自己的情绪慢下来。“圆滑”一些，避其锋芒。可以不认同父母让你做的事，但需要认同父母的感受。一句“妈，我知道你很生气”，就能表达亲人间的关怀。用提问的方法，引导互动走向。与父母争执时，不要局限在问题表面。试试用提问的方式掌控局面，与父母深入探讨问题的本质。</code></pre><p>💫共读推荐：<br>     🔹《父母的羁绊》<br>     🔷把原谅换成爱，才是家庭变好的开始。</p><h3 id="故事会-1"><a href="#故事会-1" class="headerlink" title="故事会"></a>故事会</h3><p>💞美好的亲子共读时光开始啦<br>今日共读内容<br>神农尝百草</p><p>上古时候，<br>五谷和杂草长在一起，<br>药物和百花开在一起，<br>哪些粮食可以吃，<br>哪些药草可以治病，<br>没人分得清。</p><p>黎民百姓靠打猎过日子，<br>然而天上的飞禽越来越少，<br>地上的走兽越来越稀，<br>人们就只好饿肚子。</p><p>老百姓的疾苦，<br>神农氏看在眼里，<br>决定走遍天下，<br>分辨出五谷和草药。</p><p>神农翻山越岭寻找各种<br>可以吃的种子果实，<br>和能够治病的药草。</p><p>山上长满奇花异草，<br>远远就闻到香气，<br>神农亲自采摘下来，<br>放到嘴里尝。</p><p>白天,<br>他到山上尝百草，<br>晚上映着火光，<br>把尝百草的经验详细记载下来：<br>哪些草是苦的，<br>哪些草是甜的，<br>哪些热，哪些凉，<br>哪些能充饥，<br>哪些能医病，<br>都记载得清清楚楚。</p><p>尝完一山花草，<br>又到另一山去尝，<br>踏遍了山山岭岭，<br>尝遍了每一种草。</p><p>他尝出了能充饥的果实、种子，<br>就叫人把种子带回去，<br>让百姓种植，<br>于是有了麦、 稻、黍（shǔ)、稷（jì)、菽(shū）五谷。</p><p>在路上遇到生病的百姓，<br>神农就用自己知道的药草为百姓治疗，<br>并把药方记录下来。</p><p>最后神农尝出了三百六十五种草药，<br>并写成 《神农本草》，<br>从此人们有了粮食吃，<br>病了也有药医治了。</p><p><img src="../assets/image-20231225175731589.png" alt="image-20231225175731589"></p><p><img src="../assets/image-20231225175736124.png" alt="image-20231225175736124"></p><h1 id="12-25日周一"><a href="#12-25日周一" class="headerlink" title="12.25日周一"></a>12.25日周一</h1><p>📚12/18—12/24一周书目单</p><p>1.《不抱怨的世界》<br>2.《列奥纳多·达·芬奇传》<br>3.《你就是孩子最好的玩具》<br>4.《鸢屋经营哲学》<br>5.《魔术师时代：哲学的黄金十年》<br>6.《沃顿商学院最受欢迎的谈判课》<br>7.《帮孩子摆脱焦虑》<br>8.《为未知而教，为未来而学》<br>9.《中国儒学三千年》<br>10.《海子的诗》<br>11.《跨越不可能》<br>12.《苦乐参半》<br>13.《不被父母控制的人生》</p><p>12.25-12.30日组队书单</p><p>思考输出，齐头并进</p><p>1⃣《理解未来的7个原则》</p><p>2⃣《目标感》</p><p>3⃣《常识工作法》</p><p>4⃣《为未知而教，为未来而学》</p><p>5⃣️《作文六要》</p><p>新的一年，读书明智，思考修心</p><h2 id="英语角-3"><a href="#英语角-3" class="headerlink" title="英语角"></a>英语角</h2><p>A glance at her eyebrows and eyes<br>可叹惊鸿一瞥，误入眉眼，欢喜多年。</p><h2 id="23-12-25晨读"><a href="#23-12-25晨读" class="headerlink" title="23.12.25晨读"></a>23.12.25晨读</h2><p>🍁2023.12.25晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>许多问题的解决方法都很显眼。简单地看待问题，是解决问题的第一步。<br>——来自帆书《如何成为更聪明的人》</p><p>👉【精彩选段】<br><strong>思考远远不止推理那么简单。</strong>思考<strong>既涵盖了推理，也包含了非理性的心理加工过程，诸如创新、解决问题、分析、综合和评估。</strong></p><p>👉【作者之语】<br><strong>敏捷思维就是教会我们“怎样在不确定的世界中生活而不会感到迟疑和不知所措”。</strong></p><p>👉【樊老师之语】<br>天才就是能够摆脱所受训练，做出有效反应的人。本书作者提出了一个新概念<strong>“敏捷思维”</strong>，目的就是梳理思考技巧，释放大脑智慧的潜力。让人们<strong>不仅不做大众观念的回收者，还能源源不断地想出好点子，在生活和工作中做出好决定。</strong></p><p>❤著名党史、军史专家<br>刘统先生的最新纪实作品<br>——《转折》</p><p>🎀本书采用<strong>图文结合的编排方式。</strong><br>根据实地考察成果，<br>作者绘制了转战陕北的详细路线图，<br>并辅以 80 余幅，<br>局部战斗形势图和历史现场照片，<br>将专业性与通俗性相结合，<br>为我们再现那个艰苦卓绝而充满希望的年代。</p><p>💟现在让我们一同打开《转折》，<br>走近为中国人民解放奋斗的先辈们。</p><h2 id="今日简报-2"><a href="#今日简报-2" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月25日   星期一</p><p>1、中疾控：目前<strong>南北方省份流感活动基本是同步的，都处于高发状态。</strong></p><p>2、<strong>我国已培养研究生超1100万，北京6岁以上人口每百人有9个读过研。</strong></p><p>3、国家卫健委：积石山地震伤员均得到及时有效救治，灾区正常诊疗服务已恢复。</p><p>4、河南安阳6名村民吃火锅时3死3伤！镇政府：系一氧化碳中毒。</p><p>5、搜索量暴增980%！65万人争抢1万个课程名额！“夜校”火爆出圈。</p><p>6、中国首批孤独症辅助犬“毕业”，将温柔陪伴“星星的孩子”。</p><p>7、云南农村5桌以上聚餐需报备，按规模大小实行分级指导。</p><p>8、印度公司两名高管被曝在印度遭逮捕！vivo回应：深感震惊，将采取法律措施。</p><p>9、浙江衢州200米街道开15家金店，三四线城市黄金消费火爆。</p><p>10、日本核污水排海前将不再确认氚浓度。</p><p>11、中国化妆品在韩国火了！对韩出口额暴涨190%。</p><p>12、豫园灯会点亮巴黎！700多只上海南翔小笼馒头不到一小时售罄。</p><h2 id="名人语录-3"><a href="#名人语录-3" class="headerlink" title="名人语录"></a>名人语录</h2><p>✨许多问题的解决方法都很显眼。简单地看待问题，是解决方法的第一步。<br>  来帆书APP 听《如何成为更聪明的人》</p><p>正念的核心是专注于当下的状态，不要被过去或未来的事情所干扰。<strong>无论是做事情还是与人交往</strong>，都应该全神贯注地投入其中，体验当下的感受。</p><p>当我们专注于当下时，就能更好地掌握自己的情绪和思维，避免因过去或未来的事情而感到焦虑和烦恼。</p><h2 id="12-25日独白"><a href="#12-25日独白" class="headerlink" title="12.25日独白"></a>12.25日独白</h2><p><strong>越是艰难，越要勇于攀爬，因为每一步不好走的路都是上坡路。当保持耐心，专注投入某件事时，就会发现心态变强了，困难也就变弱了。</strong><br><strong>新的一周，早安！</strong></p><p>晚上 吃饭伴音是多线程安全+ 调度</p><p>整理web 网页+ 听会议</p><p>安排论文任务+整理文章草稿，明天试着再发一篇文章</p><h2 id="日知日进-3"><a href="#日知日进-3" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.25【日知日进】日历<br>📚日知书目:《曾国藩的正面与侧面》</p><p>✨日进思考：“行事过于方刚的人，看似是强者，实则是弱者。”</p><pre><code> 🔹一个针尖如果特别长，长得都能从麻袋里扎出来，那么过不了几天它就会断了。太锋利的东西，保持不了太长时间。太锋芒毕露的人，也会平白给自己增添许多障碍。真正的强者能够在必要时选择圆滑柔软，做到和光同尘，不必事事刚强，也能为人称道。</code></pre><p>💫迁移阅读：<br>     🔹子曰：“君子和而不同，小人同而不和。”<br>     🔷孔子说：“君子追求与人和谐而不是完全相同、盲目附和，小人追求与人相同、盲目附和而不能和人相处。”</p><p>go学习过程中了解到的新概念</p><blockquote><p>ebay公司</p><p>ebay是最早进入国内的国际电商平台之一，早期阶段外贸产品销售中一直有优势，随着后期阿里巴巴崛起，ebay在国际产品销售舞台中所具有的作用也越来越小，不过总体来说在欧美等国家依然有很多用户购买中国产品时，会关注到在ebay平台上购买，ebay所涉及的外贸产品类别是异常多样化的，例如国内所生产的艺术品，化工产品，手工艺品以及各种类型的家电产品等，在ebay平台上都可以销售出去。那么ebay平台销售产品需要关注哪些问题呢?</p><p>作者：跨境电商钦雨论坛<br>链接：<a href="https://www.zhihu.com/question/287008109/answer/1130378964">https://www.zhihu.com/question/287008109/answer/1130378964</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-3c8bfd2b59a5335b2fc54ccc0bfa7963_720w.webp?source=1def8aca" alt="img"></p><blockquote><p>讲课老师孟老师从ebay 跳槽到 腾讯T4</p></blockquote><p>CS144 课程介绍和学习</p><p><a href="https://kangyupl.gitee.io/cs144.github.io/">CS 144: Introduction to Computer Networking (gitee.io)</a></p><blockquote><p> agent 概念<a href="https://blog.csdn.net/VucNdnrzk8iwX/article/details/79434572">什么是agent?-CSDN博客</a></p><p>ddd 概念 </p><p>Domain-Driven Design 领域驱动设计 ，是一种通过将实现连接到持续进化的模型来满足复杂需求的软件开发方法。</p><p> <a href="https://zhuanlan.zhihu.com/p/411866735">最近爆火的DDD到底是什么？一文带你落地DDD - 知乎 (zhihu.com)</a></p><p> go学习过程中的 一些可以参考的库源码 <strong><a href="https://github.com/kubernetes/sample-controller">sample-controller</a></strong></p><p><a href="https://github.com/kubernetes/sample-controller">kubernetes/sample-controller: Repository for sample controller. Complements sample-apiserver (github.com)</a></p><p> csig  腾讯6大部门之一，一个小分支</p></blockquote><p><a href="https://zhuanlan.zhihu.com/p/164882387">腾讯六大事业群介绍（下） - 知乎 (zhihu.com)</a></p><blockquote><p>腾讯t4 </p><p><a href="https://www.zhihu.com/question/29910703">腾讯T4专家工程师是什么水平？ - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/gu-yu">游戏开发随笔 - 知乎 (zhihu.com)</a>   曾经的腾讯t4专家的一个知乎专栏，2019年更新</p></blockquote><h2 id="故事会-2"><a href="#故事会-2" class="headerlink" title="故事会"></a>故事会</h2><p>《沈园的故事》<br>作者：夏雨晴</p><p>一个宋朝的园林，能够一代代传下来，到今天还依然有名，也许只有绍兴的沈园了。<br>  沈园的出名却是由一曲爱情悲剧引起的。诗人陆游和表妹唐琬在园壁上题写的两阙《钗头凤》是其中的热点。<br>陆游也许是宋朝最好的一个诗人，但肯定不是一个值得唐琬为他而死的人。<br>表妹唐琬是在一个秋天忧郁而逝的，临终前，她还在念着表哥那阙被后人传唱的《钗头凤》。自从这个春天，和陆游在沈园不期而遇后，病榻之上的唐琬就在低吟这阙伤感的宋词。<br>一枝梅花落在了诗人的眼里，这是南宋的春天，年迈的陆游再次踏进了沈园。在斑驳的园壁前，诗人看到了自己四十八年前题写的一阙旧词：红酥手，黄藤酒，满城春色宫墙柳。东风恶，欢情薄，一怀愁绪，几年离索。错，错，错。　春如旧，人空瘦，泪痕红浥鲛绡透。桃花落，闲池阁。山盟虽在，锦书难托。莫，莫，莫!<br>唐琬在临终的日子里，一遍遍回想自己和表哥那段幸福的岁月。<br>陆游二十岁时初娶表妹唐琬，两人诗书唱和，绣花扑蝶，就像旧小说中才子佳人的典型故事。<br>可惜这样的日子太短了，唐琬只记得有一天，婆婆对她说，他们两个太相爱了，这会荒废儿子的学业，妨碍功名的。<br>唐琬至死都没有想通，相爱也会是一种罪名。不过她更没相通的是，那个据说在大风雨之夜出生在淮河一条船上的诗人，后来又横戈跃马抗击金兵的表哥，竟然违不了父母之命，在一纸休书上签下了羞答答的大名。<br>陆游四十八年后重游沈园，发现了园壁间一阙褪色的旧词，也叫《钗头凤》，这是唐琬的词迹：<br>世情薄，人情恶，雨送黄昏花易落。晓风干，泪痕残。欲笺心事，独语斜阑。难，难，难。人成各，今非昨，病魂常似秋千索。角声寒，夜阑珊。怕人寻问。咽泪装欢。瞒，瞒，瞒!<br>在南宋的春天，一枝q梅花斜在了诗人的眼里，隔着梅花，陆游没能握住风中的一双红酥手。</p><blockquote></blockquote><h1 id="12-26日-周二"><a href="#12-26日-周二" class="headerlink" title="12.26日 周二"></a>12.26日 周二</h1><blockquote><p>早上8:55</p><p>做志愿者  写作业 免费听课 </p><p>关注读写力  </p><p>一个社群，一个社区</p></blockquote><h2 id="英语角-4"><a href="#英语角-4" class="headerlink" title="英语角"></a>英语角</h2><p>It’s late in the world and it’s autumn.<br>人间忽晚，山河已秋。</p><h2 id="23-12-26晨读"><a href="#23-12-26晨读" class="headerlink" title="23.12.26晨读"></a>23.12.26晨读</h2><p>☃️2023.12.26晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>不要让他人随意评判你，只有你能做自己的最终评判者。<br>——来自帆书·非凡精读《强势》</p><p>👉【精彩选段】<br>沟通中我们有权说“我不明白”，并不是因为我们笨，而是对方有责任解释清楚他们的逻辑和意图。</p><p>👉【作者之语】<br>当人际关系的矛盾层出不穷时，“强势”是一种以不变应万变的处理方式。</p><p>👉【主讲老师之语】<br>人际关系中，矛盾无处不在，每个人都可能遭受他人的“隐形操控”。这些隐形操控会让你怀疑自己、产生社交焦虑，甚至导致心理问题。这本书提供了<strong>三种强势做法、七类强势话术和十大强势原则</strong>，帮你在人际关系中快速识别操控、摆脱操控，成为“社牛”，强势做自己。</p><p>❄️2023.12.26晨读拓展</p><p>👉三种强势做法：<br>1.做决策<br>2.回应别人<br>3.帮助别人</p><p>👉七类强势话术：<br>01 “我是一张坏唱片”法<br>02 可行折中法<br>03 自由信息法<br>04 自我表露法<br>05 迷惑法<br>06 否定决断法<br>07 否定询问法</p><p>👉十大强势原则：<br>1️⃣也是基本的原则：你有权坚持自己的行为、想法和情感，并对产生的一切后果负责；<br>2️⃣坚持你要做的，不必解释；<br>3️⃣帮不到别人，也不用内疚；<br>4️⃣你有权改变想法；<br>5️⃣犯错不可怕，但要承担后果；<br>6️⃣你有权说“我不知道”；<br>7️⃣要与人交往，但不要刻意讨好；<br>8️⃣你有权做出“不合逻辑”的决定；<br>9️⃣你有权说“我不明白”；<br>🔟你有权说“我不在乎”。</p><h2 id="今日简报-3"><a href="#今日简报-3" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月26日   星期二</p><p>1、中办、国办：<strong>鼓励落实带薪年假安排职工除夕休息</strong>。</p><p>2、甘肃获捐款和中央部委下拨资金超18亿元。</p><p>3、杭州湾跨海铁路桥主体控制性工程开工。</p><p>4、教育部启动首批全国中小学科学教育实验区实验校建设。</p><p>5、四部门发文：严禁公立医院举债购置大型医用设备。</p><p>6、印尼一中资工厂爆炸，外交部：已致包括4名中方员工在内13人死亡。</p><p>7、美国政府5天逮捕5万名非法移民。</p><p>8、土耳其在伊拉克和叙利亚北部实施空袭。</p><p>9、<strong>腾讯调整微信支付和视频号组织架构，后续加大直播带货投入。</strong></p><p>10、多地辟谣医保统筹额度12月底清零。</p><p>11、中国或成为全球<strong>第一大汽车出口国</strong>：全球<strong>超6成新能源汽车销量来自中国</strong>。</p><p>12、一批侵犯企业权益的营销号被查处，营销号发企业不实消息只为索要公关费。</p><h2 id="名人语录-4"><a href="#名人语录-4" class="headerlink" title="名人语录"></a>名人语录</h2><p>✨不要让他人随意评判你，只有你能做自己的最终评判者。<br>    来帆书APP 听《强势》</p><p>有人说“我们不快乐的原因，是不知如何安静地待在房间里心平气和地和自己相处。”和自己相处好了，才会最舒服！步入中年，最好的活法，是让心安静下来，学着和自己相处。减少不必要的社交 ；降低过火的欲望；平复不安的情绪。过低温的生活，保持低温的心态，修好这颗心，余生皆好运</p><p>不要等待<br>时机永远不会恰到好处<br>—— —- 拿破仑·希尔</p><h2 id="12-26日独白"><a href="#12-26日独白" class="headerlink" title="12.26日独白"></a>12.26日独白</h2><p>不必纠结过去，更不必担忧未来，因为明天的答案，由你度过的每一个今天写就。新的一天，把握当下，努力向上，早安！</p><blockquote><p>重启后要做的事情</p><p>安装 caj软件  安装fumo软件</p></blockquote><p>大家平时的输出要有意识的分享哈<br>不管是问题 还是你的经验输出<br>我这次参加新精英的超级个体IP营 真的领悟到了那句</p><p>先发声，才有好事发生【持续分享，黄金万两 也是一个厉害的人物说的话】</p><p>真正的含义<br>利他分享不仅是他人认识你的被动社交名片</p><p>还可以帮你快速拿到反馈的<br>真的两周内我听完了二三十节课 感觉自己有准备的出发，结果真的做的时候发现还有很多未知的问题等着我</p><p>这时候又只有就问题找到能够解决的问题的同学老师咨询 再往前走~</p><p>我这次最大的收获就是打破了完美主义 先做起来才知道结果</p><p>我记得古典老师出了一个自制力的课 他说时间规划是从自知之明开始,因为我们平时做的这些规划，很多时候都不贴合自己的实际情况，那你真的就很难坚持下去,坚持不下去你就会啊，觉得自己不行啊，low逼啊</p><p>然后你就自我放弃 啊，过段时间好了伤疤忘了疼，又开始做计划</p><p>周而复始【焦虑 归根结底都是因为能力达不到目标造成的】【我明年的核心任务就是学会做规划</p><p>那我今年花了一年时间来做复盘———那时间放的足够的长 我就能看到自己的进步】</p><p>这就是我明年重点要解决的问题——当我忙的时候，我也要把读写放上</p><p>另外成年人的学习一定要主动 知道点什么就做什么 不要像学生一样 老师没说就不做</p><p>真的害死人</p><p>这是我曾经踩过的坑🕳️🕳️🕳️</p><p>超体营的时候 古典老师也是很绝啊</p><p>说世界只会奖励那些行动的人</p><p>我们每周提问都是：</p><p>你的问题是什么？<br>为此你做了什么？</p><p>就像我这次过去参加最终路演的产品</p><p>都不是我最初的产品 是最后一周临时做出来</p><p>那我们组的老师说 丽雯的产品都是吃百家饭做出来的</p><p>所以说大家一定要去打破这个思维上的禁锢~</p><p>不要怕麻烦 你得到哪怕一点点的东西，那个才真的是你~</p><p>否则和你有什么关系呢？</p><blockquote><p>想都是问题，做才是答案，完成比完美更重要</p><p>行动才是打败焦虑最好的办法，大脑吸收了营养才不至于因空洞而胡思乱想</p></blockquote><p>其实就是要资源为我服务~</p><p>我是这么理解的</p><p>我自己在这里带了这么久 我今年上半年很大一个转变就是：==从书从第一页开始读==</p><p>==到为我所有 思维转变==</p><p>==其实想想作者怎么谋篇布局和我有什么关系？==</p><p>==我只需要带走对我有帮助的一点就好呀==</p><p>其他的说再多再好都是虚的</p><p>当我明白这个理的时候 我就开始聚焦</p><p>==让身边的资源为我的成长服务啦==</p><p>读→理解→消化→输出，缺一不可</p><h2 id="日知日进-4"><a href="#日知日进-4" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.26【日知日进】日历<br>📚日知书目:《哲学的慰藉》</p><p>✨日进思考：“家徒四壁，不必汗颜；黄金万两，无可炫耀。”</p><pre><code> 🔹我们常常以为赚取更多钱财、买车、买包，就会更快乐；谈一场恋爱，就会更快乐……然而物质带来的快乐转瞬即逝。古希腊哲学家伊壁鸠鲁认为自由是能够真正快乐的重要条件，拿时间、精力换取金钱时，我们恰恰失去了自由。《瓦尔登湖》的作者梭罗说，他只要赚够一年所需的“热量”，工作六周就够了，剩下的时间谁也不伺候，就坐在湖边看书、走路、玩、观察自然，这才叫真正的自由。</code></pre><p>💫迁移阅读：<br>     🔹子曰：“贤哉回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！”<br>     🔷孔子说：“真是个大贤人啊，颜回！用一个竹筐盛饭，用一只瓢喝水，住在简陋的巷子里。别人都忍受不了那穷困的忧愁，颜回却能照样快活。真是个大贤人啊，颜回！”</p><p><img src="../assets/image-20231226115145223.png" alt="image-20231226115145223"></p><h1 id="12-27日-周三"><a href="#12-27日-周三" class="headerlink" title="12.27日 周三"></a>12.27日 周三</h1><h2 id="英语角-5"><a href="#英语角-5" class="headerlink" title="英语角"></a>英语角</h2><p>My journey is the stars and the sea.<br>我的征途是星辰大海。</p><h2 id="23-12-27晨读"><a href="#23-12-27晨读" class="headerlink" title="23.12.27晨读"></a>23.12.27晨读</h2><p>🍁2023.12.27晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>获得他人力量支持的人，拥有更强的免疫系统，更少患病。<br>——来自帆书《他人的力量》</p><p>👉【精彩选段】<br>迷失在负面的自我评价之后，没有人还能具有很好的能力表现。第二层次通过自我怀疑和自我贬低来毁灭高能力表现。</p><p>👉【作者之语】<br>你的表现有多好，不仅仅取决于你的技能如何，还取决于谁同你一起做了什么。</p><p>👉【樊老师之语】<br>作为群居动物，我们很难摆脱他人对我们的影响。本书为我们详细梳理了四个层次的人际相处模式，其中几乎涵盖人际交往中所有可能的情况，教给我们如何构建受益一生的人际关系。</p><p>🍁2023.12.27晨读拓展</p><p>👉【四种人际关系】</p><p>第一种人际关系叫作孤立状态，就是一个完全没有连接的wifi，你是独自的一个人。<br>第二种关系叫作坏的连接关系。就是这个人跟你关系很亲密，经常在一起交流，但问题是他让你特别不舒服，就是他总是责怪你，总是指出你的问题，总是要求你。<br>第三种关系，叫作看似美好的连接关系。<br>第四种连接关系最主要的特点就是互相关心，说实话，并且富有建设性。</p><h2 id="今日简报-4"><a href="#今日简报-4" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月27日    星期三</p><p>1、外交部：中方将对美国1家机构和2名个人采取反制措施。</p><p>2、甘肃震区已有序复商复市复课。</p><p>3、中国全面暂停进口，日本：将努力扩大扇贝对韩国及欧盟出口。</p><p>4、俄国家杜马主席：俄国家杜马网站将推出中文版本。</p><p>5、伊朗一高级顾问在叙利亚遭以军空袭死亡，伊朗总统莱希：以色列将“付出代价”。</p><p>6、步入全球第一方阵！我国自研空中出租车亮相。</p><p>7、伊朗准备与埃及复交，结束40余载恩怨。</p><p>8、沪深交易所公布除夕当日休市。</p><p>9、657个药品说明书试点大字版等改造。</p><p>10、广西市场监管局发布典型案例：病死猪被处理后卖出货值超600万。</p><p>11、电信诈骗从缅北转移到了印度、迪拜，印度约120万人口从事电信诈骗。</p><p>12、央媒：本溪花28亿除不净雪令人质疑。</p><p>✨获得他人力量支持的人，总有更强的免疫系统，更少患病。<br>   来帆书APP 听《他人的力量》</p><h2 id="名人语录-5"><a href="#名人语录-5" class="headerlink" title="名人语录"></a>名人语录</h2><p>很多时候，完成比完美更重要，在一次次完成中迭代，就是进步！</p><h2 id="12-27日独白"><a href="#12-27日独白" class="headerlink" title="12.27日独白"></a>12.27日独白</h2><blockquote><p>不是…… 而是…… 的结构 和理解（深入读书 深入理解）</p><ol><li>主要内容是 信息时代 成功方式 分析时间 做好判断  抓住机会 形成跃迁</li><li>成功不是线性的，而是非线性的  跳跃式的</li><li>失败的人分类（1. 管不住专注力的  2.不能分辨信息的 3.不能联机学习的）</li><li>看书的方法论（目录  序言（内容序 流量序  自序）附录  出版页信息）</li><li>高手成功是借力  （怎样借力  PEST ）</li><li>长周期 短周期看问题的不同之处</li><li>读书人的四个分类 </li></ol></blockquote><p>路线 go &amp;&amp;java 确定 </p><p>java 笔记 整理对比</p><blockquote></blockquote><h2 id="日知日进-5"><a href="#日知日进-5" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.27【日知日进】日历<br>📚日知书目:《危机领导力》</p><p>✨日进思考：“不愿意驶入风暴的人，永远无法取得胜利。”</p><pre><code> 🔹只有愿意驶进风暴，愿意在最激烈的战况中尝试，迎接未知的挑战，才有可能取得胜利。面对一个新兴的市场，没有人知道它能不能做得起来，但如果这时你敢于冲进去，你就拥有了先发优势。当然，勇敢不等于冒进，在驶入风暴前要做好充分的预估，对涉及的人、事、物进行评判，了解做这件事所需的能力、资金、人员的极限。</code></pre><p>💫迁移阅读：<br>     🔹子曰：“工欲善其事，必先利其器。居是邦也，事其大夫之贤者，友其士之仁者。”<br>     🔷孔子说：“工匠要想做好工，必须先把器具打磨锋利。住在这个国家，就要侍奉大夫中的贤人，结交士中的仁人。”</p><p><img src="../assets/image-20231227121322638.png" alt="image-20231227121322638"></p><h1 id="12-28日周四"><a href="#12-28日周四" class="headerlink" title="12.28日周四"></a>12.28日周四</h1><h2 id="英语角-6"><a href="#英语角-6" class="headerlink" title="英语角"></a>英语角</h2><p>you are as warm as the sunset glow.<br>你与晚霞同样温暖。</p><h2 id="23-12-晨读"><a href="#23-12-晨读" class="headerlink" title="23.12.  晨读"></a>23.12.  晨读</h2><p>🍁2023.12.28晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>火是各处可烧的，水是各处可流的，日月是各处可照的，爱情是各处可到的。<br>——来自帆书·李蕾讲经典《边城》</p><p>👉【精彩选段】<br>月光如银子，无处不可照及，山上篁竹在月光下皆成为黑色。身边草丛中虫声繁密如落雨。</p><p>👉【作者之语】<br>“一切充满了善，然而到处是不凑巧。”既然是不凑巧，因之素朴的善，终究难免产生悲剧。</p><p>👉【李蕾老师之语】<br>20世纪中国文学的无冕之王《边城》，让我们沉浸式体验湘西边城的风景与风情，体味人性的善良美好，给漂泊的心灵找到一个宁静的栖息之所。《边城》之后，每个人都想寻找翠翠，为什么呢？因为这个故事充满了爱与美。</p><h2 id="今日简报-5"><a href="#今日简报-5" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月28日   星期四</p><p>1、全国人大法工委：全面禁燃烟花爆竹不合法，部分地方需修改禁燃令。</p><p>2、5个严禁！四部门进一步规范义务教育课后服务。</p><p>3、我国“八纵八横”高铁网主通道已建成3.61万公里。</p><p>4、北京1月1日起禁止老头乐上路。</p><p>5、各项指标全部合格，我国首艘大洋钻探船完成首次试航。</p><p>6、京东也将支持仅退款，三大电商平台均将支持仅退款。</p><p>7、杭州至南昌高铁全线贯通运营，串起世界级黄金旅游线。</p><p>8、麦当劳中国部分产品今起涨价：运营成本变化，平均涨幅为3%。</p><p>9、围炉煮茶要小心！浙江一家医院一周接诊13名一氧化碳中毒者。</p><p>10、韩国首尔明年将启用无人机监控交通状况。</p><p>11、美国数百万人收到极端天气警告，超2000架次航班延误。</p><p>12、弥补战时预算缺口，以色列财政部建议关停10个政府部门。</p><h2 id="名人语录-6"><a href="#名人语录-6" class="headerlink" title="名人语录"></a>名人语录</h2><p>心里装着太阳，才能闪亮发光，幸福不在别处，就在我们自己</p><p>幸福，既不在过去也不在未来，而是在当下。</p><p>就好像不要接到工作就开始做一样，要思考工作的结果、意义：和上级澄清目标，理清和兄弟团队的关系，这些都要考虑到。</p><h2 id="12-28日独白"><a href="#12-28日独白" class="headerlink" title="12.28日独白"></a>12.28日独白</h2><blockquote><p>任务安排</p></blockquote><ol><li>路线 java go 指定 ⭐⭐⭐⭐⭐</li><li>动作识别任务⭐⭐⭐⭐</li><li>笔记系统 三习⭐⭐⭐⭐</li><li>周报准备⭐⭐⭐⭐⭐</li><li>日读安排⭐⭐⭐</li></ol><h2 id="日知日进-6"><a href="#日知日进-6" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.28【日知日进】日历<br>📚日知书目:《当良知沉睡》</p><p>✨日进思考：“最大的伤害不是别人插在我们身上的刀，而是我们经常把刀拔出来看一看，再插回去。”</p><pre><code> 🔹好好地生活是对伤害我们的人最好的报复。不要对他们耿耿于怀，对他们最好的报复是我们自己过得更愉快，我们的生活蒸蒸日上，不断向前走。我们的眼睛要向前看，而不是每天生活在过去的伤害中。对我们造成最大伤害的，不是别人插在我们身上的刀，而是我们经常把那把刀拔出来看一看，然后很生气，再插回去，这种反复的伤害才是影响最大的。所以要向前走，与伤害擦肩而过。</code></pre><p>💫迁移阅读：<br>     🔹子贡曰：“伯夷、叔齐何人也？”曰：“古之贤人也。”曰：“怨乎？”曰：“求仁而得仁，又何怨？”<br>     🔷自贡问道：“伯夷和叔齐是怎样的人呢？”孔子说：“他们是古代贤人啊。”子贡说：“他们会有怨悔吗？”孔子说：“他们追求仁德，便得到了仁德，又怎么会有怨悔呢？”</p><p><img src="../assets/image-20231228100910051.png" alt="image-20231228100910051"></p><h1 id="12-29日周五"><a href="#12-29日周五" class="headerlink" title="12.29日周五"></a>12.29日周五</h1><h2 id="英语角-7"><a href="#英语角-7" class="headerlink" title="英语角"></a>英语角</h2><p>The wind stops at autumn water I stop at you.<br>风，止于秋水，我，止于你。</p><h2 id="23-12-29晨读"><a href="#23-12-29晨读" class="headerlink" title="23.12.29晨读"></a>23.12.29晨读</h2><p>❄️2023.12.29晨读<br>一日之计在于晨，美好的一天从晨读开始。</p><p>⬇️今日晨读内容：<br>与有趣的灵魂相遇，是我们认知这个世界最美好的方式。<br>——来自帆书·非凡精读《如何结交比你更优秀的人》</p><p>👉【精彩选段】<br>孔子说“远而不疏，近而不狎”，讲的就是分寸感。人和人要有距离，再亲近也不能狎昵戏弄。</p><blockquote><p>狎昵”，汉语词语，拼音是xia(2) ni(4)，意思是过于亲近而态度不庄重。解释<br>亦作“狎曜”<br>1.亲近;亲昵<br>2.指男女淫猥苟合<br>3.过于亲近而态度不庄重。<br>出处<br>生就视,容华若仙,惊喜拥入,穷极狎昵《聊斋志异“·胡四姐》</p></blockquote><p>👉【作者之语】<br>结交更优秀的人是一个帮助我的学业、事业成功的非常重要的点。如何构建人和人之间的关系，重点是人和人，不是你的标签、我的标签。</p><p>👉【主讲老师之语】<br>《如何结交比你更优秀的人》这本书，就是帮你构建你的人脉力，使你成为真正的人脉达人，能够在身边织成一个有效的人脉网络，能够让这个人脉网络帮助你的事业更加成功，使你的生活更加幸福。</p><h2 id="今日简报-6"><a href="#今日简报-6" class="headerlink" title="今日简报"></a>今日简报</h2><p>今日简报</p><p>12月29日   星期五</p><p>1、元旦假期成都、北京、三亚成热门租车目的地，港澳游客入境数量大增。</p><p>2、多地限制涉电诈人员子女上学就业，人大法工委：废止地方涉罪人员近亲连坐规定。</p><p>3、印度庭审vivo高管更多细节曝光，为打压中企印检方竟当庭提交密信。</p><p>4、纽约时报起诉微软和OpenAI侵权：擅用报道训练大模型。</p><p>5、调查数据显示30岁人群未婚率超25%，25岁人群有七成未婚。</p><p>6、山东潍坊冷冻薯条出口量猛增，国产冷冻薯条热销海外。</p><p>7、诈骗团伙虚构2500多场竞赛收益超1400万，人民网评：警惕烧钱的野鸡竞赛。</p><p>8、小米首款电动车SU7亮相，雷军：已经小批量量产，目标用户是时代精英。</p><p>9、拖欠数千名员工年终奖，马斯克的社交平台“X”遭起诉。</p><p>10、沈阳地铁回应2天6次延误：向乘客致歉，已做好应急预案迎接跨年夜客流高峰。</p><p>11、京东零售公布加薪细节：固定薪资涨幅近100%，业绩激励上不封顶。</p><p>12、苹果手表发力健康监测功能，或持续面临法律挑战。</p><h2 id="名人语录-7"><a href="#名人语录-7" class="headerlink" title="名人语录"></a>名人语录</h2><p>与有趣的灵魂相遇，是我们认知这个世界最美好的方式。<br>   来帆书APP 听《如何结交比你更优秀的人》</p><h2 id="12-29日独白"><a href="#12-29日独白" class="headerlink" title="12.29日独白"></a>12.29日独白</h2><p>相信努力会带来收获，<br>相信用心会收获真情，<br>相信善良是行走世间最好的武器。<br>新的一天，早安！</p><p>今天是2023年的最后一个周五啦！<br>为这一年的晨读画一个完美的句号吧！</p><blockquote><p> 如愿以偿的，感恩；力所不及的，释怀。好坏都是经历，得失皆有因果。感谢自己这一年的辛苦努力，感谢伙伴们的陪伴扶助！</p></blockquote><h2 id="日知日进-7"><a href="#日知日进-7" class="headerlink" title="日知日进"></a>日知日进</h2><p>🗓️2023.12.29【日知日进】日历<br>📚日知书目:《感受爱》</p><p>✨日进思考：“聪明人要学会与不确定性共舞。”</p><pre><code> 🔹我们每天担忧的事是什么？就是怎样能够让所有的事都确定。我们希望一切东西都按照我们的想法按部就班地落实，这就叫作确定。但生活如果事事都确定是一件可怕的事，因为人生的乐趣就是来自未知，来自无常。庄子告诉人们要学会和不确定性共舞，无论天地怎么改变，我们要像飘絮一样随风而定。当我们不需要让自己安稳地站在地面，但依然感觉安稳的时候，便是天地合一。所以一个聪明人，要学会保持不确定性，不要过度担忧，这是对抗想得太多的一个最有效的方法。</code></pre><p>💫迁移阅读：<br>     🔹子曰：“直哉史鱼！邦有道如矢，邦无道如矢。君子哉蘧伯玉！邦有道则仕，邦无道则可卷而怀之。”<br>     🔷孔子说：“史鱼正直啊！国家政治清明时，他像箭一样直；国家政治黑暗，他也像箭一样直。蘧伯玉是君子啊！国家政治清明时，他就出来做官；国家政治黑暗时，就把自己的才能收藏起来（不做官）。”</p><p><img src="../assets/image-20231229162315569.png" alt="image-20231229162315569"></p>]]></content>
    
    
    <summary type="html">樊登读书会晨读——记录每天新的东西和概念，争取日更</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>java学习笔记整理</title>
    <link href="https://www.fomal.cc/posts/d5874840.html"/>
    <id>https://www.fomal.cc/posts/d5874840.html</id>
    <published>2023-12-19T01:52:12.000Z</published>
    <updated>2024-09-07T06:59:23.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="java学习现状与计划调整"><a href="#java学习现状与计划调整" class="headerlink" title="java学习现状与计划调整"></a>java学习现状与计划调整</h1><h1 id="java基础面试笔记整理"><a href="#java基础面试笔记整理" class="headerlink" title="java基础面试笔记整理"></a>java基础面试笔记整理</h1><p>12.16号 学习整理go的学习路线</p><blockquote><p>一边是极客时间的开源实践教学视频</p><p>另一边是马士兵和珊珊老师的学习视频和基础学习笔记</p><p>以及一些现有的网站和博客</p><p>综上所述，可以以开源项目为目标，争取自己扩展自己造轮子的这个思路，尝试下第一次造轮子</p><p>然后用一些辅助的笔记和博客促进go的学习，努力刷开源课程</p><p>同时借助学习go语言基础的过程，复习起来java的基础知识</p></blockquote><p>12.17号 周天</p><blockquote><p>学习完成马士兵讲解javaer转变成goer的第一讲视频后，发现要学习java 的多线程和jvm基础【比如线程池 一些内存算法等等】</p><p>整理了现有的多线程和jvm学习的视频</p><p>要求是 有完整的笔记 以及对应的详细视频讲解，如果看笔记看不懂再去看视频</p></blockquote><p>12.18日 混沌+ 焦虑的一天+晚上焦虑的睡眠</p><blockquote><p>上午 泄欲</p><p>下午没有睡午觉，整理那个尚硅谷的一个安装notecase软件的bug后，1.9版本一直有错误，换成其他版本并尝试，花费1.5h 解决后，开始刷剧</p><p>晚上去吃饭+ 刷剧 +  8.15困得不行 一觉睡到9.25</p></blockquote>]]></content>
    
    
    <summary type="html">整理java面试准备现状和java基础知识笔记</summary>
    
    
    
    
  </entry>
  
</feed>
